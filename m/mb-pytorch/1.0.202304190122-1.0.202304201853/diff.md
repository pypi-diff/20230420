# Comparing `tmp/mb_pytorch-1.0.202304190122-py3-none-any.whl.zip` & `tmp/mb_pytorch-1.0.202304201853-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,9 +1,9 @@
-Zip file size: 32186 bytes, number of entries: 37
--rw-rw-r--  2.0 unx     5066 b- defN 23-Apr-19 01:21 mb_pytorch/classification/training.py
+Zip file size: 32560 bytes, number of entries: 37
+-rw-rw-r--  2.0 unx     5621 b- defN 23-Apr-20 18:53 mb_pytorch/classification/training.py
 -rw-rw-r--  2.0 unx       44 b- defN 23-Mar-16 11:39 mb_pytorch/dataloader/__init__.py
 -rw-rw-r--  2.0 unx    12981 b- defN 23-Apr-19 01:00 mb_pytorch/dataloader/loader.py
 -rw-rw-r--  2.0 unx        0 b- defN 23-Feb-23 13:56 mb_pytorch/metalearning/__init__.py
 -rw-rw-r--  2.0 unx     1385 b- defN 23-Mar-02 03:28 mb_pytorch/metalearning/meta_utils.py
 -rw-rw-r--  2.0 unx     1030 b- defN 23-Mar-15 02:58 mb_pytorch/metalearning/proto_dataloader.py
 -rw-rw-r--  2.0 unx     2861 b- defN 23-Mar-03 23:55 mb_pytorch/metalearning/prototypical.py
 -rw-rw-r--  2.0 unx        0 b- defN 23-Mar-31 19:54 mb_pytorch/models/__init__.py
@@ -24,16 +24,16 @@
 -rw-rw-r--  2.0 unx        0 b- defN 23-Feb-23 13:56 mb_pytorch/utils/__init__.py
 -rw-rw-r--  2.0 unx     1055 b- defN 23-Mar-17 02:23 mb_pytorch/utils/compiler.py
 -rw-rw-r--  2.0 unx      257 b- defN 23-Mar-01 22:55 mb_pytorch/utils/dist.py
 -rw-rw-r--  2.0 unx     3391 b- defN 23-Apr-11 23:28 mb_pytorch/utils/extra_utils.py
 -rw-rw-r--  2.0 unx     7178 b- defN 23-Mar-15 02:58 mb_pytorch/utils/generate_emb.py
 -rw-rw-r--  2.0 unx     2582 b- defN 23-Apr-03 19:30 mb_pytorch/utils/losses.py
 -rw-rw-r--  2.0 unx     1199 b- defN 23-Apr-04 20:14 mb_pytorch/utils/metrics.py
--rw-rw-r--  2.0 unx     8976 b- defN 23-Apr-19 01:18 mb_pytorch/utils/viewer.py
+-rw-rw-r--  2.0 unx     9511 b- defN 23-Apr-20 18:53 mb_pytorch/utils/viewer.py
 -rw-rw-r--  2.0 unx      994 b- defN 23-Mar-06 13:11 mb_pytorch/utils/yaml_reader.py
--rwxrwxr-x  2.0 unx     1304 b- defN 23-Apr-19 01:22 mb_pytorch-1.0.202304190122.data/scripts/dataload_results.py
--rwxrwxr-x  2.0 unx      980 b- defN 23-Mar-15 02:59 mb_pytorch-1.0.202304190122.data/scripts/emb.py
--rw-rw-r--  2.0 unx      329 b- defN 23-Apr-19 01:22 mb_pytorch-1.0.202304190122.dist-info/METADATA
--rw-rw-r--  2.0 unx       92 b- defN 23-Apr-19 01:22 mb_pytorch-1.0.202304190122.dist-info/WHEEL
--rw-rw-r--  2.0 unx       11 b- defN 23-Apr-19 01:22 mb_pytorch-1.0.202304190122.dist-info/top_level.txt
-?rw-rw-r--  2.0 unx     3348 b- defN 23-Apr-19 01:22 mb_pytorch-1.0.202304190122.dist-info/RECORD
-37 files, 94613 bytes uncompressed, 26720 bytes compressed:  71.8%
+-rwxrwxr-x  2.0 unx     1304 b- defN 23-Apr-20 19:42 mb_pytorch-1.0.202304201853.data/scripts/dataload_results.py
+-rwxrwxr-x  2.0 unx      980 b- defN 23-Mar-15 02:59 mb_pytorch-1.0.202304201853.data/scripts/emb.py
+-rw-rw-r--  2.0 unx      329 b- defN 23-Apr-20 19:42 mb_pytorch-1.0.202304201853.dist-info/METADATA
+-rw-rw-r--  2.0 unx       92 b- defN 23-Apr-20 19:42 mb_pytorch-1.0.202304201853.dist-info/WHEEL
+-rw-rw-r--  2.0 unx       11 b- defN 23-Apr-20 19:42 mb_pytorch-1.0.202304201853.dist-info/top_level.txt
+?rw-rw-r--  2.0 unx     3348 b- defN 23-Apr-20 19:42 mb_pytorch-1.0.202304201853.dist-info/RECORD
+37 files, 95703 bytes uncompressed, 27094 bytes compressed:  71.7%
```

## zipnote {}

```diff
@@ -87,26 +87,26 @@
 
 Filename: mb_pytorch/utils/viewer.py
 Comment: 
 
 Filename: mb_pytorch/utils/yaml_reader.py
 Comment: 
 
-Filename: mb_pytorch-1.0.202304190122.data/scripts/dataload_results.py
+Filename: mb_pytorch-1.0.202304201853.data/scripts/dataload_results.py
 Comment: 
 
-Filename: mb_pytorch-1.0.202304190122.data/scripts/emb.py
+Filename: mb_pytorch-1.0.202304201853.data/scripts/emb.py
 Comment: 
 
-Filename: mb_pytorch-1.0.202304190122.dist-info/METADATA
+Filename: mb_pytorch-1.0.202304201853.dist-info/METADATA
 Comment: 
 
-Filename: mb_pytorch-1.0.202304190122.dist-info/WHEEL
+Filename: mb_pytorch-1.0.202304201853.dist-info/WHEEL
 Comment: 
 
-Filename: mb_pytorch-1.0.202304190122.dist-info/top_level.txt
+Filename: mb_pytorch-1.0.202304201853.dist-info/top_level.txt
 Comment: 
 
-Filename: mb_pytorch-1.0.202304190122.dist-info/RECORD
+Filename: mb_pytorch-1.0.202304201853.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## mb_pytorch/classification/training.py

```diff
@@ -3,15 +3,15 @@
 import torch
 from ..training.train_params import train_helper
 import tqdm
 from torch.utils.tensorboard import SummaryWriter
 import os
 from mb_utils.src.logging import logger
 import numpy as np
-from ..utils.viewer import gradcam_viewer,create_img_grid
+from ..utils.viewer import gradcam_viewer,create_img_grid,plot_classes_pred
 
 __all__ = ['classification_train_loop']
 
 def classification_train_loop( k_data,data_model,model,train_loader,val_loader,loss_attr,optimizer,scheduler=None,writer=None,logger=None,gradcam=None,gradcam_rgb=False,device='cpu'):
     """
     Function to train the model
     Args:
@@ -56,15 +56,15 @@
                 logger.info(f'Epoch {i+1} - Batch {j+1} - Train Loss: {current_loss.item()}')
             
             
             #get grad cam images
             if gradcam and writer is not None:
                 x_grad = x[0,:].to('cpu')
                 x_grad = x_grad.unsqueeze(0)
-                y_grad = y[0].to('cpu')
+                #y_grad = y[0].to('cpu')
                 use_cuda=False
                 if device.type != 'cpu':
                     use_cuda = True
                 for cam_layers in gradcam:
                     grad_img = gradcam_viewer(cam_layers,model,x_grad,gradcam_rgb=gradcam_rgb,use_cuda=use_cuda)
                     if grad_img is not None:
                         grad_img = np.transpose(grad_img,(2,0,1))
@@ -81,54 +81,67 @@
             logger.info(f'Epoch {i+1} - Train Loss: {avg_train_loss}')
     
         if writer is not None:
             writer.add_graph(model, x)
             writer.add_scalar('Loss/train', avg_train_loss, global_step=i)
             for name, param in model.named_parameters():
                 writer.add_histogram(name, param, global_step=i)
-
-        
+            
         #validation loop
 
         val_loss = 0
         val_acc = 0
         new_val_loss = 0
-        num_samples = 0
+        #num_samples = 0
     
         model.eval()
         with torch.no_grad():
             for x_val, y_val in val_loader:
                 x_val, y_val = x_val.to(device), y_val.to(device)
                 output = model(x_val)
                 val_loss += loss_attr(output, y_val).item() * x_val.size(0)
-                _, preds = torch.max(output, 1) #no need of softmax. max returns the index of the max value
+                pred_val, preds = torch.max(output, 1) #no need of softmax. max returns the index of the max value
                 val_acc += torch.sum(preds == y_val.data)
-                new_val_loss = val_loss/len(val_loader.dataset)
-                num_samples += x_val.size(0)
+                new_val_loss = val_loss/x_val.size(0)
+                #num_samples += x_val.size(0)
                 if logger: 
                     logger.info(f'Epoch {i+1} - Batch {j+1} - Val Loss: {new_val_loss:.3f}')
             
             avg_val_loss = val_loss / len(val_loader.dataset)
             val_acc = val_acc/len(val_loader.dataset)
             #val_loss /= num_samples
             #val_acc = val_acc / num_samples
             if logger:
                 logger.info(f'Epoch {i+1} -Avg Val Loss: {avg_val_loss:.3f}')
                 logger.info(f'Epoch {i+1} - Val Accuracy: {val_acc:.3f}')
     
         if writer is not None:
             writer.add_scalar('Loss/val', val_loss, global_step=i)
             writer.add_scalar('Accuracy/val', val_acc, global_step=i)
+            writer.add_figure('predictions vs. actuals', fig1, global_step=i)
+    
+        if i==0:
+            print(preds)
+            print(y_val.data)
     
         # save best model
         if i == 0:
             best_val_loss = float('inf')
         if val_loss < best_val_loss:
             best_val_loss = val_loss
             best_model = model.state_dict()
 
             path = os.path.join(k_data['work_dir'], 'best_model.pth')
             torch.save(best_model, path)
             if logger:
                 logger.info(f'Epoch {i+1} - Best Model Saved')
         
+        #get classes/labels in a dict for the last batch
+        if writer is not None:
+            if len(x_val)<4:
+                logger.info('Batch size of last batch is less than 4. Cannot plot classes')
+            else:
+                prob_val = torch.nn.functional.softmax(output, dim=1)
+                fig1 = plot_classes_pred(x_val, y_val, prob_val, preds)
+
+
```

## mb_pytorch/utils/viewer.py

```diff
@@ -6,15 +6,15 @@
 from torchvision.utils import draw_bounding_boxes, draw_segmentation_masks
 import torchvision.transforms.functional as TF
 from pytorch_grad_cam import GradCAM
 from pytorch_grad_cam.utils.image import show_cam_on_image
 import io
 import PIL
 
-__all__ = ['show_images', 'show_segmentation_masks', 'show_bounding_boxes', 'show_label_on_img','model_viewer','new_show_cam_on_image','gradcam_viewer']
+__all__ = ['show_images', 'show_segmentation_masks', 'show_bounding_boxes', 'show_label_on_img','model_viewer','new_show_cam_on_image','gradcam_viewer','plot_classes_pred']
 
 def show_images(imgs, figsize=(12.0, 12.0)):
     """Displays a single image or list of images. 
     Args:
         imgs (Union[List[torch.Tensor], torch.Tensor]): A list of images
             of shape (3, H, W) or a single image of shape (3, H, W).
         figsize (Tuple[float, float]): size of figure to display.
@@ -237,7 +237,18 @@
             return cr
         if cr is not None:        
             #cam_img = new_show_cam_on_image(x_grad[0].numpy(),cr,use_rgb=gradcam_rgb)
             x_grad_new = x_grad[0].permute(1,2,0).numpy()
             cam_img = show_cam_on_image(x_grad_new, cr,use_rgb=gradcam_rgb)
     return cam_img
     
+def plot_classes_pred(images,labels,predictions_prob,preds):
+    fig = plt.figure(figsize=(12, 48))
+    for idx in np.arange(4):
+        ax = fig.add_subplot(1, 4, idx+1, xticks=[], yticks=[])
+        plt.imshow(images[idx], one_channel=True)
+        ax.set_title("{0}, {1:.1f}%\n(label: {2})".format(
+                    preds[idx],
+                    predictions_prob[idx] * 100.0,
+                    labels[idx]),
+                    color=("green" if preds[idx]==labels[idx].item() else "red"))
+    return fig
```

## Comparing `mb_pytorch-1.0.202304190122.data/scripts/dataload_results.py` & `mb_pytorch-1.0.202304201853.data/scripts/dataload_results.py`

 * *Files identical despite different names*

## Comparing `mb_pytorch-1.0.202304190122.data/scripts/emb.py` & `mb_pytorch-1.0.202304201853.data/scripts/emb.py`

 * *Files identical despite different names*

## Comparing `mb_pytorch-1.0.202304190122.dist-info/RECORD` & `mb_pytorch-1.0.202304201853.dist-info/RECORD`

 * *Files 10% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-mb_pytorch/classification/training.py,sha256=IJjKRth2SnBlTW8c-ryO5HyKBluEYPEFPaDZBUeyNi8,5066
+mb_pytorch/classification/training.py,sha256=Qg8FZ3Nbl5doM3MEV0HXwqK9lwXj1rhfZ3NgPjPbfs8,5621
 mb_pytorch/dataloader/__init__.py,sha256=nB0xPAHbI91Ra1dDkWR1l4td5A4k9xko-I5Jdgv5apI,44
 mb_pytorch/dataloader/loader.py,sha256=Cn6N9MYVRxpm-fZznk6uu0nzdWCPQWKyK93DNVlKUvk,12981
 mb_pytorch/metalearning/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 mb_pytorch/metalearning/meta_utils.py,sha256=mgHYiQIIcYQ1pVTJcrjquSXpQstdYD1q8iXO09Zao1s,1385
 mb_pytorch/metalearning/proto_dataloader.py,sha256=WvrfZYkYMxorocCkR_zHS_AC8W_ML9YndB-P6evkdcc,1030
 mb_pytorch/metalearning/prototypical.py,sha256=qFVf6VF3s8zskGqbM3geJV-dfkdO3tRaf3P8U_KR-cE,2861
 mb_pytorch/models/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
@@ -23,15 +23,15 @@
 mb_pytorch/utils/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 mb_pytorch/utils/compiler.py,sha256=ixu-wc--ykhsjXTZINw34MLmZFg7L-dLkYs7p-nn6i4,1055
 mb_pytorch/utils/dist.py,sha256=7-ZdntmiugRWYnT5wileo8mYTuV1dbjVl4ffJsfnfAw,257
 mb_pytorch/utils/extra_utils.py,sha256=-MaT-x3gwgEOVLpg-tWm5yLFtI0CxpV0QUlXx-rqu08,3391
 mb_pytorch/utils/generate_emb.py,sha256=2iK8wRIrYfaLpEgjdbFnDqGU5ux-1JhncQoeboW_6LQ,7178
 mb_pytorch/utils/losses.py,sha256=OLCPLkJH46IofSSVly2xdcklVv7Q5OFFEGtVrJcV7V0,2582
 mb_pytorch/utils/metrics.py,sha256=Kqmdu9llSjR8aRp3IVlmy6PqeQexf0ZXjTJUcEtvcfI,1199
-mb_pytorch/utils/viewer.py,sha256=Rtq4zsa1io725_74ys8gNTDlN6HV5EgwJVSq4skjXio,8976
+mb_pytorch/utils/viewer.py,sha256=yAveqPnb5A_6c2BzfToZDen4qLBKuA5Ne5WeFUSYSvw,9511
 mb_pytorch/utils/yaml_reader.py,sha256=Azgr_5qttsH_BBVsCtfccFMvK6IEjTRYhd5qp4S5uzk,994
-mb_pytorch-1.0.202304190122.data/scripts/dataload_results.py,sha256=8IFAH7WX-nSJ7V532rr3Cl7R37v0jhSakw0JCd9dalE,1304
-mb_pytorch-1.0.202304190122.data/scripts/emb.py,sha256=5jSbTGNOhusDTvHZeaTwk4pmJa4HIdkRGd98s0L4Rl0,980
-mb_pytorch-1.0.202304190122.dist-info/METADATA,sha256=2f2jxa8gb327DzsIEId_NVQ0Xy3g1TDwWbcRf8OP_ZQ,329
-mb_pytorch-1.0.202304190122.dist-info/WHEEL,sha256=g4nMs7d-Xl9-xC9XovUrsDHGXt-FT0E17Yqo92DEfvY,92
-mb_pytorch-1.0.202304190122.dist-info/top_level.txt,sha256=2m_aBiEfjq3pZM2NtYSlTqlgoQxH6WaK8_8SsRicIvg,11
-mb_pytorch-1.0.202304190122.dist-info/RECORD,,
+mb_pytorch-1.0.202304201853.data/scripts/dataload_results.py,sha256=8IFAH7WX-nSJ7V532rr3Cl7R37v0jhSakw0JCd9dalE,1304
+mb_pytorch-1.0.202304201853.data/scripts/emb.py,sha256=5jSbTGNOhusDTvHZeaTwk4pmJa4HIdkRGd98s0L4Rl0,980
+mb_pytorch-1.0.202304201853.dist-info/METADATA,sha256=2aqkJXUD0YIQuevftw61TwDaUIk9EnoJi3m1x7b9rr0,329
+mb_pytorch-1.0.202304201853.dist-info/WHEEL,sha256=g4nMs7d-Xl9-xC9XovUrsDHGXt-FT0E17Yqo92DEfvY,92
+mb_pytorch-1.0.202304201853.dist-info/top_level.txt,sha256=2m_aBiEfjq3pZM2NtYSlTqlgoQxH6WaK8_8SsRicIvg,11
+mb_pytorch-1.0.202304201853.dist-info/RECORD,,
```

