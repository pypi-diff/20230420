# Comparing `tmp/redscrap-0.0.3-py3-none-any.whl.zip` & `tmp/redscrap-0.0.4-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,44 +1,44 @@
-Zip file size: 43563 bytes, number of entries: 42
+Zip file size: 44543 bytes, number of entries: 42
 -rw-r--r--  2.0 unx        0 b- defN 23-Mar-23 21:27 __init__.py
 -rw-r--r--  2.0 unx        0 b- defN 23-Apr-04 23:07 common/__init__.py
 -rw-r--r--  2.0 unx        0 b- defN 23-Apr-17 15:16 common/constants/__init__.py
--rw-r--r--  2.0 unx    14149 b- defN 23-Apr-17 14:38 common/constants/common_constants.py
+-rw-r--r--  2.0 unx    15781 b- defN 23-Apr-17 22:04 common/constants/common_constants.py
 -rw-r--r--  2.0 unx    16883 b- defN 23-Apr-17 11:30 common/constants/logging_constants.py
 -rw-r--r--  2.0 unx        0 b- defN 23-Apr-17 15:18 common/exceptions/__init__.py
 -rw-r--r--  2.0 unx     1107 b- defN 23-Apr-16 09:33 common/exceptions/main_exceptions.py
 -rw-r--r--  2.0 unx        0 b- defN 23-Apr-17 14:24 common/io_operations/__init__.py
--rw-r--r--  2.0 unx     8697 b- defN 23-Apr-17 15:16 common/io_operations/image_downloader.py
--rw-r--r--  2.0 unx    12460 b- defN 23-Apr-17 15:16 common/io_operations/io_operations.py
--rw-r--r--  2.0 unx     2064 b- defN 23-Apr-15 16:10 common/io_operations/request_manager.py
+-rw-r--r--  2.0 unx     9548 b- defN 23-Apr-19 22:20 common/io_operations/image_downloader.py
+-rw-r--r--  2.0 unx    12419 b- defN 23-Apr-19 22:34 common/io_operations/io_operations.py
+-rw-r--r--  2.0 unx     7818 b- defN 23-Apr-19 22:42 common/io_operations/request_factory.py
 -rw-r--r--  2.0 unx        0 b- defN 23-Apr-04 10:01 common/logging/__init__.py
 -rw-r--r--  2.0 unx     9755 b- defN 23-Apr-17 15:17 common/logging/logging_setup.py
 -rw-r--r--  2.0 unx     6015 b- defN 23-Apr-17 15:17 common/logging/loguru_setup.py
 -rw-r--r--  2.0 unx        0 b- defN 23-Apr-05 13:26 common/logging/utils/__init__.py
 -rw-r--r--  2.0 unx      992 b- defN 23-Apr-14 21:28 common/logging/utils/color_formatter.py
 -rw-r--r--  2.0 unx     1615 b- defN 23-Apr-15 16:07 common/logging/utils/log_rotator.py
 -rw-r--r--  2.0 unx     1182 b- defN 23-Apr-14 22:20 common/logging/utils/logging_wrappers.py
 -rw-r--r--  2.0 unx     2092 b- defN 23-Apr-15 18:33 common/logging/utils/loguru_wrappers.py
 -rw-r--r--  2.0 unx     1288 b- defN 23-Apr-17 15:17 common/logging/utils/padding_formatter.py
 -rw-r--r--  2.0 unx        0 b- defN 23-Apr-17 15:17 common/utils/__init__.py
 -rw-r--r--  2.0 unx     1640 b- defN 23-Apr-15 16:12 common/utils/string_builder.py
 -rw-r--r--  2.0 unx        0 b- defN 23-Apr-04 23:07 common/validations/__init__.py
--rw-r--r--  2.0 unx     6703 b- defN 23-Apr-17 15:26 common/validations/parameter_validations.py
--rw-r--r--  2.0 unx     6889 b- defN 23-Apr-17 15:28 common/validations/reddit_api_validations.py
+-rw-r--r--  2.0 unx     6786 b- defN 23-Apr-19 20:57 common/validations/parameter_validations.py
+-rw-r--r--  2.0 unx     6837 b- defN 23-Apr-19 22:00 common/validations/reddit_api_validations.py
 -rw-r--r--  2.0 unx     3517 b- defN 23-Apr-17 15:16 common/validations/url_validations.py
 -rw-r--r--  2.0 unx        0 b- defN 23-Mar-23 21:27 core/__init__.py
--rw-r--r--  2.0 unx     4647 b- defN 23-Apr-17 15:17 core/main.py
+-rw-r--r--  2.0 unx     5429 b- defN 23-Apr-19 22:58 core/main.py
 -rw-r--r--  2.0 unx        0 b- defN 23-Apr-08 10:02 core/api/__init__.py
--rw-r--r--  2.0 unx     6018 b- defN 23-Apr-17 15:28 core/api/reddit_api.py
+-rw-r--r--  2.0 unx     5282 b- defN 23-Apr-19 22:00 core/api/reddit_api.py
 -rw-r--r--  2.0 unx        0 b- defN 23-Apr-14 19:19 core/helper/__init__.py
--rw-r--r--  2.0 unx    11320 b- defN 23-Apr-17 15:26 core/helper/main_helper.py
+-rw-r--r--  2.0 unx    11253 b- defN 23-Apr-19 21:58 core/helper/main_helper.py
 -rw-r--r--  2.0 unx        0 b- defN 23-Apr-04 16:57 core/scraper/__init__.py
--rw-r--r--  2.0 unx     9431 b- defN 23-Apr-17 16:11 core/scraper/comment_scraper.py
--rw-r--r--  2.0 unx    10874 b- defN 23-Apr-17 16:39 core/scraper/scraper_helper.py
--rw-r--r--  2.0 unx     9415 b- defN 23-Apr-17 16:37 core/scraper/thread_scraper.py
--rw-r--r--  2.0 unx     1089 b- defN 23-Apr-17 17:48 redscrap-0.0.3.dist-info/LICENSE
--rw-r--r--  2.0 unx     1151 b- defN 23-Apr-17 17:48 redscrap-0.0.3.dist-info/METADATA
--rw-r--r--  2.0 unx       92 b- defN 23-Apr-17 17:48 redscrap-0.0.3.dist-info/WHEEL
--rw-r--r--  2.0 unx       44 b- defN 23-Apr-17 17:48 redscrap-0.0.3.dist-info/entry_points.txt
--rw-r--r--  2.0 unx       21 b- defN 23-Apr-17 17:48 redscrap-0.0.3.dist-info/top_level.txt
--rw-rw-r--  2.0 unx     3602 b- defN 23-Apr-17 17:48 redscrap-0.0.3.dist-info/RECORD
-42 files, 154752 bytes uncompressed, 37743 bytes compressed:  75.6%
+-rw-r--r--  2.0 unx     9320 b- defN 23-Apr-19 22:36 core/scraper/comment_scraper.py
+-rw-r--r--  2.0 unx    11223 b- defN 23-Apr-19 19:55 core/scraper/scraper_helper.py
+-rw-r--r--  2.0 unx     9547 b- defN 23-Apr-19 22:35 core/scraper/thread_scraper.py
+-rw-r--r--  2.0 unx     1089 b- defN 23-Apr-19 23:01 redscrap-0.0.4.dist-info/LICENSE
+-rw-r--r--  2.0 unx     1151 b- defN 23-Apr-19 23:01 redscrap-0.0.4.dist-info/METADATA
+-rw-r--r--  2.0 unx       92 b- defN 23-Apr-19 23:01 redscrap-0.0.4.dist-info/WHEEL
+-rw-r--r--  2.0 unx       44 b- defN 23-Apr-19 23:01 redscrap-0.0.4.dist-info/entry_points.txt
+-rw-r--r--  2.0 unx       21 b- defN 23-Apr-19 23:01 redscrap-0.0.4.dist-info/top_level.txt
+-rw-rw-r--  2.0 unx     3602 b- defN 23-Apr-19 23:01 redscrap-0.0.4.dist-info/RECORD
+42 files, 163328 bytes uncompressed, 38723 bytes compressed:  76.3%
```

## zipnote {}

```diff
@@ -24,15 +24,15 @@
 
 Filename: common/io_operations/image_downloader.py
 Comment: 
 
 Filename: common/io_operations/io_operations.py
 Comment: 
 
-Filename: common/io_operations/request_manager.py
+Filename: common/io_operations/request_factory.py
 Comment: 
 
 Filename: common/logging/__init__.py
 Comment: 
 
 Filename: common/logging/logging_setup.py
 Comment: 
@@ -102,26 +102,26 @@
 
 Filename: core/scraper/scraper_helper.py
 Comment: 
 
 Filename: core/scraper/thread_scraper.py
 Comment: 
 
-Filename: redscrap-0.0.3.dist-info/LICENSE
+Filename: redscrap-0.0.4.dist-info/LICENSE
 Comment: 
 
-Filename: redscrap-0.0.3.dist-info/METADATA
+Filename: redscrap-0.0.4.dist-info/METADATA
 Comment: 
 
-Filename: redscrap-0.0.3.dist-info/WHEEL
+Filename: redscrap-0.0.4.dist-info/WHEEL
 Comment: 
 
-Filename: redscrap-0.0.3.dist-info/entry_points.txt
+Filename: redscrap-0.0.4.dist-info/entry_points.txt
 Comment: 
 
-Filename: redscrap-0.0.3.dist-info/top_level.txt
+Filename: redscrap-0.0.4.dist-info/top_level.txt
 Comment: 
 
-Filename: redscrap-0.0.3.dist-info/RECORD
+Filename: redscrap-0.0.4.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## common/constants/common_constants.py

```diff
@@ -42,14 +42,81 @@
         output_path (Path): Output path for subreddits.
         resolutions (defaultdict): a dictionary containing a tuple of integers representing a resolution as the key and the
         corresponding resolution label as the value.
         user_agent (str): User agent information for Reddit.
     """
 
     @property
+    def click_scrape_subreddits_option_help_msg(self):
+        """
+        Returns:
+            (str): Logs default output directory.
+        """
+
+        return "Scrape subreddits threads"
+
+    @property
+    def click_scrape_user_option_help_msg(self):
+        """
+        Returns:
+            (str): Logs default output directory.
+        """
+
+        return "Scrape user threads"
+
+    @property
+    def click_number_of_threads_option_help_msg(self):
+        """
+        Returns:
+            (str): Logs default output directory.
+        """
+
+        return "Number of threads to scrape"
+
+    @property
+    def click_filter_threads_option_help_msg(self):
+        """
+        Returns:
+            (str): Logs default output directory.
+        """
+
+        return "Filter threads"
+
+    @property
+    def click_details_option_help_msg(self):
+        """
+        Returns:
+            (str): Logs default output directory.
+        """
+
+        return "If enable outputs the detailed list of threads of each subreddit provided into an individual file"
+
+    @property
+    def click_sorting_filter_option_help_msg(self):
+        return "Thread sorting filter"
+
+    @property
+    def click_output_option_help_msg(self):
+        """
+        Returns:
+            (str): Logs default output directory.
+        """
+
+        return "The directory to output the downloads"
+
+    @property
+    def click_verbose_option_help_msg(self):
+        """
+        Returns:
+            (str): Logs default output directory.
+        """
+
+        return "Enables verbose mode"
+
+    @property
     def logs_default_output_directory(self):
         """
         Returns:
             (str): Logs default output directory.
         """
         return "logs"
```

## common/io_operations/image_downloader.py

```diff
@@ -1,13 +1,12 @@
 import glob
 from contextlib import contextmanager
 from pathlib import Path
 from typing import List
 from urllib.parse import urlparse
-import requests  # type: ignore
 from loguru import logger  # type: ignore
 from tqdm import tqdm
 import httpx
 
 from common.logging.utils.loguru_wrappers import logger_wraps  # type: ignore
 from common.validations.url_validations import UrlValidations  # type: ignore
 from common.constants.common_constants import CommonConstants  # type: ignore
@@ -89,93 +88,103 @@
         if not output_directory.exists():
             output_directory.mkdir(parents=True)
 
         # Make a GET request to the specified URL using httpx.
         transport = httpx.HTTPTransport(retries=3)
         client = httpx.Client(transport=transport)
 
-        failed_urls = subreddits_img_list
+        failed_urls = []
         downloaded_urls = []
 
         subreddits_img_list = list(set(subreddits_img_list))
 
         try:
             if verbose:
                 # Wrapping the loop with tqdm and customize the appearance of the progress bar
-                with tqdm(
-                        total=len(subreddits_img_list),
-                        desc="Downloading images",
-                        ncols=100,
-                        colour="green",
-                        unit_scale=True,
-                        dynamic_ncols=True) as pbar:
+                with tqdm(total=len(subreddits_img_list), desc="Downloading images", ncols=100, colour="green",
+                          unit_scale=True, dynamic_ncols=True) as pbar:
                     for url in subreddits_img_list:
-                        response = client.get(url)
+                        try:
+                            response = client.get(url, timeout=10.0)  # Set a 10-second timeout
 
-                        if response.status_code == 200:
-                            self.download_img_url(url, response.content, output_directory, failed_urls, downloaded_urls)
+                            if response.status_code == 200:
+                                self.download_img_url(url, response.read(), output_directory)
+                                downloaded_urls.append(url)
+                            else:
+                                failed_urls.append(url)
+                        except httpx.TimeoutException:
+                            failed_urls.append(url)  # Add the URL to the failed_urls list if the request times out.
 
                         pbar.update(1)
             else:
                 for url in subreddits_img_list:
-                    response = client.get(url)
-
-                    if response.status_code == 200:
-                        self.download_img_url(url, response.content, output_directory, failed_urls, downloaded_urls)
+                    try:
+                        response = client.get(url, timeout=10.0)  # Set a 10-second timeout
 
+                        if response.status_code == 200:
+                            self.download_img_url(url, response.content, output_directory)
+                            downloaded_urls.append(url)
+                        else:
+                            failed_urls.append(url)
+                    except httpx.TimeoutException:
+                        failed_urls.append(url)  # Add the URL to the failed_urls list if the request times out.
         finally:
             client.close()
 
-        self.generate_download_report(output_directory, subreddits_img_list, failed_urls, verbose)
+        self.generate_download_report(output_directory, subreddits_img_list, downloaded_urls, failed_urls, verbose)
 
         logger.debug("[6] FINISHED IMAGE DOWNLOAD STEP")
 
     @contextmanager
-    def download_img_url(self, url: str, payload, output_directory: Path, failed_urls, downloaded_urls) -> None:
-        # Raise an exception if the HTTP request fails.
+    def download_img_url(self, url: str, payload: bytes, output_directory: Path):
+        """
+        Downloads an image from a URL to a specified directory.
 
+        Args:
+            url (str): The URL of the image to download.
+            payload (bytes): The image file data.
+            output_directory (Path): The output directory where the image will be saved.
+        """
         # set the output file path
         output_file = output_directory / Path(urlparse(url).path).name
 
         msg = "Response: {}".format(self.constants.check_mark_symbol)
         logger.debug(msg)
 
-        failed_urls.remove(url)
-        downloaded_urls.append(url)
-
         # If the response is ok and the output file doesn't exist, save it to it's destination
         if not output_file.exists():
             with open(output_file, "wb") as ifile:
                 ifile.write(payload)
                 msg = "{} - Downloaded {} to target folder {}".format(
                     self.constants.check_mark_symbol, url, output_file)
                 logger.debug(msg)
         else:
             msg = "{} - Skipping: {} already exists in the target folder {}".format(
                 self.constants.cross_symbol, url, output_directory)
             logger.debug(msg)
 
     @logger_wraps()
-    def generate_download_report(self, output_dir: Path, subreddits_img_list: List[str], failed_urls: List[str],
-                                 verbose: bool):
+    def generate_download_report(self, output_dir: Path, subreddits_img_list: List[str],
+                                 downloaded_urls: List[str], failed_urls: List[str], verbose: bool):
         """
         Generates report for the downloading process of the subreddits images
 
         Args:
             output_dir (Path): Output path
             subreddits_img_list (List[str]): List of images scraped from the subreddit
-            failed_urls (List[str]): List of urls that failed the downloading process
+            failed_urls (List[str]): List of failed downloaded urls
+            downloaded_urls (List[str]): List of successfully downloaded urls
             verbose: (bool): controls the verbosity level
         """
         image_files = glob.glob(str(output_dir) + "/*.jpg") + glob.glob(str(output_dir) + "/*.png") + glob.glob(
             str(output_dir) + "/*.gif")
 
         total_url_list_length = len(subreddits_img_list)
         failed_urls_length = len(subreddits_img_list) - len(image_files) \
             if (len(subreddits_img_list) - len(image_files)) > 0 else 0
         downloaded_urls_length = (total_url_list_length - failed_urls_length)
 
         info_msg = "From {} urls scrapped, {} urls where downloaded successfully and {} urls failed download".format(
             total_url_list_length, downloaded_urls_length, failed_urls_length)
         logger.info(info_msg) if verbose else None
-        debug_msg = "Failed urls: {}".format(failed_urls)
+        debug_msg = "Successful urls: {}, \n Failed urls: {}".format(downloaded_urls, failed_urls)
         logger.debug(debug_msg)
```

## common/io_operations/io_operations.py

```diff
@@ -3,20 +3,21 @@
 import mimetypes
 import os
 import shutil
 import sys
 from contextlib import contextmanager
 from pathlib import Path
 from typing import Any
-from loguru import logger       # type: ignore
+
+from PIL import Image  # type: ignore
+from loguru import logger  # type: ignore
 from tqdm import tqdm
-from PIL import Image       # type: ignore
 
-from common.constants.common_constants import CommonConstants     # type: ignore
-from common.logging.logging_setup import LoggingSetup       # type: ignore
+from common.constants.common_constants import CommonConstants  # type: ignore
+from common.logging.logging_setup import LoggingSetup  # type: ignore
 
 
 class IOOperations:
     """
     The IOOperations class provides methods for input-output operations, including writing detailed post information,
     validating directories, sorting files by MIME type and resolution, and deleting original files.
 
@@ -24,22 +25,25 @@
     the PIL library for image processing.
 
     Methods:
         - def write_detailed_post_information(payload: Any, operation: str, filename) -> None:
             Writes payload to a file with the given filename, using the specified operation mode. The file format is
             determined by the extension of the filename. output_post_detailed_information(payload): Writes all image
             URLs in payload to a JSON file named src/output/image_urls.json, sorted by subreddit and post.
+
         - def validate_directories(input_dir):
             Checks if the input_dir directory exists. If it does not exist, logs an
             error message and exits the program.
+
         - def sort_by_mime_type_and_resolution(input_dir: Path, output_dir: Path, remove: bool):
             Sorts all files in the input_dir directory by MIME type and resolution. The sorted files are copied to the
             corresponding folders in the output_dir directory, and original files are deleted if the remove flag is set
              to True.
-        -def delete_original_files(input_dir, remove): Deletes all original files in the input_dir directory if the
+
+        - def delete_original_files(input_dir, remove): Deletes all original files in the input_dir directory if the
         remove flag is set to True.
     """
 
     def __init__(self) -> None:
         super().__init__()
         self.logging_funcs = LoggingSetup()
         self.constants = CommonConstants()
@@ -134,25 +138,23 @@
         input_dir_path = Path(input_dir)
         if not input_dir_path.exists():
             logger.error("Input directory does not exist")
             sys.exit()
 
     #  @contextmanager decorator allows the image file to be opened and closed automatically
     @contextmanager
-    def write_detailed_post_information(self, payload: Any, operation: str, filename: str, verbose: bool) -> None:
+    def write_detailed_post_information(
+            self, payload: Any, operation: str, filename: str, verbose: bool):
         """Write payload to a file in the specified format.
 
         Args:
             payload: The data to be written to the file.
             operation: The file operation mode ('w' for write, 'a' for append, etc.).
             filename: The path and filename of the file to write to.
             verbose: Boolean flag that controls the verbosity output
-
-        Returns:
-            None
         """
         logger.debug("[9] STARTED WRITING SUBREDDIT REPORT INFORMATION STEP")
 
         extension = Path(filename).suffixes[-1]
 
         with open(filename, operation, encoding="utf-8") as file:
             if extension == ".txt":
```

## common/validations/parameter_validations.py

```diff
@@ -1,17 +1,17 @@
-from typing import List, Optional
-from loguru import logger        # type: ignore
 import re
-import requests      # type: ignore
+from typing import List, Optional
+from loguru import logger  # type: ignore
 
-from common.exceptions.main_exceptions import SubredditNotFoundException         # type: ignore
-from common.logging.logging_setup import LoggingSetup        # type: ignore
-from common.constants.common_constants import CommonConstants      # type: ignore
-from common.logging.utils.loguru_wrappers import logger_wraps        # type: ignore
-from core.api.reddit_api import RedditApi        # type: ignore
+from common.constants.common_constants import CommonConstants  # type: ignore
+from common.exceptions.main_exceptions import SubredditNotFoundException  # type: ignore
+from common.logging.logging_setup import LoggingSetup  # type: ignore
+from common.logging.utils.loguru_wrappers import logger_wraps  # type: ignore
+from core.api.reddit_api import RedditApi  # type: ignore
+from common.io_operations.request_factory import RequestFactory     # type: ignore
 
 
 class ParameterValidations:
     """
         A class that provides method to validate the application parameters
 
         Methods:
@@ -20,21 +20,21 @@
                 separated by those characters. Returns the list of values.
 
             def validate_user(self, token: str, reddit_user: str) -> Optional[bool]:
                 Checks if the given username exists on Reddit API.
 
             def validate_user_v2(self, reddit_user: str) -> Optional[bool]:
                 Checks if the given username exists on Reddit API.
-
     """
 
     def __init__(self) -> None:
         super().__init__()
         self.logging_setup = LoggingSetup()
         self.main_constants = CommonConstants()
+        self.request_factory = RequestFactory()
         self.reddit_api = RedditApi(self.main_constants.client_id, self.main_constants.secret_token,
                                     self.main_constants.username, self.main_constants.password)
 
     @logger_wraps()
     def validate_subreddits_parameter(self, input_str: str) -> list[str]:
         """
         Validates that input string only contains commas and semicolons, and splits it into a list of values separated
@@ -114,15 +114,15 @@
         """
 
         url: str = "{}/api/username_available?user={}".format(self.main_constants.reddit_api_base_url, reddit_user)
 
         # add authorization to our headers dictionary
         headers = self.reddit_api.generate_headers(token)
 
-        res = requests.get(url, headers=headers)
+        res = self.request_factory.get(url, headers=headers)
         exists = None
         if res.status_code == 200:
             if res is False:
                 # If the enpoint returns False, it means the username isn't available and as such the user exists
                 msg = "USER {} EXISTS: {}".format(reddit_user, self.main_constants.check_mark_symbol)
                 logger.debug(msg)
                 exists = True
@@ -140,15 +140,15 @@
 
         Returns:
             bool: True if the user exists, False otherwise.
         """
         url: str = "{}/user/{}/about.json".format(self.main_constants.reddit_url, reddit_user)
         headers: dict[str, str] = self.main_constants.reddit_headers
 
-        response = requests.get(url, headers=headers)
+        response = self.request_factory.get(url, headers=headers)
 
         if response.status_code == 200:
             logger.debug("USER EXISTS", True)
             # User exists
             exists = True
         else:
             logger.debug("USER DOESN'T EXIST", True)
```

## common/validations/reddit_api_validations.py

```diff
@@ -1,44 +1,44 @@
 from typing import Optional, List
 from loguru import logger       # type: ignore
-import requests     # type: ignore
 import json
 
-from common.exceptions.main_exceptions import SubredditNotFoundException, UserNotFoundException, \
-    TokenErrorException  # type: ignore
+from common.exceptions.main_exceptions import SubredditNotFoundException, UserNotFoundException, TokenErrorException  # type: ignore
 from common.logging.logging_setup import LoggingSetup   # type: ignore
-from common.constants.common_constants import CommonConstants     # type: ignore
-from common.io_operations.request_manager import RequestManager
+from common.constants.common_constants import CommonConstants       # type: ignore
+from common.io_operations.request_factory import RequestFactory       # type: ignore
 from core.api.reddit_api import RedditApi       # type: ignore
 
 
 class RedditApiValidations:
     """
     A class that provides a number of methods to expose the reddit API
 
     Methods:
-        - def validate_subreddits_list(self, subreddits):
-            Validates a list of subreddits
-        - def validate_subreddit(self, subreddit):
-            Validates if a subreddit exists
+        - def validate_subreddits_list(self, subreddits): Validates a list of subreddits
+
+        - def validate_subreddit(self, subreddit): Validates if a subreddit exists
+
         - def check_if_subreddit_exists(self, token: str, subreddit: str) -> Optional[bool]:
-            Checks if the given username exists on Reddit API.
+            Checks if the given subreddits exists on Reddit API.
+
         - def validate_reddit_user(self, reddit_user: str, verbose: bool) -> Optional[bool]:
             Checks if the given username exists on Reddit API.
+
         - def validate_user(self, token: str, reddit_user: str) -> Optional[bool]:
             Checks if the given username exists on Reddit API.
+
         - def validate_user_v2(self, reddit_user: str) -> Optional[bool]:
             Checks if the given username exists on Reddit API.
     """
 
-
     def __init__(self):
         self.logging_setup = LoggingSetup()
         self.main_constants = CommonConstants()
-        self.request_manager = RequestManager()
+        self.request_factory = RequestFactory()
         self.reddit_api = RedditApi(self.main_constants.client_id, self.main_constants.secret_token,
                                     self.main_constants.username, self.main_constants.password)
 
     def validate_subreddits_list(self, subreddits: List[str], verbose: bool):
         """
         Validates a list of subreddits
 
@@ -71,17 +71,17 @@
             According to Reddit's API rules changed the client's User-Agent string to something unique and descriptive,
             including the target platform, a unique application identifier, a version string, and your username
             as contact information, in the following format
         """
 
         # add authorization to our headers dictionary
         url = "{}/r/{}/about".format(self.main_constants.reddit_api_base_url, subreddit)
-        token = self.reddit_api.generate_reddit_api_token(verbose)
+        token = self.reddit_api.generate_reddit_api_token()
         headers = self.reddit_api.generate_headers(token)
-        res = self.request_manager.request_page(url, headers)
+        res = self.request_factory.get(url, headers=headers)
 
         is_valid: bool = False
         if res.status_code == 200:
             res_dict = json.loads(res.text)
             res_data = res_dict["data"]
 
             if res_data.get("url") is not None:
@@ -117,17 +117,17 @@
 
             this check can also be accomplished by targeting this endpoint:
                 url = "{constants.reddit_api_base_url}/api/v1/user/{username}/trophies"
         """
 
         url: str = "{}/api/username_available.json?user={}".format(
             self.main_constants.reddit_api_base_url, reddit_user)
-        token = self.reddit_api.generate_reddit_api_token(verbose)
+        token = self.reddit_api.generate_reddit_api_token()
         headers = self.reddit_api.generate_headers(token)
-        res = self.request_manager.request_page(url, headers)
+        res = self.request_factory.get(url, headers=headers)
 
         exists: bool = False
 
         if res.status_code == 200:
             # If the enpoint returns False, it means the username isn't available and as such the user exists
             if res.json() is False:
                 exists = True
```

## core/main.py

```diff
@@ -1,31 +1,37 @@
+import time
 from pathlib import Path
 from loguru import logger  # type: ignore
 import click
 
-# import logging
 from common.io_operations.io_operations import IOOperations  # type: ignore
 from common.logging.loguru_setup import LoguruSetup  # type: ignore
 from common.constants.logging_constants import LoggingConstants  # type: ignore
-from core.helper.main_helper import MainHelper
+from common.constants.common_constants import CommonConstants  # type: ignore
+from core.helper.main_helper import MainHelper  # type: ignore
 
 logging_constants = LoggingConstants()
+constants = CommonConstants()
 io_operations = IOOperations()
 main_helper = MainHelper()
+main = click.Group(help="Reddit Scraper")
 
-main = click.Group(help="JSON tools")
 
-
-@main.command("user", help="Scrape user threads")
+@main.command("user", help=constants.click_scrape_user_option_help_msg)
 @click.argument('reddit_user', type=str, nargs=-1)
-@click.option("-n", "--number_results", type=int, help="Number of threads to scrape")
-@click.option("-s", "--sorting_filter", type=click.Choice(["top", "hot", "new"]), default="hot", help="Filter threads")
-@click.option("-o", "--output", type=str, help="The directory to output the downloads")
-@click.option("-v", "--verbose", is_flag=True, default=False, help="Enables verbose mode")
+@click.option("-n", "--number_results", type=click.INT, help=constants.click_number_of_threads_option_help_msg)
+@click.option("-s", "--sorting_filter", type=click.Choice(["top", "hot", "new"]), default="hot",
+              help=constants.click_sorting_filter_option_help_msg)
+@click.option("-o", "--output", type=str, help=constants.click_output_option_help_msg)
+@click.option("-v", "--verbose", is_flag=True, default=False, help=constants.click_verbose_option_help_msg)
 def main_scrape_user(number_results, sorting_filter, reddit_user, output, verbose):
+
+    if number_results <= 0:
+        raise click.BadParameter('Number must be bigger than zero.')
+
     # Setups directories used in the application
     with logger.catch(reraise=True):
         logger.remove()
         output_directory = io_operations.init_directories(output)
 
     # Setups logging for the application
     LoguruSetup.script_logger_config_dict(
@@ -46,34 +52,44 @@
         logging_constants.default_log_backtrace,
         logging_constants.default_log_diagnose,
         logging_constants.default_log_enqueue,
         logging_constants.default_log_catch,
         False,  # enables/disables debug mode logs
     )
 
+    s = time.perf_counter()
+
     main_helper.scrape_user(
         "".join(reddit_user) if len(reddit_user) > 0 else None,
         sorting_filter, number_results, output_directory, verbose)
 
+    elapsed = time.perf_counter() - s
+    logger.info(f"{__file__} executed in {elapsed:0.2f} seconds.")
+
 
-@main.command("subreddits", help="Scrape subreddits threads")
+@main.command("subreddits", help=constants.click_scrape_subreddits_option_help_msg)
 @click.argument('subreddits', type=str, nargs=-1)
-@click.option("-n", "--number_results", type=int, help="Number of threads to scrape")
-@click.option("-s", "--sorting_filter", type=click.Choice(["top", "hot", "new"]), default="hot", help="Filter threads")
+@click.option("-n", "--number_results", type=click.INT,
+              help=constants.click_number_of_threads_option_help_msg)
+@click.option("-s", "--sorting_filter", type=click.Choice(["top", "hot", "new"]), default="hot",
+              help=constants.click_sorting_filter_option_help_msg)
 @click.option("-d", "--details", is_flag=True, default=False,
-              help="If enable outputs the detailed list of threads of each subreddit provided into an individual file")
-@click.option("-o", "--output", type=str, help="The directory to output the downloads")
-@click.option("-v", "--verbose", is_flag=True, default=False, help="Enables verbose mode")
+              help=constants.click_details_option_help_msg)
+@click.option("-o", "--output", type=str, help=constants.click_output_option_help_msg)
+@click.option("-v", "--verbose", is_flag=True, default=False, help=constants.click_verbose_option_help_msg)
 def main_scrape_subreddits(number_results, sorting_filter, subreddits, details, output, verbose):
+
+    if number_results <= 0:
+        raise click.BadParameter('Number must be bigger than zero.')
+
     # Setups directories used in the application
     with logger.catch(reraise=True):
         logger.remove()
         output_directory = io_operations.init_directories(output)
 
-
     # Setups logging for the application
     LoguruSetup.script_logger_config_dict(
         logger,
         output_directory,
         Path(logging_constants.log_filename).name,
         logging_constants.default_log_stfout_level,
         logging_constants.default_log_format,
@@ -89,14 +105,20 @@
         logging_constants.default_log_backtrace,
         logging_constants.default_log_diagnose,
         logging_constants.default_log_enqueue,
         logging_constants.default_log_catch,
         False,  # enables/disables debug mode logs
     )
 
+    s = time.perf_counter()
+
     main_helper.scrape_subreddit(
         ", ".join(subreddits) if len(subreddits) > 0 else None
         , sorting_filter, number_results, details, output_directory, verbose)
 
+    elapsed = time.perf_counter() - s
+    logger.info(f"{__file__} executed in {elapsed:0.2f} seconds.")
+
 
 if __name__ == "__main__":
     exit(main())  # pylint: disable=no-value-for-parameter
+
```

## core/api/reddit_api.py

```diff
@@ -1,16 +1,14 @@
-import logging
 from typing import Optional, Dict, Union
 from loguru import logger       # type: ignore
-import requests     # type: ignore
 
-from common.exceptions.main_exceptions import SubredditNotFoundException, \
-    UserNotFoundException, TokenErrorException  # type: ignore
+from common.exceptions.main_exceptions import UserNotFoundException, TokenErrorException  # type: ignore
 from common.logging.logging_setup import LoggingSetup   # type: ignore
 from common.constants.common_constants import CommonConstants     # type: ignore
+from common.io_operations.request_factory import RequestFactory     # type: ignore
 
 
 class RedditApi:
     """
         A class that provides functions to access Reddit API.
 
     Methods:
@@ -26,14 +24,15 @@
         def generate_params_for_reddit_api_req(self, after: Optional[str], before: Optional[str], count: Optional[int],
             limit: Optional[int], show: Optional[str], sr_detail: bool) -> Dict[str, str]:
             Generate a dictionary of parameters to be used in a Reddit API request.
     """
     def __init__(self, client_id, secret_token, username, password):
         self.logging_setup = LoggingSetup()
         self.main_constants = CommonConstants()
+        self.request_factory = RequestFactory()
         self.client_id = client_id
         self.secret_token = secret_token
         self.username = username
         self.password = password
 
     def generate_headers(self, token: str) -> Dict[str, str]:
         """
@@ -44,93 +43,65 @@
 
         Returns:
             Dict[str, str]: A dictionary of headers with authorization token added.
         """
         headers: dict[str, str] = {**self.main_constants.reddit_headers, **{"Authorization": "bearer {}".format(token)}}
         return headers
 
-    # noinspection PyUnresolvedReferences
-    def generate_reddit_api_token(self, verbose: Optional[bool]) -> str:
+    def generate_reddit_api_token(self) -> str:
         """Generate a Reddit API token using the user's credentials.
 
-        Args:
-            verbose (Optional[bool]): Whether to log additional debug information. Defaults to False.
-
         Raises:
             TokenErrorException: If the request to obtain the token fails.
 
         Returns:
             str: The generated Reddit API token.
         """
 
-        auth = requests.auth.HTTPBasicAuth(self.client_id, self.secret_token)
-        data = {
-            "grant_type": "password",
-            "username": self.username,
-            "password": self.password,
-        }
-        headers = self.main_constants.reddit_headers
-
-        res = requests.post(
-            "https://www.reddit.com/api/v1/access_token",
-            auth=auth,
-            data=data,
-            headers=headers,
-        )
-
-        if not res.ok:
-            if verbose:
-                logger.exception("Request failed with code {}".format(res.status_code))
-            raise TokenErrorException("Failed to obtain Reddit API token")
-
-        token = res.json()["access_token"]
+        response = self.request_factory.get_access_token(self.client_id, self.secret_token, self.username,
+                                                         self.password, self.main_constants)
+        token = response["access_token"]
         msg = "Generated Reddit API token: {}".format(token)
-        logger.exception(msg)
+        logger.info(msg)
         return token
 
-    def get_logged_user_profile(self, verbose: Optional[bool]) -> Union[Dict, None]:
+    def get_logged_user_profile(self) -> Union[Dict, None]:
         """
         Retrieve the logged-in user's profile information.
 
-        Args:
-            verbose (Optional[bool]): If True, prints the API request information and response.
-
         Returns:
             Union[Dict, None]: Returns a dictionary of the user profile information if successful,
             otherwise returns None.
 
         Raises:
             UserNotFoundException: If the user profile is unavailable or could not be reached.
         """
         # Generating the URL leading to the desired subreddit
-        url = "https://oauth.reddit.com/api/v1/me"
-        token = self.generate_reddit_api_token(verbose)
+        url = "{}/api/v1/me".format(self.main_constants.reddit_api_base_url)
+        token = self.generate_reddit_api_token()
         headers = {**self.main_constants.reddit_headers, **{"Authorization": "bearer {}".format(token)}}
 
         response = None
 
         try:
-            res = requests.get(url, headers=headers)
+            res = self.request_factory.get(url, headers=headers)
 
-            if res.ok:
+            if res:
                 response = res.json()
 
         except UserNotFoundException as exc:
-            msg = (
-                "Your user profile is unavailable or couldn't be reached at this moment"
-            )
-            logging.exception(msg)
+            msg = "Your user profile is unavailable or couldn't be reached at this moment"
+            logger.exception(msg)
             raise UserNotFoundException(msg) from exc
 
         return response
 
-    def generate_params_for_reddit_api_req(
-        self, after: Optional[str], before: Optional[str], count: Optional[int],
-        limit: Optional[int], show: Optional[str], sr_detail: Optional[bool]
-    ) -> Dict[str, object]:
+    def generate_params_for_reddit_api_req( self, after: Optional[str], before: Optional[str], count: Optional[int],
+                                            limit: Optional[int], show: Optional[str], sr_detail: Optional[bool]) \
+            -> Dict[str, object]:
         """
         Generate a dictionary of parameters to be used in a Reddit API request.
 
         Args:
             after (Optional[str]): The fullname of the post to start after.
             before (Optional[str]): The fullname of the post to start before.
             count (Optional[int]): The number of items in the listing to skip.
```

## core/helper/main_helper.py

```diff
@@ -1,24 +1,25 @@
 import glob
 from pathlib import Path
-from typing import Any, Optional, List, Iterable
-from loguru import logger  # type: ignore
+from typing import Any, Optional, List, Union
+
 from bs4 import ResultSet
+from loguru import logger  # type: ignore
 
-from common.exceptions.main_exceptions import MissingRequiredParameter
+from common.constants.common_constants import CommonConstants  # type: ignore
+from common.constants.logging_constants import LoggingConstants  # type: ignore
+from common.exceptions.main_exceptions import MissingRequiredParameter  # type: ignore
 from common.io_operations.image_downloader import ImageDownloader  # type: ignore
 from common.io_operations.io_operations import IOOperations  # type: ignore
 from common.logging.loguru_setup import LoguruSetup  # type: ignore
-from common.constants.logging_constants import LoggingConstants  # type: ignore
-from common.constants.common_constants import CommonConstants  # type: ignore
 from common.validations.parameter_validations import ParameterValidations  # type: ignore
 from common.validations.reddit_api_validations import RedditApiValidations  # type: ignore
 from core.api.reddit_api import RedditApi  # type: ignore
+from core.scraper.scraper_helper import ScraperHelper  # type: ignore
 from core.scraper.thread_scraper import ThreadScraper  # type: ignore
-from core.scraper.scraper_helper import ScraperHelper   # type: ignore
 
 
 class MainHelper:
     """
     A class that provides helper functions for the main application.
 
     Methods:
@@ -28,15 +29,14 @@
 
         scrape_user(reddit_user: str, verbose: bool, number_results: int) -> None:
             Scrapes a Reddit user's submissions, downloads the images, and exports detailed thread information.
 
         scrape_subreddit(subreddits: Optional[List[str]], sorting_type: str, number_results: Optional[int],
             details: bool, verbose: bool) -> None:
             Scrapes posts and comments from a subreddit.
-
     """
 
     def __init__(self) -> None:
         super().__init__()
         self.parameter_validations = ParameterValidations()
         self.image_downloader = ImageDownloader()
         self.io_operations = IOOperations()
@@ -49,62 +49,63 @@
             self.main_constants.password,
         )
         self.thread_scraper = ThreadScraper()
         self.reddit_api_validations = RedditApiValidations()
         self.scraper_helper = ScraperHelper()
 
     def export_threads_detailed_information(self,
-                                            user_or_subreddit: str,
+                                            user_or_subreddit: Union[str, list[str]],
                                             export_mode: str,
                                             threads_list: dict[str, dict[str, dict[str, Any] | ResultSet | Any] | Any],
                                             output_directory: str,
                                             verbose: Optional[bool]) -> None:
         """Write all the img urls to a json file, sorted by subreddit and post.
 
         Args:
             output_directory (str): Directory to output thread detailed information
-            user_or_subreddit (str): The name of the user or subreddit to export data for.
-            export_mode (str): The export mode, which can be "single", "multiple", or "user".
-            threads_list (List[dict]): A list of dictionaries containing thread information.
-            verbose (Optional[bool]): Whether to print verbose output.
+            user_or_subreddit (str): The name of the user or subreddit to export data for
+            export_mode (str): The export mode, which can be "single", "multiple", or "user"
+            threads_list (List[dict]): A list of dictionaries containing thread information
+            verbose (Optional[bool]): Whether to print verbose output
 
         Returns:
             None
         """
 
-        detailed_report = ""
-
         match export_mode:
             case "single":
-                detailed_report = "{}/reports/subreddits/{}_{}_summary.{}".format(
-                    output_directory, self.main_constants.current_date, user_or_subreddit, "json")
+                for subreddit in user_or_subreddit:
+                    detailed_report = "{}/reports/subreddits/{}_{}_summary.{}".format(
+                        output_directory, self.main_constants.current_date, subreddit, "json")
+                    subreddit_threads_list = threads_list[subreddit]
+                    self.io_operations.write_detailed_post_information(
+                        subreddit_threads_list, "w", detailed_report, verbose)
             case "multiple":
+                filename = ' '.join(map(str, user_or_subreddit)).replace(" ", "_")
                 detailed_report = "{}/reports/subreddits/{}_{}_summary.{}".format(
-                    output_directory, self.main_constants.current_date, user_or_subreddit, "json")
+                    output_directory, self.main_constants.current_date, filename, "json")
+                self.io_operations.write_detailed_post_information(
+                    threads_list, "w", detailed_report, verbose)
             case "user":
                 detailed_report = "{}/reports/users/{}_{}_summary.{}".format(
                     output_directory, self.main_constants.current_date, user_or_subreddit, "json")
-
-        self.io_operations.write_detailed_post_information(
-            threads_list, "w", detailed_report, verbose)
+                self.io_operations.write_detailed_post_information(
+                    threads_list, "w", detailed_report, verbose)
 
     def scrape_user(self, reddit_user: str, sort: str, number_results: int, output_directory: str,
                     verbose: Optional[bool]) -> None:
         """
         Scrape a Reddit user's submissions, download the images, and export detailed thread information.
 
         Args:
             reddit_user (str): The name of the Reddit user to scrape.
             sort:  (str): The type of posts to be scraped: hot, new, top
             verbose (Optional[bool]): Whether to print verbose output.
             output_directory: (str): Directory to output the downloaded files and reports
             number_results (int): The number of results to scrape.
-
-        Returns:
-            None
         """
         user_reddit_profile_to_scrape = self.main_constants.user_profile_to_scrape
 
         if reddit_user is None and user_reddit_profile_to_scrape is not None:
             reddit_user = user_reddit_profile_to_scrape
         elif (reddit_user is not None and user_reddit_profile_to_scrape is not None or reddit_user is not None
               and user_reddit_profile_to_scrape is None):
@@ -139,87 +140,73 @@
                     verbose,
                 )
 
             self.export_threads_detailed_information(reddit_user, "user", user_threads, output_directory, verbose)
 
         logger.info("Finished scraping threads for subreddits: {}".format(reddit_user)) if verbose else None
 
-    def scrape_subreddit(self, subreddits: Iterable[str], sorting_type: str, number_results: Optional[int],
+    def scrape_subreddit(self, subreddits: Union[str, List[str]], sorting_type: str, number_results: Optional[int],
                          details: bool, output_directory: str, verbose: bool) -> None:
         """
         Scrape posts and comments from a subreddit.
 
         Args:
-
             subreddits (List[str], optional): A list of subreddits to scrape. If None, the user's subreddits will
                     be used.
             sorting_type (str): A string indicating how to sort the posts. Valid values: 'hot', 'new', 'top',
                 'controversial', 'rising'.
             number_results (int, optional): The maximum number of posts to scrape. If None, all posts will be scraped.
             details (bool): If True, exports detailed information about each post to a JSON file.
             output_directory: (str): Directory to output the downloaded files and reports
             verbose (bool): If True, displays logging information during the scraping process.
-
-        Returns:
-            None
         """
 
         user_subreddits_list = self.main_constants.user_subreddits_list
 
         if subreddits is None and user_subreddits_list is not None:
             subreddits = user_subreddits_list
-        elif (subreddits is not None and user_subreddits_list is not None or subreddits is not None
-              and user_subreddits_list is None):
+        elif subreddits is not None and user_subreddits_list is not None or subreddits is not None \
+                and user_subreddits_list is None:
             subreddits = subreddits
         else:
             msg = "Missing subreddits. If you want to scrape a subreddit or multiple subreddits, provide one or " \
-                  "multiple subreddits separated by a comma or semi-colo"
+                  "multiple subreddits separated by a comma or semi-colon"
             raise MissingRequiredParameter(msg)
 
         # Validate Args
         subreddits = self.parameter_validations.validate_subreddits_parameter(subreddits)
 
         # Validate Subreddit
         self.reddit_api_validations.validate_subreddits_list(subreddits, verbose)
 
         subreddits_detailed_information_dict: dict = {}
 
-        # Scrape posts and comments
+        # Scrape threads and comments, extract image urls and downloads them
         for subreddit in subreddits:
-            # Scrapes n number of threads for the given subreddits, according to provided parameter max_count
             subreddit_threads_list = self.thread_scraper.scrape_threads(
                 subreddit, sorting_type, "subreddit", verbose,
                 number_results if number_results is not None else self.main_constants.user_num_threads_to_scrape)
 
             subreddits_detailed_information_dict = {**subreddits_detailed_information_dict,
                                                     **subreddit_threads_list}
 
-            img_output_dir = Path("{}/downloads/subreddit/{}".format(output_directory, subreddit))
-
-            # Downloads scraped img urls
             self.image_downloader.download_img_url_list(
                 subreddit, subreddit_threads_list, "subreddit", output_directory, verbose)
 
+            img_output_dir = Path("{}/downloads/subreddit/{}".format(output_directory, subreddit))
+
             image_files = glob.glob(str(img_output_dir) + "/*.jpg") + glob.glob(
                 str(img_output_dir) + "/*.png") + glob.glob(
                 str(img_output_dir) + "/*.gif")
 
-            # If the image list size is bigger than 0, sort the downloaded images by mime type and resolution
             if len(image_files) > 0:
-                self.io_operations.sort_by_mime_type_and_resolution(
-                    img_output_dir,
-                    img_output_dir,
-                    True,
-                    verbose,
-                )
+                self.io_operations.sort_by_mime_type_and_resolution(img_output_dir, img_output_dir, True, verbose)
 
-            # If the user wants to export all subreddit data to one file or multiple files, can pass the detail option
-            if details:
-                self.export_threads_detailed_information(subreddit, "single",
-                                                         subreddit_threads_list, output_directory, verbose)
-
-        if not details:
-            self.export_threads_detailed_information(' '.join(map(str, subreddits)).replace(" ", "_"), "multiple",
-                                                     subreddits_detailed_information_dict, output_directory, verbose)
+        if details:
+            self.export_threads_detailed_information(subreddits, "single", subreddits_detailed_information_dict,
+                                                     output_directory, verbose)
+        else:
+            self.export_threads_detailed_information(subreddits, "multiple", subreddits_detailed_information_dict,
+                                                     output_directory, verbose)
 
         logger.info("Finished scraping threads for subreddits: {}".format(" ".join(map(str, subreddits)))) \
             if verbose else None
```

## core/scraper/comment_scraper.py

```diff
@@ -1,21 +1,21 @@
 import typing
 from typing import List, Dict, Tuple, Any
 
 import bs4
-from bs4 import BeautifulSoup
-from loguru import logger  # type: ignore
 import validators  # type: ignore
+from bs4 import element
+from loguru import logger  # type: ignore
 
-from common.logging.logging_setup import LoggingSetup  # type: ignore
 from common.constants.common_constants import CommonConstants  # type: ignore
 from common.io_operations.image_downloader import ImageDownloader  # type: ignore
+from common.logging.logging_setup import LoggingSetup  # type: ignore
 from common.logging.utils.loguru_wrappers import logger_wraps  # type: ignore
 from common.validations.url_validations import UrlValidations  # type: ignore
-from core.scraper.scraper_helper import ScraperHelper
+from core.scraper.scraper_helper import ScraperHelper  # type: ignore
 
 
 class CommentScraper:
     """
     A class for scraping comments and replies from a given HTML element.
 
     Attributes:
@@ -23,31 +23,31 @@
         constants (CommonConstants): An instance of ConstantsNamespace for constants.
         validations (UrlValidations): An instance of UrlValidations for validating URLs.
         image_downloader (ImageDownloader): An instance of ImageDownloader for downloading images.
         scraper_helper (ScraperHelper): An instance of ScraperHelper for helper methods.
 
     Methods:
         def scrape_comments(soup): Scrape comments from the given comments' element.
+
         def scrape_replies(reply_divs): Scrapes the replies from the given reply divs and returns a list of dictionaries
         representing each reply.
     """
 
     def __init__(self) -> None:
         super().__init__()
         self.logging_funcs = LoggingSetup()
         self.constants = CommonConstants()
         self.validations = UrlValidations()
         self.image_downloader = ImageDownloader()
         self.scraper_helper = ScraperHelper()
-        self.processed_comments = []
-        self.img_urls = []
+        self.processed_comments: List[str] = []
+        self.img_urls: List[str] = []
 
-    # noinspection PyUnresolvedReferences
     @logger_wraps()
-    def scrape_comments(self, soup: BeautifulSoup) -> Tuple[List[Dict[str, any]], List[str]]:
+    def scrape_comments(self, soup) -> Tuple[List[Dict[str, Any]], List[str]]:
         """
         Scrape comments from the given comments' element.
 
         Args:
             soup (bs4.BeautifulSoup): A BeautifulSoup object representing the HTML or XML source.
 
         Returns:
@@ -67,33 +67,32 @@
                         - 'hasChildren': A boolean indicating whether the given reply has children replies or not.
                         - 'urls': A list of strings representing the URLs found in the given comment.
                         - 'replies': A list of nested dictionaries representing the children replies for the given
                             reply.
                     - A list of strings representing the URLs of any images found in the comments.
         """
 
-        comments: List[Dict[str, any]] = []
-        self.img_urls: List[str] = []
-        self.processed_comments: List[str] = []
+        comments: List[Dict[str, Any]] = []
+        self.img_urls = []
+        self.processed_comments = []
 
-        comment_area: bs4.element.Tag = soup.find("div", attrs={"class": "commentarea"})
+        comment_area = soup.find("div", attrs={"class": "commentarea"})
         comments_link_listing = comment_area.find("div", attrs={"class", "sitetable"})
 
         for comment_ele in comments_link_listing:
-            c_ele: bs4.element.Tag = comment_ele
+            c_ele: element.Tag = comment_ele
             if "thing" in c_ele.attrs["class"]:
                 if c_ele.attrs["data-permalink"] not in self.processed_comments:
                     comment = self.process_comment(c_ele)
                     comments.append(comment)
 
         logger.debug("Processed Replies: {}".format(len(self.processed_comments)))
 
         return comments, list(set(self.img_urls))
 
-    # noinspection PyUnresolvedReferences
     @logger_wraps()
     def scrape_replies(self, reply_divs: List[Any]) -> Tuple[List[Dict[str, Any]], List[str]]:
         """
         Scrapes the replies from the given reply divs and returns a list of dictionaries representing each reply.
 
         Args:
             reply_divs: A list of reply divs to scrape from.
@@ -123,39 +122,39 @@
         for reply_div in reply_divs:
             if reply_div.attrs["data-permalink"] not in self.processed_comments:
                 reply = self.process_reply(reply_div)
                 replies.append(reply)
 
         return replies, list(set(img_urls))
 
-    def process_comment(self, comment_element: bs4.element.Tag):
+    def process_comment(self, comment_element: element.Tag):
         comment_has_children, comment_num_children = self.scraper_helper.define_children_fields(comment_element)
         urls = self.scraper_helper.construct_urls_list(comment_element)
         self.img_urls = self.img_urls + urls
 
-        comment = {"text": comment_element.find("div", class_="md").text.strip(),
+        comment = {"text": comment_element.find("div", class_="md").text.strip(),   # type: ignore
                    "author": self.scraper_helper.construct_author_dict(comment_element),
                    "rating": self.scraper_helper.construct_rating_dict(comment_element),
                    "datetime": self.scraper_helper.construct_time_dict(comment_element),
                    "url": comment_element.attrs["data-permalink"], "hasChildren": comment_has_children,
                    "numChildren": comment_num_children, "urls": urls, "replies": []}
 
         child_div = comment_element.find("div", attrs={"class", "child"})
-        reply_divs = child_div.find_all("div", attrs={"class", "comment"})
+        reply_divs = child_div.find_all("div", attrs={"class", "comment"})      # type: ignore
 
         self.processed_comments.append(comment_element.attrs["data-permalink"])
 
         if reply_divs:
             replies, img_urls = self.scrape_replies(reply_divs)
             processed_replies: List[Dict[str, Any]] = self.scraper_helper.remove_empty_lists(replies)
             comment["replies"] = processed_replies
 
         return comment
 
-    def process_reply(self, reply_element: bs4.element.Tag):
+    def process_reply(self, reply_element):  # --check-untyped-defs
         reply_has_children, reply_num_children = self.scraper_helper.define_children_fields(reply_element)
         urls = self.scraper_helper.construct_urls_list(reply_element)
         self.img_urls = self.img_urls + urls
 
         reply = {"text": reply_element.find("div", class_="md").text.strip(),
                  "author": self.scraper_helper.construct_author_dict(reply_element),
                  "rating": self.scraper_helper.construct_rating_dict(reply_element),
@@ -164,12 +163,11 @@
                  "numChildren": reply_num_children, "urls": urls, "replies": []}
 
         self.processed_comments.append(reply_element.attrs["data-permalink"])
 
         nested_reply_divs = reply_element.find_all('div', class_='comment')
         if nested_reply_divs:
             processed_replies, img_urls = self.scrape_replies(nested_reply_divs)
-            post_processed_replies: List[Dict[str, Any]] = \
-                self.scraper_helper.remove_empty_lists(processed_replies)
+            post_processed_replies = self.scraper_helper.remove_empty_lists(processed_replies)
             reply['replies'] = post_processed_replies
 
         return reply
```

## core/scraper/scraper_helper.py

```diff
@@ -1,14 +1,15 @@
 import glob
 import re
-from loguru import logger
 from pathlib import Path
-from typing import Dict, Tuple, List, Any
-from bs4 import BeautifulSoup, element
-import validators
+from typing import Dict, Tuple, List, Any, Union
+
+import validators  # type: ignore
+from bs4 import BeautifulSoup, Tag
+from loguru import logger
 
 from common.constants.common_constants import CommonConstants  # type: ignore
 from common.validations.url_validations import UrlValidations  # type: ignore
 
 
 class ScraperHelper:
     """
@@ -31,50 +32,50 @@
 
     def __init__(self) -> None:
         super().__init__()
         self.constants = CommonConstants()
         self.validations = UrlValidations()
 
     # noinspection PyUnresolvedReferences
-    def construct_author_dict(self, div_ele: element.PageElement) -> Dict[str, str]:
+    def construct_author_dict(self, div_ele: Tag) -> Dict[str, Union[str, List[str]]]:
         """
         Constructs a dictionary with the author's username and profile URL.
 
         Args:
             div_ele: A BeautifulSoup object representing a div element that contains the author's information.
 
         Returns:
             A dictionary with the author's username and profile URL.
 
         """
-        author_el = div_ele.find("a", class_="author")
+        author_el: Tag = div_ele.find("a", class_="author")  # type: ignore
 
         author_username = author_el.text.strip()
         profile = author_el["href"]
 
         author = {"username": author_username, "profile": profile}
 
         return author
 
     # noinspection PyUnresolvedReferences
-    def construct_rating_dict(self, div_ele: element.PageElement) -> Dict[str, str]:
+    def construct_rating_dict(self, div_ele: Tag) -> Dict[str, str]:
         """
         Constructs a dictionary with the comment's rating scores.
 
         Args:
             div_ele: A BeautifulSoup object representing a div element that contains the comment's rating scores.
 
         Returns:
             A dictionary with the comment's rating scores.
         """
         rating = {}
 
-        score_dislikes = div_ele.find("span", attrs={"class": "dislikes"})
-        score_unvoted = div_ele.find("span", attrs={"class": "unvoted"})
-        score_likes = div_ele.find("span", attrs={"class": "likes"})
+        score_dislikes: Tag = div_ele.find("span", attrs={"class": "dislikes"})  # type: ignore
+        score_unvoted: Tag = div_ele.find("span", attrs={"class": "unvoted"})  # type: ignore
+        score_likes: Tag = div_ele.find("span", attrs={"class": "likes"})  # type: ignore
 
         if score_dislikes is not None:
             parent_comment_author_score_dislikes = score_dislikes.text
             rating["score_dislikes"] = parent_comment_author_score_dislikes
 
         if score_unvoted is not None:
             parent_comment_author_score_unvoted = score_unvoted.text
@@ -83,30 +84,30 @@
         if score_likes is not None:
             parent_comment_author_score_likes = score_likes.text
             rating["score_likes"] = parent_comment_author_score_likes
 
         return rating
 
     # noinspection PyUnresolvedReferences
-    def construct_thread_rating_dict(self, div_ele: element.PageElement) -> Dict[str, str]:
+    def construct_thread_rating_dict(self, div_ele: Tag) -> Dict[str, str]:
         """
         Constructs a dictionary with the thread's rating scores.
 
         Args:
             div_ele: A BeautifulSoup object representing a div element that contains the thread's rating scores.
 
         Returns:
             A dictionary with the thread's rating scores.
 
         """
         rating = {}
 
-        score_dislikes = div_ele.find("div", attrs={"class": "dislikes"})
-        score_unvoted = div_ele.find("div", attrs={"class": "unvoted"})
-        score_likes = div_ele.find("div", attrs={"class": "likes"})
+        score_dislikes: Tag = div_ele.find("div", attrs={"class": "dislikes"})  # type: ignore
+        score_unvoted: Tag = div_ele.find("div", attrs={"class": "unvoted"})  # type: ignore
+        score_likes: Tag = div_ele.find("div", attrs={"class": "likes"})  # type: ignore
 
         if score_dislikes is not None:
             parent_comment_author_score_dislikes = score_dislikes.text
             rating["score_dislikes"] = parent_comment_author_score_dislikes
 
         if score_unvoted is not None:
             parent_comment_author_score_unvoted = score_unvoted.text
@@ -115,65 +116,65 @@
         if score_likes is not None:
             parent_comment_author_score_likes = score_likes.text
             rating["score_likes"] = parent_comment_author_score_likes
 
         return rating
 
     # noinspection PyUnresolvedReferences
-    def construct_time_dict(self, div_ele: element.PageElement) -> Dict[str, str]:
+    def construct_time_dict(self, div_ele: Tag) -> Dict[str, str]:
         """
         Constructs a dictionary with the time information of the comment or thread.
 
         Args:
             div_ele: A BeautifulSoup object representing a div element that contains the time information.
 
         Returns:
             A dictionary with the time information of the comment or thread.
 
         """
         time = {}
 
-        time_el = div_ele.find("time", attrs={"class": ["live-timestamp"]})
+        time_el: Tag = div_ele.find("time", attrs={"class": ["live-timestamp"]})  # type: ignore
 
         if time_el is not None:
             time["time"] = time_el.attrs["title"]
             time["datetime"] = time_el.attrs["datetime"]
             time["time_since_posting"] = time_el.text
 
         return time
 
     # noinspection PyUnresolvedReferences
-    def define_children_fields(self, div_ele: element.PageElement) -> Tuple[bool, int]:
+    def define_children_fields(self, div_ele: Tag) -> Tuple[bool, int]:
         """
         Defines the number of children and whether a comment or thread has children.
 
         Args:
             div_ele: A BeautifulSoup object representing a div element that contains the information about children.
 
         Returns:
             A tuple with the boolean value indicating whether the comment or thread has children, and the number of
             children (an integer) if there are any.
 
         """
-        num_children_el = div_ele.find("a", attrs={"class": ["numchildren"]})
+        num_children_el: Tag = div_ele.find("a", attrs={"class": ["numchildren"]})  # type: ignore
 
         num_children = None
         has_children = None
 
         if num_children_el is not None:
             pattern = self.constants.extract_num_children
             num_children = re.findall(pattern, num_children_el.text)[0]
             num_children = int(num_children)
             has_children = True if num_children > 0 else False
 
         return has_children, num_children
 
     # noinspection PyUnresolvedReferences
 
-    def construct_urls_list(self, div_ele: element.PageElement) -> List[str]:
+    def construct_urls_list(self, div_ele: Tag) -> List[str]:
         """
         Constructs a list of URLs from the div element that contains the URLs.
 
         Args:
             div_ele: A BeautifulSoup object representing a div element that contains URLs.
 
         Returns:
@@ -216,34 +217,35 @@
             list: A list of image file paths.
         """
 
         image_files = glob.glob(str(directory) + "/*.jpg") + glob.glob(str(directory) + "/*.png") \
                       + glob.glob(str(directory) + "/*.gif")
         return image_files
 
-    def define_threads_based_on_search_parameter(self, scrape_mode, request, subreddit_or_user):
-        threads = None
+    def define_threads_based_on_search_parameter(self, scrape_mode: str, request, subreddit_or_user):
+
         soup: BeautifulSoup = BeautifulSoup(request.text, "html.parser")
 
         match scrape_mode:
             case "subreddit":
-                threads_list_element = soup.find(
-                    "div", attrs={"class": "sitetable linklisting"})
-                threads = threads_list_element.find_all(
-                    "div", attrs={"data-subreddit-prefixed": "r/{}".format(subreddit_or_user)})
+                threads_list_element: Tag = soup.find(
+                    "div", attrs={"class": "sitetable linklisting"})  # type: ignore
+                subreddit_threads: List[Tag] = threads_list_element.find_all(
+                    "div", attrs={"data-subreddit-prefixed": "r/{}".format(subreddit_or_user)})  # type: ignore
+                return subreddit_threads
             case "user":
-                threads_list_element = soup.find("div", attrs={"class": "sitetable linklisting"})
-                threads = threads_list_element.find_all("div", attrs={"data-author": subreddit_or_user})
-
-        return threads
+                threads_list_element: Tag = soup.find("div", attrs={"class": "sitetable linklisting"})  # type: ignore
+                user_threads: List[Tag] = threads_list_element.find_all("div", attrs={
+                    "data-author": subreddit_or_user})  # type: ignore
+                return user_threads
 
     def process_thread_images(self, thread_img_ele, thread_images):
         if thread_img_ele is not None:
-            thread_image_a_tags = thread_img_ele.find_all("a", attrs={"class", "may-blank"})
-            thread_iframe_tag = thread_img_ele.find("iframe", attrs={"class", "media-embed"})
+            thread_image_a_tags: List[Tag] = thread_img_ele.find_all("a", attrs={"class", "may-blank"})  # type: ignore
+            thread_iframe_tag: Tag = thread_img_ele.find("iframe", attrs={"class", "media-embed"})  # type: ignore
 
             if thread_image_a_tags is not None and len(thread_image_a_tags) >= 1:
                 for thread_image_a_tag in thread_image_a_tags:
                     logger.debug(thread_image_a_tag.attrs["href"])
                     if self.validations.validate_if_url_is_a_valid_img_link(thread_image_a_tag.attrs["href"],
                                                                             self.constants.possible_urls):
                         thread_images.append(thread_image_a_tag.attrs["href"])
```

## core/scraper/thread_scraper.py

```diff
@@ -1,59 +1,58 @@
-import logging
 import time
-from typing import Any, Optional, Dict, Union
-import requests  # type: ignore
+from typing import Any, Optional, Dict, Union, List, Tuple
 from loguru import logger  # type: ignore
 from bs4 import BeautifulSoup, ResultSet
 
 from common.exceptions.main_exceptions import TokenErrorException  # type: ignore
 from common.logging.logging_setup import LoggingSetup  # type: ignore
 from common.constants.common_constants import CommonConstants  # type: ignore
 from common.logging.utils.loguru_wrappers import logger_wraps   # type: ignore
 from common.validations.url_validations import UrlValidations  # type: ignore
-from common.io_operations.request_manager import RequestManager  # type: ignore
 from common.io_operations.image_downloader import ImageDownloader  # type: ignore
 from common.io_operations.io_operations import IOOperations  # type: ignore
 from core.scraper.comment_scraper import CommentScraper  # type: ignore
 from core.api.reddit_api import RedditApi  # type: ignore
 from core.scraper.scraper_helper import ScraperHelper  # type: ignore
+from common.io_operations.request_factory import RequestFactory     # type: ignore
 
 
 class ThreadScraper:
     """
     A class that provides methods to scrape threads from subreddits or users
 
     Methods:
-        def scrape_threads(self, subreddit_or_user: str, sort: str, scrape_mode: str, verbose: bool,
+        - def scrape_threads(self, subreddit_or_user: str, sort: str, scrape_mode: str, verbose: bool,
             max_counter: Optional[int] = None ) -> Dict[str, Union[Dict[str, Union[Dict[str, Any], Dict[str, Any],
             ResultSet[Any], Dict[str, Any], Any]], Any]]:
             Scrape threads from a subreddit or user and return the results.
-        def scrape_single_thread(self, link: str, verbose: bool):
+
+        - def scrape_single_thread(self, link: str, verbose: bool):
             Scrapes the given Reddit thread URL and returns a tuple containing a list of image URLs and a dictionary of
             comments.
     """
 
     def __init__(self) -> None:
         super().__init__()
         self.logging_funcs = LoggingSetup()
         self.constants = CommonConstants()
         self.validations = UrlValidations()
         self.comment_scrapper = CommentScraper()
-        self.request_manager = RequestManager()
         self.io_operations = IOOperations()
         self.image_downloader = ImageDownloader()
         self.scraper_helper = ScraperHelper()
+        self.request_factory = RequestFactory()
         self.reddit_api = RedditApi(self.constants.client_id, self.constants.secret_token, self.constants.username,
                                     self.constants.password)
 
     @logger_wraps()
-    def scrape_threads(self, subreddit_or_user: str, sort: str, scrape_mode: str, verbose: bool,
-                       max_counter: Optional[int]
-                       ) -> Dict[
-        str, Union[Dict[str, Union[Dict[str, Any], Dict[str, Any], ResultSet[Any], Dict[str, Any], Any]], Any]]:
+    def scrape_threads(self, subreddit_or_user: str, sort: str, scrape_mode: str, verbose: bool, max_counter: int
+                       ) \
+            -> Dict[
+                str, Union[Dict[str, Union[Dict[str, Any], Dict[str, Any], ResultSet[Any], Dict[str, Any], Any]], Any]]:
         """
         Scrape threads from a subreddit or user and return the results.
 
         Args:
             subreddit_or_user (str): The name of the subreddit or user from which to scrape threads.
             sort (str): The method to sort the threads, such as "hot" or "top".
             scrape_mode (str): The mode in which to scrape threads, either "subreddit" or "user".
@@ -65,27 +64,24 @@
                 Any]]:
                 A dictionary of thread URLs and their corresponding information, such as their author, datetime,
                  rating, URLs, and comments.
         """
         params: Optional[Dict[str, Any]] = None
 
         if scrape_mode == "subreddit":
-            endpoint = "/r/{}/{}".format(subreddit_or_user, sort)
+            endpoint = "/r/{}/{}/".format(subreddit_or_user, sort)
             url = "{}{}".format(self.constants.old_reddit_url, endpoint)
         else:
             # Generating the URL leading to the desired subreddit
             endpoint = "/user/{}/{}".format(subreddit_or_user, "submitted")
             url = "{}{}".format(self.constants.old_reddit_url, endpoint)
             params = self.reddit_api.generate_params_for_reddit_api_req(None, None, None, None, "all", None)
 
-        req = self.request_manager.request_page(
-            url,
-            self.constants.user_agent,
-            params if params is not None else None
-        )
+        req = self.request_factory.get(url, headers=self.constants.user_agent,
+                                       params=params if params is not None else None)
 
         threads_urls: Dict[
             str, Union[Dict[str, Union[Dict[str, Any], Dict[str, Any], ResultSet[Any], Dict[str, Any], Any]], Any]
         ] = {}  # noqa comments
         thread_img_urls = {}
 
         counter = 1
@@ -138,63 +134,63 @@
                     next_button = soup.find("span", class_="next-button")
                     if next_button is not None:
                         next_page_link = next_button.find("a").attrs["href"]  # type: ignore
                     else:
                         full = True
                         break
 
-                    req = requests.get(next_page_link, headers=self.constants.user_agent, timeout=10)
-                    soup: BeautifulSoup = BeautifulSoup(req.text, "html.parser")
+                    req = self.request_factory.get(next_page_link, headers=self.constants.user_agent)
+                    soup = BeautifulSoup(req.text, "html.parser")
                 except TokenErrorException as exc:
-                    self.logging_funcs.print_exception_log(str(exc), verbose)
+                    logger.exception(str(exc))
                     break
 
             threads_urls[subreddit_or_user] = thread_img_urls
 
             logger.debug("[5] FINISHED SCRAPING STEP")
         else:
             message = "Error fetching results.. Try again!"
             logger.exception(message, verbose)
 
         return threads_urls
 
-    def scrape_single_thread(self, link: str, verbose: bool):
+    def scrape_single_thread(self, link: str, verbose: bool) -> Tuple[list[dict[str, Any]], list[str]]:
         """
         Scrapes the given Reddit thread URL and returns a tuple containing a list of image URLs and a dictionary of
-         comments.
+        comments.
 
         Args:
             link (str): The URL of the Reddit thread to be scraped.
             verbose (bool): Whether to print verbose logging messages.
 
         Returns:
-            (tuple[Any, dict[str, dict[str, Any]]]): A tuple containing a list of image URLs and a dictionary of comments.
+            Tuple[List[str], Dict[str, Dict[str, Any]]]:
+                A tuple containing a list of image URLs and a dictionary of
+                comments. The first element of the tuple is a list of strings representing image URLs, and the second
+                element is a dictionary of comments. The dictionary has keys that correspond to the comment IDs and
+                values that are dictionaries containing information about each comment.
         """
 
         msg = "Scraping thread: {}{}".format(self.constants.old_reddit_url, link)
         logger.info(msg) if verbose else None
 
         # Using a user-agent to mimic browser activity
-        req = self.request_manager.request_page(self.constants.old_reddit_url + link, self.constants.user_agent)
-
-        if req is not None:
-            # Parsing through the web page for obtaining the right html tags
-            # and scraping the details required
-            soup: BeautifulSoup = BeautifulSoup(req.text, "html.parser")
+        req = self.request_factory.get(self.constants.old_reddit_url + link, headers=self.constants.user_agent)
 
-            thread_images = []
+        # Parsing through the web page for obtaining the right html tags
+        # and scraping the details required
+        soup: BeautifulSoup = BeautifulSoup(req.text, "html.parser")
 
-            thread_img_ele = soup.find("div", attrs={"class", "expando"})
+        thread_images: List[str] = []
 
-            self.scraper_helper.process_thread_images(thread_img_ele, thread_images)
-            thread_comments, img_urls = self.comment_scrapper.scrape_comments(soup)
+        thread_img_ele = soup.find("div", attrs={"class", "expando"})
 
-            thread_images = list(set(thread_images))
+        self.scraper_helper.process_thread_images(thread_img_ele, thread_images)
+        thread_comments, img_urls = self.comment_scrapper.scrape_comments(soup)
 
-            # Remove all elements from thread_images that contain the value "crop=smart"
-            thread_images = [x for x in thread_images if "crop=smart" not in x]
+        thread_images = list(set(thread_images))
 
-            logging.debug(thread_images)
-            logging.debug(len(thread_images))
+        # Remove all elements from thread_images that contain the value "crop=smart"
+        thread_images = [x for x in thread_images if "crop=smart" not in x]
+        img_urls = img_urls + thread_images
 
-            img_urls = img_urls + thread_images
-            return thread_comments, img_urls
+        return thread_comments, img_urls
```

## Comparing `redscrap-0.0.3.dist-info/LICENSE` & `redscrap-0.0.4.dist-info/LICENSE`

 * *Files identical despite different names*

## Comparing `redscrap-0.0.3.dist-info/METADATA` & `redscrap-0.0.4.dist-info/METADATA`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: redscrap
-Version: 0.0.3
+Version: 0.0.4
 Summary: A Reddit scraper that can scrape a subreddit or a user for images and download them
 Home-page: https://github.com/martimdLima
 Author: Martim Lima
 Author-email: martim.d.lima@protonmail.com
 Keywords: development,setup,setuptools
 Classifier: Development Status :: 1 - Planning
 Classifier: Programming Language :: Python :: 3.0
```

## Comparing `redscrap-0.0.3.dist-info/RECORD` & `redscrap-0.0.4.dist-info/RECORD`

 * *Files 10% similar despite different names*

```diff
@@ -1,42 +1,42 @@
 __init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 common/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 common/constants/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-common/constants/common_constants.py,sha256=myVfKDu3pqmIcLh6hpP8Fqrud1ZKsVEKlZKhmrEVtdE,14149
+common/constants/common_constants.py,sha256=TVSYBsjk1uGEtdfR7mTOW6PGd_1k-5IBQRxqIzrpryg,15781
 common/constants/logging_constants.py,sha256=8rxVQv6mZ7DPzPr8Fhwn8r8Nph1B4sAPro3x-Diimj0,16883
 common/exceptions/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 common/exceptions/main_exceptions.py,sha256=_G2i2FRMvvjbNB5IAe4BvING-Q_lSSKd_tIxAxKbbBw,1107
 common/io_operations/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-common/io_operations/image_downloader.py,sha256=ti-MqELgWmJ0__PkLuzVD6h0SmekWwVxBhS5WtZr9Y0,8697
-common/io_operations/io_operations.py,sha256=_L2bwdxleBaAYOVVnNJzPV8IJ6oJWEcMm5NuV2UE_hs,12460
-common/io_operations/request_manager.py,sha256=X6CzhIAVhBX18TNn18d3K4H82yEZkiSJMJE6XvsBO9o,2064
+common/io_operations/image_downloader.py,sha256=DJq4qnmXd2nbr35x8_VrUSJkVT9uAdkv_oMzlaHQpI8,9548
+common/io_operations/io_operations.py,sha256=IPnGwWkyM-6w9lQcODkvs4Aj_Uo4wunZ6YkMqPoDpzk,12419
+common/io_operations/request_factory.py,sha256=km-rcUANfQg_4tltdXa_roMuPq7-UNjD_bgifE8or6E,7818
 common/logging/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 common/logging/logging_setup.py,sha256=2K7hmtu46eaDADtclS_Myp_K4PR07vQaCnku30y0UzY,9755
 common/logging/loguru_setup.py,sha256=2WVGLWZli5jiznXrkaMDxCdLko_lXGiPDUQ-Tzx3_Qs,6015
 common/logging/utils/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 common/logging/utils/color_formatter.py,sha256=VhCwDpzIHsZsKfYd1IlBfx7N-reSe82Lm_ZRu1S1_BI,992
 common/logging/utils/log_rotator.py,sha256=NCq_LITtCvm306_vr7fKwL5UyDwIViYTp9NVGiviL0o,1615
 common/logging/utils/logging_wrappers.py,sha256=GbX_i5tvrNus2AUaI-zbORfzBamIXnR2RiDt3Ek2R3s,1182
 common/logging/utils/loguru_wrappers.py,sha256=lGHLMv12BasGc6viDg8LjTCk0KRZUtRHOeOLPx7558A,2092
 common/logging/utils/padding_formatter.py,sha256=rKkjBT-sxboLaURvZKvWKmE8hI1nWWt5xSEyl1jpKlc,1288
 common/utils/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 common/utils/string_builder.py,sha256=5xQeSv_secDR0Y7_EPhwRaTdcKkgJdXWejGVUwZFEPs,1640
 common/validations/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-common/validations/parameter_validations.py,sha256=NwRb38zvOxNj3d-XoR3Q77jtjqpaTVK7umn5miItsio,6703
-common/validations/reddit_api_validations.py,sha256=30SjcpYtKWEPMWc8_5qUQ2bMOMOw7svoZoAEGNTumM8,6889
+common/validations/parameter_validations.py,sha256=d0uerwxuBIzBdVWDLRqBFay8yC9qeoJDAHKkMX-cn9s,6786
+common/validations/reddit_api_validations.py,sha256=6_y2ozhHA8ClHG2qSmZCHKhz-QJoFXqrODfJDjx3Tbk,6837
 common/validations/url_validations.py,sha256=X0EcuNmFbh1pgauVa5_aHi280qa77mmxGIzdemHhfNM,3517
 core/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-core/main.py,sha256=dAa5KyC5WJb_-4ZCWMs8KMXvZYZH3E-95afhb6kqbzU,4647
+core/main.py,sha256=GJiHHt7aSUPJYMdGswXOOKvEOi7KMF27dH5CaC2HXGY,5429
 core/api/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-core/api/reddit_api.py,sha256=CWZsIE56T4hnFD1U6CTZzkgOQQdXcIoAzjinOpeRePc,6018
+core/api/reddit_api.py,sha256=CnSJo2JQVMpVdaoJvhva9GwzIXK5TaKw6Yl6Modl0CA,5282
 core/helper/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-core/helper/main_helper.py,sha256=qQxatToJO8Zxy805Pk8g6b7dBhPT62UYxmjsIMc5rEU,11320
+core/helper/main_helper.py,sha256=F3c5jYNSa0XyjmQ4iktr-iA46bj_1G0x1LaNZLvH2SU,11253
 core/scraper/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-core/scraper/comment_scraper.py,sha256=wz_lm1KmcNgnLshTdB-1o-itjMJof4z-iGhbjD7VOwg,9431
-core/scraper/scraper_helper.py,sha256=BVpm0WOqTyL06nRMzn7f6YLUJWw6I4Figy8meWBlr_A,10874
-core/scraper/thread_scraper.py,sha256=IxDvnQjE8ofwvwaAHJiXCj0a4FqYyesI1yjElhaJ7lA,9415
-redscrap-0.0.3.dist-info/LICENSE,sha256=mYe2gnkfOYGs2FsK2prdfAcYAiURyyuHydEVbCYc534,1089
-redscrap-0.0.3.dist-info/METADATA,sha256=qnWLmq0M6fWoZgNEygAuQouDpdA0WDLTQRWhVyFBq6s,1151
-redscrap-0.0.3.dist-info/WHEEL,sha256=pkctZYzUS4AYVn6dJ-7367OJZivF2e8RA9b_ZBjif18,92
-redscrap-0.0.3.dist-info/entry_points.txt,sha256=tTH8sFcsimNSFmh7Yzw6B2XRu9QRdlwhNvnk_T2ELbM,44
-redscrap-0.0.3.dist-info/top_level.txt,sha256=DTy351DnTAnYmG3Y2LB89rn_9i2efCXvT-e7vBITiKo,21
-redscrap-0.0.3.dist-info/RECORD,,
+core/scraper/comment_scraper.py,sha256=3E71o1s0fofzLQX7hNF5edBof8ylgEsIffsIGSlgaV8,9320
+core/scraper/scraper_helper.py,sha256=EMcniYrISmeYfE4Wo2loBsiOD2DWClR4K_3QpFUbFM4,11223
+core/scraper/thread_scraper.py,sha256=S6uc9U-AxGJIIMqI2YENTQORCTV_eAGsHHtLJjN0jwE,9547
+redscrap-0.0.4.dist-info/LICENSE,sha256=mYe2gnkfOYGs2FsK2prdfAcYAiURyyuHydEVbCYc534,1089
+redscrap-0.0.4.dist-info/METADATA,sha256=WTDXUFml4bEWnzzZvirqmB56tT80QfMMXlPOfCisURM,1151
+redscrap-0.0.4.dist-info/WHEEL,sha256=pkctZYzUS4AYVn6dJ-7367OJZivF2e8RA9b_ZBjif18,92
+redscrap-0.0.4.dist-info/entry_points.txt,sha256=tTH8sFcsimNSFmh7Yzw6B2XRu9QRdlwhNvnk_T2ELbM,44
+redscrap-0.0.4.dist-info/top_level.txt,sha256=DTy351DnTAnYmG3Y2LB89rn_9i2efCXvT-e7vBITiKo,21
+redscrap-0.0.4.dist-info/RECORD,,
```

