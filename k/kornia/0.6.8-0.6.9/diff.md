# Comparing `tmp/kornia-0.6.8.tar.gz` & `tmp/kornia-0.6.9.tar.gz`

## filetype from file(1)

```diff
@@ -1 +1 @@
-gzip compressed data, was "dist/kornia-0.6.8.tar", last modified: Thu Oct 13 09:38:05 2022, max compression
+gzip compressed data, was "dist/kornia-0.6.9.tar", last modified: Wed Dec 21 21:44:56 2022, max compression
```

## Comparing `kornia-0.6.8.tar` & `kornia-0.6.9.tar`

### file list

```diff
@@ -1,333 +1,348 @@
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-10-13 09:38:05.000000 kornia-0.6.8/
--rw-r--r--   0 runner    (1001) docker     (121)    10173 2022-10-13 09:37:54.000000 kornia-0.6.8/LICENSE
--rw-r--r--   0 runner    (1001) docker     (121)    11168 2022-10-13 09:38:05.000000 kornia-0.6.8/PKG-INFO
--rw-r--r--   0 runner    (1001) docker     (121)     9763 2022-10-13 09:37:54.000000 kornia-0.6.8/README.md
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-10-13 09:38:05.000000 kornia-0.6.8/kornia/
--rw-r--r--   0 runner    (1001) docker     (121)      887 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/__init__.py
--rw-r--r--   0 runner    (1001) docker     (121)      557 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/__tmp__.py
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-10-13 09:38:05.000000 kornia-0.6.8/kornia/augmentation/
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-10-13 09:38:05.000000 kornia-0.6.8/kornia/augmentation/_2d/
--rw-r--r--   0 runner    (1001) docker     (121)      138 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/augmentation/_2d/__init__.py
--rw-r--r--   0 runner    (1001) docker     (121)     2288 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/augmentation/_2d/base.py
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-10-13 09:38:05.000000 kornia-0.6.8/kornia/augmentation/_2d/geometric/
--rw-r--r--   0 runner    (1001) docker     (121)      972 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/augmentation/_2d/geometric/__init__.py
--rw-r--r--   0 runner    (1001) docker     (121)     6490 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/augmentation/_2d/geometric/affine.py
--rw-r--r--   0 runner    (1001) docker     (121)     4915 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/augmentation/_2d/geometric/base.py
--rw-r--r--   0 runner    (1001) docker     (121)     5987 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/augmentation/_2d/geometric/center_crop.py
--rw-r--r--   0 runner    (1001) docker     (121)    12799 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/augmentation/_2d/geometric/crop.py
--rw-r--r--   0 runner    (1001) docker     (121)     3688 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/augmentation/_2d/geometric/elastic_transform.py
--rw-r--r--   0 runner    (1001) docker     (121)     3976 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/augmentation/_2d/geometric/fisheye.py
--rw-r--r--   0 runner    (1001) docker     (121)     3234 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/augmentation/_2d/geometric/horizontal_flip.py
--rw-r--r--   0 runner    (1001) docker     (121)     3112 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/augmentation/_2d/geometric/pad.py
--rw-r--r--   0 runner    (1001) docker     (121)     4341 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/augmentation/_2d/geometric/perspective.py
--rw-r--r--   0 runner    (1001) docker     (121)     4920 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/augmentation/_2d/geometric/resize.py
--rw-r--r--   0 runner    (1001) docker     (121)     6486 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/augmentation/_2d/geometric/resized_crop.py
--rw-r--r--   0 runner    (1001) docker     (121)     4911 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/augmentation/_2d/geometric/rotation.py
--rw-r--r--   0 runner    (1001) docker     (121)     2911 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/augmentation/_2d/geometric/thin_plate_spline.py
--rw-r--r--   0 runner    (1001) docker     (121)     2685 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/augmentation/_2d/geometric/vertical_flip.py
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-10-13 09:38:05.000000 kornia-0.6.8/kornia/augmentation/_2d/intensity/
--rw-r--r--   0 runner    (1001) docker     (121)     1500 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/augmentation/_2d/intensity/__init__.py
--rw-r--r--   0 runner    (1001) docker     (121)     1140 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/augmentation/_2d/intensity/base.py
--rw-r--r--   0 runner    (1001) docker     (121)     2389 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/augmentation/_2d/intensity/box_blur.py
--rw-r--r--   0 runner    (1001) docker     (121)     2014 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/augmentation/_2d/intensity/channel_shuffle.py
--rw-r--r--   0 runner    (1001) docker     (121)     3850 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/augmentation/_2d/intensity/color_jiggle.py
--rw-r--r--   0 runner    (1001) docker     (121)     4715 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/augmentation/_2d/intensity/color_jitter.py
--rw-r--r--   0 runner    (1001) docker     (121)     2243 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/augmentation/_2d/intensity/denormalize.py
--rw-r--r--   0 runner    (1001) docker     (121)     2083 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/augmentation/_2d/intensity/equalize.py
--rw-r--r--   0 runner    (1001) docker     (121)     3715 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/augmentation/_2d/intensity/erasing.py
--rw-r--r--   0 runner    (1001) docker     (121)     2998 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/augmentation/_2d/intensity/gamma.py
--rw-r--r--   0 runner    (1001) docker     (121)     2807 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/augmentation/_2d/intensity/gaussian_blur.py
--rw-r--r--   0 runner    (1001) docker     (121)     2085 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/augmentation/_2d/intensity/gaussian_noise.py
--rw-r--r--   0 runner    (1001) docker     (121)     2574 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/augmentation/_2d/intensity/grayscale.py
--rw-r--r--   0 runner    (1001) docker     (121)     2302 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/augmentation/_2d/intensity/invert.py
--rw-r--r--   0 runner    (1001) docker     (121)     4599 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/augmentation/_2d/intensity/motion_blur.py
--rw-r--r--   0 runner    (1001) docker     (121)     2152 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/augmentation/_2d/intensity/normalize.py
--rw-r--r--   0 runner    (1001) docker     (121)     6509 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/augmentation/_2d/intensity/planckian_jitter.py
--rw-r--r--   0 runner    (1001) docker     (121)     7499 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/augmentation/_2d/intensity/plasma.py
--rw-r--r--   0 runner    (1001) docker     (121)     2650 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/augmentation/_2d/intensity/posterize.py
--rw-r--r--   0 runner    (1001) docker     (121)     4502 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/augmentation/_2d/intensity/random_rgb_shift.py
--rw-r--r--   0 runner    (1001) docker     (121)     2533 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/augmentation/_2d/intensity/sharpness.py
--rw-r--r--   0 runner    (1001) docker     (121)     3143 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/augmentation/_2d/intensity/solarize.py
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-10-13 09:38:05.000000 kornia-0.6.8/kornia/augmentation/_2d/mix/
--rw-r--r--   0 runner    (1001) docker     (121)      269 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/augmentation/_2d/mix/__init__.py
--rw-r--r--   0 runner    (1001) docker     (121)    13723 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/augmentation/_2d/mix/base.py
--rw-r--r--   0 runner    (1001) docker     (121)    11771 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/augmentation/_2d/mix/cutmix.py
--rw-r--r--   0 runner    (1001) docker     (121)     3266 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/augmentation/_2d/mix/jigsaw.py
--rw-r--r--   0 runner    (1001) docker     (121)     9445 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/augmentation/_2d/mix/mixup.py
--rw-r--r--   0 runner    (1001) docker     (121)    10021 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/augmentation/_2d/mix/mosaic.py
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-10-13 09:38:05.000000 kornia-0.6.8/kornia/augmentation/_3d/
--rw-r--r--   0 runner    (1001) docker     (121)       96 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/augmentation/_3d/__init__.py
--rw-r--r--   0 runner    (1001) docker     (121)     2171 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/augmentation/_3d/base.py
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-10-13 09:38:05.000000 kornia-0.6.8/kornia/augmentation/_3d/geometric/
--rw-r--r--   0 runner    (1001) docker     (121)      602 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/augmentation/_3d/geometric/__init__.py
--rw-r--r--   0 runner    (1001) docker     (121)     7331 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/augmentation/_3d/geometric/affine.py
--rw-r--r--   0 runner    (1001) docker     (121)     4835 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/augmentation/_3d/geometric/center_crop.py
--rw-r--r--   0 runner    (1001) docker     (121)     7229 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/augmentation/_3d/geometric/crop.py
--rw-r--r--   0 runner    (1001) docker     (121)     3348 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/augmentation/_3d/geometric/depthical_flip.py
--rw-r--r--   0 runner    (1001) docker     (121)     2807 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/augmentation/_3d/geometric/horizontal_flip.py
--rw-r--r--   0 runner    (1001) docker     (121)     3942 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/augmentation/_3d/geometric/perspective.py
--rw-r--r--   0 runner    (1001) docker     (121)     5435 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/augmentation/_3d/geometric/rotation.py
--rw-r--r--   0 runner    (1001) docker     (121)     3332 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/augmentation/_3d/geometric/vertical_flip.py
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-10-13 09:38:05.000000 kornia-0.6.8/kornia/augmentation/_3d/intensity/
--rw-r--r--   0 runner    (1001) docker     (121)      149 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/augmentation/_3d/intensity/__init__.py
--rw-r--r--   0 runner    (1001) docker     (121)     2574 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/augmentation/_3d/intensity/equalize.py
--rw-r--r--   0 runner    (1001) docker     (121)     5807 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/augmentation/_3d/intensity/motion_blur.py
--rw-r--r--   0 runner    (1001) docker     (121)     3278 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/augmentation/__init__.py
--rw-r--r--   0 runner    (1001) docker     (121)    11130 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/augmentation/base.py
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-10-13 09:38:05.000000 kornia-0.6.8/kornia/augmentation/container/
--rw-r--r--   0 runner    (1001) docker     (121)      382 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/augmentation/container/__init__.py
--rw-r--r--   0 runner    (1001) docker     (121)    19910 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/augmentation/container/augment.py
--rw-r--r--   0 runner    (1001) docker     (121)     6864 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/augmentation/container/base.py
--rw-r--r--   0 runner    (1001) docker     (121)     4195 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/augmentation/container/dispatcher.py
--rw-r--r--   0 runner    (1001) docker     (121)    19203 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/augmentation/container/image.py
--rw-r--r--   0 runner    (1001) docker     (121)    19460 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/augmentation/container/patch.py
--rw-r--r--   0 runner    (1001) docker     (121)    24335 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/augmentation/container/utils.py
--rw-r--r--   0 runner    (1001) docker     (121)    13181 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/augmentation/container/video.py
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-10-13 09:38:05.000000 kornia-0.6.8/kornia/augmentation/random_generator/
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-10-13 09:38:05.000000 kornia-0.6.8/kornia/augmentation/random_generator/_2d/
--rw-r--r--   0 runner    (1001) docker     (121)     1106 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/augmentation/random_generator/_2d/__init__.py
--rw-r--r--   0 runner    (1001) docker     (121)    15172 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/augmentation/random_generator/_2d/affine.py
--rw-r--r--   0 runner    (1001) docker     (121)     4704 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/augmentation/random_generator/_2d/color_jiggle.py
--rw-r--r--   0 runner    (1001) docker     (121)     4922 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/augmentation/random_generator/_2d/color_jitter.py
--rw-r--r--   0 runner    (1001) docker     (121)    23431 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/augmentation/random_generator/_2d/crop.py
--rw-r--r--   0 runner    (1001) docker     (121)    11941 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/augmentation/random_generator/_2d/cutmix.py
--rw-r--r--   0 runner    (1001) docker     (121)     5615 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/augmentation/random_generator/_2d/func.py
--rw-r--r--   0 runner    (1001) docker     (121)     2128 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/augmentation/random_generator/_2d/jigsaw.py
--rw-r--r--   0 runner    (1001) docker     (121)     5504 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/augmentation/random_generator/_2d/mixup.py
--rw-r--r--   0 runner    (1001) docker     (121)     4570 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/augmentation/random_generator/_2d/mosaic.py
--rw-r--r--   0 runner    (1001) docker     (121)     8063 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/augmentation/random_generator/_2d/motion_blur.py
--rw-r--r--   0 runner    (1001) docker     (121)     7065 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/augmentation/random_generator/_2d/perspective.py
--rw-r--r--   0 runner    (1001) docker     (121)     3699 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/augmentation/random_generator/_2d/plain_uniform.py
--rw-r--r--   0 runner    (1001) docker     (121)     1159 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/augmentation/random_generator/_2d/planckian_jitter.py
--rw-r--r--   0 runner    (1001) docker     (121)     3845 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/augmentation/random_generator/_2d/posterize.py
--rw-r--r--   0 runner    (1001) docker     (121)     2941 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/augmentation/random_generator/_2d/probability.py
--rw-r--r--   0 runner    (1001) docker     (121)    11632 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/augmentation/random_generator/_2d/rectangle_earase.py
--rw-r--r--   0 runner    (1001) docker     (121)     3816 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/augmentation/random_generator/_2d/resize.py
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-10-13 09:38:05.000000 kornia-0.6.8/kornia/augmentation/random_generator/_3d/
--rw-r--r--   0 runner    (1001) docker     (121)      320 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/augmentation/random_generator/_3d/__init__.py
--rw-r--r--   0 runner    (1001) docker     (121)    18499 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/augmentation/random_generator/_3d/affine.py
--rw-r--r--   0 runner    (1001) docker     (121)    15489 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/augmentation/random_generator/_3d/crop.py
--rw-r--r--   0 runner    (1001) docker     (121)     8879 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/augmentation/random_generator/_3d/motion_blur.py
--rw-r--r--   0 runner    (1001) docker     (121)     6810 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/augmentation/random_generator/_3d/perspective.py
--rw-r--r--   0 runner    (1001) docker     (121)     5092 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/augmentation/random_generator/_3d/rotation.py
--rw-r--r--   0 runner    (1001) docker     (121)      152 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/augmentation/random_generator/__init__.py
--rw-r--r--   0 runner    (1001) docker     (121)     3697 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/augmentation/random_generator/base.py
--rw-r--r--   0 runner    (1001) docker     (121)      395 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/augmentation/random_generator/utils.py
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-10-13 09:38:05.000000 kornia-0.6.8/kornia/augmentation/utils/
--rw-r--r--   0 runner    (1001) docker     (121)     1204 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/augmentation/utils/__init__.py
--rw-r--r--   0 runner    (1001) docker     (121)    10379 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/augmentation/utils/helpers.py
--rw-r--r--   0 runner    (1001) docker     (121)     7548 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/augmentation/utils/param_validation.py
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-10-13 09:38:05.000000 kornia-0.6.8/kornia/color/
--rw-r--r--   0 runner    (1001) docker     (121)     2282 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/color/__init__.py
--rw-r--r--   0 runner    (1001) docker     (121)     5452 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/color/gray.py
--rw-r--r--   0 runner    (1001) docker     (121)     7036 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/color/hls.py
--rw-r--r--   0 runner    (1001) docker     (121)     4392 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/color/hsv.py
--rw-r--r--   0 runner    (1001) docker     (121)     5625 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/color/lab.py
--rw-r--r--   0 runner    (1001) docker     (121)     5428 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/color/luv.py
--rw-r--r--   0 runner    (1001) docker     (121)     9624 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/color/raw.py
--rw-r--r--   0 runner    (1001) docker     (121)    12923 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/color/rgb.py
--rw-r--r--   0 runner    (1001) docker     (121)     3391 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/color/xyz.py
--rw-r--r--   0 runner    (1001) docker     (121)     4016 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/color/ycbcr.py
--rw-r--r--   0 runner    (1001) docker     (121)    12970 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/color/yuv.py
--rw-r--r--   0 runner    (1001) docker     (121)     2978 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/constants.py
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-10-13 09:38:05.000000 kornia-0.6.8/kornia/contrib/
--rw-r--r--   0 runner    (1001) docker     (121)     1071 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/contrib/__init__.py
--rw-r--r--   0 runner    (1001) docker     (121)      836 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/contrib/classification.py
--rw-r--r--   0 runner    (1001) docker     (121)     2007 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/contrib/connected_components.py
--rw-r--r--   0 runner    (1001) docker     (121)     8698 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/contrib/diamond_square.py
--rw-r--r--   0 runner    (1001) docker     (121)     3512 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/contrib/distance_transform.py
--rw-r--r--   0 runner    (1001) docker     (121)     1206 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/contrib/edge_detection.py
--rw-r--r--   0 runner    (1001) docker     (121)    13744 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/contrib/extract_patches.py
--rw-r--r--   0 runner    (1001) docker     (121)    15124 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/contrib/face_detection.py
--rw-r--r--   0 runner    (1001) docker     (121)     2891 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/contrib/histogram_matching.py
--rw-r--r--   0 runner    (1001) docker     (121)     6250 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/contrib/image_stitching.py
--rw-r--r--   0 runner    (1001) docker     (121)      812 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/contrib/lambda_module.py
--rw-r--r--   0 runner    (1001) docker     (121)     8179 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/contrib/vit.py
--rw-r--r--   0 runner    (1001) docker     (121)     9743 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/contrib/vit_mobile.py
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-10-13 09:38:05.000000 kornia-0.6.8/kornia/core/
--rw-r--r--   0 runner    (1001) docker     (121)      233 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/core/__init__.py
--rw-r--r--   0 runner    (1001) docker     (121)      396 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/core/_backend.py
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-10-13 09:38:05.000000 kornia-0.6.8/kornia/enhance/
--rw-r--r--   0 runner    (1001) docker     (121)     2053 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/enhance/__init__.py
--rw-r--r--   0 runner    (1001) docker     (121)    52178 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/enhance/adjust.py
--rw-r--r--   0 runner    (1001) docker     (121)     3097 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/enhance/core.py
--rw-r--r--   0 runner    (1001) docker     (121)    15520 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/enhance/equalization.py
--rw-r--r--   0 runner    (1001) docker     (121)     9847 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/enhance/histogram.py
--rw-r--r--   0 runner    (1001) docker     (121)     9734 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/enhance/normalize.py
--rw-r--r--   0 runner    (1001) docker     (121)      613 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/enhance/shift_rgb.py
--rw-r--r--   0 runner    (1001) docker     (121)    13381 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/enhance/zca.py
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-10-13 09:38:05.000000 kornia-0.6.8/kornia/feature/
--rw-r--r--   0 runner    (1001) docker     (121)     3348 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/feature/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-10-13 09:38:05.000000 kornia-0.6.8/kornia/feature/adalam/
--rw-r--r--   0 runner    (1001) docker     (121)       74 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/feature/adalam/__init__.py
--rw-r--r--   0 runner    (1001) docker     (121)    13749 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/feature/adalam/adalam.py
--rw-r--r--   0 runner    (1001) docker     (121)    16299 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/feature/adalam/core.py
--rw-r--r--   0 runner    (1001) docker     (121)     6814 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/feature/adalam/ransac.py
--rw-r--r--   0 runner    (1001) docker     (121)     3922 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/feature/adalam/utils.py
--rw-r--r--   0 runner    (1001) docker     (121)    10299 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/feature/affine_shape.py
--rw-r--r--   0 runner    (1001) docker     (121)    11372 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/feature/defmo.py
--rw-r--r--   0 runner    (1001) docker     (121)     7648 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/feature/hardnet.py
--rw-r--r--   0 runner    (1001) docker     (121)     7435 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/feature/hynet.py
--rw-r--r--   0 runner    (1001) docker     (121)    14487 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/feature/integrated.py
--rw-r--r--   0 runner    (1001) docker     (121)    12211 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/feature/keynet.py
--rw-r--r--   0 runner    (1001) docker     (121)    20033 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/feature/laf.py
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-10-13 09:38:05.000000 kornia-0.6.8/kornia/feature/loftr/
--rw-r--r--   0 runner    (1001) docker     (121)       25 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/feature/loftr/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-10-13 09:38:05.000000 kornia-0.6.8/kornia/feature/loftr/backbone/
--rw-r--r--   0 runner    (1001) docker     (121)      429 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/feature/loftr/backbone/__init__.py
--rw-r--r--   0 runner    (1001) docker     (121)     6858 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/feature/loftr/backbone/resnet_fpn.py
--rw-r--r--   0 runner    (1001) docker     (121)     7533 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/feature/loftr/loftr.py
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-10-13 09:38:05.000000 kornia-0.6.8/kornia/feature/loftr/loftr_module/
--rw-r--r--   0 runner    (1001) docker     (121)       93 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/feature/loftr/loftr_module/__init__.py
--rw-r--r--   0 runner    (1001) docker     (121)     2922 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/feature/loftr/loftr_module/fine_preprocess.py
--rw-r--r--   0 runner    (1001) docker     (121)     2991 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/feature/loftr/loftr_module/linear_attention.py
--rw-r--r--   0 runner    (1001) docker     (121)     3757 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/feature/loftr/loftr_module/transformer.py
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-10-13 09:38:05.000000 kornia-0.6.8/kornia/feature/loftr/utils/
--rw-r--r--   0 runner    (1001) docker     (121)        0 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/feature/loftr/utils/__init__.py
--rw-r--r--   0 runner    (1001) docker     (121)    10506 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/feature/loftr/utils/coarse_matching.py
--rw-r--r--   0 runner    (1001) docker     (121)     2893 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/feature/loftr/utils/fine_matching.py
--rw-r--r--   0 runner    (1001) docker     (121)     2084 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/feature/loftr/utils/geometry.py
--rw-r--r--   0 runner    (1001) docker     (121)     2962 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/feature/loftr/utils/position_encoding.py
--rw-r--r--   0 runner    (1001) docker     (121)     5589 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/feature/loftr/utils/supervision.py
--rw-r--r--   0 runner    (1001) docker     (121)    16147 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/feature/matching.py
--rw-r--r--   0 runner    (1001) docker     (121)    23902 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/feature/mkd.py
--rw-r--r--   0 runner    (1001) docker     (121)     9602 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/feature/orientation.py
--rw-r--r--   0 runner    (1001) docker     (121)    12250 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/feature/responses.py
--rw-r--r--   0 runner    (1001) docker     (121)     9989 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/feature/scale_space_detector.py
--rw-r--r--   0 runner    (1001) docker     (121)    11215 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/feature/siftdesc.py
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-10-13 09:38:05.000000 kornia-0.6.8/kornia/feature/sold2/
--rw-r--r--   0 runner    (1001) docker     (121)       68 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/feature/sold2/__init__.py
--rw-r--r--   0 runner    (1001) docker     (121)    13903 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/feature/sold2/backbones.py
--rw-r--r--   0 runner    (1001) docker     (121)    15465 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/feature/sold2/sold2.py
--rw-r--r--   0 runner    (1001) docker     (121)    26477 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/feature/sold2/sold2_detector.py
--rw-r--r--   0 runner    (1001) docker     (121)     2760 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/feature/sosnet.py
--rw-r--r--   0 runner    (1001) docker     (121)     2227 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/feature/tfeat.py
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-10-13 09:38:05.000000 kornia-0.6.8/kornia/filters/
--rw-r--r--   0 runner    (1001) docker     (121)     2242 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/filters/__init__.py
--rw-r--r--   0 runner    (1001) docker     (121)     3548 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/filters/blur.py
--rw-r--r--   0 runner    (1001) docker     (121)    10046 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/filters/blur_pool.py
--rw-r--r--   0 runner    (1001) docker     (121)     8434 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/filters/canny.py
--rw-r--r--   0 runner    (1001) docker     (121)    10195 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/filters/dexined.py
--rw-r--r--   0 runner    (1001) docker     (121)    11037 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/filters/filter.py
--rw-r--r--   0 runner    (1001) docker     (121)     3816 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/filters/gaussian.py
--rw-r--r--   0 runner    (1001) docker     (121)    25227 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/filters/kernels.py
--rw-r--r--   0 runner    (1001) docker     (121)     8457 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/filters/kernels_geometry.py
--rw-r--r--   0 runner    (1001) docker     (121)     3035 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/filters/laplacian.py
--rw-r--r--   0 runner    (1001) docker     (121)     2568 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/filters/median.py
--rw-r--r--   0 runner    (1001) docker     (121)     8302 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/filters/motion.py
--rw-r--r--   0 runner    (1001) docker     (121)     9309 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/filters/sobel.py
--rw-r--r--   0 runner    (1001) docker     (121)     2553 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/filters/unsharp.py
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-10-13 09:38:05.000000 kornia-0.6.8/kornia/geometry/
--rw-r--r--   0 runner    (1001) docker     (121)      278 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/geometry/__init__.py
--rw-r--r--   0 runner    (1001) docker     (121)    23306 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/geometry/bbox.py
--rw-r--r--   0 runner    (1001) docker     (121)    41542 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/geometry/boxes.py
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-10-13 09:38:05.000000 kornia-0.6.8/kornia/geometry/calibration/
--rw-r--r--   0 runner    (1001) docker     (121)      246 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/geometry/calibration/__init__.py
--rw-r--r--   0 runner    (1001) docker     (121)     5976 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/geometry/calibration/distort.py
--rw-r--r--   0 runner    (1001) docker     (121)    12368 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/geometry/calibration/pnp.py
--rw-r--r--   0 runner    (1001) docker     (121)     7525 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/geometry/calibration/undistort.py
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-10-13 09:38:05.000000 kornia-0.6.8/kornia/geometry/camera/
--rw-r--r--   0 runner    (1001) docker     (121)      257 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/geometry/camera/__init__.py
--rw-r--r--   0 runner    (1001) docker     (121)     5184 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/geometry/camera/perspective.py
--rw-r--r--   0 runner    (1001) docker     (121)    26981 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/geometry/camera/pinhole.py
--rw-r--r--   0 runner    (1001) docker     (121)    11840 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/geometry/camera/stereo.py
--rw-r--r--   0 runner    (1001) docker     (121)    52456 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/geometry/conversions.py
--rw-r--r--   0 runner    (1001) docker     (121)    16770 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/geometry/depth.py
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-10-13 09:38:05.000000 kornia-0.6.8/kornia/geometry/epipolar/
--rw-r--r--   0 runner    (1001) docker     (121)     1839 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/geometry/epipolar/__init__.py
--rw-r--r--   0 runner    (1001) docker     (121)     7355 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/geometry/epipolar/_metrics.py
--rw-r--r--   0 runner    (1001) docker     (121)    11190 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/geometry/epipolar/essential.py
--rw-r--r--   0 runner    (1001) docker     (121)    11544 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/geometry/epipolar/fundamental.py
--rw-r--r--   0 runner    (1001) docker     (121)      839 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/geometry/epipolar/numeric.py
--rw-r--r--   0 runner    (1001) docker     (121)     7011 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/geometry/epipolar/projection.py
--rw-r--r--   0 runner    (1001) docker     (121)     1618 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/geometry/epipolar/scene.py
--rw-r--r--   0 runner    (1001) docker     (121)     2904 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/geometry/epipolar/triangulation.py
--rw-r--r--   0 runner    (1001) docker     (121)    14513 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/geometry/homography.py
--rw-r--r--   0 runner    (1001) docker     (121)     9393 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/geometry/linalg.py
--rw-r--r--   0 runner    (1001) docker     (121)     6148 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/geometry/line.py
--rw-r--r--   0 runner    (1001) docker     (121)    12572 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/geometry/quaternion.py
--rw-r--r--   0 runner    (1001) docker     (121)    10571 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/geometry/ransac.py
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-10-13 09:38:05.000000 kornia-0.6.8/kornia/geometry/subpix/
--rw-r--r--   0 runner    (1001) docker     (121)      769 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/geometry/subpix/__init__.py
--rw-r--r--   0 runner    (1001) docker     (121)     5871 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/geometry/subpix/dsnt.py
--rw-r--r--   0 runner    (1001) docker     (121)     6540 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/geometry/subpix/nms.py
--rw-r--r--   0 runner    (1001) docker     (121)    24871 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/geometry/subpix/spatial_soft_argmax.py
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-10-13 09:38:05.000000 kornia-0.6.8/kornia/geometry/transform/
--rw-r--r--   0 runner    (1001) docker     (121)      266 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/geometry/transform/__init__.py
--rw-r--r--   0 runner    (1001) docker     (121)    37356 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/geometry/transform/affwarp.py
--rw-r--r--   0 runner    (1001) docker     (121)    13586 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/geometry/transform/crop2d.py
--rw-r--r--   0 runner    (1001) docker     (121)    13750 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/geometry/transform/crop3d.py
--rw-r--r--   0 runner    (1001) docker     (121)     4134 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/geometry/transform/elastic_transform.py
--rw-r--r--   0 runner    (1001) docker     (121)     3656 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/geometry/transform/flips.py
--rw-r--r--   0 runner    (1001) docker     (121)     4873 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/geometry/transform/homography_warper.py
--rw-r--r--   0 runner    (1001) docker     (121)    10235 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/geometry/transform/image_registrator.py
--rw-r--r--   0 runner    (1001) docker     (121)    50782 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/geometry/transform/imgwarp.py
--rw-r--r--   0 runner    (1001) docker     (121)    14561 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/geometry/transform/pyramid.py
--rw-r--r--   0 runner    (1001) docker     (121)    10879 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/geometry/transform/thin_plate_spline.py
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-10-13 09:38:05.000000 kornia-0.6.8/kornia/grad_estimator/
--rw-r--r--   0 runner    (1001) docker     (121)       55 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/grad_estimator/__init__.py
--rw-r--r--   0 runner    (1001) docker     (121)     4997 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/grad_estimator/ste.py
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-10-13 09:38:05.000000 kornia-0.6.8/kornia/io/
--rw-r--r--   0 runner    (1001) docker     (121)       42 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/io/__init__.py
--rw-r--r--   0 runner    (1001) docker     (121)     4229 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/io/io.py
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-10-13 09:38:05.000000 kornia-0.6.8/kornia/losses/
--rw-r--r--   0 runner    (1001) docker     (121)     1264 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/losses/__init__.py
--rw-r--r--   0 runner    (1001) docker     (121)     3709 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/losses/depth_smooth.py
--rw-r--r--   0 runner    (1001) docker     (121)     3952 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/losses/dice.py
--rw-r--r--   0 runner    (1001) docker     (121)     2560 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/losses/divergence.py
--rw-r--r--   0 runner    (1001) docker     (121)    10219 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/losses/focal.py
--rw-r--r--   0 runner    (1001) docker     (121)    10244 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/losses/hausdorff.py
--rw-r--r--   0 runner    (1001) docker     (121)     4646 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/losses/lovasz_hinge.py
--rw-r--r--   0 runner    (1001) docker     (121)     5155 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/losses/lovasz_softmax.py
--rw-r--r--   0 runner    (1001) docker     (121)     6536 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/losses/ms_ssim.py
--rw-r--r--   0 runner    (1001) docker     (121)     1748 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/losses/psnr.py
--rw-r--r--   0 runner    (1001) docker     (121)     3791 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/losses/ssim.py
--rw-r--r--   0 runner    (1001) docker     (121)     2572 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/losses/total_variation.py
--rw-r--r--   0 runner    (1001) docker     (121)     4715 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/losses/tversky.py
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-10-13 09:38:05.000000 kornia-0.6.8/kornia/metrics/
--rw-r--r--   0 runner    (1001) docker     (121)      453 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/metrics/__init__.py
--rw-r--r--   0 runner    (1001) docker     (121)      857 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/metrics/accuracy.py
--rw-r--r--   0 runner    (1001) docker     (121)      726 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/metrics/average_meter.py
--rw-r--r--   0 runner    (1001) docker     (121)     3106 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/metrics/confusion_matrix.py
--rw-r--r--   0 runner    (1001) docker     (121)     8389 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/metrics/mean_average_precision.py
--rw-r--r--   0 runner    (1001) docker     (121)     4697 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/metrics/mean_iou.py
--rw-r--r--   0 runner    (1001) docker     (121)     1634 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/metrics/psnr.py
--rw-r--r--   0 runner    (1001) docker     (121)     6013 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/metrics/ssim.py
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-10-13 09:38:05.000000 kornia-0.6.8/kornia/morphology/
--rw-r--r--   0 runner    (1001) docker     (121)       26 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/morphology/__init__.py
--rw-r--r--   0 runner    (1001) docker     (121)    22676 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/morphology/morphology.py
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-10-13 09:38:05.000000 kornia-0.6.8/kornia/testing/
--rw-r--r--   0 runner    (1001) docker     (121)    12355 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/testing/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-10-13 09:38:05.000000 kornia-0.6.8/kornia/tracking/
--rw-r--r--   0 runner    (1001) docker     (121)       79 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/tracking/__init__.py
--rw-r--r--   0 runner    (1001) docker     (121)     6044 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/tracking/planar_tracker.py
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-10-13 09:38:05.000000 kornia-0.6.8/kornia/utils/
--rw-r--r--   0 runner    (1001) docker     (121)      967 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/utils/__init__.py
--rw-r--r--   0 runner    (1001) docker     (121)     1982 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/utils/_compat.py
--rw-r--r--   0 runner    (1001) docker     (121)    12277 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/utils/draw.py
--rw-r--r--   0 runner    (1001) docker     (121)     4072 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/utils/grid.py
--rw-r--r--   0 runner    (1001) docker     (121)     8931 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/utils/helpers.py
--rw-r--r--   0 runner    (1001) docker     (121)     8605 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/utils/image.py
--rw-r--r--   0 runner    (1001) docker     (121)     1792 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/utils/memory.py
--rw-r--r--   0 runner    (1001) docker     (121)     1393 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/utils/misc.py
--rw-r--r--   0 runner    (1001) docker     (121)     1725 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/utils/one_hot.py
--rw-r--r--   0 runner    (1001) docker     (121)     3245 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/utils/pointcloud_io.py
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-10-13 09:38:05.000000 kornia-0.6.8/kornia/x/
--rw-r--r--   0 runner    (1001) docker     (121)      236 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/x/__init__.py
--rw-r--r--   0 runner    (1001) docker     (121)     3202 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/x/callbacks.py
--rw-r--r--   0 runner    (1001) docker     (121)     7697 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/x/trainer.py
--rw-r--r--   0 runner    (1001) docker     (121)     4537 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/x/trainers.py
--rw-r--r--   0 runner    (1001) docker     (121)     2977 2022-10-13 09:37:55.000000 kornia-0.6.8/kornia/x/utils.py
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-10-13 09:38:05.000000 kornia-0.6.8/kornia.egg-info/
--rw-r--r--   0 runner    (1001) docker     (121)    11168 2022-10-13 09:38:05.000000 kornia-0.6.8/kornia.egg-info/PKG-INFO
--rw-r--r--   0 runner    (1001) docker     (121)    10192 2022-10-13 09:38:05.000000 kornia-0.6.8/kornia.egg-info/SOURCES.txt
--rw-r--r--   0 runner    (1001) docker     (121)        1 2022-10-13 09:38:05.000000 kornia-0.6.8/kornia.egg-info/dependency_links.txt
--rw-r--r--   0 runner    (1001) docker     (121)      533 2022-10-13 09:38:05.000000 kornia-0.6.8/kornia.egg-info/requires.txt
--rw-r--r--   0 runner    (1001) docker     (121)        7 2022-10-13 09:38:05.000000 kornia-0.6.8/kornia.egg-info/top_level.txt
--rw-r--r--   0 runner    (1001) docker     (121)        1 2022-10-13 09:38:05.000000 kornia-0.6.8/kornia.egg-info/zip-safe
--rw-r--r--   0 runner    (1001) docker     (121)      315 2022-10-13 09:37:55.000000 kornia-0.6.8/pyproject.toml
--rw-r--r--   0 runner    (1001) docker     (121)     2901 2022-10-13 09:38:05.000000 kornia-0.6.8/setup.cfg
--rw-r--r--   0 runner    (1001) docker     (121)      316 2022-10-13 09:37:55.000000 kornia-0.6.8/setup.py
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-10-13 09:38:05.000000 kornia-0.6.8/test/
--rw-r--r--   0 runner    (1001) docker     (121)    27323 2022-10-13 09:37:55.000000 kornia-0.6.8/test/test_contrib.py
--rw-r--r--   0 runner    (1001) docker     (121)    10407 2022-10-13 09:37:55.000000 kornia-0.6.8/test/test_metrics.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2022-12-21 21:44:56.000000 kornia-0.6.9/
+-rw-r--r--   0 runner    (1001) docker     (122)    10173 2022-12-21 21:44:52.000000 kornia-0.6.9/LICENSE
+-rw-r--r--   0 runner    (1001) docker     (122)    11168 2022-12-21 21:44:56.000000 kornia-0.6.9/PKG-INFO
+-rw-r--r--   0 runner    (1001) docker     (122)     9763 2022-12-21 21:44:52.000000 kornia-0.6.9/README.md
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2022-12-21 21:44:56.000000 kornia-0.6.9/kornia/
+-rw-r--r--   0 runner    (1001) docker     (122)      887 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2022-12-21 21:44:56.000000 kornia-0.6.9/kornia/augmentation/
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2022-12-21 21:44:56.000000 kornia-0.6.9/kornia/augmentation/_2d/
+-rw-r--r--   0 runner    (1001) docker     (122)      138 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/augmentation/_2d/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2288 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/augmentation/_2d/base.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2022-12-21 21:44:56.000000 kornia-0.6.9/kornia/augmentation/_2d/geometric/
+-rw-r--r--   0 runner    (1001) docker     (122)      972 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/augmentation/_2d/geometric/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6490 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/augmentation/_2d/geometric/affine.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4924 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/augmentation/_2d/geometric/base.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5987 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/augmentation/_2d/geometric/center_crop.py
+-rw-r--r--   0 runner    (1001) docker     (122)    13339 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/augmentation/_2d/geometric/crop.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3977 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/augmentation/_2d/geometric/elastic_transform.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3892 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/augmentation/_2d/geometric/fisheye.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3235 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/augmentation/_2d/geometric/horizontal_flip.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3112 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/augmentation/_2d/geometric/pad.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4375 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/augmentation/_2d/geometric/perspective.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5173 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/augmentation/_2d/geometric/resize.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6730 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/augmentation/_2d/geometric/resized_crop.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4672 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/augmentation/_2d/geometric/rotation.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2911 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/augmentation/_2d/geometric/thin_plate_spline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2686 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/augmentation/_2d/geometric/vertical_flip.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2022-12-21 21:44:56.000000 kornia-0.6.9/kornia/augmentation/_2d/intensity/
+-rw-r--r--   0 runner    (1001) docker     (122)     1778 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/augmentation/_2d/intensity/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1140 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/augmentation/_2d/intensity/base.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2389 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/augmentation/_2d/intensity/box_blur.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3124 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/augmentation/_2d/intensity/brightness.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2014 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/augmentation/_2d/intensity/channel_shuffle.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3796 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/augmentation/_2d/intensity/color_jiggle.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4661 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/augmentation/_2d/intensity/color_jitter.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3061 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/augmentation/_2d/intensity/contrast.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2243 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/augmentation/_2d/intensity/denormalize.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2083 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/augmentation/_2d/intensity/equalize.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3668 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/augmentation/_2d/intensity/erasing.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2952 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/augmentation/_2d/intensity/gamma.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2807 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/augmentation/_2d/intensity/gaussian_blur.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2560 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/augmentation/_2d/intensity/gaussian_noise.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2574 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/augmentation/_2d/intensity/grayscale.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2887 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/augmentation/_2d/intensity/hue.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2302 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/augmentation/_2d/intensity/invert.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5372 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/augmentation/_2d/intensity/motion_blur.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2152 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/augmentation/_2d/intensity/normalize.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6725 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/augmentation/_2d/intensity/planckian_jitter.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7316 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/augmentation/_2d/intensity/plasma.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2620 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/augmentation/_2d/intensity/posterize.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4418 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/augmentation/_2d/intensity/random_rgb_shift.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2968 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/augmentation/_2d/intensity/saturation.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2478 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/augmentation/_2d/intensity/sharpness.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3067 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/augmentation/_2d/intensity/solarize.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2022-12-21 21:44:56.000000 kornia-0.6.9/kornia/augmentation/_2d/mix/
+-rw-r--r--   0 runner    (1001) docker     (122)      242 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/augmentation/_2d/mix/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     9014 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/augmentation/_2d/mix/base.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5984 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/augmentation/_2d/mix/cutmix.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3234 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/augmentation/_2d/mix/jigsaw.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5235 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/augmentation/_2d/mix/mixup.py
+-rw-r--r--   0 runner    (1001) docker     (122)    10043 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/augmentation/_2d/mix/mosaic.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2022-12-21 21:44:56.000000 kornia-0.6.9/kornia/augmentation/_3d/
+-rw-r--r--   0 runner    (1001) docker     (122)       96 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/augmentation/_3d/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2171 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/augmentation/_3d/base.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2022-12-21 21:44:56.000000 kornia-0.6.9/kornia/augmentation/_3d/geometric/
+-rw-r--r--   0 runner    (1001) docker     (122)      602 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/augmentation/_3d/geometric/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7401 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/augmentation/_3d/geometric/affine.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4835 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/augmentation/_3d/geometric/center_crop.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7223 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/augmentation/_3d/geometric/crop.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3349 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/augmentation/_3d/geometric/depthical_flip.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2808 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/augmentation/_3d/geometric/horizontal_flip.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4007 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/augmentation/_3d/geometric/perspective.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5503 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/augmentation/_3d/geometric/rotation.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3333 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/augmentation/_3d/geometric/vertical_flip.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2022-12-21 21:44:56.000000 kornia-0.6.9/kornia/augmentation/_3d/intensity/
+-rw-r--r--   0 runner    (1001) docker     (122)      149 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/augmentation/_3d/intensity/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2574 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/augmentation/_3d/intensity/equalize.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5763 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/augmentation/_3d/intensity/motion_blur.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3413 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/augmentation/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    11554 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/augmentation/base.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2022-12-21 21:44:56.000000 kornia-0.6.9/kornia/augmentation/container/
+-rw-r--r--   0 runner    (1001) docker     (122)      382 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/augmentation/container/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    19658 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/augmentation/container/augment.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6424 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/augmentation/container/base.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4195 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/augmentation/container/dispatcher.py
+-rw-r--r--   0 runner    (1001) docker     (122)    18586 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/augmentation/container/image.py
+-rw-r--r--   0 runner    (1001) docker     (122)    18999 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/augmentation/container/patch.py
+-rw-r--r--   0 runner    (1001) docker     (122)    26301 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/augmentation/container/utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)    12955 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/augmentation/container/video.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2022-12-21 21:44:56.000000 kornia-0.6.9/kornia/augmentation/random_generator/
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2022-12-21 21:44:56.000000 kornia-0.6.9/kornia/augmentation/random_generator/_2d/
+-rw-r--r--   0 runner    (1001) docker     (122)     1046 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/augmentation/random_generator/_2d/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     9345 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/augmentation/random_generator/_2d/affine.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4695 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/augmentation/random_generator/_2d/color_jiggle.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4913 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/augmentation/random_generator/_2d/color_jitter.py
+-rw-r--r--   0 runner    (1001) docker     (122)    13710 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/augmentation/random_generator/_2d/crop.py
+-rw-r--r--   0 runner    (1001) docker     (122)    11941 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/augmentation/random_generator/_2d/cutmix.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2128 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/augmentation/random_generator/_2d/jigsaw.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5504 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/augmentation/random_generator/_2d/mixup.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4570 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/augmentation/random_generator/_2d/mosaic.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4610 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/augmentation/random_generator/_2d/motion_blur.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4145 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/augmentation/random_generator/_2d/perspective.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3708 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/augmentation/random_generator/_2d/plain_uniform.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1158 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/augmentation/random_generator/_2d/planckian_jitter.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2368 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/augmentation/random_generator/_2d/posterize.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2917 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/augmentation/random_generator/_2d/probability.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6013 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/augmentation/random_generator/_2d/rectangle_earase.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3663 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/augmentation/random_generator/_2d/resize.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2022-12-21 21:44:56.000000 kornia-0.6.9/kornia/augmentation/random_generator/_3d/
+-rw-r--r--   0 runner    (1001) docker     (122)      320 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/augmentation/random_generator/_3d/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    11810 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/augmentation/random_generator/_3d/affine.py
+-rw-r--r--   0 runner    (1001) docker     (122)     9803 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/augmentation/random_generator/_3d/crop.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5173 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/augmentation/random_generator/_3d/motion_blur.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3638 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/augmentation/random_generator/_3d/perspective.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3204 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/augmentation/random_generator/_3d/rotation.py
+-rw-r--r--   0 runner    (1001) docker     (122)      152 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/augmentation/random_generator/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3728 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/augmentation/random_generator/base.py
+-rw-r--r--   0 runner    (1001) docker     (122)      395 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/augmentation/random_generator/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2022-12-21 21:44:56.000000 kornia-0.6.9/kornia/augmentation/utils/
+-rw-r--r--   0 runner    (1001) docker     (122)     1204 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/augmentation/utils/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    10609 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/augmentation/utils/helpers.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7403 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/augmentation/utils/param_validation.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2022-12-21 21:44:56.000000 kornia-0.6.9/kornia/color/
+-rw-r--r--   0 runner    (1001) docker     (122)     2373 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/color/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5452 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/color/gray.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5439 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/color/hls.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4392 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/color/hsv.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5625 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/color/lab.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5428 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/color/luv.py
+-rw-r--r--   0 runner    (1001) docker     (122)     9624 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/color/raw.py
+-rw-r--r--   0 runner    (1001) docker     (122)    12923 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/color/rgb.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2440 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/color/sepia.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3391 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/color/xyz.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4016 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/color/ycbcr.py
+-rw-r--r--   0 runner    (1001) docker     (122)    12970 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/color/yuv.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3344 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/constants.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2022-12-21 21:44:56.000000 kornia-0.6.9/kornia/contrib/
+-rw-r--r--   0 runner    (1001) docker     (122)     1071 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/contrib/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)      836 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/contrib/classification.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2007 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/contrib/connected_components.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8743 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/contrib/diamond_square.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3512 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/contrib/distance_transform.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1206 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/contrib/edge_detection.py
+-rw-r--r--   0 runner    (1001) docker     (122)    13646 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/contrib/extract_patches.py
+-rw-r--r--   0 runner    (1001) docker     (122)    15450 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/contrib/face_detection.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2891 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/contrib/histogram_matching.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6057 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/contrib/image_stitching.py
+-rw-r--r--   0 runner    (1001) docker     (122)      814 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/contrib/lambda_module.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8179 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/contrib/vit.py
+-rw-r--r--   0 runner    (1001) docker     (122)     9460 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/contrib/vit_mobile.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2022-12-21 21:44:56.000000 kornia-0.6.9/kornia/core/
+-rw-r--r--   0 runner    (1001) docker     (122)      473 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/core/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)      568 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/core/_backend.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4267 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/core/tensor_wrapper.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2022-12-21 21:44:56.000000 kornia-0.6.9/kornia/enhance/
+-rw-r--r--   0 runner    (1001) docker     (122)     2053 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/enhance/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    52178 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/enhance/adjust.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3422 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/enhance/core.py
+-rw-r--r--   0 runner    (1001) docker     (122)    15520 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/enhance/equalization.py
+-rw-r--r--   0 runner    (1001) docker     (122)     9847 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/enhance/histogram.py
+-rw-r--r--   0 runner    (1001) docker     (122)     9733 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/enhance/normalize.py
+-rw-r--r--   0 runner    (1001) docker     (122)      613 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/enhance/shift_rgb.py
+-rw-r--r--   0 runner    (1001) docker     (122)    13321 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/enhance/zca.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2022-12-21 21:44:56.000000 kornia-0.6.9/kornia/feature/
+-rw-r--r--   0 runner    (1001) docker     (122)     3348 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/feature/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2022-12-21 21:44:56.000000 kornia-0.6.9/kornia/feature/adalam/
+-rw-r--r--   0 runner    (1001) docker     (122)       74 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/feature/adalam/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    11891 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/feature/adalam/adalam.py
+-rw-r--r--   0 runner    (1001) docker     (122)    18566 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/feature/adalam/core.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7014 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/feature/adalam/ransac.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3986 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/feature/adalam/utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)    10285 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/feature/affine_shape.py
+-rw-r--r--   0 runner    (1001) docker     (122)    11407 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/feature/defmo.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7534 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/feature/hardnet.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7372 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/feature/hynet.py
+-rw-r--r--   0 runner    (1001) docker     (122)    14250 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/feature/integrated.py
+-rw-r--r--   0 runner    (1001) docker     (122)    12366 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/feature/keynet.py
+-rw-r--r--   0 runner    (1001) docker     (122)    19385 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/feature/laf.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2022-12-21 21:44:56.000000 kornia-0.6.9/kornia/feature/loftr/
+-rw-r--r--   0 runner    (1001) docker     (122)       25 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/feature/loftr/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2022-12-21 21:44:56.000000 kornia-0.6.9/kornia/feature/loftr/backbone/
+-rw-r--r--   0 runner    (1001) docker     (122)      429 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/feature/loftr/backbone/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6858 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/feature/loftr/backbone/resnet_fpn.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8010 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/feature/loftr/loftr.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2022-12-21 21:44:56.000000 kornia-0.6.9/kornia/feature/loftr/loftr_module/
+-rw-r--r--   0 runner    (1001) docker     (122)       93 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/feature/loftr/loftr_module/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2922 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/feature/loftr/loftr_module/fine_preprocess.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2991 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/feature/loftr/loftr_module/linear_attention.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3757 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/feature/loftr/loftr_module/transformer.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2022-12-21 21:44:56.000000 kornia-0.6.9/kornia/feature/loftr/utils/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/feature/loftr/utils/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    10506 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/feature/loftr/utils/coarse_matching.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2893 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/feature/loftr/utils/fine_matching.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2084 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/feature/loftr/utils/geometry.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2995 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/feature/loftr/utils/position_encoding.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5601 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/feature/loftr/utils/supervision.py
+-rw-r--r--   0 runner    (1001) docker     (122)    16164 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/feature/matching.py
+-rw-r--r--   0 runner    (1001) docker     (122)    23687 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/feature/mkd.py
+-rw-r--r--   0 runner    (1001) docker     (122)     9588 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/feature/orientation.py
+-rw-r--r--   0 runner    (1001) docker     (122)    11814 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/feature/responses.py
+-rw-r--r--   0 runner    (1001) docker     (122)    10328 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/feature/scale_space_detector.py
+-rw-r--r--   0 runner    (1001) docker     (122)    10773 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/feature/siftdesc.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2022-12-21 21:44:56.000000 kornia-0.6.9/kornia/feature/sold2/
+-rw-r--r--   0 runner    (1001) docker     (122)       68 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/feature/sold2/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    13823 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/feature/sold2/backbones.py
+-rw-r--r--   0 runner    (1001) docker     (122)    15530 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/feature/sold2/sold2.py
+-rw-r--r--   0 runner    (1001) docker     (122)    26202 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/feature/sold2/sold2_detector.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2746 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/feature/sosnet.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2213 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/feature/tfeat.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2022-12-21 21:44:56.000000 kornia-0.6.9/kornia/filters/
+-rw-r--r--   0 runner    (1001) docker     (122)     2242 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/filters/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3548 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/filters/blur.py
+-rw-r--r--   0 runner    (1001) docker     (122)    10053 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/filters/blur_pool.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8434 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/filters/canny.py
+-rw-r--r--   0 runner    (1001) docker     (122)    10332 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/filters/dexined.py
+-rw-r--r--   0 runner    (1001) docker     (122)    11037 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/filters/filter.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3816 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/filters/gaussian.py
+-rw-r--r--   0 runner    (1001) docker     (122)    24573 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/filters/kernels.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8136 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/filters/kernels_geometry.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3035 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/filters/laplacian.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2568 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/filters/median.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8302 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/filters/motion.py
+-rw-r--r--   0 runner    (1001) docker     (122)     9127 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/filters/sobel.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2553 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/filters/unsharp.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2022-12-21 21:44:56.000000 kornia-0.6.9/kornia/geometry/
+-rw-r--r--   0 runner    (1001) docker     (122)      302 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/geometry/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    23306 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/geometry/bbox.py
+-rw-r--r--   0 runner    (1001) docker     (122)    41542 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/geometry/boxes.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2022-12-21 21:44:56.000000 kornia-0.6.9/kornia/geometry/calibration/
+-rw-r--r--   0 runner    (1001) docker     (122)      246 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/geometry/calibration/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5976 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/geometry/calibration/distort.py
+-rw-r--r--   0 runner    (1001) docker     (122)    12366 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/geometry/calibration/pnp.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7525 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/geometry/calibration/undistort.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2022-12-21 21:44:56.000000 kornia-0.6.9/kornia/geometry/camera/
+-rw-r--r--   0 runner    (1001) docker     (122)      257 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/geometry/camera/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2927 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/geometry/camera/perspective.py
+-rw-r--r--   0 runner    (1001) docker     (122)    26425 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/geometry/camera/pinhole.py
+-rw-r--r--   0 runner    (1001) docker     (122)    11674 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/geometry/camera/stereo.py
+-rw-r--r--   0 runner    (1001) docker     (122)    55879 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/geometry/conversions.py
+-rw-r--r--   0 runner    (1001) docker     (122)    16394 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/geometry/depth.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2022-12-21 21:44:56.000000 kornia-0.6.9/kornia/geometry/epipolar/
+-rw-r--r--   0 runner    (1001) docker     (122)     1839 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/geometry/epipolar/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7355 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/geometry/epipolar/_metrics.py
+-rw-r--r--   0 runner    (1001) docker     (122)    11190 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/geometry/epipolar/essential.py
+-rw-r--r--   0 runner    (1001) docker     (122)    11544 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/geometry/epipolar/fundamental.py
+-rw-r--r--   0 runner    (1001) docker     (122)      839 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/geometry/epipolar/numeric.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7021 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/geometry/epipolar/projection.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1618 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/geometry/epipolar/scene.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2904 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/geometry/epipolar/triangulation.py
+-rw-r--r--   0 runner    (1001) docker     (122)    14513 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/geometry/homography.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2022-12-21 21:44:56.000000 kornia-0.6.9/kornia/geometry/liegroup/
+-rw-r--r--   0 runner    (1001) docker     (122)      124 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/geometry/liegroup/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2547 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/geometry/liegroup/_utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)    10895 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/geometry/liegroup/se2.py
+-rw-r--r--   0 runner    (1001) docker     (122)    12634 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/geometry/liegroup/se3.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8562 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/geometry/liegroup/so2.py
+-rw-r--r--   0 runner    (1001) docker     (122)    10126 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/geometry/liegroup/so3.py
+-rw-r--r--   0 runner    (1001) docker     (122)    10487 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/geometry/linalg.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6705 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/geometry/line.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4888 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/geometry/plane.py
+-rw-r--r--   0 runner    (1001) docker     (122)    12797 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/geometry/quaternion.py
+-rw-r--r--   0 runner    (1001) docker     (122)    10553 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/geometry/ransac.py
+-rw-r--r--   0 runner    (1001) docker     (122)      173 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/geometry/ray.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2022-12-21 21:44:56.000000 kornia-0.6.9/kornia/geometry/subpix/
+-rw-r--r--   0 runner    (1001) docker     (122)      769 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/geometry/subpix/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5871 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/geometry/subpix/dsnt.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6458 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/geometry/subpix/nms.py
+-rw-r--r--   0 runner    (1001) docker     (122)    24479 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/geometry/subpix/spatial_soft_argmax.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2022-12-21 21:44:56.000000 kornia-0.6.9/kornia/geometry/transform/
+-rw-r--r--   0 runner    (1001) docker     (122)      266 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/geometry/transform/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    37259 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/geometry/transform/affwarp.py
+-rw-r--r--   0 runner    (1001) docker     (122)    13706 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/geometry/transform/crop2d.py
+-rw-r--r--   0 runner    (1001) docker     (122)    13750 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/geometry/transform/crop3d.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4134 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/geometry/transform/elastic_transform.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3545 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/geometry/transform/flips.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4873 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/geometry/transform/homography_warper.py
+-rw-r--r--   0 runner    (1001) docker     (122)    10130 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/geometry/transform/image_registrator.py
+-rw-r--r--   0 runner    (1001) docker     (122)    49748 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/geometry/transform/imgwarp.py
+-rw-r--r--   0 runner    (1001) docker     (122)    14565 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/geometry/transform/pyramid.py
+-rw-r--r--   0 runner    (1001) docker     (122)    10879 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/geometry/transform/thin_plate_spline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3429 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/geometry/vector.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2022-12-21 21:44:56.000000 kornia-0.6.9/kornia/grad_estimator/
+-rw-r--r--   0 runner    (1001) docker     (122)       55 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/grad_estimator/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5054 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/grad_estimator/ste.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2022-12-21 21:44:56.000000 kornia-0.6.9/kornia/io/
+-rw-r--r--   0 runner    (1001) docker     (122)       42 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/io/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4287 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/io/io.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2022-12-21 21:44:56.000000 kornia-0.6.9/kornia/losses/
+-rw-r--r--   0 runner    (1001) docker     (122)     1264 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/losses/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3709 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/losses/depth_smooth.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3950 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/losses/dice.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2560 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/losses/divergence.py
+-rw-r--r--   0 runner    (1001) docker     (122)    10219 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/losses/focal.py
+-rw-r--r--   0 runner    (1001) docker     (122)    10217 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/losses/hausdorff.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4646 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/losses/lovasz_hinge.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5155 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/losses/lovasz_softmax.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6536 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/losses/ms_ssim.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1748 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/losses/psnr.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3791 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/losses/ssim.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2733 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/losses/total_variation.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4715 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/losses/tversky.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2022-12-21 21:44:56.000000 kornia-0.6.9/kornia/metrics/
+-rw-r--r--   0 runner    (1001) docker     (122)      453 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/metrics/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)      857 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/metrics/accuracy.py
+-rw-r--r--   0 runner    (1001) docker     (122)      865 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/metrics/average_meter.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3106 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/metrics/confusion_matrix.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8344 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/metrics/mean_average_precision.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4697 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/metrics/mean_iou.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1634 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/metrics/psnr.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6013 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/metrics/ssim.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2022-12-21 21:44:56.000000 kornia-0.6.9/kornia/morphology/
+-rw-r--r--   0 runner    (1001) docker     (122)       26 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/morphology/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    22676 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/morphology/morphology.py
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/py.typed
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2022-12-21 21:44:56.000000 kornia-0.6.9/kornia/testing/
+-rw-r--r--   0 runner    (1001) docker     (122)    17342 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/testing/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2022-12-21 21:44:56.000000 kornia-0.6.9/kornia/tracking/
+-rw-r--r--   0 runner    (1001) docker     (122)       79 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/tracking/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6073 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/tracking/planar_tracker.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2022-12-21 21:44:56.000000 kornia-0.6.9/kornia/utils/
+-rw-r--r--   0 runner    (1001) docker     (122)     1098 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/utils/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1988 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/utils/_compat.py
+-rw-r--r--   0 runner    (1001) docker     (122)    12277 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/utils/draw.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4344 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/utils/grid.py
+-rw-r--r--   0 runner    (1001) docker     (122)     9216 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/utils/helpers.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8586 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/utils/image.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1792 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/utils/memory.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2450 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/utils/misc.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1695 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/utils/one_hot.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3245 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/utils/pointcloud_io.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2022-12-21 21:44:56.000000 kornia-0.6.9/kornia/x/
+-rw-r--r--   0 runner    (1001) docker     (122)      236 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/x/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3212 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/x/callbacks.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7851 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/x/trainer.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4560 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/x/trainers.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2894 2022-12-21 21:44:52.000000 kornia-0.6.9/kornia/x/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2022-12-21 21:44:56.000000 kornia-0.6.9/kornia.egg-info/
+-rw-r--r--   0 runner    (1001) docker     (122)    11168 2022-12-21 21:44:56.000000 kornia-0.6.9/kornia.egg-info/PKG-INFO
+-rw-r--r--   0 runner    (1001) docker     (122)    10650 2022-12-21 21:44:56.000000 kornia-0.6.9/kornia.egg-info/SOURCES.txt
+-rw-r--r--   0 runner    (1001) docker     (122)        1 2022-12-21 21:44:56.000000 kornia-0.6.9/kornia.egg-info/dependency_links.txt
+-rw-r--r--   0 runner    (1001) docker     (122)      442 2022-12-21 21:44:56.000000 kornia-0.6.9/kornia.egg-info/requires.txt
+-rw-r--r--   0 runner    (1001) docker     (122)        7 2022-12-21 21:44:56.000000 kornia-0.6.9/kornia.egg-info/top_level.txt
+-rw-r--r--   0 runner    (1001) docker     (122)        1 2022-12-21 21:44:56.000000 kornia-0.6.9/kornia.egg-info/zip-safe
+-rw-r--r--   0 runner    (1001) docker     (122)      315 2022-12-21 21:44:52.000000 kornia-0.6.9/pyproject.toml
+-rw-r--r--   0 runner    (1001) docker     (122)     2812 2022-12-21 21:44:56.000000 kornia-0.6.9/setup.cfg
+-rw-r--r--   0 runner    (1001) docker     (122)      316 2022-12-21 21:44:52.000000 kornia-0.6.9/setup.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2022-12-21 21:44:56.000000 kornia-0.6.9/test/
+-rw-r--r--   0 runner    (1001) docker     (122)    30427 2022-12-21 21:44:52.000000 kornia-0.6.9/test/test_contrib.py
+-rw-r--r--   0 runner    (1001) docker     (122)    10407 2022-12-21 21:44:52.000000 kornia-0.6.9/test/test_metrics.py
```

### Comparing `kornia-0.6.8/LICENSE` & `kornia-0.6.9/LICENSE`

 * *Files identical despite different names*

### Comparing `kornia-0.6.8/PKG-INFO` & `kornia-0.6.9/PKG-INFO`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: kornia
-Version: 0.6.8
+Version: 0.6.9
 Summary: Open Source Differentiable Computer Vision Library for PyTorch
 Home-page: https://www.kornia.org
 Download-URL: https://github.com/kornia/kornia
 Author: Edgar Riba
 Author-email: edgar@kornia.org
 License: Apache-2.0
 Project-URL: Bug Tracker, https://github.com/kornia/kornia/issues
@@ -22,15 +22,15 @@
 Classifier: Natural Language :: English
 Classifier: Operating System :: OS Independent
 Classifier: Programming Language :: Python :: 3
 Classifier: Programming Language :: Python :: 3 :: Only
 Classifier: Topic :: Scientific/Engineering :: Artificial Intelligence
 Classifier: Topic :: Scientific/Engineering :: Image Processing
 Classifier: Topic :: Software Development :: Libraries
-Requires-Python: >=3.6
+Requires-Python: >=3.7
 Description-Content-Type: text/markdown
 Provides-Extra: dev
 Provides-Extra: docs
 Provides-Extra: x
 License-File: LICENSE
 
 <div align="center">
```

#### html2text {}

```diff
@@ -1,8 +1,8 @@
-Metadata-Version: 2.1 Name: kornia Version: 0.6.8 Summary: Open Source
+Metadata-Version: 2.1 Name: kornia Version: 0.6.9 Summary: Open Source
 Differentiable Computer Vision Library for PyTorch Home-page: https://
 www.kornia.org Download-URL: https://github.com/kornia/kornia Author: Edgar
 Riba Author-email: edgar@kornia.org License: Apache-2.0 Project-URL: Bug
 Tracker, https://github.com/kornia/kornia/issues Project-URL: Documentation,
 https://kornia.readthedocs.io/en/latest Project-URL: Source Code, https://
 github.com/kornia/kornia Keywords: computer vision,deep learning,pytorch
 Classifier: Development Status :: 4 - Beta Classifier: Environment :: Console
@@ -11,15 +11,15 @@
 Information Technology Classifier: Intended Audience :: Science/Research
 Classifier: License :: OSI Approved :: Apache Software License Classifier:
 Natural Language :: English Classifier: Operating System :: OS Independent
 Classifier: Programming Language :: Python :: 3 Classifier: Programming
 Language :: Python :: 3 :: Only Classifier: Topic :: Scientific/Engineering ::
 Artificial Intelligence Classifier: Topic :: Scientific/Engineering :: Image
 Processing Classifier: Topic :: Software Development :: Libraries Requires-
-Python: >=3.6 Description-Content-Type: text/markdown Provides-Extra: dev
+Python: >=3.7 Description-Content-Type: text/markdown Provides-Extra: dev
 Provides-Extra: docs Provides-Extra: x License-File: LICENSE
        [https://github.com/kornia/data/raw/main/kornia_banner_pixie.png]
 --- English | [](README_zh-CN.md)  Website  Docs  Try_it_Now
    Tutorials  Examples  Blog  Community [![PyPI python](https://
   img.shields.io/pypi/pyversions/kornia)](https://pypi.org/project/kornia) [!
 [PyPI version](https://badge.fury.io/py/kornia.svg)](https://pypi.org/project/
    kornia) [![Downloads](https://pepy.tech/badge/kornia)](https://pepy.tech/
```

### Comparing `kornia-0.6.8/README.md` & `kornia-0.6.9/README.md`

 * *Files identical despite different names*

### Comparing `kornia-0.6.8/kornia/__init__.py` & `kornia-0.6.9/kornia/__init__.py`

 * *Files identical despite different names*

### Comparing `kornia-0.6.8/kornia/augmentation/_2d/base.py` & `kornia-0.6.9/kornia/augmentation/_2d/base.py`

 * *Files identical despite different names*

### Comparing `kornia-0.6.8/kornia/augmentation/_2d/geometric/__init__.py` & `kornia-0.6.9/kornia/augmentation/_2d/geometric/__init__.py`

 * *Files identical despite different names*

### Comparing `kornia-0.6.8/kornia/augmentation/_2d/geometric/affine.py` & `kornia-0.6.9/kornia/augmentation/_2d/geometric/affine.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,15 +1,15 @@
 from typing import Any, Dict, Optional, Tuple, Union, cast
 
 import torch
-from torch import Tensor
 
 from kornia.augmentation import random_generator as rg
 from kornia.augmentation._2d.geometric.base import GeometricAugmentationBase2D
 from kornia.constants import Resample, SamplePadding
+from kornia.core import Tensor
 from kornia.geometry.conversions import deg2rad
 from kornia.geometry.transform import get_affine_matrix2d, warp_affine
 
 
 class RandomAffine(GeometricAugmentationBase2D):
     r"""Apply a random 2D affine transformation to a tensor image.
 
@@ -87,15 +87,15 @@
         align_corners: bool = False,
         padding_mode: Union[str, int, SamplePadding] = SamplePadding.ZEROS.name,
         p: float = 0.5,
         keepdim: bool = False,
         return_transform: Optional[bool] = None,
     ) -> None:
         super().__init__(p=p, return_transform=return_transform, same_on_batch=same_on_batch, keepdim=keepdim)
-        self._param_generator = cast(rg.AffineGenerator, rg.AffineGenerator(degrees, translate, scale, shear))
+        self._param_generator: rg.AffineGenerator = rg.AffineGenerator(degrees, translate, scale, shear)
         self.flags = dict(
             resample=Resample.get(resample), padding_mode=SamplePadding.get(padding_mode), align_corners=align_corners
         )
 
     def compute_transformation(self, input: Tensor, params: Dict[str, Tensor], flags: Dict[str, Any]) -> Tensor:
         return get_affine_matrix2d(
             torch.as_tensor(params["translations"], device=input.device, dtype=input.dtype),
```

### Comparing `kornia-0.6.8/kornia/augmentation/_2d/geometric/base.py` & `kornia-0.6.9/kornia/augmentation/_2d/geometric/base.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,14 +1,13 @@
 import warnings
-from typing import Any, Dict, Optional, Tuple, cast
-
-from torch import Tensor, as_tensor
+from typing import Any, Dict, Optional, Tuple
 
 from kornia.augmentation._2d.base import AugmentationBase2D
 from kornia.augmentation.utils import override_parameters
+from kornia.core import Tensor, as_tensor
 from kornia.utils.helpers import _torch_inverse_cast
 
 
 class GeometricAugmentationBase2D(AugmentationBase2D):
     r"""GeometricAugmentationBase2D base class for customized geometric augmentation implementations.
 
     Args:
@@ -106,8 +105,11 @@
             transform = self.compute_inverse_transformation(transform)
             output = self.inverse_transform(in_tensor, flags=flags, transform=transform, size=size)
         else:
             transform[to_apply] = self.compute_inverse_transformation(transform[to_apply])
             output[to_apply] = self.inverse_transform(
                 in_tensor[to_apply], transform=transform[to_apply], size=size, flags=flags
             )
-        return cast(Tensor, self.transform_output_tensor(output, input_shape)) if self.keepdim else output
+        if self.keepdim:
+            return self.transform_output_tensor(output, input_shape)
+
+        return output
```

### Comparing `kornia-0.6.8/kornia/augmentation/_2d/geometric/center_crop.py` & `kornia-0.6.9/kornia/augmentation/_2d/geometric/center_crop.py`

 * *Files identical despite different names*

### Comparing `kornia-0.6.8/kornia/augmentation/_2d/geometric/crop.py` & `kornia-0.6.9/kornia/augmentation/_2d/geometric/crop.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,17 +1,16 @@
 from typing import Any, Dict, List, Optional, Tuple, Union, cast
 
 import torch
-from torch import Tensor
-from torch.nn.functional import pad
 
 from kornia.augmentation import random_generator as rg
 from kornia.augmentation._2d.geometric.base import GeometricAugmentationBase2D
 from kornia.augmentation.utils import _transform_input, _transform_output_shape, override_parameters
 from kornia.constants import Resample
+from kornia.core import Tensor, pad, tensor
 from kornia.geometry.transform import crop_by_indices, crop_by_transform_mat, get_perspective_transform
 
 
 class RandomCrop(GeometricAugmentationBase2D):
     r"""Crop random patches of a tensor image on a given size.
 
     .. image:: _static/img/RandomCrop.png
@@ -87,15 +86,15 @@
         cropping_mode: str = "slice",
         return_transform: Optional[bool] = None,
     ) -> None:
         # Since PyTorch does not support ragged tensor. So cropping function happens batch-wisely.
         super().__init__(
             p=1.0, return_transform=return_transform, same_on_batch=same_on_batch, p_batch=p, keepdim=keepdim
         )
-        self._param_generator = cast(rg.CropGenerator, rg.CropGenerator(size))
+        self._param_generator = rg.CropGenerator(size)
         self.flags = dict(
             size=size,
             padding=padding,
             pad_if_needed=pad_if_needed,
             fill=fill,
             padding_mode=padding_mode,
             resample=Resample.get(resample),
@@ -103,31 +102,41 @@
             cropping_mode=cropping_mode,
         )
 
     def compute_padding(self, shape: torch.Size, flags: Optional[Dict[str, Any]] = None) -> List[int]:
         flags = self.flags if flags is None else flags
         if len(shape) != 4:
             raise AssertionError(f"Expected BCHW. Got {shape}.")
-        padding = [0, 0, 0, 0]
+        padding = [0, 0, 0, 0]  # left, right, top, bottom
         if flags["padding"] is not None:
             if isinstance(flags["padding"], int):
                 padding = [flags["padding"]] * 4
             elif isinstance(flags["padding"], tuple) and len(flags["padding"]) == 2:
-                padding = [flags["padding"][1], flags["padding"][1], flags["padding"][0], flags["padding"][0]]
+                padding = [flags["padding"][0], flags["padding"][0], flags["padding"][1], flags["padding"][1]]
             elif isinstance(flags["padding"], tuple) and len(flags["padding"]) == 4:
-                padding = [flags["padding"][3], flags["padding"][2], flags["padding"][1], flags["padding"][0]]
+                padding = [flags["padding"][0], flags["padding"][2], flags["padding"][1], flags["padding"][3]]
             else:
                 raise RuntimeError(f"Expect `padding` to be a scalar, or length 2/4 list. Got {flags['padding']}.")
 
-        if flags["pad_if_needed"] and shape[-2] < flags["size"][0]:
-            padding = [0, 0, (flags["size"][0] - shape[-2]), flags["size"][0] - shape[-2]]
-
-        if flags["pad_if_needed"] and shape[-1] < flags["size"][1]:
-            padding = [flags["size"][1] - shape[-1], flags["size"][1] - shape[-1], 0, 0]
-
+        if flags["pad_if_needed"]:
+            needed_padding: Tuple[int, int] = (flags["size"][0] - shape[-2], flags["size"][1] - shape[-1])  # HW
+            # If crop width is larger than input width pad equally left and right
+            if needed_padding[1] > 0:
+                # Only use the extra padding if actually needed after possible fixed padding
+                if needed_padding[1] > padding[0]:
+                    padding[0] = needed_padding[1]
+                if needed_padding[1] > padding[1]:
+                    padding[1] = needed_padding[1]
+            # If crop height is larger than input height pad equally top and bottom
+            if needed_padding[0] > 0:
+                # Only use the extra padding if actually needed after possible fixed padding
+                if needed_padding[0] > padding[2]:
+                    padding[2] = needed_padding[0]
+                if needed_padding[0] > padding[3]:
+                    padding[3] = needed_padding[0]
         return padding
 
     def precrop_padding(
         self, input: Tensor, padding: Optional[List[int]] = None, flags: Optional[Dict[str, Any]] = None
     ) -> Tensor:
         flags = self.flags if flags is None else flags
         if padding is None:
@@ -222,15 +231,15 @@
     def forward_parameters_precrop(self, batch_shape) -> Dict[str, Tensor]:
         input_pad = self.compute_padding(batch_shape)
         batch_shape_new = (
             *batch_shape[:2],
             batch_shape[2] + input_pad[2] + input_pad[3],  # original height + top + bottom padding
             batch_shape[3] + input_pad[0] + input_pad[1],  # original width + left + right padding
         )
-        padding_size = torch.tensor(tuple(input_pad), dtype=torch.long).expand(batch_shape[0], -1)
+        padding_size = tensor(tuple(input_pad), dtype=torch.long).expand(batch_shape[0], -1)
         _params = super().forward_parameters(batch_shape_new)
         _params.update({"padding_size": padding_size})
         return _params
 
     def forward(self, input: Tensor, params: Optional[Dict[str, Tensor]] = None, **kwargs) -> Tensor:
         padding_size = params.get("padding_size") if params else None
         if padding_size is not None:
@@ -241,29 +250,28 @@
         flags = override_parameters(self.flags, kwargs, in_place=False)
 
         if isinstance(input, (tuple, list)):
             ori_shape = input[0].shape
             input_temp = _transform_input(input[0])
             input_pad = self.compute_padding(input[0].shape, flags) if input_pad is None else input_pad
             _input = (self.precrop_padding(input_temp, input_pad, flags), input[1])
-            _input = _transform_output_shape(_input, ori_shape) if self.keepdim else _input  # type:ignore
+            _input = _transform_output_shape(_input, ori_shape) if self.keepdim else _input
         else:
-            input = cast(Tensor, input)
             ori_shape = input.shape
             input_temp = _transform_input(input)
             input_pad = self.compute_padding(input_temp.shape, flags) if input_pad is None else input_pad
-            _input = self.precrop_padding(input_temp, input_pad, flags)  # type: ignore
-            _input = _transform_output_shape(_input, ori_shape) if self.keepdim else _input  # type:ignore
+            _input = self.precrop_padding(input_temp, input_pad, flags)
+            _input = _transform_output_shape(_input, ori_shape) if self.keepdim else _input
         if params is not None:
             params, flags = self._process_kwargs_to_params_and_flags(params, self.flags, **kwargs)
-        out = super().forward(_input, params, **kwargs)  # type:ignore
+        out = super().forward(_input, params, **kwargs)
 
         # Update the actual input size for inverse
         if "padding_size" not in self._params:
-            _padding_size = torch.tensor(tuple(input_pad), device=input_temp.device, dtype=torch.long).expand(
+            _padding_size = tensor(tuple(input_pad), device=input_temp.device, dtype=torch.long).expand(
                 input_temp.size(0), -1
             )
             self._params.update({"padding_size": _padding_size})
 
         if not self._params["batch_prob"].all():
             # undo the pre-crop if nothing happened.
             if isinstance(out, tuple) and isinstance(input, tuple):
```

### Comparing `kornia-0.6.8/kornia/augmentation/_2d/geometric/elastic_transform.py` & `kornia-0.6.9/kornia/augmentation/_2d/geometric/elastic_transform.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,13 +1,14 @@
-from typing import Any, Dict, Optional, Tuple
+from typing import Any, Dict, Optional, Tuple, Union
 
 import torch
 from torch import Tensor
 
 from kornia.augmentation._2d.geometric.base import GeometricAugmentationBase2D
+from kornia.constants import Resample
 from kornia.geometry.transform import elastic_transform2d
 
 
 class RandomElasticTransform(GeometricAugmentationBase2D):
     r"""Add random elastic transformation to a tensor image.
 
     .. image:: _static/img/RandomElasticTransform.png
@@ -15,15 +16,17 @@
     Args:
         kernel_size: the size of the Gaussian kernel.
         sigma: The standard deviation of the Gaussian in the y and x directions,
           respectively. Larger sigma results in smaller pixel displacements.
         alpha: The scaling factor that controls the intensity of the deformation
           in the y and x directions, respectively.
         align_corners: Interpolation flag used by `grid_sample`.
-        mode: Interpolation mode used by `grid_sample`. Either 'bilinear' or 'nearest'.
+        resample: Interpolation mode used by `grid_sample`. Either 'nearest' (0) or 'bilinear' (1).
+        mode: Deprecated: Interpolation mode used by `grid_sample`. Either 'bilinear' or 'nearest'.
+          Please use the `resample` argument instead.
         padding_mode: The padding used by ```grid_sample```. Either 'zeros', 'border' or 'refection'.
         same_on_batch: apply the same transformation across the batch.
         p: probability of applying the transformation.
         keepdim: whether to keep the output shape the same as input (True) or broadcast it
             to the batch form (False).
 
     .. note::
@@ -45,37 +48,38 @@
 
     def __init__(
         self,
         kernel_size: Tuple[int, int] = (63, 63),
         sigma: Tuple[float, float] = (32.0, 32.0),
         alpha: Tuple[float, float] = (1.0, 1.0),
         align_corners: bool = False,
-        mode: str = "bilinear",
+        resample: Union[str, int, Resample] = Resample.BILINEAR.name,
         padding_mode: str = "zeros",
         same_on_batch: bool = False,
         p: float = 0.5,
         keepdim: bool = False,
         return_transform: Optional[bool] = None,
     ) -> None:
         super().__init__(
             p=p, return_transform=return_transform, same_on_batch=same_on_batch, p_batch=1.0, keepdim=keepdim
         )
+
         self.flags = dict(
             kernel_size=kernel_size,
             sigma=sigma,
             alpha=alpha,
             align_corners=align_corners,
-            mode=mode,
+            resample=Resample.get(resample),
             padding_mode=padding_mode,
         )
 
     def generate_parameters(self, shape: torch.Size) -> Dict[str, Tensor]:
         B, _, H, W = shape
         if self.same_on_batch:
-            noise = torch.rand(1, 2, H, W, device=self.device, dtype=self.dtype).repeat(B, 1, 1, 1)
+            noise = torch.rand(1, 2, H, W, device=self.device, dtype=self.dtype).expand(B, 2, H, W)
         else:
             noise = torch.rand(B, 2, H, W, device=self.device, dtype=self.dtype)
         return dict(noise=noise * 2 - 1)
 
     # TODO: It is incorrect to return identity
     def compute_transformation(self, input: Tensor, params: Dict[str, Tensor], flags: Dict[str, Any]) -> Tensor:
         return self.identity_matrix(input)
@@ -86,10 +90,10 @@
         return elastic_transform2d(
             input,
             params["noise"].to(input),
             flags["kernel_size"],
             flags["sigma"],
             flags["alpha"],
             flags["align_corners"],
-            flags["mode"],
+            flags["resample"].name.lower(),
             flags["padding_mode"],
         )
```

### Comparing `kornia-0.6.8/kornia/augmentation/_2d/geometric/fisheye.py` & `kornia-0.6.9/kornia/augmentation/_2d/geometric/fisheye.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,13 +1,12 @@
-from typing import Any, Dict, Optional, cast
-
-from torch import Tensor
+from typing import Any, Dict, Optional
 
 from kornia.augmentation import random_generator as rg
 from kornia.augmentation._2d.geometric.base import GeometricAugmentationBase2D
+from kornia.core import Tensor
 from kornia.geometry.transform import remap
 from kornia.utils import create_meshgrid
 
 
 class RandomFisheye(GeometricAugmentationBase2D):
     r"""Add random camera radial distortion.
 
@@ -51,21 +50,18 @@
     ) -> None:
         super().__init__(
             p=p, return_transform=return_transform, same_on_batch=same_on_batch, p_batch=1.0, keepdim=keepdim
         )
         self._check_tensor(center_x)
         self._check_tensor(center_y)
         self._check_tensor(gamma)
-        self._param_generator = cast(
-            rg.PlainUniformGenerator,
-            rg.PlainUniformGenerator(
-                (center_x[:, None], "center_x", None, None),
-                (center_y[:, None], "center_y", None, None),
-                (gamma[:, None], "gamma", None, None),
-            ),
+        self._param_generator = rg.PlainUniformGenerator(
+            (center_x[:, None], "center_x", None, None),
+            (center_y[:, None], "center_y", None, None),
+            (gamma[:, None], "gamma", None, None),
         )
 
     def _check_tensor(self, data: Tensor) -> None:
         if not isinstance(data, Tensor):
             raise TypeError(f"Invalid input type. Expected Tensor - got: {type(data)}")
 
         if len(data.shape) != 1 and data.shape[0] != 2:
```

### Comparing `kornia-0.6.8/kornia/augmentation/_2d/geometric/horizontal_flip.py` & `kornia-0.6.9/kornia/augmentation/_2d/geometric/horizontal_flip.py`

 * *Files 1% similar despite different names*

```diff
@@ -53,15 +53,15 @@
         tensor(True)
     """
 
     def compute_transformation(self, input: Tensor, params: Dict[str, Tensor], flags: Dict[str, Any]) -> Tensor:
         w: int = int(params["forward_input_shape"][-1])
         flip_mat: Tensor = torch.tensor([[-1, 0, w - 1], [0, 1, 0], [0, 0, 1]], device=input.device, dtype=input.dtype)
 
-        return flip_mat.repeat(input.size(0), 1, 1)
+        return flip_mat.expand(input.shape[0], 3, 3)
 
     def apply_transform(
         self, input: Tensor, params: Dict[str, Tensor], flags: Dict[str, Any], transform: Optional[Tensor] = None
     ) -> Tensor:
         return hflip(input)
 
     def inverse_transform(
```

### Comparing `kornia-0.6.8/kornia/augmentation/_2d/geometric/pad.py` & `kornia-0.6.9/kornia/augmentation/_2d/geometric/pad.py`

 * *Files identical despite different names*

### Comparing `kornia-0.6.8/kornia/augmentation/_2d/geometric/perspective.py` & `kornia-0.6.9/kornia/augmentation/_2d/geometric/perspective.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,15 +1,13 @@
-from typing import Any, Dict, Optional, Tuple, Union, cast
-
-import torch
-from torch import Tensor
+from typing import Any, Dict, Optional, Tuple, Union
 
 from kornia.augmentation import random_generator as rg
 from kornia.augmentation._2d.geometric.base import GeometricAugmentationBase2D
 from kornia.constants import Resample
+from kornia.core import Tensor, as_tensor
 from kornia.geometry.transform import get_perspective_transform, warp_perspective
 
 
 class RandomPerspective(GeometricAugmentationBase2D):
     r"""Apply a random perspective transformation to an image tensor with a given probability.
 
     .. image:: _static/img/RandomPerspective.png
@@ -65,27 +63,27 @@
         align_corners: bool = False,
         p: float = 0.5,
         keepdim: bool = False,
         sampling_method: str = "basic",
         return_transform: Optional[bool] = None,
     ) -> None:
         super().__init__(p=p, return_transform=return_transform, same_on_batch=same_on_batch, keepdim=keepdim)
-        self._param_generator = cast(
-            rg.PerspectiveGenerator, rg.PerspectiveGenerator(distortion_scale, sampling_method=sampling_method)
-        )
+        self._param_generator = rg.PerspectiveGenerator(distortion_scale, sampling_method=sampling_method)
+
         self.flags: Dict[str, Any] = dict(align_corners=align_corners, resample=Resample.get(resample))
 
     def compute_transformation(self, input: Tensor, params: Dict[str, Tensor], flags: Dict[str, Any]) -> Tensor:
         return get_perspective_transform(params["start_points"].to(input), params["end_points"].to(input))
 
     def apply_transform(
         self, input: Tensor, params: Dict[str, Tensor], flags: Dict[str, Any], transform: Optional[Tensor] = None
     ) -> Tensor:
         _, _, height, width = input.shape
-        transform = cast(Tensor, transform)
+        if not isinstance(transform, Tensor):
+            raise TypeError(f'Expected the transform be a Tensor. Gotcha {type(transform)}')
 
         return warp_perspective(
             input, transform, (height, width), mode=flags["resample"].name.lower(), align_corners=flags["align_corners"]
         )
 
     def inverse_transform(
         self,
@@ -93,10 +91,10 @@
         flags: Dict[str, Any],
         transform: Optional[Tensor] = None,
         size: Optional[Tuple[int, int]] = None,
     ) -> Tensor:
         return self.apply_transform(
             input,
             params=self._params,
-            transform=torch.as_tensor(transform, device=input.device, dtype=input.dtype),
+            transform=as_tensor(transform, device=input.device, dtype=input.dtype),
             flags=flags,
         )
```

### Comparing `kornia-0.6.8/kornia/augmentation/_2d/geometric/resize.py` & `kornia-0.6.9/kornia/augmentation/_2d/geometric/resize.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,15 +1,15 @@
-from typing import Any, Dict, Optional, Tuple, Union, cast
+from typing import Any, Dict, Optional, Tuple, Union
 
 import torch
-from torch import Tensor
 
 from kornia.augmentation import random_generator as rg
 from kornia.augmentation._2d.geometric.base import GeometricAugmentationBase2D
 from kornia.constants import Resample
+from kornia.core import Tensor
 from kornia.geometry.transform import crop_by_transform_mat, get_perspective_transform, resize
 from kornia.utils import eye_like
 
 
 class Resize(GeometricAugmentationBase2D):
     """Resize to size.
 
@@ -31,15 +31,15 @@
         align_corners: bool = True,
         antialias: bool = False,
         p: float = 1.0,
         return_transform: Optional[bool] = None,
         keepdim: bool = False,
     ) -> None:
         super().__init__(p=1.0, return_transform=return_transform, same_on_batch=True, p_batch=p, keepdim=keepdim)
-        self._param_generator = cast(rg.ResizeGenerator, rg.ResizeGenerator(resize_to=size, side=side))
+        self._param_generator = rg.ResizeGenerator(resize_to=size, side=side)
         self.flags = dict(
             size=size, side=side, resample=Resample.get(resample), align_corners=align_corners, antialias=antialias
         )
 
     def compute_transformation(self, input: Tensor, params: Dict[str, Tensor], flags: Dict[str, Any]) -> Tensor:
         if params["output_size"] == input.shape[-2:]:
             return eye_like(3, input)
@@ -60,30 +60,36 @@
             x2 = int(params["src"][i, 1, 0]) + 1
             y1 = int(params["src"][i, 0, 1])
             y2 = int(params["src"][i, 3, 1]) + 1
             out[i] = resize(
                 input[i : i + 1, :, y1:y2, x1:x2],
                 out_size,
                 interpolation=flags["resample"].name.lower(),
-                align_corners=flags["align_corners"],
+                align_corners=flags["align_corners"]
+                if flags["resample"] in [Resample.BILINEAR, Resample.BICUBIC]
+                else None,
                 antialias=flags["antialias"],
             )
         return out
 
     def inverse_transform(
         self,
         input: Tensor,
         flags: Dict[str, Any],
         transform: Optional[Tensor] = None,
         size: Optional[Tuple[int, int]] = None,
     ) -> Tensor:
-        size = cast(Tuple[int, int], size)
-        transform = cast(Tensor, transform)
+        if not isinstance(size, tuple):
+            raise TypeError(f'Expected the size be a tuple. Gotcha {type(size)}')
+
+        if not isinstance(transform, Tensor):
+            raise TypeError(f'Expected the transform be a Tensor. Gotcha {type(transform)}')
+
         return crop_by_transform_mat(
-            input, transform[:, :2, :], size, flags["resample"], flags["padding_mode"], flags["align_corners"]
+            input, transform[:, :2, :], size, flags["resample"].name.lower(), "zeros", flags["align_corners"]
         )
 
 
 class LongestMaxSize(Resize):
     """Rescale an image so that maximum side is equal to max_size, keeping the aspect ratio of the initial image.
 
     Args:
```

### Comparing `kornia-0.6.8/kornia/augmentation/_2d/geometric/resized_crop.py` & `kornia-0.6.9/kornia/augmentation/_2d/geometric/resized_crop.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,14 +1,13 @@
-from typing import Any, Dict, Optional, Tuple, Union, cast
-
-from torch import Tensor
+from typing import Any, Dict, Optional, Tuple, Union
 
 from kornia.augmentation import random_generator as rg
 from kornia.augmentation._2d.geometric.base import GeometricAugmentationBase2D
 from kornia.constants import Resample
+from kornia.core import Tensor
 from kornia.geometry.transform import crop_by_indices, crop_by_transform_mat, get_perspective_transform
 
 
 class RandomResizedCrop(GeometricAugmentationBase2D):
     r"""Crop random patches in an image tensor and resizes to a given size.
 
     .. image:: _static/img/RandomResizedCrop.png
@@ -76,15 +75,15 @@
         cropping_mode: str = "slice",
         return_transform: Optional[bool] = None,
     ) -> None:
         # Since PyTorch does not support ragged tensor. So cropping function happens all the time.
         super().__init__(
             p=1.0, return_transform=return_transform, same_on_batch=same_on_batch, p_batch=p, keepdim=keepdim
         )
-        self._param_generator = cast(rg.ResizedCropGenerator, rg.ResizedCropGenerator(size, scale, ratio))
+        self._param_generator = rg.ResizedCropGenerator(size, scale, ratio)
         self.flags = dict(
             size=size,
             resample=Resample.get(resample),
             align_corners=align_corners,
             cropping_mode=cropping_mode,
             padding_mode="zeros",
         )
@@ -96,15 +95,17 @@
             return transform
         raise NotImplementedError(f"Not supported type: {flags['cropping_mode']}.")
 
     def apply_transform(
         self, input: Tensor, params: Dict[str, Tensor], flags: Dict[str, Any], transform: Optional[Tensor] = None
     ) -> Tensor:
         if flags["cropping_mode"] == "resample":  # uses bilinear interpolation to crop
-            transform = cast(Tensor, transform)
+            if not isinstance(transform, Tensor):
+                raise TypeError(f'Expected the transform be a Tensor. Gotcha {type(transform)}')
+
             return crop_by_transform_mat(
                 input,
                 transform,
                 flags["size"],
                 mode=flags["resample"].name.lower(),
                 padding_mode="zeros",
                 align_corners=flags["align_corners"],
@@ -126,16 +127,20 @@
         transform: Optional[Tensor] = None,
         size: Optional[Tuple[int, int]] = None,
     ) -> Tensor:
         if flags["cropping_mode"] != "resample":
             raise NotImplementedError(
                 f"`inverse` is only applicable for resample cropping mode. Got {flags['cropping_mode']}."
             )
-        size = cast(Tuple[int, int], size)
-        transform = cast(Tensor, transform)
+        if not isinstance(size, tuple):
+            raise TypeError(f'Expected the size be a tuple. Gotcha {type(size)}')
+
+        if not isinstance(transform, Tensor):
+            raise TypeError(f'Expected the transform be a Tensor. Gotcha {type(transform)}')
+
         return crop_by_transform_mat(
             input,
             transform[:, :2, :],
             size,
             flags["resample"].name.lower(),
             flags["padding_mode"],
             flags["align_corners"],
```

### Comparing `kornia-0.6.8/kornia/augmentation/_2d/geometric/rotation.py` & `kornia-0.6.9/kornia/augmentation/_2d/geometric/rotation.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,17 +1,16 @@
-from typing import Any, Dict, List, Optional, Tuple, Union, cast
-
-import torch
-from torch import Tensor
+from typing import Any, Dict, List, Optional, Tuple, Union
 
 from kornia.augmentation import random_generator as rg
 from kornia.augmentation._2d.geometric.base import GeometricAugmentationBase2D
 from kornia.constants import Resample
+from kornia.core import Tensor, as_tensor
 from kornia.geometry.transform import affine
 from kornia.geometry.transform.affwarp import _compute_rotation_matrix, _compute_tensor_center
+from kornia.utils.misc import eye_like
 
 
 class RandomRotation(GeometricAugmentationBase2D):
     r"""Apply a random rotation to a tensor image or a batch of tensor images given an amount of degrees.
 
     .. image:: _static/img/RandomRotation.png
 
@@ -45,19 +44,15 @@
                   [0.0000, 0.0029, 0.0000, 0.0176],
                   [0.0029, 1.0000, 1.9883, 0.0000],
                   [0.0000, 0.0088, 1.0117, 1.9649]]]])
         >>> aug.transform_matrix
         tensor([[[ 1.0000, -0.0059,  0.0088],
                  [ 0.0059,  1.0000, -0.0088],
                  [ 0.0000,  0.0000,  1.0000]]])
-        >>> aug.inverse(out)
-        tensor([[[[9.6526e-01, 8.6824e-03, 1.7263e-02, 1.9305e+00],
-                  [8.6398e-03, 2.9485e-03, 5.8971e-03, 1.7365e-02],
-                  [2.9054e-03, 9.9416e-01, 1.9825e+00, 2.3134e-02],
-                  [2.5777e-05, 1.1640e-02, 9.9992e-01, 1.9392e+00]]]])
+        >>> inv = aug.inverse(out)
 
     To apply the exact augmenation again, you may take the advantage of the previous parameter state:
         >>> input = torch.randn(1, 3, 32, 32)
         >>> aug = RandomRotation(degrees=45.0, p=1.)
         >>> (aug(input) == aug(input, params=aug._params)).all()
         tensor(True)
     """
@@ -71,46 +66,46 @@
         same_on_batch: bool = False,
         align_corners: bool = True,
         p: float = 0.5,
         keepdim: bool = False,
         return_transform: Optional[bool] = None,
     ) -> None:
         super().__init__(p=p, return_transform=return_transform, same_on_batch=same_on_batch, keepdim=keepdim)
-        self._param_generator = cast(
-            rg.PlainUniformGenerator, rg.PlainUniformGenerator((degrees, "degrees", 0.0, (-360.0, 360.0)))
-        )
+        self._param_generator = rg.PlainUniformGenerator((degrees, "degrees", 0.0, (-360.0, 360.0)))
+
         self.flags = dict(resample=Resample.get(resample), align_corners=align_corners)
 
     def compute_transformation(self, input: Tensor, params: Dict[str, Tensor], flags: Dict[str, Any]) -> Tensor:
         # TODO: Update to use `get_rotation_matrix2d`
         angles: Tensor = params["degrees"].to(input)
 
         center: Tensor = _compute_tensor_center(input)
         rotation_mat: Tensor = _compute_rotation_matrix(angles, center.expand(angles.shape[0], -1))
 
         # rotation_mat is B x 2 x 3 and we need a B x 3 x 3 matrix
-        trans_mat: Tensor = torch.eye(3, device=input.device, dtype=input.dtype).repeat(input.shape[0], 1, 1)
+        trans_mat: Tensor = eye_like(3, input, shared_memory=False)
         trans_mat[:, 0] = rotation_mat[:, 0]
         trans_mat[:, 1] = rotation_mat[:, 1]
 
         return trans_mat
 
     def apply_transform(
         self, input: Tensor, params: Dict[str, Tensor], flags: Dict[str, Any], transform: Optional[Tensor] = None
     ) -> Tensor:
-        transform = cast(Tensor, transform)
+        if not isinstance(transform, Tensor):
+            raise TypeError(f'Expected the transform be a Tensor. Gotcha {type(transform)}')
 
         return affine(input, transform[..., :2, :3], flags["resample"].name.lower(), "zeros", flags["align_corners"])
 
     def inverse_transform(
         self,
         input: Tensor,
         flags: Dict[str, Any],
         transform: Optional[Tensor] = None,
         size: Optional[Tuple[int, int]] = None,
     ) -> Tensor:
         return self.apply_transform(
             input,
             params=self._params,
-            transform=torch.as_tensor(transform, device=input.device, dtype=input.dtype),
+            transform=as_tensor(transform, device=input.device, dtype=input.dtype),
             flags=flags,
         )
```

### Comparing `kornia-0.6.8/kornia/augmentation/_2d/geometric/thin_plate_spline.py` & `kornia-0.6.9/kornia/augmentation/_2d/geometric/thin_plate_spline.py`

 * *Files 3% similar despite different names*

```diff
@@ -49,15 +49,15 @@
             p=p, return_transform=return_transform, same_on_batch=same_on_batch, p_batch=1.0, keepdim=keepdim
         )
         self.flags = dict(align_corners=align_corners)
         self.dist = torch.distributions.Uniform(-scale, scale)
 
     def generate_parameters(self, shape: torch.Size) -> Dict[str, Tensor]:
         B, _, _, _ = shape
-        src = torch.tensor([[[-1.0, -1.0], [-1.0, 1.0], [1.0, -1.0], [1.0, 1.0], [0.0, 0.0]]]).repeat(B, 1, 1)  # Bx5x2
+        src = torch.tensor([[[-1.0, -1.0], [-1.0, 1.0], [1.0, -1.0], [1.0, 1.0], [0.0, 0.0]]]).expand(B, 5, 2)  # Bx5x2
         dst = src + self.dist.rsample(src.shape)
         return dict(src=src, dst=dst)
 
     # TODO: It is incorrect to return identity
     def compute_transformation(self, input: Tensor, params: Dict[str, Tensor], flags: Dict[str, Any]) -> Tensor:
         return self.identity_matrix(input)
```

### Comparing `kornia-0.6.8/kornia/augmentation/_2d/geometric/vertical_flip.py` & `kornia-0.6.9/kornia/augmentation/_2d/geometric/vertical_flip.py`

 * *Files 5% similar despite different names*

```diff
@@ -46,15 +46,15 @@
         tensor(True)
     """
 
     def compute_transformation(self, input: Tensor, params: Dict[str, Tensor], flags: Dict[str, Any]) -> Tensor:
         h: int = int(params["forward_input_shape"][-2])
         flip_mat: Tensor = torch.tensor([[1, 0, 0], [0, -1, h - 1], [0, 0, 1]], device=input.device, dtype=input.dtype)
 
-        return flip_mat.repeat(input.size(0), 1, 1)
+        return flip_mat.expand(input.shape[0], 3, 3)
 
     def apply_transform(
         self, input: Tensor, params: Dict[str, Tensor], flags: Dict[str, Any], transform: Optional[Tensor] = None
     ) -> Tensor:
         return vflip(input)
 
     def inverse_transform(
```

### Comparing `kornia-0.6.8/kornia/augmentation/_2d/intensity/__init__.py` & `kornia-0.6.9/kornia/augmentation/_2d/intensity/__init__.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,20 +1,24 @@
 from kornia.augmentation._2d.intensity.box_blur import RandomBoxBlur
+from kornia.augmentation._2d.intensity.brightness import RandomBrightness
 from kornia.augmentation._2d.intensity.channel_shuffle import RandomChannelShuffle
 from kornia.augmentation._2d.intensity.color_jiggle import ColorJiggle
 from kornia.augmentation._2d.intensity.color_jitter import ColorJitter
+from kornia.augmentation._2d.intensity.contrast import RandomContrast
 from kornia.augmentation._2d.intensity.denormalize import Denormalize
 from kornia.augmentation._2d.intensity.equalize import RandomEqualize
 from kornia.augmentation._2d.intensity.erasing import RandomErasing
 from kornia.augmentation._2d.intensity.gamma import RandomGamma
 from kornia.augmentation._2d.intensity.gaussian_blur import RandomGaussianBlur
 from kornia.augmentation._2d.intensity.gaussian_noise import RandomGaussianNoise
 from kornia.augmentation._2d.intensity.grayscale import RandomGrayscale
+from kornia.augmentation._2d.intensity.hue import RandomHue
 from kornia.augmentation._2d.intensity.invert import RandomInvert
 from kornia.augmentation._2d.intensity.motion_blur import RandomMotionBlur
 from kornia.augmentation._2d.intensity.normalize import Normalize
 from kornia.augmentation._2d.intensity.planckian_jitter import RandomPlanckianJitter
 from kornia.augmentation._2d.intensity.plasma import RandomPlasmaBrightness, RandomPlasmaContrast, RandomPlasmaShadow
 from kornia.augmentation._2d.intensity.posterize import RandomPosterize
 from kornia.augmentation._2d.intensity.random_rgb_shift import RandomRGBShift
+from kornia.augmentation._2d.intensity.saturation import RandomSaturation
 from kornia.augmentation._2d.intensity.sharpness import RandomSharpness
 from kornia.augmentation._2d.intensity.solarize import RandomSolarize
```

### Comparing `kornia-0.6.8/kornia/augmentation/_2d/intensity/base.py` & `kornia-0.6.9/kornia/augmentation/_2d/intensity/base.py`

 * *Files identical despite different names*

### Comparing `kornia-0.6.8/kornia/augmentation/_2d/intensity/box_blur.py` & `kornia-0.6.9/kornia/augmentation/_2d/intensity/box_blur.py`

 * *Files identical despite different names*

### Comparing `kornia-0.6.8/kornia/augmentation/_2d/intensity/channel_shuffle.py` & `kornia-0.6.9/kornia/augmentation/_2d/intensity/channel_shuffle.py`

 * *Files identical despite different names*

### Comparing `kornia-0.6.8/kornia/augmentation/_2d/intensity/color_jiggle.py` & `kornia-0.6.9/kornia/augmentation/_2d/intensity/color_jiggle.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,14 +1,13 @@
-from typing import Any, Dict, List, Optional, Tuple, Union, cast
-
-from torch import Tensor
+from typing import Any, Dict, List, Optional, Tuple, Union
 
 from kornia.augmentation import random_generator as rg
 from kornia.augmentation._2d.intensity.base import IntensityAugmentationBase2D
 from kornia.constants import pi
+from kornia.core import Tensor
 from kornia.enhance import adjust_brightness, adjust_contrast, adjust_hue, adjust_saturation
 
 
 class ColorJiggle(IntensityAugmentationBase2D):
     r"""Apply a random transformation to the brightness, contrast, saturation and hue of a tensor image.
 
     .. image:: _static/img/ColorJiggle.png
@@ -67,17 +66,15 @@
         return_transform: Optional[bool] = None,
     ) -> None:
         super().__init__(p=p, return_transform=return_transform, same_on_batch=same_on_batch, keepdim=keepdim)
         self.brightness = brightness
         self.contrast = contrast
         self.saturation = saturation
         self.hue = hue
-        self._param_generator = cast(
-            rg.ColorJiggleGenerator, rg.ColorJiggleGenerator(brightness, contrast, saturation, hue)
-        )
+        self._param_generator = rg.ColorJiggleGenerator(brightness, contrast, saturation, hue)
 
     def apply_transform(
         self, input: Tensor, params: Dict[str, Tensor], flags: Dict[str, Any], transform: Optional[Tensor] = None
     ) -> Tensor:
 
         transforms = [
             lambda img: adjust_brightness(img, params["brightness_factor"] - 1),
```

### Comparing `kornia-0.6.8/kornia/augmentation/_2d/intensity/color_jitter.py` & `kornia-0.6.9/kornia/augmentation/_2d/intensity/color_jitter.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,15 +1,14 @@
 import warnings
-from typing import Any, Dict, List, Optional, Tuple, Union, cast
-
-from torch import Tensor
+from typing import Any, Dict, List, Optional, Tuple, Union
 
 from kornia.augmentation import random_generator as rg
 from kornia.augmentation._2d.intensity.base import IntensityAugmentationBase2D
 from kornia.constants import pi
+from kornia.core import Tensor
 from kornia.enhance import (
     adjust_brightness_accumulative,
     adjust_contrast_with_mean_subtraction,
     adjust_hue,
     adjust_saturation_with_gray_subtraction,
 )
 
@@ -88,17 +87,15 @@
                 category=DeprecationWarning,
             )
 
         self.brightness = brightness
         self.contrast = contrast
         self.saturation = saturation
         self.hue = hue
-        self._param_generator = cast(
-            rg.ColorJitterGenerator, rg.ColorJitterGenerator(brightness, contrast, saturation, hue)
-        )
+        self._param_generator = rg.ColorJitterGenerator(brightness, contrast, saturation, hue)
 
     def apply_transform(
         self, input: Tensor, params: Dict[str, Tensor], flags: Dict[str, Any], transform: Optional[Tensor] = None
     ) -> Tensor:
 
         transforms = [
             lambda img: adjust_brightness_accumulative(img, params["brightness_factor"]),
```

### Comparing `kornia-0.6.8/kornia/augmentation/_2d/intensity/denormalize.py` & `kornia-0.6.9/kornia/augmentation/_2d/intensity/denormalize.py`

 * *Files identical despite different names*

### Comparing `kornia-0.6.8/kornia/augmentation/_2d/intensity/equalize.py` & `kornia-0.6.9/kornia/augmentation/_2d/intensity/equalize.py`

 * *Files identical despite different names*

### Comparing `kornia-0.6.8/kornia/augmentation/_2d/intensity/erasing.py` & `kornia-0.6.9/kornia/augmentation/_2d/intensity/erasing.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,14 +1,12 @@
-from typing import Any, Dict, Optional, Tuple, Union, cast
-
-import torch
-from torch import Tensor
+from typing import Any, Dict, Optional, Tuple, Union
 
 from kornia.augmentation import random_generator as rg
 from kornia.augmentation._2d.intensity.base import IntensityAugmentationBase2D
+from kornia.core import Tensor, where
 from kornia.geometry.bbox import bbox_generator, bbox_to_mask
 
 
 class RandomErasing(IntensityAugmentationBase2D):
     r"""Erase a random rectangle of a tensor image according to a probability p value.
 
     .. image:: _static/img/RandomErasing.png
@@ -65,20 +63,20 @@
         keepdim: bool = False,
         return_transform: Optional[bool] = None,
     ) -> None:
         super().__init__(p=p, return_transform=return_transform, same_on_batch=same_on_batch, keepdim=keepdim)
         self.scale = scale
         self.ratio = ratio
         self.value: float = float(value)
-        self._param_generator = cast(rg.RectangleEraseGenerator, rg.RectangleEraseGenerator(scale, ratio, float(value)))
+        self._param_generator = rg.RectangleEraseGenerator(scale, ratio, float(value))
 
     def apply_transform(
         self, input: Tensor, params: Dict[str, Tensor], flags: Dict[str, Any], transform: Optional[Tensor] = None
     ) -> Tensor:
         _, c, h, w = input.size()
         values = params["values"].unsqueeze(-1).unsqueeze(-1).unsqueeze(-1).repeat(1, *input.shape[1:]).to(input)
 
         bboxes = bbox_generator(params["xs"], params["ys"], params["widths"], params["heights"])
         mask = bbox_to_mask(bboxes, w, h)  # Returns B, H, W
         mask = mask.unsqueeze(1).repeat(1, c, 1, 1).to(input)  # Transform to B, c, H, W
-        transformed = torch.where(mask == 1.0, values, input)
+        transformed = where(mask == 1.0, values, input)
         return transformed
```

### Comparing `kornia-0.6.8/kornia/augmentation/_2d/intensity/gamma.py` & `kornia-0.6.9/kornia/augmentation/_2d/intensity/gamma.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,13 +1,12 @@
-from typing import Any, Dict, Optional, Tuple, cast
-
-from torch import Tensor
+from typing import Any, Dict, Optional, Tuple
 
 from kornia.augmentation import random_generator as rg
 from kornia.augmentation._2d.intensity.base import IntensityAugmentationBase2D
+from kornia.core import Tensor
 from kornia.enhance.adjust import adjust_gamma
 
 
 class RandomGamma(IntensityAugmentationBase2D):
     r"""Apply a random transformation to the gamma of a tensor image.
 
     This implementation aligns PIL. Hence, the output is close to TorchVision.
@@ -59,17 +58,16 @@
         gain: Tuple[float, float] = (1.0, 1.0),
         same_on_batch: bool = False,
         p: float = 1.0,
         keepdim: bool = False,
         return_transform: Optional[bool] = None,
     ) -> None:
         super().__init__(p=p, return_transform=return_transform, same_on_batch=same_on_batch, keepdim=keepdim)
-        self._param_generator = cast(
-            rg.PlainUniformGenerator,
-            rg.PlainUniformGenerator((gamma, "gamma_factor", None, None), (gain, "gain_factor", None, None)),
+        self._param_generator = rg.PlainUniformGenerator(
+            (gamma, "gamma_factor", None, None), (gain, "gain_factor", None, None)
         )
 
     def apply_transform(
         self, input: Tensor, params: Dict[str, Tensor], flags: Dict[str, Any], transform: Optional[Tensor] = None
     ) -> Tensor:
         gamma_factor = params["gamma_factor"].to(input)
         gain_factor = params["gain_factor"].to(input)
```

### Comparing `kornia-0.6.8/kornia/augmentation/_2d/intensity/gaussian_blur.py` & `kornia-0.6.9/kornia/augmentation/_2d/intensity/gaussian_blur.py`

 * *Files identical despite different names*

### Comparing `kornia-0.6.8/kornia/augmentation/_2d/intensity/gaussian_noise.py` & `kornia-0.6.9/kornia/augmentation/_2d/intensity/gaussian_noise.py`

 * *Files 12% similar despite different names*

```diff
@@ -2,14 +2,23 @@
 
 import torch
 from torch import Tensor
 
 from kornia.augmentation._2d.intensity.base import IntensityAugmentationBase2D
 
 
+def _randn_like(input: Tensor, mean: float, std: float) -> Tensor:
+    x = torch.randn_like(input)  # Generating on GPU is fastest with `torch.randn_like(...)`
+    if std != 1.0:  # `if` is cheaper than multiplication
+        x *= std
+    if mean != 0.0:  # `if` is cheaper than addition
+        x += mean
+    return x
+
+
 class RandomGaussianNoise(IntensityAugmentationBase2D):
     r"""Add gaussian noise to a batch of multi-dimensional images.
 
     .. image:: _static/img/RandomGaussianNoise.png
 
     Args:
         mean: The mean of the gaussian distribution.
@@ -43,14 +52,18 @@
     ) -> None:
         super().__init__(
             p=p, return_transform=return_transform, same_on_batch=same_on_batch, p_batch=1.0, keepdim=keepdim
         )
         self.flags = dict(mean=mean, std=std)
 
     def generate_parameters(self, shape: torch.Size) -> Dict[str, Tensor]:
-        noise = torch.randn(shape)
-        return dict(noise=noise)
+        return {}
 
     def apply_transform(
         self, input: Tensor, params: Dict[str, Tensor], flags: Dict[str, Any], transform: Optional[Tensor] = None
     ) -> Tensor:
-        return input + params["noise"].to(input.device) * flags["std"] + flags["mean"]
+        if "gaussian_noise" in params:
+            gaussian_noise = params["gaussian_noise"]
+        else:
+            gaussian_noise = _randn_like(input, mean=flags["mean"], std=flags["std"])
+            self._params["gaussian_noise"] = gaussian_noise
+        return input + gaussian_noise
```

### Comparing `kornia-0.6.8/kornia/augmentation/_2d/intensity/grayscale.py` & `kornia-0.6.9/kornia/augmentation/_2d/intensity/grayscale.py`

 * *Files identical despite different names*

### Comparing `kornia-0.6.8/kornia/augmentation/_2d/intensity/invert.py` & `kornia-0.6.9/kornia/augmentation/_2d/intensity/invert.py`

 * *Files identical despite different names*

### Comparing `kornia-0.6.8/kornia/augmentation/_2d/intensity/motion_blur.py` & `kornia-0.6.9/kornia/augmentation/_2d/intensity/motion_blur.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,15 +1,15 @@
 from typing import Any, Dict, List, Optional, Tuple, Union, cast
 
 import torch
-from torch import Tensor
 
 from kornia.augmentation import random_generator as rg
 from kornia.augmentation._2d.intensity.base import IntensityAugmentationBase2D
 from kornia.constants import BorderType, Resample
+from kornia.core import Tensor, tensor
 from kornia.filters import motion_blur
 
 
 class RandomMotionBlur(IntensityAugmentationBase2D):
     r"""Perform motion blur on 2D images (4D tensor).
 
     .. image:: _static/img/RandomMotionBlur.png
@@ -77,20 +77,31 @@
         keepdim: bool = False,
         return_transform: Optional[bool] = None,
     ) -> None:
         super().__init__(p=p, return_transform=return_transform, same_on_batch=same_on_batch, keepdim=keepdim)
         self._param_generator = rg.MotionBlurGenerator(kernel_size, angle, direction)
         self.flags = dict(border_type=BorderType.get(border_type), resample=Resample.get(resample))
 
+    def generate_parameters(self, batch_shape: torch.Size) -> Dict[str, Tensor]:
+        params = super().generate_parameters(batch_shape)
+        params["idx"] = tensor([0]) if batch_shape[0] == 0 else torch.randint(batch_shape[0], (1,))
+        return params
+
     def apply_transform(
         self, input: Tensor, params: Dict[str, Tensor], flags: Dict[str, Any], transform: Optional[Tensor] = None
     ) -> Tensor:
         # sample a kernel size
         kernel_size_list: List[int] = params["ksize_factor"].tolist()
-        idx: int = cast(int, torch.randint(len(kernel_size_list), (1,)).item())
+
+        # 1. We have to apply the same kernel size to all samples in the batch, thus we take the previously
+        # selected random index --- `params["idx"][0]` --- to determine the applied kernel size.
+        # 2. The `VideoSequential` flattens the first two dimensions, effectively creating a larger batch.
+        # Its method `VideoSequential.__repeat_param_across_channels__` repeats the previously selected index,
+        # creating a tensor with equal values. Hence, taking the first one (`params["idx"][0]`) is legit.
+        idx: int = cast(int, params["idx"][0])
         return motion_blur(
             input,
             kernel_size=kernel_size_list[idx],
             angle=params["angle_factor"],
             direction=params["direction_factor"],
             border_type=flags["border_type"].name.lower(),
             mode=flags["resample"].name.lower(),
```

### Comparing `kornia-0.6.8/kornia/augmentation/_2d/intensity/normalize.py` & `kornia-0.6.9/kornia/augmentation/_2d/intensity/normalize.py`

 * *Files identical despite different names*

### Comparing `kornia-0.6.8/kornia/augmentation/_2d/intensity/planckian_jitter.py` & `kornia-0.6.9/kornia/augmentation/_2d/intensity/planckian_jitter.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,13 +1,12 @@
-from typing import Any, Dict, List, Optional, Union, cast
-
-from torch import Tensor, stack
+from typing import Any, Dict, List, Optional, Union
 
 from kornia.augmentation import random_generator as rg
 from kornia.augmentation._2d.intensity.base import IntensityAugmentationBase2D
+from kornia.core import Tensor, stack
 
 _planckian_coeffs = {
     'blackbody': Tensor(
         [
             [0.6743, 0.4029, 0.0013],
             [0.6281, 0.4241, 0.1665],
             [0.5919, 0.4372, 0.2513],
@@ -166,26 +165,32 @@
             select_from = [select_from]
 
         if select_from is not None:
             self.register_buffer('pl', _planckian_coeffs_ratio[mode][select_from])
         else:
             self.register_buffer('pl', _planckian_coeffs_ratio[mode])
 
+        if not isinstance(self.pl, Tensor):
+            raise TypeError(f'Expected the `pl` property be a Tensor. Gotcha {type(self.pl)}')
+
         # the range of the sampling parameters
         _param_min: float = 0.0
-        _param_max: float = float(cast(Tensor, self.pl).shape[0])
+        _param_max: float = float(self.pl.shape[0])
 
-        self._param_generator = cast(rg.PlanckianJitterGenerator, rg.PlanckianJitterGenerator([_param_min, _param_max]))
+        self._param_generator = rg.PlanckianJitterGenerator([_param_min, _param_max])
 
     def apply_transform(
         self, input: Tensor, params: Dict[str, Tensor], flags: Dict[str, Any], transform: Optional[Tensor] = None
     ) -> Tensor:
 
         list_idx = params['idx'].tolist()
-        coeffs = cast(Tensor, self.pl)[list_idx]
+
+        if not isinstance(self.pl, Tensor):
+            raise TypeError(f'Expected the `pl` property be a Tensor. Gotcha {type(self.pl)}')
+        coeffs = self.pl[list_idx]
 
         r_w = coeffs[:, 0][..., None, None]
         b_w = coeffs[:, 1][..., None, None]
 
         r = input[..., 0, :, :] * r_w
         g = input[..., 1, :, :]
         b = input[..., 2, :, :] * b_w
```

### Comparing `kornia-0.6.8/kornia/augmentation/_2d/intensity/plasma.py` & `kornia-0.6.9/kornia/augmentation/_2d/intensity/plasma.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,14 +1,13 @@
-from typing import Any, Dict, Optional, Tuple, cast
-
-from torch import Tensor
+from typing import Any, Dict, Optional, Tuple
 
 from kornia.augmentation import random_generator as rg
 from kornia.augmentation._2d.intensity.base import IntensityAugmentationBase2D
 from kornia.contrib import diamond_square
+from kornia.core import Tensor
 
 
 class RandomPlasmaBrightness(IntensityAugmentationBase2D):
     r"""Adds brightness to the image based on a fractal map generated by the diamond square algorithm.
 
     .. image:: _static/img/RandomPlasmaBrightness.png
 
@@ -43,17 +42,16 @@
         p: float = 0.5,
         keepdim: bool = False,
         return_transform: Optional[bool] = None,
     ) -> None:
         super().__init__(
             p=p, return_transform=return_transform, same_on_batch=same_on_batch, p_batch=1.0, keepdim=keepdim
         )
-        self._param_generator = cast(
-            rg.PlainUniformGenerator,
-            rg.PlainUniformGenerator((roughness, "roughness", None, None), (intensity, "intensity", None, None)),
+        self._param_generator = rg.PlainUniformGenerator(
+            (roughness, "roughness", None, None), (intensity, "intensity", None, None)
         )
 
     def apply_transform(
         self, image: Tensor, params: Dict[str, Tensor], flags: Dict[str, Any], transform: Optional[Tensor] = None
     ) -> Tensor:
         B, C, H, W = image.shape
         roughness = params["roughness"].to(image)
@@ -97,17 +95,15 @@
         p: float = 0.5,
         keepdim: bool = False,
         return_transform: Optional[bool] = None,
     ) -> None:
         super().__init__(
             p=p, return_transform=return_transform, same_on_batch=same_on_batch, p_batch=1.0, keepdim=keepdim
         )
-        self._param_generator = cast(
-            rg.PlainUniformGenerator, rg.PlainUniformGenerator((roughness, "roughness", None, None))
-        )
+        self._param_generator = rg.PlainUniformGenerator((roughness, "roughness", None, None))
 
     def apply_transform(
         self, image: Tensor, params: Dict[str, Tensor], flags: Dict[str, Any], transform: Optional[Tensor] = None
     ) -> Tensor:
         B, C, H, W = image.shape
         roughness = params["roughness"].to(image)
         contrast_map = 4 * diamond_square((B, C, H, W), roughness, device=image.device, dtype=image.dtype)
@@ -152,21 +148,18 @@
         p: float = 0.5,
         keepdim: bool = False,
         return_transform: Optional[bool] = None,
     ) -> None:
         super().__init__(
             p=p, return_transform=return_transform, same_on_batch=same_on_batch, p_batch=1.0, keepdim=keepdim
         )
-        self._param_generator = cast(
-            rg.PlainUniformGenerator,
-            rg.PlainUniformGenerator(
-                (roughness, "roughness", None, None),
-                (shade_intensity, "shade_intensity", None, None),
-                (shade_quantity, "shade_quantity", None, None),
-            ),
+        self._param_generator = rg.PlainUniformGenerator(
+            (roughness, "roughness", None, None),
+            (shade_intensity, "shade_intensity", None, None),
+            (shade_quantity, "shade_quantity", None, None),
         )
 
     def apply_transform(
         self, image: Tensor, params: Dict[str, Tensor], flags: Dict[str, Any], transform: Optional[Tensor] = None
     ) -> Tensor:
         B, _, H, W = image.shape
         roughness = params["roughness"].to(image)
```

### Comparing `kornia-0.6.8/kornia/augmentation/_2d/intensity/posterize.py` & `kornia-0.6.9/kornia/augmentation/_2d/intensity/posterize.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,13 +1,12 @@
-from typing import Any, Dict, Optional, Tuple, Union, cast
-
-from torch import Tensor
+from typing import Any, Dict, Optional, Tuple, Union
 
 from kornia.augmentation import random_generator as rg
 from kornia.augmentation._2d.intensity.base import IntensityAugmentationBase2D
+from kornia.core import Tensor
 from kornia.enhance import posterize
 
 
 class RandomPosterize(IntensityAugmentationBase2D):
     r"""Posterize given tensor image or a batch of tensor images randomly.
 
     .. image:: _static/img/RandomPosterize.png
@@ -52,13 +51,13 @@
         same_on_batch: bool = False,
         p: float = 0.5,
         keepdim: bool = False,
         return_transform: Optional[bool] = None,
     ) -> None:
         super().__init__(p=p, return_transform=return_transform, same_on_batch=same_on_batch, keepdim=keepdim)
         # TODO: the generator should receive the device
-        self._param_generator = cast(rg.PosterizeGenerator, rg.PosterizeGenerator(bits))
+        self._param_generator = rg.PosterizeGenerator(bits)
 
     def apply_transform(
         self, input: Tensor, params: Dict[str, Tensor], flags: Dict[str, Any], transform: Optional[Tensor] = None
     ) -> Tensor:
         return posterize(input, params["bits_factor"].to(input.device))
```

### Comparing `kornia-0.6.8/kornia/augmentation/_2d/intensity/random_rgb_shift.py` & `kornia-0.6.9/kornia/augmentation/_2d/intensity/random_rgb_shift.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,13 +1,12 @@
-from typing import Any, Dict, Optional, cast
-
-from torch import Tensor
+from typing import Any, Dict, Optional
 
 from kornia.augmentation import random_generator as rg
 from kornia.augmentation._2d.intensity.base import IntensityAugmentationBase2D
+from kornia.core import Tensor
 from kornia.enhance import shift_rgb
 
 
 class RandomRGBShift(IntensityAugmentationBase2D):
     """Randomly shift each channel of an image.
 
     Args:
@@ -81,20 +80,17 @@
         b_shift_limit: float = 0.5,
         same_on_batch: bool = False,
         p: float = 0.5,
         keepdim: bool = False,
         return_transform: Optional[bool] = None,
     ) -> None:
         super().__init__(p=p, return_transform=return_transform, same_on_batch=same_on_batch, keepdim=keepdim)
-        self._param_generator = cast(
-            rg.PlainUniformGenerator,
-            rg.PlainUniformGenerator(
-                (r_shift_limit, "r_shift", 0, (-r_shift_limit, r_shift_limit)),
-                (g_shift_limit, "g_shift", 0, (-g_shift_limit, g_shift_limit)),
-                (b_shift_limit, "b_shift", 0, (-b_shift_limit, b_shift_limit)),
-            ),
+        self._param_generator = rg.PlainUniformGenerator(
+            (r_shift_limit, "r_shift", 0, (-r_shift_limit, r_shift_limit)),
+            (g_shift_limit, "g_shift", 0, (-g_shift_limit, g_shift_limit)),
+            (b_shift_limit, "b_shift", 0, (-b_shift_limit, b_shift_limit)),
         )
 
     def apply_transform(
         self, inp: Tensor, params: Dict[str, Tensor], flags: Dict[str, Any], transform: Optional[Tensor] = None
     ) -> Tensor:
         return shift_rgb(inp, params['r_shift'], params['g_shift'], params['b_shift'])
```

### Comparing `kornia-0.6.8/kornia/augmentation/_2d/intensity/sharpness.py` & `kornia-0.6.9/kornia/augmentation/_2d/intensity/sharpness.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,13 +1,12 @@
-from typing import Any, Dict, Optional, Tuple, Union, cast
-
-from torch import Tensor
+from typing import Any, Dict, Optional, Tuple, Union
 
 from kornia.augmentation import random_generator as rg
 from kornia.augmentation._2d.intensity.base import IntensityAugmentationBase2D
+from kornia.core import Tensor
 from kornia.enhance import sharpness
 
 
 class RandomSharpness(IntensityAugmentationBase2D):
     r"""Sharpen given tensor image or a batch of tensor images randomly.
 
     .. image:: _static/img/RandomSharpness.png
@@ -49,16 +48,14 @@
         sharpness: Union[Tensor, float, Tuple[float, float], Tensor] = 0.5,
         same_on_batch: bool = False,
         p: float = 0.5,
         keepdim: bool = False,
         return_transform: Optional[bool] = None,
     ) -> None:
         super().__init__(p=p, return_transform=return_transform, same_on_batch=same_on_batch, keepdim=keepdim)
-        self._param_generator = cast(
-            rg.PlainUniformGenerator, rg.PlainUniformGenerator((sharpness, "sharpness", 0.0, (0, float("inf"))))
-        )
+        self._param_generator = rg.PlainUniformGenerator((sharpness, "sharpness", 0.0, (0, float("inf"))))
 
     def apply_transform(
         self, input: Tensor, params: Dict[str, Tensor], flags: Dict[str, Any], transform: Optional[Tensor] = None
     ) -> Tensor:
         factor = params["sharpness"]
         return sharpness(input, factor)
```

### Comparing `kornia-0.6.8/kornia/augmentation/_2d/intensity/solarize.py` & `kornia-0.6.9/kornia/augmentation/_2d/intensity/solarize.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,13 +1,12 @@
-from typing import Any, Dict, List, Optional, Tuple, Union, cast
-
-from torch import Tensor
+from typing import Any, Dict, List, Optional, Tuple, Union
 
 from kornia.augmentation import random_generator as rg
 from kornia.augmentation._2d.intensity.base import IntensityAugmentationBase2D
+from kornia.core import Tensor
 from kornia.enhance import solarize
 
 
 class RandomSolarize(IntensityAugmentationBase2D):
     r"""Solarize given tensor image or a batch of tensor images randomly.
 
     .. image:: _static/img/RandomSolarize.png
@@ -55,19 +54,16 @@
         additions: Union[Tensor, float, Tuple[float, float], List[float]] = 0.1,
         same_on_batch: bool = False,
         p: float = 0.5,
         keepdim: bool = False,
         return_transform: Optional[bool] = None,
     ) -> None:
         super().__init__(p=p, return_transform=return_transform, same_on_batch=same_on_batch, keepdim=keepdim)
-        self._param_generator = cast(
-            rg.PlainUniformGenerator,
-            rg.PlainUniformGenerator(
-                (thresholds, "thresholds", 0.5, (0.0, 1.0)), (additions, "additions", 0.0, (-0.5, 0.5))
-            ),
+        self._param_generator = rg.PlainUniformGenerator(
+            (thresholds, "thresholds", 0.5, (0.0, 1.0)), (additions, "additions", 0.0, (-0.5, 0.5))
         )
 
     def apply_transform(
         self, input: Tensor, params: Dict[str, Tensor], flags: Dict[str, Any], transform: Optional[Tensor] = None
     ) -> Tensor:
         thresholds = params["thresholds"]
         additions: Optional[Tensor]
```

### Comparing `kornia-0.6.8/kornia/augmentation/_2d/mix/jigsaw.py` & `kornia-0.6.9/kornia/augmentation/_2d/mix/jigsaw.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-from typing import Any, Dict, List, Optional, Tuple, Union, cast
+from typing import Any, Dict, List, Optional, Tuple, Union
 
 import torch
 
 from kornia.augmentation import random_generator as rg
 from kornia.augmentation._2d.mix.base import MixAugmentationBaseV2
 from kornia.constants import DataKey
 from kornia.core import Tensor
@@ -44,15 +44,15 @@
         data_keys: List[Union[str, int, DataKey]] = [DataKey.INPUT],
         p: float = 0.5,
         same_on_batch: bool = False,
         keepdim: bool = False,
         ensure_perm: bool = True,
     ) -> None:
         super().__init__(p=p, p_batch=1.0, same_on_batch=same_on_batch, keepdim=keepdim, data_keys=data_keys)
-        self._param_generator = cast(rg.JigsawGenerator, rg.JigsawGenerator(grid, ensure_perm))
+        self._param_generator = rg.JigsawGenerator(grid, ensure_perm)
         self.flags = dict(grid=grid)
 
     def apply_transform(
         self, input: Tensor, params: Dict[str, Tensor], maybe_flags: Optional[Dict[str, Any]] = None
     ) -> Tensor:
         # different from the Base class routine. This function will not refer to any non-transformation images.
         to_apply = params['batch_prob']
```

### Comparing `kornia-0.6.8/kornia/augmentation/_2d/mix/mosaic.py` & `kornia-0.6.9/kornia/augmentation/_2d/mix/mosaic.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,16 +1,15 @@
-from typing import Any, Dict, List, Optional, Tuple, Union, cast
+from typing import Any, Dict, List, Optional, Tuple, Union
 
 import torch
-from torch.nn.functional import pad
 
 from kornia.augmentation import random_generator as rg
 from kornia.augmentation._2d.mix.base import MixAugmentationBaseV2
 from kornia.constants import DataKey, Resample
-from kornia.core import Tensor
+from kornia.core import Tensor, as_tensor, concatenate, pad, zeros
 from kornia.geometry.boxes import Boxes
 from kornia.geometry.transform import crop_by_indices, crop_by_transform_mat, get_perspective_transform
 from kornia.testing import KORNIA_UNWRAP
 from kornia.utils import eye_like
 
 __all__ = ["RandomMosaic"]
 
@@ -72,17 +71,16 @@
         padding_mode: str = "constant",
         resample: Union[str, int, Resample] = Resample.BILINEAR.name,
         align_corners: bool = True,
         cropping_mode: str = "slice",
     ) -> None:
         super().__init__(p=p, p_batch=1.0, same_on_batch=False, keepdim=keepdim, data_keys=data_keys)
         self.start_ratio_range = start_ratio_range
-        self._param_generator = cast(
-            rg.MosaicGenerator, rg.MosaicGenerator(output_size, mosaic_grid, start_ratio_range)
-        )
+        self._param_generator = rg.MosaicGenerator(output_size, mosaic_grid, start_ratio_range)
+
         self.flags = dict(
             mosaic_grid=mosaic_grid,
             output_size=output_size,
             min_bbox_size=min_bbox_size,
             padding_mode=padding_mode,
             resample=Resample.get(resample),
             align_corners=align_corners,
@@ -90,19 +88,19 @@
         )
 
     def apply_transform_mask(self, input: Tensor, params: Dict[str, Tensor], flags: Dict[str, Any]) -> Tensor:
         raise NotImplementedError
 
     @torch.no_grad()
     def apply_transform_boxes(self, input: Boxes, params: Dict[str, Tensor], flags: Dict[str, Any]) -> Boxes:
-        src_box = torch.as_tensor(params["src"], device=input.device, dtype=input.dtype)
-        dst_box = torch.as_tensor(params["dst"], device=input.device, dtype=input.dtype)
+        src_box = as_tensor(params["src"], device=input.device, dtype=input.dtype)
+        dst_box = as_tensor(params["dst"], device=input.device, dtype=input.dtype)
         # Boxes is BxNx4x2 only.
-        batch_shapes = torch.as_tensor(params["batch_shapes"], device=input.device, dtype=input.dtype)
-        offset = torch.zeros((len(params["batch_prob"]), 2), device=input.device, dtype=input.dtype)  # Bx2
+        batch_shapes = as_tensor(params["batch_shapes"], device=input.device, dtype=input.dtype)
+        offset = zeros((len(params["batch_prob"]), 2), device=input.device, dtype=input.dtype)  # Bx2
         # NOTE: not a pretty good line I think.
         offset_end = dst_box[0, 2].repeat(input.data.shape[0], 1)
         idx = torch.arange(0, input.data.shape[0], device=input.device, dtype=torch.long)[params["batch_prob"]]
 
         maybe_out_boxes: Optional[Boxes] = None
         for i in range(flags['mosaic_grid'][0]):
             for j in range(flags['mosaic_grid'][1]):
@@ -136,31 +134,33 @@
         out = []
         for i in range(flags['mosaic_grid'][0]):
             out_row = []
             for j in range(flags['mosaic_grid'][1]):
                 img_idx = flags['mosaic_grid'][1] * i + j
                 image = input[params["permutation"][:, img_idx]]
                 out_row.append(image)
-            out.append(torch.cat(out_row, dim=-2))
-        return torch.cat(out, dim=-1)
+            out.append(concatenate(out_row, -2))
+        return concatenate(out, -1)
 
     def compute_transformation(self, input: Tensor, params: Dict[str, Tensor], flags: Dict[str, Any]) -> Tensor:
         if flags["cropping_mode"] == "resample":
             transform: Tensor = get_perspective_transform(params["src"].to(input), params["dst"].to(input))
             return transform
         if flags["cropping_mode"] == "slice":  # Skip the computation for slicing.
             return eye_like(3, input)
         raise NotImplementedError(f"Not supported type: {flags['cropping_mode']}.")
 
     def _crop_images(
         self, input: Tensor, params: Dict[str, Tensor], flags: Dict[str, Any], transform: Optional[Tensor] = None
     ) -> Tensor:
         flags = self.flags if flags is None else flags
         if flags["cropping_mode"] == "resample":  # uses bilinear interpolation to crop
-            transform = cast(Tensor, transform)
+            if not isinstance(transform, Tensor):
+                raise TypeError(f'Expected the transform to be a Tensor. Gotcha {type(transform)}')
+
             # Fit the arg to F.pad
             if flags['padding_mode'] == "constant":
                 padding_mode = "zeros"
             elif flags['padding_mode'] == "replicate":
                 padding_mode = "border"
             elif flags['padding_mode'] == "reflect":
                 padding_mode = "reflection"
```

### Comparing `kornia-0.6.8/kornia/augmentation/_3d/base.py` & `kornia-0.6.9/kornia/augmentation/_3d/base.py`

 * *Files identical despite different names*

### Comparing `kornia-0.6.8/kornia/augmentation/_3d/geometric/__init__.py` & `kornia-0.6.9/kornia/augmentation/_3d/geometric/__init__.py`

 * *Files identical despite different names*

### Comparing `kornia-0.6.8/kornia/augmentation/_3d/geometric/affine.py` & `kornia-0.6.9/kornia/augmentation/_3d/geometric/affine.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,14 +1,13 @@
-from typing import Any, Dict, Optional, Tuple, Union, cast
-
-from torch import Tensor
+from typing import Any, Dict, Optional, Tuple, Union
 
 from kornia.augmentation import random_generator as rg
 from kornia.augmentation._3d.base import AugmentationBase3D
 from kornia.constants import Resample
+from kornia.core import Tensor
 from kornia.geometry import deg2rad, get_affine_matrix3d, warp_affine3d
 
 
 class RandomAffine3D(AugmentationBase3D):
     r"""Apply affine transformation 3D volumes (5D tensor).
 
     The transformation is computed so that the center is kept invariant.
@@ -122,15 +121,15 @@
         super().__init__(p=p, return_transform=return_transform, same_on_batch=same_on_batch, keepdim=keepdim)
         self.degrees = degrees
         self.shears = shears
         self.translate = translate
         self.scale = scale
 
         self.flags = dict(resample=Resample.get(resample), align_corners=align_corners)
-        self._param_generator = cast(rg.AffineGenerator3D, rg.AffineGenerator3D(degrees, translate, scale, shears))
+        self._param_generator = rg.AffineGenerator3D(degrees, translate, scale, shears)
 
     def compute_transformation(self, input: Tensor, params: Dict[str, Tensor], flags: Dict[str, Any]) -> Tensor:
         transform: Tensor = get_affine_matrix3d(
             params["translations"],
             params["center"],
             params["scale"],
             params["angles"],
@@ -142,15 +141,17 @@
             deg2rad(params["szy"]),
         ).to(input)
         return transform
 
     def apply_transform(
         self, input: Tensor, params: Dict[str, Tensor], flags: Dict[str, Any], transform: Optional[Tensor] = None
     ) -> Tensor:
-        transform = cast(Tensor, transform)
+        if not isinstance(transform, Tensor):
+            raise TypeError(f'Expected the transform to be a Tensor. Gotcha {type(transform)}')
+
         return warp_affine3d(
             input,
             transform[:, :3, :],
             (input.shape[-3], input.shape[-2], input.shape[-1]),
             flags["resample"].name.lower(),
             align_corners=flags["align_corners"],
         )
```

### Comparing `kornia-0.6.8/kornia/augmentation/_3d/geometric/center_crop.py` & `kornia-0.6.9/kornia/augmentation/_3d/geometric/center_crop.py`

 * *Files identical despite different names*

### Comparing `kornia-0.6.8/kornia/augmentation/_3d/geometric/crop.py` & `kornia-0.6.9/kornia/augmentation/_3d/geometric/crop.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,15 +1,13 @@
-from typing import Any, Dict, Optional, Tuple, Union, cast
-
-from torch import Tensor
-from torch.nn.functional import pad
+from typing import Any, Dict, Optional, Tuple, Union
 
 from kornia.augmentation import random_generator as rg
 from kornia.augmentation._3d.base import AugmentationBase3D
 from kornia.constants import Resample
+from kornia.core import Tensor, pad
 from kornia.geometry import crop_by_transform_mat3d, get_perspective_transform3d
 
 
 class RandomCrop3D(AugmentationBase3D):
     r"""Apply random crop on 3D volumes (5D tensor).
 
     Crops random sub-volumes on a given size.
@@ -89,26 +87,26 @@
             padding=padding,
             pad_if_needed=pad_if_needed,
             padding_mode=padding_mode,
             fill=fill,
             resample=Resample.get(resample),
             align_corners=align_corners,
         )
-        self._param_generator = cast(rg.CropGenerator3D, rg.CropGenerator3D(size, None))
+        self._param_generator = rg.CropGenerator3D(size, None)
 
     def precrop_padding(self, input: Tensor, flags: Optional[Dict[str, Any]] = None) -> Tensor:
         flags = self.flags if flags is None else flags
         padding = flags["padding"]
         if padding is not None:
             if isinstance(padding, int):
                 padding = [padding, padding, padding, padding, padding, padding]
             elif isinstance(padding, (tuple, list)) and len(padding) == 3:
                 padding = [padding[0], padding[0], padding[1], padding[1], padding[2], padding[2]]
             elif isinstance(padding, (tuple, list)) and len(padding) == 6:
-                padding = [padding[0], padding[1], padding[2], padding[3], padding[4], padding[5]]  # type: ignore
+                padding = [padding[0], padding[1], padding[2], padding[3], padding[4], padding[5]]
             else:
                 raise ValueError(f"`padding` must be an integer, 3-element-list or 6-element-list. Got {padding}.")
             input = pad(input, padding, value=flags["fill"], mode=flags["padding_mode"])
 
         if flags["pad_if_needed"] and input.shape[-3] < flags["size"][0]:
             padding = [0, 0, 0, 0, flags["size"][0] - input.shape[-3], flags["size"][0] - input.shape[-3]]
             input = pad(input, padding, value=flags["fill"], mode=flags["padding_mode"])
@@ -127,16 +125,18 @@
         transform: Tensor = get_perspective_transform3d(params["src"].to(input), params["dst"].to(input))
         transform = transform.expand(input.shape[0], -1, -1)
         return transform
 
     def apply_transform(
         self, input: Tensor, params: Dict[str, Tensor], flags: Dict[str, Any], transform: Optional[Tensor] = None
     ) -> Tensor:
-        transform = cast(Tensor, transform)
+        if not isinstance(transform, Tensor):
+            raise TypeError(f'Expected the transform to be a Tensor. Gotcha {type(transform)}')
+
         return crop_by_transform_mat3d(
             input, transform, flags["size"], mode=flags["resample"].name.lower(), align_corners=flags["align_corners"]
         )
 
-    def forward(self, input: Tensor, params: Optional[Dict[str, Tensor]] = None, **kwargs) -> Tensor:  # type: ignore
+    def forward(self, input: Tensor, params: Optional[Dict[str, Tensor]] = None, **kwargs) -> Tensor:
         # TODO: need to align 2D implementations
         input = self.precrop_padding(input)
-        return super().forward(input, params)  # type:ignore
+        return super().forward(input, params)
```

### Comparing `kornia-0.6.8/kornia/augmentation/_3d/geometric/depthical_flip.py` & `kornia-0.6.9/kornia/augmentation/_3d/geometric/depthical_flip.py`

 * *Files 2% similar despite different names*

```diff
@@ -67,13 +67,13 @@
         super().__init__(p=p, return_transform=return_transform, same_on_batch=same_on_batch, keepdim=keepdim)
 
     def compute_transformation(self, input: Tensor, params: Dict[str, Tensor], flags: Dict[str, Any]) -> Tensor:
         d: int = input.shape[-3]
         flip_mat: Tensor = torch.tensor(
             [[1, 0, 0, 0], [0, 1, 0, 0], [0, 0, -1, d - 1], [0, 0, 0, 1]], device=input.device, dtype=input.dtype
         )
-        return flip_mat.repeat(input.size(0), 1, 1)
+        return flip_mat.expand(input.shape[0], 4, 4)
 
     def apply_transform(
         self, input: Tensor, params: Dict[str, Tensor], flags: Dict[str, Any], transform: Optional[Tensor] = None
     ) -> Tensor:
         return torch.flip(input, [-3])
```

### Comparing `kornia-0.6.8/kornia/augmentation/_3d/geometric/horizontal_flip.py` & `kornia-0.6.9/kornia/augmentation/_3d/geometric/horizontal_flip.py`

 * *Files 6% similar despite different names*

```diff
@@ -61,13 +61,13 @@
         super().__init__(p=p, return_transform=return_transform, same_on_batch=same_on_batch, keepdim=keepdim)
 
     def compute_transformation(self, input: Tensor, params: Dict[str, Tensor], flags: Dict[str, Any]) -> Tensor:
         w: int = input.shape[-1]
         flip_mat: Tensor = torch.tensor(
             [[-1, 0, 0, w - 1], [0, 1, 0, 0], [0, 0, 1, 0], [0, 0, 0, 1]], device=input.device, dtype=input.dtype
         )
-        return flip_mat.repeat(input.size(0), 1, 1)
+        return flip_mat.expand(input.shape[0], 4, 4)
 
     def apply_transform(
         self, input: Tensor, params: Dict[str, Tensor], flags: Dict[str, Any], transform: Optional[Tensor] = None
     ) -> Tensor:
         return torch.flip(input, [-1])
```

### Comparing `kornia-0.6.8/kornia/augmentation/_3d/geometric/perspective.py` & `kornia-0.6.9/kornia/augmentation/_3d/geometric/perspective.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,14 +1,13 @@
-from typing import Any, Dict, Optional, Union, cast
-
-from torch import Tensor
+from typing import Any, Dict, Optional, Union
 
 from kornia.augmentation import random_generator as rg
 from kornia.augmentation._3d.base import AugmentationBase3D
 from kornia.constants import Resample
+from kornia.core import Tensor
 from kornia.geometry import get_perspective_transform3d, warp_perspective3d
 
 
 class RandomPerspective3D(AugmentationBase3D):
     r"""Apply andom perspective transformation to 3D volumes (5D tensor).
 
     Args:
@@ -72,23 +71,25 @@
         align_corners: bool = False,
         p: float = 0.5,
         keepdim: bool = False,
         return_transform: Optional[bool] = None,
     ) -> None:
         super().__init__(p=p, return_transform=return_transform, same_on_batch=same_on_batch, keepdim=keepdim)
         self.flags = dict(resample=Resample.get(resample), align_corners=align_corners)
-        self._param_generator = cast(rg.PerspectiveGenerator3D, rg.PerspectiveGenerator3D(distortion_scale))
+        self._param_generator = rg.PerspectiveGenerator3D(distortion_scale)
 
     def compute_transformation(self, input: Tensor, params: Dict[str, Tensor], flags: Dict[str, Any]) -> Tensor:
         return get_perspective_transform3d(params["start_points"], params["end_points"]).to(input)
 
     def apply_transform(
         self, input: Tensor, params: Dict[str, Tensor], flags: Dict[str, Any], transform: Optional[Tensor] = None
     ) -> Tensor:
-        transform = cast(Tensor, transform)
+        if not isinstance(transform, Tensor):
+            raise TypeError(f'Expected the transform to be a Tensor. Gotcha {type(transform)}')
+
         return warp_perspective3d(
             input,
             transform,
             (input.shape[-3], input.shape[-2], input.shape[-1]),
             flags=flags["resample"].name.lower(),
             align_corners=flags["align_corners"],
         )
```

### Comparing `kornia-0.6.8/kornia/augmentation/_3d/geometric/rotation.py` & `kornia-0.6.9/kornia/augmentation/_3d/geometric/rotation.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,15 +1,14 @@
-from typing import Any, Dict, Optional, Tuple, Union, cast
-
-from torch import Tensor
+from typing import Any, Dict, Optional, Tuple, Union
 
 import kornia
 from kornia.augmentation import random_generator as rg
 from kornia.augmentation._3d.base import AugmentationBase3D
 from kornia.constants import Resample
+from kornia.core import Tensor
 from kornia.geometry import affine3d
 from kornia.geometry.transform.affwarp import _compute_rotation_matrix3d, _compute_tensor_center3d
 
 
 class RandomRotation3D(AugmentationBase3D):
     r"""Apply random rotations to 3D volumes (5D tensor).
 
@@ -84,15 +83,15 @@
         align_corners: bool = False,
         p: float = 0.5,
         keepdim: bool = False,
         return_transform: Optional[bool] = None,
     ) -> None:
         super().__init__(p=p, return_transform=return_transform, same_on_batch=same_on_batch, keepdim=keepdim)
         self.flags = dict(resample=Resample.get(resample), align_corners=align_corners)
-        self._param_generator = cast(rg.RotationGenerator3D, rg.RotationGenerator3D(degrees))
+        self._param_generator = rg.RotationGenerator3D(degrees)
 
     def compute_transformation(self, input: Tensor, params: Dict[str, Tensor], flags: Dict[str, Any]) -> Tensor:
         yaw: Tensor = params["yaw"].to(input)
         pitch: Tensor = params["pitch"].to(input)
         roll: Tensor = params["roll"].to(input)
 
         center: Tensor = _compute_tensor_center3d(input)
@@ -105,9 +104,11 @@
         trans_mat[:, 2] = rotation_mat[:, 2]
 
         return trans_mat
 
     def apply_transform(
         self, input: Tensor, params: Dict[str, Tensor], flags: Dict[str, Any], transform: Optional[Tensor] = None
     ) -> Tensor:
-        transform = cast(Tensor, transform)
+        if not isinstance(transform, Tensor):
+            raise TypeError(f'Expected the transform to be a Tensor. Gotcha {type(transform)}')
+
         return affine3d(input, transform[..., :3, :4], flags["resample"].name.lower(), "zeros", flags["align_corners"])
```

### Comparing `kornia-0.6.8/kornia/augmentation/_3d/geometric/vertical_flip.py` & `kornia-0.6.9/kornia/augmentation/_3d/geometric/vertical_flip.py`

 * *Files 1% similar despite different names*

```diff
@@ -67,13 +67,13 @@
         super().__init__(p=p, return_transform=return_transform, same_on_batch=same_on_batch, keepdim=keepdim)
 
     def compute_transformation(self, input: Tensor, params: Dict[str, Tensor], flags: Dict[str, Any]) -> Tensor:
         h: int = input.shape[-2]
         flip_mat: Tensor = torch.tensor(
             [[1, 0, 0, 0], [0, -1, 0, h - 1], [0, 0, 1, 0], [0, 0, 0, 1]], device=input.device, dtype=input.dtype
         )
-        return flip_mat.repeat(input.size(0), 1, 1)
+        return flip_mat.expand(input.shape[0], 4, 4)
 
     def apply_transform(
         self, input: Tensor, params: Dict[str, Tensor], flags: Dict[str, Any], transform: Optional[Tensor] = None
     ) -> Tensor:
         return torch.flip(input, [-2])
```

### Comparing `kornia-0.6.8/kornia/augmentation/_3d/intensity/equalize.py` & `kornia-0.6.9/kornia/augmentation/_3d/intensity/equalize.py`

 * *Files identical despite different names*

### Comparing `kornia-0.6.8/kornia/augmentation/_3d/intensity/motion_blur.py` & `kornia-0.6.9/kornia/augmentation/_3d/intensity/motion_blur.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,14 +1,13 @@
-from typing import Any, Dict, Optional, Tuple, Union, cast
-
-from torch import Tensor
+from typing import Any, Dict, Optional, Tuple, Union
 
 from kornia.augmentation import random_generator as rg
 from kornia.augmentation._3d.base import AugmentationBase3D
 from kornia.constants import BorderType, Resample
+from kornia.core import Tensor
 from kornia.filters import motion_blur3d
 
 
 class RandomMotionBlur3D(AugmentationBase3D):
     r"""Apply random motion blur on 3D volumes (5D tensor).
 
     Args:
@@ -92,23 +91,23 @@
         keepdim: bool = False,
         return_transform: Optional[bool] = None,
     ) -> None:
         super().__init__(
             p=p, return_transform=return_transform, same_on_batch=same_on_batch, p_batch=1.0, keepdim=keepdim
         )
         self.flags = dict(border_type=BorderType.get(border_type), resample=Resample.get(resample))
-        self._param_generator = cast(rg.MotionBlurGenerator3D, rg.MotionBlurGenerator3D(kernel_size, angle, direction))
+        self._param_generator = rg.MotionBlurGenerator3D(kernel_size, angle, direction)
 
     def compute_transformation(self, input: Tensor, params: Dict[str, Tensor], flags: Dict[str, Any]) -> Tensor:
         return self.identity_matrix(input)
 
     def apply_transform(
         self, input: Tensor, params: Dict[str, Tensor], flags: Dict[str, Any], transform: Optional[Tensor] = None
     ) -> Tensor:
-        kernel_size: int = cast(int, params["ksize_factor"].unique().item())
+        kernel_size = int(params["ksize_factor"].unique().item())
         angle = params["angle_factor"]
         direction = params["direction_factor"]
         return motion_blur3d(
             input,
             kernel_size,
             angle,
             direction,
```

### Comparing `kornia-0.6.8/kornia/augmentation/__init__.py` & `kornia-0.6.9/kornia/augmentation/__init__.py`

 * *Files 2% similar despite different names*

```diff
@@ -4,53 +4,55 @@
     ColorJitter,
     Denormalize,
     LongestMaxSize,
     Normalize,
     PadTo,
     RandomAffine,
     RandomBoxBlur,
+    RandomBrightness,
     RandomChannelShuffle,
+    RandomContrast,
     RandomCrop,
-    RandomCutMix,
     RandomCutMixV2,
     RandomElasticTransform,
     RandomEqualize,
     RandomErasing,
     RandomFisheye,
     RandomGamma,
     RandomGaussianBlur,
     RandomGaussianNoise,
     RandomGrayscale,
     RandomHorizontalFlip,
+    RandomHue,
     RandomInvert,
     RandomJigsaw,
-    RandomMixUp,
     RandomMixUpV2,
     RandomMosaic,
     RandomMotionBlur,
     RandomPerspective,
     RandomPlanckianJitter,
     RandomPlasmaBrightness,
     RandomPlasmaContrast,
     RandomPlasmaShadow,
     RandomPosterize,
     RandomResizedCrop,
     RandomRGBShift,
     RandomRotation,
+    RandomSaturation,
     RandomSharpness,
     RandomSolarize,
     RandomThinPlateSpline,
     RandomVerticalFlip,
     Resize,
     SmallestMaxSize,
 )
 from kornia.augmentation._2d.base import AugmentationBase2D
 from kornia.augmentation._2d.geometric.base import GeometricAugmentationBase2D
 from kornia.augmentation._2d.intensity.base import IntensityAugmentationBase2D
-from kornia.augmentation._2d.mix.base import MixAugmentationBase
+from kornia.augmentation._2d.mix.base import MixAugmentationBaseV2
 from kornia.augmentation._3d import (
     CenterCrop3D,
     RandomAffine3D,
     RandomCrop3D,
     RandomDepthicalFlip3D,
     RandomEqualize3D,
     RandomHorizontalFlip3D,
@@ -69,43 +71,47 @@
     VideoSequential,
 )
 
 __all__ = [
     "AugmentationBase2D",
     "GeometricAugmentationBase2D",
     "IntensityAugmentationBase2D",
-    "MixAugmentationBase",
+    "MixAugmentationBaseV2",
     "CenterCrop",
     "ColorJiggle",
     "ColorJitter",
     "Normalize",
     "Denormalize",
     "LongestMaxSize",
     "PadTo",
     "RandomAffine",
     "RandomBoxBlur",
-    "RandomCrop",
+    "RandomBrightness",
     "RandomChannelShuffle",
+    "RandomContrast",
+    "RandomCrop",
     "RandomErasing",
     "RandomElasticTransform",
     "RandomFisheye",
     "RandomGamma",
     "RandomGrayscale",
     "RandomGaussianBlur",
     "RandomGaussianNoise",
     "RandomHorizontalFlip",
+    "RandomHue",
     "RandomVerticalFlip",
     "RandomPerspective",
     "RandomPlanckianJitter",
     "RandomPlasmaShadow",
     "RandomPlasmaBrightness",
     "RandomPlasmaContrast",
     "RandomResizedCrop",
     "RandomRotation",
     "RandomRGBShift",
+    "RandomSaturation",
     "RandomSolarize",
     "RandomSharpness",
     "RandomPosterize",
     "RandomEqualize",
     "RandomMotionBlur",
     "RandomInvert",
     "RandomThinPlateSpline",
```

### Comparing `kornia-0.6.8/kornia/augmentation/base.py` & `kornia-0.6.9/kornia/augmentation/base.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,22 +1,33 @@
 from enum import Enum
-from typing import Any, Dict, Optional, Tuple, Union
+from typing import Any, Callable, Dict, Optional, Tuple, Union
 
 import torch
-import torch.nn as nn
-from torch import Tensor
 from torch.distributions import Bernoulli
 
 from kornia.augmentation.random_generator import RandomGeneratorBase
 from kornia.augmentation.utils import _adapted_sampling, _transform_output_shape, override_parameters
+from kornia.core import Module, Tensor, tensor
 
 TensorWithTransformMat = Union[Tensor, Tuple[Tensor, Tensor]]
 
 
-class _BasicAugmentationBase(nn.Module):
+# Trick mypy into not applying contravariance rules to inputs by defining
+# forward as a value, rather than a function.  See also
+# https://github.com/python/mypy/issues/8795
+# Based on the trick that torch.nn.Module does for the forward method
+def _apply_transform_unimplemented(self, *input: Any) -> Tensor:
+    r"""Defines the computation performed at every call.
+
+    Should be overridden by all subclasses.
+    """
+    raise NotImplementedError(f"Module [{type(self).__name__}] is missing the required \"apply_tranform\" function")
+
+
+class _BasicAugmentationBase(Module):
     r"""_BasicAugmentationBase base class for customized augmentation implementations.
 
     Plain augmentation base class without the functionality of transformation matrix calculations.
     By default, the random computations will be happened on CPU with ``torch.get_default_dtype()``.
     To change this behaviour, please use ``set_rng_device_and_dtype``.
 
     For automatically generating the corresponding ``__repr__`` with full customized parameters, you may need to
@@ -46,14 +57,16 @@
             self._p_gen = Bernoulli(self.p)
         if p_batch != 0.0 or p_batch != 1.0:
             self._p_batch_gen = Bernoulli(self.p_batch)
         self._param_generator: Optional[RandomGeneratorBase] = None
         self.flags: Dict[str, Any] = {}
         self.set_rng_device_and_dtype(torch.device('cpu'), torch.get_default_dtype())
 
+    apply_transform: Callable[..., Tensor] = _apply_transform_unimplemented
+
     def __repr__(self) -> str:
         txt = f"p={self.p}, p_batch={self.p_batch}, same_on_batch={self.same_on_batch}"
         if isinstance(self._param_generator, RandomGeneratorBase):
             txt = f"{str(self._param_generator)}, {txt}"
         for k, v in self.flags.items():
             if isinstance(v, Enum):
                 txt += f", {k}={v.name.lower()}"
@@ -64,26 +77,23 @@
     def __unpack_input__(self, input: Tensor) -> Tensor:
         return input
 
     def transform_tensor(self, input: Tensor) -> Tensor:
         """Standardize input tensors."""
         raise NotImplementedError
 
-    def transform_output_tensor(self, output: Tensor, output_shape: Tuple) -> Tensor:
+    def transform_output_tensor(self, output: Tensor, output_shape: Tuple[int, ...]) -> Tensor:
         """Standardize output tensors."""
         return _transform_output_shape(output, output_shape) if self.keepdim else output
 
     def generate_parameters(self, batch_shape: torch.Size) -> Dict[str, Tensor]:
         if self._param_generator is not None:
             return self._param_generator(batch_shape, self.same_on_batch)
         return {}
 
-    def apply_transform(self, input: Tensor, params: Dict[str, Tensor], flags: Dict[str, Any]) -> Tensor:
-        raise NotImplementedError
-
     def set_rng_device_and_dtype(self, device: torch.device, dtype: torch.dtype) -> None:
         """Change the random generation device and dtype.
 
         Note:
             The generated random numbers are not reproducible across different devices and dtypes.
         """
         self.device = device
@@ -92,26 +102,26 @@
             self._param_generator.set_rng_device_and_dtype(device, dtype)
 
     def __batch_prob_generator__(
         self, batch_shape: torch.Size, p: float, p_batch: float, same_on_batch: bool
     ) -> Tensor:
         batch_prob: Tensor
         if p_batch == 1:
-            batch_prob = torch.tensor([True])
+            batch_prob = tensor([True])
         elif p_batch == 0:
-            batch_prob = torch.tensor([False])
+            batch_prob = tensor([False])
         else:
             batch_prob = _adapted_sampling((1,), self._p_batch_gen, same_on_batch).bool()
 
         if batch_prob.sum().item() == 1:
             elem_prob: Tensor
             if p == 1:
-                elem_prob = torch.tensor([True] * batch_shape[0])
+                elem_prob = tensor([True] * batch_shape[0])
             elif p == 0:
-                elem_prob = torch.tensor([False] * batch_shape[0])
+                elem_prob = tensor([False] * batch_shape[0])
             else:
                 elem_prob = _adapted_sampling((batch_shape[0],), self._p_gen, same_on_batch).bool()
             batch_prob = batch_prob * elem_prob
         else:
             batch_prob = batch_prob.repeat(batch_shape[0])
         return batch_prob
 
@@ -136,22 +146,22 @@
         to_apply = self.__batch_prob_generator__(batch_shape, self.p, self.p_batch, self.same_on_batch)
         _params = self.generate_parameters(torch.Size((int(to_apply.sum().item()), *batch_shape[1:])))
         if _params is None:
             _params = {}
         _params['batch_prob'] = to_apply
         # Added another input_size parameter for geometric transformations
         # This might be needed for correctly inversing.
-        input_size = torch.tensor(batch_shape, dtype=torch.long)
+        input_size = tensor(batch_shape, dtype=torch.long)
         _params.update({'forward_input_shape': input_size})
         return _params
 
     def apply_func(self, input: Tensor, params: Dict[str, Tensor], flags: Dict[str, Any]) -> Tensor:
         return self.apply_transform(input, params, flags)
 
-    def forward(self, input: Tensor, params: Optional[Dict[str, Tensor]] = None, **kwargs) -> Tensor:  # type: ignore
+    def forward(self, input: Tensor, params: Optional[Dict[str, Tensor]] = None, **kwargs) -> Tensor:
         """Perform forward operations.
 
         Args:
             input: the input tensor.
             params: the corresponding parameters for an operation.
                 If None, a new parameter suite will be generated.
             **kwargs: key-value pairs to override the parameters and flags.
@@ -165,15 +175,15 @@
         input_shape = in_tensor.shape
         in_tensor = self.transform_tensor(in_tensor)
         batch_shape = in_tensor.shape
         if params is None:
             params = self.forward_parameters(batch_shape)
 
         if 'batch_prob' not in params:
-            params['batch_prob'] = torch.tensor([True] * batch_shape[0])
+            params['batch_prob'] = tensor([True] * batch_shape[0])
 
         params, flags = self._process_kwargs_to_params_and_flags(params, self.flags, **kwargs)
 
         output = self.apply_func(in_tensor, params, flags)
         return self.transform_output_tensor(output, input_shape) if self.keepdim else output
 
 
@@ -225,15 +235,15 @@
         raise NotImplementedError
 
     def apply_transform(
         self, input: Tensor, params: Dict[str, Tensor], flags: Dict[str, Any], transform: Optional[Tensor] = None
     ) -> Tensor:
         raise NotImplementedError
 
-    def apply_func(  # type: ignore
+    def apply_func(
         self, in_tensor: Tensor, params: Dict[str, Tensor], flags: Optional[Dict[str, Any]] = None
     ) -> Tensor:
         if flags is None:
             flags = self.flags
         to_apply = params['batch_prob']
 
         # if no augmentation needed
```

### Comparing `kornia-0.6.8/kornia/augmentation/container/augment.py` & `kornia-0.6.9/kornia/augmentation/container/augment.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,12 +1,10 @@
 import warnings
 from itertools import zip_longest
-from typing import Any, Dict, List, Optional, Tuple, Union, cast
-
-from torch import Tensor
+from typing import Any, Dict, List, Optional, Tuple, Union
 
 from kornia.augmentation import (
     AugmentationBase3D,
     GeometricAugmentationBase2D,
     IntensityAugmentationBase2D,
     RandomErasing,
 )
@@ -14,14 +12,15 @@
 from kornia.augmentation.base import _AugmentationBase
 from kornia.augmentation.container.base import SequentialBase
 from kornia.augmentation.container.image import ImageSequential, ParamItem
 from kornia.augmentation.container.patch import PatchSequential
 from kornia.augmentation.container.utils import ApplyInverse
 from kornia.augmentation.container.video import VideoSequential
 from kornia.constants import DataKey, Resample
+from kornia.core import Tensor
 from kornia.geometry.boxes import Boxes
 from kornia.utils import eye_like
 
 __all__ = ["AugmentationSequential"]
 
 
 class AugmentationSequential(ImageSequential):
@@ -140,24 +139,22 @@
     """
 
     def __init__(
         self,
         *args: Union[_AugmentationBase, ImageSequential],
         data_keys: List[Union[str, int, DataKey]] = [DataKey.INPUT],
         same_on_batch: Optional[bool] = None,
-        return_transform: Optional[bool] = None,
         keepdim: Optional[bool] = None,
         random_apply: Union[int, bool, Tuple[int, int]] = False,
         random_apply_weights: Optional[List[float]] = None,
         extra_args: Dict[DataKey, Dict[str, Any]] = {DataKey.MASK: dict(resample=Resample.NEAREST, align_corners=True)},
     ) -> None:
         super().__init__(
             *args,
             same_on_batch=same_on_batch,
-            return_transform=return_transform,
             keepdim=keepdim,
             random_apply=random_apply,
             random_apply_weights=random_apply_weights,
         )
 
         self.data_keys = [DataKey.get(inp) for inp in data_keys]
 
@@ -187,67 +184,69 @@
         else:
             return eye_like(3, input)
 
     @property
     def transform_matrix(self) -> Optional[Tensor]:
         return self._transform_matrix
 
-    def inverse(  # type: ignore
+    def inverse(  # type: ignore[override]
         self,
         *args: Tensor,
         params: Optional[List[ParamItem]] = None,
         data_keys: Optional[List[Union[str, int, DataKey]]] = None,
     ) -> Union[Tensor, List[Tensor]]:
         """Reverse the transformation applied.
 
         Number of input tensors must align with the number of``data_keys``. If ``data_keys`` is not set, use
         ``self.data_keys`` by default.
         """
         if data_keys is None:
-            data_keys = cast(List[Union[str, int, DataKey]], self.data_keys)
-
-        _data_keys: List[DataKey] = [DataKey.get(inp) for inp in data_keys]
+            _data_keys = self.data_keys
+        else:
+            _data_keys = [DataKey.get(inp) for inp in data_keys]
 
-        if len(args) != len(data_keys):
-            raise AssertionError(
-                "The number of inputs must align with the number of data_keys, "
-                f"Got {len(args)} and {len(data_keys)}."
-            )
+        self._validate_args_datakeys(*args, data_keys=_data_keys)
 
         args = self._arguments_preproc(*args, data_keys=_data_keys)
 
         if params is None:
             if self._params is None:
                 raise ValueError(
                     "No parameters available for inversing, please run a forward pass first "
                     "or passing valid params into this function."
                 )
             params = self._params
 
-        outputs: List[Tensor] = [None] * len(data_keys)  # type: ignore
-        for idx, (arg, dcate) in enumerate(zip(args, data_keys)):
+        outputs: List[Optional[Tensor]] = [None] * len(_data_keys)
+        for idx, (arg, dcate) in enumerate(zip(args, _data_keys)):
 
-            if DataKey.INPUT in self.extra_args:
-                extra_args = self.extra_args[DataKey.INPUT]
+            if dcate in self.extra_args:
+                extra_args = self.extra_args[dcate]
             else:
                 extra_args = {}
 
             if dcate == DataKey.INPUT and isinstance(arg, (tuple, list)):
                 input, _ = arg  # ignore the transformation matrix whilst inverse
             # Using tensors straight-away
             elif isinstance(arg, (Boxes,)):
                 input = arg.data  # all boxes are in (B, N, 4, 2) format now.
             else:
                 input = arg
-            for (name, module), param in zip_longest(list(self.get_forward_sequence(params))[::-1], params[::-1]):
+            for (name, module), _param in zip_longest(list(self.get_forward_sequence(params))[::-1], params[::-1]):
                 if isinstance(module, (_AugmentationBase, ImageSequential)):
-                    # TODO(jian): verify what's happening here. Mypy was complaining,
-                    param = params[name] if name in params else param  # type: ignore
+                    _mb = [p for p in params if name in p]
+                    if len(_mb) > 0:
+                        param = _mb[0]
+                    elif isinstance(_param, ParamItem):
+                        param = _param
+                    else:
+                        param = None
                 else:
                     param = None
+
                 if (
                     isinstance(module, IntensityAugmentationBase2D)
                     and dcate in DataKey
                     and not isinstance(module, RandomErasing)
                 ):
                     pass  # Do nothing
                 elif isinstance(module, ImageSequential) and module.is_intensity_only() and dcate in DataKey:
@@ -273,29 +272,36 @@
                     raise NotImplementedError(f"data_key {dcate} is not implemented for {module}.")
             if isinstance(arg, (Boxes,)):
                 arg._data = input
                 outputs[idx] = arg.to_tensor()
             else:
                 outputs[idx] = input
 
-        if len(outputs) == 1 and isinstance(outputs, (tuple, list)):
-            return outputs[0]
+        _outputs = [i for i in outputs if isinstance(i, Tensor)]
+
+        if len(_outputs) == 1 and isinstance(_outputs, list):
+            return _outputs[0]
 
-        return outputs
+        return _outputs
 
-    def __packup_output__(  # type: ignore
+    def __packup_output__(  # type: ignore[override]
         self, output: List[Tensor], label: Optional[Tensor] = None
-    ) -> Union[Tensor, Tuple[Tensor, Optional[Tensor]], List[Tensor], Tuple[List[Tensor], Optional[Tensor]]]:
-        if len(output) == 1 and isinstance(output, (tuple, list)) and self.return_label:
-            return output[0], label
-        if len(output) == 1 and isinstance(output, (tuple, list)):
-            return output[0]
+    ) -> Union[Tensor, List[Tensor], Tuple[Union[Tensor, List[Tensor]], Optional[Tensor]]]:
+
+        _out: Union[Tensor, List[Tensor]]
+
+        if len(output) == 1 and isinstance(output, list):
+            _out = output[0]
+        else:
+            _out = output
+
         if self.return_label:
-            return output, label
-        return output
+            return _out, label
+
+        return _out
 
     def _validate_args_datakeys(self, *args: Tensor, data_keys: List[DataKey]):
         if len(args) != len(data_keys):
             raise AssertionError(
                 f"The number of inputs must align with the number of data_keys. Got {len(args)} and {len(data_keys)}."
             )
         # TODO: validate args batching, and its consistency
@@ -310,73 +316,73 @@
                     mode = "vertices_plus"
                 elif DataKey.get(dcate) in [DataKey.BBOX_XYXY]:
                     mode = "xyxy"
                 elif DataKey.get(dcate) in [DataKey.BBOX_XYWH]:
                     mode = "xywh"
                 else:
                     raise ValueError(f"Unsupported mode `{DataKey.get(dcate).name}`.")
-                inp.append(Boxes.from_tensor(arg, mode=mode))  # type: ignore
+                inp.append(Boxes.from_tensor(arg, mode=mode))
             else:
                 raise NotImplementedError(f"input type of {dcate} is not implemented.")
         return inp
 
-    def forward(  # type: ignore
+    def forward(  # type: ignore[override]
         self,
         *args: Tensor,
         label: Optional[Tensor] = None,
         params: Optional[List[ParamItem]] = None,
         data_keys: Optional[List[Union[str, int, DataKey]]] = None,
-    ) -> Union[Tensor, Tuple[Tensor, Optional[Tensor]], List[Tensor], Tuple[List[Tensor], Optional[Tensor]]]:
+    ) -> Union[Tensor, List[Tensor], Tuple[Union[Tensor, List[Tensor]], Optional[Tensor]]]:
         """Compute multiple tensors simultaneously according to ``self.data_keys``."""
-        _data_keys: List[DataKey]
         if data_keys is None:
             _data_keys = self.data_keys
         else:
             _data_keys = [DataKey.get(inp) for inp in data_keys]
-            self.data_keys = _data_keys
+
         self._validate_args_datakeys(*args, data_keys=_data_keys)
 
         args = self._arguments_preproc(*args, data_keys=_data_keys)
 
         if params is None:
             # image data must exist if params is not provided.
             if DataKey.INPUT in _data_keys:
-                _input = args[_data_keys.index(DataKey.INPUT)]
-                inp = _input
+                inp = args[_data_keys.index(DataKey.INPUT)]
                 if isinstance(inp, (tuple, list)):
                     raise ValueError(f"`INPUT` should be a tensor but `{type(inp)}` received.")
                 # A video input shall be BCDHW while an image input shall be BCHW
                 if self.contains_video_sequential or self.contains_3d_augmentation:
                     _, out_shape = self.autofill_dim(inp, dim_range=(3, 5))
                 else:
                     _, out_shape = self.autofill_dim(inp, dim_range=(2, 4))
                 params = self.forward_parameters(out_shape)
             else:
                 raise ValueError("`params` must be provided whilst INPUT is not in data_keys.")
 
-        outputs: List[Tensor] = [None] * len(_data_keys)  # type: ignore
+        outputs: List[Optional[Tensor]] = [None] * len(_data_keys)
 
         self.return_label = self.return_label or label is not None or self.contains_label_operations(params)
 
         for idx, (arg, dcate) in enumerate(zip(args, _data_keys)):
             # Forward the param to all input data keys
             if dcate in self.extra_args:
                 extra_args = self.extra_args[dcate]
             else:
                 extra_args = {}
 
             if dcate == DataKey.INPUT:
                 _inp = args[idx]
 
-                _out = super().forward(_inp, label, params=params, extra_args=extra_args)  # type: ignore
+                _out = super().forward(_inp, label, params=params, extra_args=extra_args)
                 self._transform_matrix = self.get_transformation_matrix(_inp, params=params)
-                if self.return_label:
-                    _input, label = cast(Tuple[Tensor, Tensor], _out)
-                else:
-                    _input = cast(Tensor, _out)
+
+                if self.return_label and isinstance(_out, tuple):
+                    _input, label = _out
+                elif isinstance(_out, Tensor):
+                    _input = _out
+
                 outputs[idx] = _input
                 # NOTE: Skip the rest here.
                 continue
 
             # Using tensors straight-away
             if isinstance(arg, (Boxes,)):
                 input = arg.data  # all boxes are in (B, N, 4, 2) format now.
@@ -415,9 +421,9 @@
                     raise NotImplementedError(f"data_key {dcate} is not implemented for {module}.")
 
             if isinstance(arg, (Boxes,)):
                 arg._data = input
                 outputs[idx] = arg.to_tensor()
             else:
                 outputs[idx] = input
-
-        return self.__packup_output__(outputs, label)
+        _outputs = [i for i in outputs if isinstance(i, Tensor)]
+        return self.__packup_output__(_outputs, label)
```

### Comparing `kornia-0.6.8/kornia/augmentation/container/base.py` & `kornia-0.6.9/kornia/augmentation/container/base.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,23 +1,24 @@
 from collections import OrderedDict
-from typing import Any, Iterator, List, NamedTuple, Optional, Tuple, Union
+from typing import Any, Dict, Iterator, List, NamedTuple, Optional, Tuple, Union
 
 import torch
 import torch.nn as nn
 
-from kornia.augmentation import MixAugmentationBase
-from kornia.augmentation._2d.mix.base import MixAugmentationBaseV2
+from kornia.augmentation import MixAugmentationBaseV2
 from kornia.augmentation.base import _AugmentationBase
+from kornia.core import Module, Tensor
 
 __all__ = ["SequentialBase", "ParamItem"]
 
 
 class ParamItem(NamedTuple):
     name: str
-    data: Optional[Union[dict, list]]
+    # TODO: add type List['ParamItem'] when mypy > 0.991 be available (see python/mypy#14200)
+    data: Optional[Union[Dict[str, Tensor], List]]  # type: ignore [type-arg]
 
 
 class SequentialBase(nn.Sequential):
     r"""SequentialBase for creating kornia modulized processing pipeline.
 
     Args:
         *args : a list of kornia augmentation and image operation modules.
@@ -25,110 +26,94 @@
             If None, it will not overwrite the function-wise settings.
         return_transform: if ``True`` return the matrix describing the transformation
             applied to each. If None, it will not overwrite the function-wise settings.
         keepdim: whether to keep the output shape the same as input (True) or broadcast it
             to the batch form (False). If None, it will not overwrite the function-wise settings.
     """
 
-    def __init__(
-        self,
-        *args: nn.Module,
-        same_on_batch: Optional[bool] = None,
-        return_transform: Optional[bool] = None,
-        keepdim: Optional[bool] = None,
-    ) -> None:
+    def __init__(self, *args: Module, same_on_batch: Optional[bool] = None, keepdim: Optional[bool] = None) -> None:
         # To name the modules properly
         _args = OrderedDict()
         for idx, mod in enumerate(args):
-            if not isinstance(mod, nn.Module):
-                raise NotImplementedError(f"Only nn.Module are supported at this moment. Got {mod}.")
+            if not isinstance(mod, Module):
+                raise NotImplementedError(f"Only Module are supported at this moment. Got {mod}.")
             _args.update({f"{mod.__class__.__name__}_{idx}": mod})
         super().__init__(_args)
         self._same_on_batch = same_on_batch
-        self._return_transform = return_transform
         self._keepdim = keepdim
         self._params: Optional[List[ParamItem]] = None
-        self.update_attribute(same_on_batch, return_transform, keepdim)
+        self.update_attribute(same_on_batch, keepdim)
 
     def update_attribute(
         self,
         same_on_batch: Optional[bool] = None,
         return_transform: Optional[bool] = None,
         keepdim: Optional[bool] = None,
     ) -> None:
         for mod in self.children():
             # MixAugmentation does not have return transform
-            if isinstance(mod, (_AugmentationBase, MixAugmentationBase, MixAugmentationBaseV2)):
+            if isinstance(mod, (_AugmentationBase, MixAugmentationBaseV2)):
                 if same_on_batch is not None:
                     mod.same_on_batch = same_on_batch
                 if keepdim is not None:
                     mod.keepdim = keepdim
             if isinstance(mod, _AugmentationBase):
                 if return_transform is not None:
                     mod.return_transform = return_transform
             if isinstance(mod, SequentialBase):
                 mod.update_attribute(same_on_batch, return_transform, keepdim)
 
-    def get_submodule(self, target: str) -> nn.Module:
+    def get_submodule(self, target: str) -> Module:
         """Get submodule.
 
         This code is taken from torch 1.9.0 since it is not introduced
         back to torch 1.7.1. We included this for maintaining more
         backward torch versions.
 
         Args:
             target: The fully-qualified string name of the submodule
                 to look for. (See above example for how to specify a
                 fully-qualified string.)
 
         Returns:
-            torch.nn.Module: The submodule referenced by ``target``
+            Module: The submodule referenced by ``target``
 
         Raises:
             AttributeError: If the target string references an invalid
                 path or resolves to something that is not an
-                ``nn.Module``
+                ``Module``
         """
         if target == "":
             return self
 
         atoms: List[str] = target.split(".")
-        mod: torch.nn.Module = self
+        mod: Module = self
 
         for item in atoms:
 
             if not hasattr(mod, item):
                 raise AttributeError(mod._get_name() + " has no " "attribute `" + item + "`")
 
             mod = getattr(mod, item)
 
-            if not isinstance(mod, torch.nn.Module):
-                raise AttributeError("`" + item + "` is not " "an nn.Module")
+            if not isinstance(mod, Module):
+                raise AttributeError("`" + item + "` is not " "an Module")
 
         return mod
 
     @property
     def same_on_batch(self) -> Optional[bool]:
         return self._same_on_batch
 
     @same_on_batch.setter
     def same_on_batch(self, same_on_batch: Optional[bool]) -> None:
         self._same_on_batch = same_on_batch
         self.update_attribute(same_on_batch=same_on_batch)
 
     @property
-    def return_transform(self) -> Optional[bool]:
-        return self._return_transform
-
-    @return_transform.setter
-    def return_transform(self, return_transform: Optional[bool]) -> None:
-        self._return_transform = return_transform
-        self.update_attribute(return_transform=return_transform)
-
-    @property
     def keepdim(self) -> Optional[bool]:
         return self._keepdim
 
     @keepdim.setter
     def keepdim(self, keepdim: Optional[bool]) -> None:
         self._keepdim = keepdim
         self.update_attribute(keepdim=keepdim)
@@ -144,34 +129,34 @@
         else:
             self._params.append(param)
 
     # TODO: Implement this for all submodules.
     def forward_parameters(self, batch_shape: torch.Size) -> List[ParamItem]:
         raise NotImplementedError
 
-    def get_children_by_indices(self, indices: torch.Tensor) -> Iterator[Tuple[str, nn.Module]]:
+    def get_children_by_indices(self, indices: Tensor) -> Iterator[Tuple[str, Module]]:
         modules = list(self.named_children())
         for idx in indices:
             yield modules[idx]
 
-    def get_children_by_params(self, params: List[ParamItem]) -> Iterator[Tuple[str, nn.Module]]:
+    def get_children_by_params(self, params: List[ParamItem]) -> Iterator[Tuple[str, Module]]:
         modules = list(self.named_children())
         # TODO: Wrong params passed here when nested ImageSequential
         for param in params:
             yield modules[list(dict(self.named_children()).keys()).index(param.name)]
 
-    def get_params_by_module(self, named_modules: Iterator[Tuple[str, nn.Module]]) -> Iterator[ParamItem]:
+    def get_params_by_module(self, named_modules: Iterator[Tuple[str, Module]]) -> Iterator[ParamItem]:
         # This will not take module._params
         for name, _ in named_modules:
             yield ParamItem(name, None)
 
-    def contains_label_operations(self, params: List) -> bool:
+    def contains_label_operations(self, params: List[ParamItem]) -> bool:
         raise NotImplementedError
 
-    def autofill_dim(self, input: torch.Tensor, dim_range: Tuple[int, int] = (2, 4)) -> Tuple[torch.Size, torch.Size]:
+    def autofill_dim(self, input: Tensor, dim_range: Tuple[int, int] = (2, 4)) -> Tuple[torch.Size, torch.Size]:
         """Fill tensor dim to the upper bound of dim_range.
 
         If input tensor dim is smaller than the lower bound of dim_range, an error will be thrown out.
         """
         ori_shape = input.shape
         if len(ori_shape) < dim_range[0] or len(ori_shape) > dim_range[1]:
             raise RuntimeError(f"input shape expected to be in {dim_range} while got {ori_shape}.")
```

### Comparing `kornia-0.6.8/kornia/augmentation/container/dispatcher.py` & `kornia-0.6.9/kornia/augmentation/container/dispatcher.py`

 * *Files identical despite different names*

### Comparing `kornia-0.6.8/kornia/augmentation/container/image.py` & `kornia-0.6.9/kornia/augmentation/container/image.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,26 +1,24 @@
 from itertools import zip_longest
 from typing import Any, Dict, Iterator, List, Optional, Tuple, Type, Union, cast
 
 import torch
-import torch.nn as nn
-from torch import Tensor
 
 import kornia
 from kornia.augmentation import (
     GeometricAugmentationBase2D,
     IntensityAugmentationBase2D,
-    MixAugmentationBase,
+    MixAugmentationBaseV2,
     RandomCrop,
 )
-from kornia.augmentation._2d.mix.base import MixAugmentationBaseV2
 from kornia.augmentation.base import _AugmentationBase
 from kornia.augmentation.container.base import ParamItem, SequentialBase
 from kornia.augmentation.container.utils import ApplyInverseInterface, InputApplyInverse
 from kornia.augmentation.utils import override_parameters
+from kornia.core import Module, Tensor, as_tensor
 
 __all__ = ["ImageSequential"]
 
 
 class ImageSequential(SequentialBase):
     r"""Sequential for creating kornia image processing pipeline.
 
@@ -50,22 +48,21 @@
         >>> input, label = torch.randn(2, 3, 5, 6), torch.tensor([0, 1])
         >>> aug_list = ImageSequential(
         ...     kornia.color.BgrToRgb(),
         ...     kornia.augmentation.ColorJiggle(0.1, 0.1, 0.1, 0.1, p=1.0),
         ...     kornia.filters.MedianBlur((3, 3)),
         ...     kornia.augmentation.RandomAffine(360, p=1.0),
         ...     kornia.enhance.Invert(),
-        ...     kornia.augmentation.RandomMixUp(p=1.0),
+        ...     kornia.augmentation.RandomMixUpV2(p=1.0),
         ...     same_on_batch=True,
         ...     random_apply=10,
         ... )
         >>> out, lab = aug_list(input, label=label)
         >>> lab
-        tensor([[0.0000, 1.0000, 0.1214],
-                [1.0000, 0.0000, 0.1214]])
+        tensor([0, 1])
         >>> out.shape
         torch.Size([2, 3, 5, 6])
 
         Reproduce with provided params.
         >>> out2, lab2 = aug_list(input, label=label, params=aug_list._params)
         >>> torch.equal(out, out2), torch.equal(lab, lab2)
         (True, True)
@@ -85,36 +82,30 @@
         >>> out= aug_list(input)
         >>> out.shape
         torch.Size([2, 3, 5, 6])
     """
 
     def __init__(
         self,
-        *args: nn.Module,
+        *args: Module,
         same_on_batch: Optional[bool] = None,
-        return_transform: Optional[bool] = None,
         keepdim: Optional[bool] = None,
         random_apply: Union[int, bool, Tuple[int, int]] = False,
         random_apply_weights: Optional[List[float]] = None,
         if_unsupported_ops: str = "raise",
     ) -> None:
-        if return_transform is not None:
-            raise ValueError(
-                "`return_transform` is deprecated. Please access"
-                " `.transform_matrix` in `AugmentationSequential` instead."
-            )
-        super().__init__(*args, same_on_batch=same_on_batch, return_transform=return_transform, keepdim=keepdim)
+        super().__init__(*args, same_on_batch=same_on_batch, keepdim=keepdim)
 
-        self.random_apply: Union[Tuple[int, int], bool] = self._read_random_apply(random_apply, len(args))
+        self.random_apply = self._read_random_apply(random_apply, len(args))
         if random_apply_weights is not None and len(random_apply_weights) != len(self):
             raise ValueError(
                 "The length of `random_apply_weights` must be as same as the number of operations."
                 f"Got {len(random_apply_weights)} and {len(self)}."
             )
-        self.random_apply_weights = torch.as_tensor(random_apply_weights or torch.ones((len(self),)))
+        self.random_apply_weights = as_tensor(random_apply_weights or torch.ones((len(self),)))
         self.return_label: Optional[bool] = None
         self.apply_inverse_func: Type[ApplyInverseInterface] = InputApplyInverse
         self.if_unsupported_ops = if_unsupported_ops
 
     def _read_random_apply(
         self, random_apply: Union[int, bool, Tuple[int, int]], max_length: int
     ) -> Union[Tuple[int, int], bool]:
@@ -141,21 +132,25 @@
             and len(random_apply) == 2
             and isinstance(random_apply[0], (int,))
             and isinstance(random_apply[0], (int,))
         ):
             raise AssertionError(f"Expect a tuple of (int, int). Got {random_apply}.")
         return random_apply
 
-    def get_random_forward_sequence(self, with_mix: bool = True) -> Tuple[Iterator[Tuple[str, nn.Module]], bool]:
+    def get_random_forward_sequence(self, with_mix: bool = True) -> Tuple[Iterator[Tuple[str, Module]], bool]:
         """Get a forward sequence when random apply is in need.
 
         Note:
             Mix augmentations (e.g. RandomMixUp) will be only applied once even in a random forward.
         """
-        num_samples = int(torch.randint(*self.random_apply, (1,)).item())  # type: ignore
+        if isinstance(self.random_apply, tuple):
+            num_samples = int(torch.randint(*self.random_apply, (1,)).item())
+        else:
+            raise TypeError(f'random apply should be a tuple. Gotcha {type(self.random_apply)}')
+
         multinomial_weights = self.random_apply_weights.clone()
         # Mix augmentation can only be applied once per forward
         mix_indices = self.get_mix_augmentation_indices(self.named_children())
         # kick out the mix augmentations
         multinomial_weights[mix_indices] = 0
         indices = torch.multinomial(
             multinomial_weights,
@@ -170,26 +165,23 @@
             if (torch.rand(1) < ((len(mix_indices) + len(indices)) / len(self))).item():
                 indices[-1] = torch.multinomial((~multinomial_weights.bool()).float(), 1)
                 indices = indices[torch.randperm(len(indices))]
                 mix_added = True
 
         return self.get_children_by_indices(indices), mix_added
 
-    def get_mix_augmentation_indices(self, named_modules: Iterator[Tuple[str, nn.Module]]) -> List[int]:
+    def get_mix_augmentation_indices(self, named_modules: Iterator[Tuple[str, Module]]) -> List[int]:
         """Get all the mix augmentations since they are label-involved.
 
         Special operations needed for label-involved augmentations.
         """
-        indices = []
-        for idx, (_, child) in enumerate(named_modules):
-            if isinstance(child, (MixAugmentationBase,)):  # NOTE: MixV2 will not be a special op in the future.
-                indices.append(idx)
-        return indices
+        # NOTE: MixV2 will not be a special op in the future.
+        return [idx for idx, (_, child) in enumerate(named_modules) if isinstance(child, MixAugmentationBaseV2)]
 
-    def get_forward_sequence(self, params: Optional[List[ParamItem]] = None) -> Iterator[Tuple[str, nn.Module]]:
+    def get_forward_sequence(self, params: Optional[List[ParamItem]] = None) -> Iterator[Tuple[str, Module]]:
         if params is None:
             # Mix augmentation can only be applied once per forward
             mix_indices = self.get_mix_augmentation_indices(self.named_children())
 
             if self.random_apply:
                 return self.get_random_forward_sequence()[0]
 
@@ -203,32 +195,32 @@
 
         return self.get_children_by_params(params)
 
     def apply_to_input(
         self,
         input: Tensor,
         label: Optional[Tensor],
-        module: Optional[nn.Module],
+        module: Optional[Module],
         param: ParamItem,
         extra_args: Dict[str, Any],
     ) -> Tuple[Tensor, Optional[Tensor]]:
         if module is None:
             module = self.get_submodule(param.name)
-        return self.apply_inverse_func.apply_trans(input, label, module, param, extra_args)  # type: ignore
+        return self.apply_inverse_func.apply_trans(input, label, module, param, extra_args)
 
     def forward_parameters(self, batch_shape: torch.Size) -> List[ParamItem]:
-        named_modules: Iterator[Tuple[str, nn.Module]] = self.get_forward_sequence()
+        named_modules: Iterator[Tuple[str, Module]] = self.get_forward_sequence()
 
         params: List[ParamItem] = []
-        mod_param: Union[dict, list]
+        mod_param: Union[Dict[str, Tensor], List[ParamItem]]
         for name, module in named_modules:
             if isinstance(module, RandomCrop):
                 mod_param = module.forward_parameters_precrop(batch_shape)
                 param = ParamItem(name, mod_param)
-            elif isinstance(module, (_AugmentationBase, MixAugmentationBase, MixAugmentationBaseV2, ImageSequential)):
+            elif isinstance(module, (_AugmentationBase, MixAugmentationBaseV2, ImageSequential)):
                 mod_param = module.forward_parameters(batch_shape)
                 param = ParamItem(name, mod_param)
             else:
                 param = ParamItem(name, None)
             batch_shape = _get_new_batch_shape(param, batch_shape)
             params.append(param)
         return params
@@ -236,18 +228,20 @@
     def contains_label_operations(self, params: List[ParamItem]) -> bool:
         """Check if current sequential contains label-involved operations like MixUp."""
         for param in params:
             if param.name.startswith("RandomMixUp_") or param.name.startswith("RandomCutMix_"):
                 return True
         return False
 
-    def __packup_output__(self, output: Tensor, label: Optional[Tensor] = None) -> Union[Tensor, Tuple[Tensor, Tensor]]:
+    def __packup_output__(
+        self, output: Tensor, label: Optional[Tensor] = None
+    ) -> Union[Tensor, Tuple[Tensor, Optional[Tensor]]]:
         if self.return_label:
-            return output, label  # type: ignore
             # Implicitly indicating the label cannot be optional since there is a mix aug
+            return output, label
         return output
 
     def identity_matrix(self, input) -> Tensor:
         """Return identity matrix."""
         return kornia.eye_like(3, input)
 
     def get_transformation_matrix(
@@ -263,59 +257,60 @@
             input: the input tensor.
             params: params for the sequence.
             recompute: if to recompute the transformation matrix according to the params.
                 default: False.
         """
         if params is None:
             raise NotImplementedError("requires params to be provided.")
-        named_modules: Iterator[Tuple[str, nn.Module]] = self.get_forward_sequence(params)
+        named_modules: Iterator[Tuple[str, Module]] = self.get_forward_sequence(params)
 
         # Define as 1 for broadcasting
         res_mat: Optional[Tensor] = None
         for (_, module), param in zip(named_modules, params if params is not None else []):
-            if isinstance(module, (_AugmentationBase,)) and not isinstance(
-                module, (MixAugmentationBase, MixAugmentationBaseV2)
+            if (
+                isinstance(module, (_AugmentationBase,))
+                and not isinstance(module, MixAugmentationBaseV2)
+                and isinstance(param.data, dict)
             ):
-                pdata = cast(Dict[str, Tensor], param.data)
-                to_apply = pdata['batch_prob']  # type: ignore
+                to_apply = param.data['batch_prob']
                 ori_shape = input.shape
                 try:
                     input = module.transform_tensor(input)
                 except ValueError:
                     # Ignore error for 5-dim video
                     pass
                 # Standardize shape
                 if recompute:
                     mat: Tensor = self.identity_matrix(input)
                     flags = override_parameters(module.flags, extra_args, in_place=False)
-                    mat[to_apply] = module.compute_transformation(input[to_apply], param.data, flags)  # type: ignore
+                    mat[to_apply] = module.compute_transformation(input[to_apply], param.data, flags)
                 else:
-                    mat = torch.as_tensor(module._transform_matrix, device=input.device, dtype=input.dtype)
+                    mat = as_tensor(module._transform_matrix, device=input.device, dtype=input.dtype)
                 res_mat = mat if res_mat is None else mat @ res_mat
                 input = module.transform_output_tensor(input, ori_shape)
                 if module.keepdim and ori_shape != input.shape:
                     res_mat = res_mat.squeeze()
             elif isinstance(module, (ImageSequential,)):
                 # If not augmentationSequential
                 if isinstance(module, (kornia.augmentation.AugmentationSequential,)) and not recompute:
-                    mat = torch.as_tensor(module._transform_matrix, device=input.device, dtype=input.dtype)
+                    mat = as_tensor(module._transform_matrix, device=input.device, dtype=input.dtype)
                 else:
                     maybe_param_data = cast(Optional[List[ParamItem]], param.data)
                     _mat = module.get_transformation_matrix(
                         input, maybe_param_data, recompute=recompute, extra_args=extra_args
-                    )  # type: ignore
+                    )
                     mat = module.identity_matrix(input) if _mat is None else _mat
                 res_mat = mat if res_mat is None else mat @ res_mat
         return res_mat
 
     def is_intensity_only(self, strict: bool = True) -> bool:
         """Check if all transformations are intensity-based.
 
         Args:
-            strict: if strict is False, it will allow non-augmentation nn.Modules to be passed.
+            strict: if strict is False, it will allow non-augmentation Modules to be passed.
                 e.g. `kornia.enhance.AdjustBrightness` will be recognized as non-intensity module
                 if strict is set to True.
 
         Note: patch processing would break the continuity of labels (e.g. bbounding boxes, masks).
         """
         for arg in self.children():
             if isinstance(arg, (ImageSequential,)) and not arg.is_intensity_only(strict):
@@ -343,51 +338,50 @@
                 raise ValueError(
                     "No parameters available for inversing, please run a forward pass first "
                     "or passing valid params into this function."
                 )
             params = self._params
 
         for (name, module), param in zip_longest(list(self.get_forward_sequence(params))[::-1], params[::-1]):
-            maybe_param: Optional[ParamItem] = None
             if isinstance(module, (_AugmentationBase, ImageSequential)):
-                maybe_param = params[name] if name in params else param  # type: ignore
+                _mb: List[ParamItem] = [p for p in params if name in p]
+                maybe_param = _mb if len(_mb) > 0 else [param]
 
             if isinstance(module, IntensityAugmentationBase2D):
                 pass  # Do nothing
             elif isinstance(module, ImageSequential) and module.is_intensity_only():
                 pass  # Do nothing
-            elif isinstance(module, ImageSequential) and maybe_param is not None:
-                param_data = cast(List[ParamItem], cast(ParamItem, maybe_param).data)
-                input = module.inverse(input, param_data, extra_args=extra_args)
+            elif isinstance(module, ImageSequential) and isinstance(maybe_param, ParamItem):
+                input = module.inverse(input, maybe_param, extra_args=extra_args)
             elif isinstance(module, (GeometricAugmentationBase2D,)):
                 input = self.apply_inverse_func.inverse(input, module, param, extra_args=extra_args)
             else:
                 pass
                 # raise NotImplementedError(f"`inverse` is not implemented for {module}.")
 
         return input
 
-    def forward(  # type: ignore
+    def forward(
         self,
         input: Tensor,
         label: Optional[Tensor] = None,
         params: Optional[List[ParamItem]] = None,
         extra_args: Dict[str, Any] = {},
-    ) -> Union[Tensor, Tuple[Tensor, Tensor]]:
+    ) -> Union[Tensor, Tuple[Tensor, Optional[Tensor]]]:
         self.clear_state()
         if params is None:
             inp = input
             _, out_shape = self.autofill_dim(inp, dim_range=(2, 4))
             params = self.forward_parameters(out_shape)
         if self.return_label is None:
             self.return_label = label is not None or self.contains_label_operations(params)
         for param in params:
             module = self.get_submodule(param.name)
-            input, label = self.apply_to_input(input, label, module, param=param, extra_args=extra_args)  # type: ignore
-            if isinstance(module, (_AugmentationBase, MixAugmentationBase, MixAugmentationBaseV2, SequentialBase)):
+            input, label = self.apply_to_input(input, label, module, param=param, extra_args=extra_args)
+            if isinstance(module, (_AugmentationBase, MixAugmentationBaseV2, SequentialBase)):
                 param = ParamItem(param.name, module._params)
             else:
                 param = ParamItem(param.name, None)
             self.update_params(param)
         return self.__packup_output__(input, label)
```

### Comparing `kornia-0.6.8/kornia/augmentation/container/patch.py` & `kornia-0.6.9/kornia/augmentation/container/patch.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,19 +1,19 @@
 from itertools import cycle, islice
-from typing import Iterator, List, NamedTuple, Optional, Tuple, Union
+from typing import Any, Dict, Iterator, List, NamedTuple, Optional, Tuple, Union
 
 import torch
-import torch.nn as nn
 
-from kornia.augmentation import MixAugmentationBase
-from kornia.augmentation._2d.mix.base import MixAugmentationBaseV2
+from kornia.augmentation import MixAugmentationBaseV2
 from kornia.augmentation.base import _AugmentationBase
 from kornia.augmentation.container.base import SequentialBase
 from kornia.augmentation.container.image import ImageSequential, ParamItem
 from kornia.contrib.extract_patches import extract_tensor_patches
+from kornia.core import Module, Tensor, concatenate
+from kornia.core import pad as fpad
 
 __all__ = ["PatchSequential"]
 
 
 class PatchParamItem(NamedTuple):
     indices: List[int]
     param: ParamItem
@@ -108,15 +108,15 @@
         >>> out = seq(input)
         >>> out.shape
         torch.Size([2, 3, 224, 224])
     """
 
     def __init__(
         self,
-        *args: nn.Module,
+        *args: Module,
         grid_size: Tuple[int, int] = (4, 4),
         padding: str = "same",
         same_on_batch: Optional[bool] = None,
         keepdim: Optional[bool] = None,
         patchwise_apply: bool = True,
         random_apply: Union[int, bool, Tuple[int, int]] = False,
         random_apply_weights: Optional[List[float]] = None,
@@ -147,40 +147,40 @@
         )
         if padding not in ("same", "valid"):
             raise ValueError(f"`padding` must be either `same` or `valid`. Got {padding}.")
         self.grid_size = grid_size
         self.padding = padding
         self.patchwise_apply = patchwise_apply
 
-    def contains_label_operations(self, params: List[PatchParamItem]) -> bool:  # type: ignore
+    def contains_label_operations(self, params: List[PatchParamItem]) -> bool:  # type: ignore[override]
         for param in params:
             if param.param.name.startswith("RandomMixUp") or param.param.name.startswith("RandomCutMix"):
                 return True
         return False
 
     def compute_padding(
-        self, input: torch.Tensor, padding: str, grid_size: Optional[Tuple[int, int]] = None
+        self, input: Tensor, padding: str, grid_size: Optional[Tuple[int, int]] = None
     ) -> Tuple[int, int, int, int]:
         if grid_size is None:
             grid_size = self.grid_size
         if padding == "valid":
             ph, pw = input.size(-2) // grid_size[0], input.size(-1) // grid_size[1]
             return (-pw // 2, pw // 2 - pw, -ph // 2, ph // 2 - ph)
         if padding == "same":
             ph = input.size(-2) - input.size(-2) // grid_size[0] * grid_size[0]
             pw = input.size(-1) - input.size(-1) // grid_size[1] * grid_size[1]
             return (pw // 2, pw - pw // 2, ph // 2, ph - ph // 2)
         raise NotImplementedError(f"Expect `padding` as either 'valid' or 'same'. Got {padding}.")
 
     def extract_patches(
         self,
-        input: torch.Tensor,
+        input: Tensor,
         grid_size: Optional[Tuple[int, int]] = None,
         pad: Optional[Tuple[int, int, int, int]] = None,
-    ) -> torch.Tensor:
+    ) -> Tensor:
         """Extract patches from tensor.
 
         Example:
             >>> import kornia.augmentation as K
             >>> pas = PatchSequential(K.ColorJiggle(0.1, 0.1, 0.1, 0.1, p=1.0), patchwise_apply=False)
             >>> pas.extract_patches(torch.arange(16).view(1, 1, 4, 4), grid_size=(2, 2))
             tensor([[[[[ 0,  1],
@@ -206,27 +206,24 @@
             <BLANKLINE>
                      [[[28, 29, 30]]],
             <BLANKLINE>
             <BLANKLINE>
                      [[[31, 32, 33]]]]])
         """
         if pad is not None:
-            input = torch.nn.functional.pad(input, list(pad))
+            input = fpad(input, list(pad))
         if grid_size is None:
             grid_size = self.grid_size
         window_size = (input.size(-2) // grid_size[-2], input.size(-1) // grid_size[-1])
         stride = window_size
         return extract_tensor_patches(input, window_size, stride)
 
     def restore_from_patches(
-        self,
-        patches: torch.Tensor,
-        grid_size: Tuple[int, int] = (4, 4),
-        pad: Optional[Tuple[int, int, int, int]] = None,
-    ) -> torch.Tensor:
+        self, patches: Tensor, grid_size: Tuple[int, int] = (4, 4), pad: Optional[Tuple[int, int, int, int]] = None
+    ) -> Tensor:
         """Restore input from patches.
 
         Example:
             >>> import kornia.augmentation as K
             >>> pas = PatchSequential(K.ColorJiggle(0.1, 0.1, 0.1, 0.1, p=1.0), patchwise_apply=False)
             >>> out = pas.extract_patches(torch.arange(16).view(1, 1, 4, 4), grid_size=(2, 2))
             >>> pas.restore_from_patches(out, grid_size=(2, 2))
@@ -234,36 +231,36 @@
                       [ 4,  5,  6,  7],
                       [ 8,  9, 10, 11],
                       [12, 13, 14, 15]]]])
         """
         if grid_size is None:
             grid_size = self.grid_size
         patches_tensor = patches.view(-1, grid_size[0], grid_size[1], *patches.shape[-3:])
-        restored_tensor = torch.cat(torch.chunk(patches_tensor, grid_size[0], dim=1), -2).squeeze(1)
-        restored_tensor = torch.cat(torch.chunk(restored_tensor, grid_size[1], dim=1), -1).squeeze(1)
+        restored_tensor = concatenate(torch.chunk(patches_tensor, grid_size[0], 1), -2).squeeze(1)
+        restored_tensor = concatenate(torch.chunk(restored_tensor, grid_size[1], 1), -1).squeeze(1)
 
         if pad is not None:
-            restored_tensor = torch.nn.functional.pad(restored_tensor, [-i for i in pad])
+            restored_tensor = fpad(restored_tensor, [-i for i in pad])
         return restored_tensor
 
-    def forward_parameters(self, batch_shape: torch.Size) -> List[PatchParamItem]:  # type: ignore
+    def forward_parameters(self, batch_shape: torch.Size) -> List[PatchParamItem]:  # type: ignore[override]
         out_param: List[PatchParamItem] = []
         if not self.patchwise_apply:
             params = self.generate_parameters(torch.Size([1, batch_shape[0] * batch_shape[1], *batch_shape[2:]]))
             indices = torch.arange(0, batch_shape[0] * batch_shape[1])
-            [out_param.append(PatchParamItem(indices.tolist(), p)) for p, _ in params]  # type: ignore
+            out_param = [PatchParamItem(indices.tolist(), p) for p, _ in params]
             # "append" of "list" does not return a value
         elif not self.same_on_batch:
             params = self.generate_parameters(torch.Size([batch_shape[0] * batch_shape[1], 1, *batch_shape[2:]]))
-            [out_param.append(PatchParamItem([i], p)) for p, i in params]  # type: ignore
+            out_param = [PatchParamItem([i], p) for p, i in params]
             # "append" of "list" does not return a value
         else:
             params = self.generate_parameters(torch.Size([batch_shape[1], batch_shape[0], *batch_shape[2:]]))
             indices = torch.arange(0, batch_shape[0] * batch_shape[1], step=batch_shape[1])
-            [out_param.append(PatchParamItem((indices + i).tolist(), p)) for p, i in params]  # type: ignore
+            out_param = [PatchParamItem((indices + i).tolist(), p) for p, i in params]
             # "append" of "list" does not return a value
         return out_param
 
     def generate_parameters(self, batch_shape: torch.Size) -> Iterator[Tuple[ParamItem, int]]:
         """Get multiple forward sequence but maximumly one mix augmentation in between.
 
         Args:
@@ -273,68 +270,60 @@
         if not self.same_on_batch and self.random_apply:
             # diff_on_batch and random_apply => patch-wise augmentation
             with_mix = False
             for i in range(batch_shape[0]):
                 seq, mix_added = self.get_random_forward_sequence(with_mix=with_mix)
                 with_mix = mix_added
                 for s in seq:
-                    if isinstance(
-                        s[1], (_AugmentationBase, MixAugmentationBase, SequentialBase, MixAugmentationBaseV2)
-                    ):
+                    if isinstance(s[1], (_AugmentationBase, SequentialBase, MixAugmentationBaseV2)):
                         yield ParamItem(s[0], s[1].forward_parameters(torch.Size(batch_shape[1:]))), i
                     else:
                         yield ParamItem(s[0], None), i
         elif not self.same_on_batch and not self.random_apply:
             for i, nchild in enumerate(self.named_children()):
-                if isinstance(
-                    nchild[1], (_AugmentationBase, MixAugmentationBase, SequentialBase, MixAugmentationBaseV2)
-                ):
+                if isinstance(nchild[1], (_AugmentationBase, SequentialBase, MixAugmentationBaseV2)):
                     yield ParamItem(nchild[0], nchild[1].forward_parameters(torch.Size(batch_shape[1:]))), i
                 else:
                     yield ParamItem(nchild[0], None), i
         elif not self.random_apply:
             # same_on_batch + not random_apply => location-wise augmentation
             for i, nchild in enumerate(islice(cycle(self.named_children()), batch_shape[0])):
-                if isinstance(
-                    nchild[1], (_AugmentationBase, MixAugmentationBase, SequentialBase, MixAugmentationBaseV2)
-                ):
+                if isinstance(nchild[1], (_AugmentationBase, SequentialBase, MixAugmentationBaseV2)):
                     yield ParamItem(nchild[0], nchild[1].forward_parameters(torch.Size(batch_shape[1:]))), i
                 else:
                     yield ParamItem(nchild[0], None), i
         else:
             # same_on_batch + random_apply => location-wise augmentation
             with_mix = False
             for i in range(batch_shape[0]):
                 seq, mix_added = self.get_random_forward_sequence(with_mix=with_mix)
                 with_mix = mix_added
                 for s in seq:
-                    if isinstance(
-                        s[1], (_AugmentationBase, MixAugmentationBase, SequentialBase, MixAugmentationBaseV2)
-                    ):
+                    if isinstance(s[1], (_AugmentationBase, SequentialBase, MixAugmentationBaseV2)):
                         yield ParamItem(s[0], s[1].forward_parameters(torch.Size(batch_shape[1:]))), i
                     else:
                         yield ParamItem(s[0], None), i
 
     def apply_by_param(
-        self, input: torch.Tensor, label: Optional[torch.Tensor], params: PatchParamItem
-    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], PatchParamItem]:
-        _input: torch.Tensor
+        self, input: Tensor, label: Optional[Tensor], params: PatchParamItem
+    ) -> Tuple[Tensor, Optional[Tensor], PatchParamItem]:
+        _input: Tensor
         in_shape = input.shape
         _input = input[params.indices]
 
-        _label: Optional[torch.Tensor]
+        _label: Optional[Tensor]
         if label is not None:
             _label = label[params.indices]
         else:
             _label = label
 
         module = self.get_submodule(params.param.name)
         output, out_label = self.apply_to_input(_input, _label, module, params.param, extra_args={})
 
-        if isinstance(module, (_AugmentationBase, MixAugmentationBase, SequentialBase, MixAugmentationBaseV2)):
+        if isinstance(module, (_AugmentationBase, SequentialBase, MixAugmentationBaseV2)):
             out_param = ParamItem(params.param.name, module._params)
         else:
             out_param = ParamItem(params.param.name, None)
 
         if isinstance(output, (tuple,)) and isinstance(input, (tuple,)):
             input[0][params.indices] = output[0]
             input[1][params.indices] = output[1]
@@ -367,49 +356,51 @@
                     torch.ones(in_shape[0], *out_label.shape[1:], device=out_label.device, dtype=out_label.dtype) * -1
                 )
             _label[params.indices] = out_label
 
         return input, _label, PatchParamItem(params.indices, param=out_param)
 
     def forward_by_params(
-        self, input: torch.Tensor, label: Optional[torch.Tensor], params: List[PatchParamItem]
-    ) -> Union[torch.Tensor, Tuple[torch.Tensor, Optional[torch.Tensor]]]:
-        _input: torch.Tensor
+        self, input: Tensor, label: Optional[Tensor], params: List[PatchParamItem]
+    ) -> Union[Tensor, Tuple[Tensor, Optional[Tensor]]]:
+        _input: Tensor
         in_shape = input.shape
         _input = input.reshape(-1, *in_shape[-3:])
 
         if label is not None:
-            label = torch.cat([label] * in_shape[1], dim=0)
+            label = concatenate([label] * in_shape[1], 0)
 
         self.clear_state()
         for patch_param in params:
             _input, label, out_param = self.apply_by_param(_input, label, params=patch_param)
             self.update_params(out_param)
         _input = _input.reshape(in_shape)
         return _input, label
 
-    def inverse(self, input: torch.Tensor, params: List[ParamItem]) -> torch.Tensor:  # type: ignore
+    def inverse(
+        self, input: Tensor, params: Optional[List[ParamItem]] = None, extra_args: Dict[str, Any] = {}
+    ) -> Tensor:
         """Inverse transformation.
 
         Used to inverse a tensor according to the performed transformation by a forward pass, or with respect to
         provided parameters.
         """
         if self.is_intensity_only():
             return input
 
         raise NotImplementedError("PatchSequential inverse cannot be used with geometric transformations.")
 
-    def forward(  # type: ignore
-        self, input: torch.Tensor, label: Optional[torch.Tensor] = None, params: Optional[List[PatchParamItem]] = None
-    ) -> Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:
+    def forward(  # type: ignore[override]
+        self, input: Tensor, label: Optional[Tensor] = None, params: Optional[List[PatchParamItem]] = None
+    ) -> Union[Tensor, Tuple[Tensor, Optional[Tensor]]]:
         """Input transformation will be returned if input is a tuple."""
         # BCHW -> B(patch)CHW
         if isinstance(input, (tuple,)):
             raise ValueError("tuple input is not currently supported.")
-        _input: torch.Tensor
+        _input: Tensor
 
         pad = self.compute_padding(input, self.padding)
         input = self.extract_patches(input, self.grid_size, pad)
 
         if params is None:
             params = self.forward_parameters(input.shape)
```

### Comparing `kornia-0.6.8/kornia/augmentation/container/utils.py` & `kornia-0.6.9/kornia/augmentation/container/utils.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,51 +1,44 @@
 import warnings
 from abc import ABCMeta, abstractmethod
 from functools import partial
 from typing import Any, Callable, Dict, Iterator, List, Optional, Tuple, Type, Union, cast
 
 import torch
-import torch.nn as nn
-from torch import Tensor
 
 import kornia  # lazy loading for circular dependencies
-from kornia.augmentation import GeometricAugmentationBase2D, MixAugmentationBase, RandomCrop, RandomErasing
-from kornia.augmentation._2d.mix.base import MixAugmentationBaseV2
+from kornia.augmentation import GeometricAugmentationBase2D, MixAugmentationBaseV2, RandomCrop, RandomErasing
 from kornia.augmentation.base import _AugmentationBase
 from kornia.augmentation.container.base import ParamItem
 from kornia.augmentation.utils import override_parameters
 from kornia.constants import DataKey
+from kornia.core import Module, Tensor, as_tensor
 from kornia.geometry.bbox import transform_bbox
 from kornia.geometry.linalg import transform_points
 from kornia.testing import KORNIA_UNWRAP
 from kornia.utils.helpers import _torch_inverse_cast
 
 
-def _get_geometric_only_param(module: "kornia.augmentation.ImageSequential", param: List[ParamItem]) -> List[ParamItem]:
-    named_modules: Iterator[Tuple[str, nn.Module]] = module.get_forward_sequence(param)
+def _get_geometric_only_param(module: 'kornia.augmentation.ImageSequential', param: List[ParamItem]) -> List[ParamItem]:
+    named_modules: Iterator[Tuple[str, Module]] = module.get_forward_sequence(param)
 
     res: List[ParamItem] = []
     for (_, mod), p in zip(named_modules, param):
         if isinstance(mod, (GeometricAugmentationBase2D,)):
             res.append(p)
     return res
 
 
 class ApplyInverseInterface(metaclass=ABCMeta):
     """Abstract interface for applying and inversing transformations."""
 
     @classmethod
     @abstractmethod
     def apply_trans(
-        cls,
-        input: Tensor,
-        label: Optional[Tensor],
-        module: nn.Module,
-        param: ParamItem,
-        extra_args: Dict[str, Any] = {},
+        cls, input: Tensor, label: Optional[Tensor], module: Module, param: ParamItem, extra_args: Dict[str, Any] = {}
     ) -> Tuple[Tensor, Optional[Tensor]]:
         """Apply a transformation with respect to the parameters.
 
         Args:
             input: the input tensor.
             label: the optional label tensor.
             module: any torch Module but only kornia augmentation modules will count
@@ -53,15 +46,15 @@
             param: the corresponding parameters to the module.
         """
         raise NotImplementedError
 
     @classmethod
     @abstractmethod
     def inverse(
-        cls, input: Tensor, module: nn.Module, param: Optional[ParamItem] = None, extra_args: Dict[str, Any] = {}
+        cls, input: Tensor, module: Module, param: Optional[ParamItem] = None, extra_args: Dict[str, Any] = {}
     ) -> Tensor:
         """Inverse a transformation with respect to the parameters.
 
         Args:
             input: the input tensor.
             module: any torch Module but only kornia augmentation modules will count
                 to apply transformations.
@@ -69,24 +62,19 @@
         """
         raise NotImplementedError
 
 
 class ApplyInverseImpl(ApplyInverseInterface):
     """Standard matrix apply and inverse methods."""
 
-    apply_func: Callable
+    apply_func: Callable[[Tensor, Tensor], Tensor]
 
     @classmethod
     def apply_trans(
-        cls,
-        input: Tensor,
-        label: Optional[Tensor],
-        module: nn.Module,
-        param: ParamItem,
-        extra_args: Dict[str, Any] = {},
+        cls, input: Tensor, label: Optional[Tensor], module: Module, param: ParamItem, extra_args: Dict[str, Any] = {}
     ) -> Tuple[Tensor, Optional[Tensor]]:
         """Apply a transformation with respect to the parameters.
 
         Args:
             input: the input tensor.
             label: the optional label tensor.
             module: any torch Module but only kornia augmentation modules will count
@@ -94,54 +82,55 @@
             param: the corresponding parameters to the module.
         """
         mat: Optional[Tensor]
         if hasattr(module, "transform_matrix") and module.transform_matrix is not None:
             mat = cast(Tensor, module.transform_matrix)
         else:
             mat = cls._get_transformation(input, module, param, extra_args=extra_args)
-        mat = torch.as_tensor(mat, device=input.device, dtype=input.dtype)
+        mat = as_tensor(mat, device=input.device, dtype=input.dtype)
+
         to_apply = None
-        if isinstance(module, _AugmentationBase):
-            to_apply = param.data['batch_prob']  # type: ignore
-        if isinstance(module, kornia.augmentation.ImageSequential):
+        if isinstance(module, _AugmentationBase) and isinstance(param.data, dict):
+            to_apply = param.data['batch_prob']
+        elif isinstance(module, kornia.augmentation.ImageSequential):
             to_apply = torch.ones(input.shape[0], device=input.device, dtype=input.dtype).bool()
 
         # If any inputs need to be transformed.
         if mat is not None and to_apply is not None and to_apply.sum() != 0 and input.numel() > 0:
             input[to_apply] = cls.apply_func(mat[to_apply], input[to_apply])
 
         return input, label
 
     @classmethod
     def inverse(
-        cls, input: Tensor, module: nn.Module, param: Optional[ParamItem] = None, extra_args: Dict[str, Any] = {}
+        cls, input: Tensor, module: Module, param: Optional[ParamItem] = None, extra_args: Dict[str, Any] = {}
     ) -> Tensor:
         """Inverse a transformation with respect to the parameters.
 
         Args:
             input: the input tensor.
             module: any torch Module but only kornia augmentation modules will count
                 to apply transformations.
             param: the corresponding parameters to the module.
         """
         mat: Optional[Tensor]
         if hasattr(module, "transform_matrix") and module.transform_matrix is not None:
             mat = cast(Tensor, module.transform_matrix)
         else:
             mat = cls._get_transformation(input, module, param, extra_args=extra_args)
-        mat = torch.as_tensor(mat, device=input.device, dtype=input.dtype)
+        mat = as_tensor(mat, device=input.device, dtype=input.dtype)
 
         if mat is not None:
             transform: Tensor = cls._get_inverse_transformation(mat)
-            input = cls.apply_func(torch.as_tensor(transform, device=input.device, dtype=input.dtype), input)
+            input = cls.apply_func(as_tensor(transform, device=input.device, dtype=input.dtype), input)
         return input
 
     @classmethod
     def _get_transformation(
-        cls, input: Tensor, module: nn.Module, maybe_param: Optional[ParamItem] = None, extra_args: Dict[str, Any] = {}
+        cls, input: Tensor, module: Module, maybe_param: Optional[ParamItem] = None, extra_args: Dict[str, Any] = {}
     ) -> Optional[Tensor]:
 
         if (
             isinstance(module, (GeometricAugmentationBase2D, kornia.augmentation.ImageSequential))
             and maybe_param is None
         ):
             raise ValueError(f"Parameters of transformation matrix for {module} has not been computed.")
@@ -166,34 +155,27 @@
 
 class InputApplyInverse(ApplyInverseImpl):
     """Apply and inverse transformations for (image) input tensors."""
 
     data_key = DataKey.INPUT
 
     @classmethod
-    def apply_trans(  # type: ignore
-        cls,
-        input: Tensor,
-        label: Optional[Tensor],
-        module: nn.Module,
-        param: ParamItem,
-        extra_args: Dict[str, Any] = {},
+    def apply_trans(
+        cls, input: Tensor, label: Optional[Tensor], module: Module, param: ParamItem, extra_args: Dict[str, Any] = {}
     ) -> Tuple[Tensor, Optional[Tensor]]:
         """Apply a transformation with respect to the parameters.
 
         Args:
             input: the input tensor.
             label: the optional label tensor.
             module: any torch Module but only kornia augmentation modules will count
                 to apply transformations.
             param: the corresponding parameters to the module.
         """
-        if isinstance(module, (MixAugmentationBase,)):
-            input, label = module(input, label=label, params=param.data)
-        elif isinstance(module, (_AugmentationBase, MixAugmentationBaseV2)):
+        if isinstance(module, (_AugmentationBase, MixAugmentationBaseV2)):
             input = module(input, params=param.data, **extra_args)
         elif isinstance(module, kornia.augmentation.ImageSequential):
             temp = module.apply_inverse_func
             temp2 = module.return_label
             module.apply_inverse_func = InputApplyInverse
             module.return_label = True
             if isinstance(module, kornia.augmentation.AugmentationSequential):
@@ -210,48 +192,66 @@
                 input = (module(input[0]), input[1])
             else:
                 input = module(input)
         return input, label
 
     @classmethod
     def inverse(
-        cls, input: Tensor, module: nn.Module, param: Optional[ParamItem] = None, extra_args: Dict[str, Any] = {}
+        cls, input: Tensor, module: Module, param: Optional[ParamItem] = None, extra_args: Dict[str, Any] = {}
     ) -> Tensor:
         """Inverse a transformation with respect to the parameters.
 
         Args:
             input: the input tensor.
             module: any torch Module but only kornia augmentation modules will count
                 to apply transformations.
             param: the corresponding parameters to the module.
         """
         if isinstance(module, GeometricAugmentationBase2D):
-            input = module.inverse(
-                input, params=None if param is None else cast(Dict, param.data), extra_args=extra_args
-            )
+            if param is None:
+                _params_geo = None
+            elif isinstance(param, ParamItem) and isinstance(param.data, dict):
+                _params_geo = param.data
+            else:
+                raise TypeError(f'Expected param (ParamItem.data) be a dictionary. Gotcha {type(param.data)}')
+
+            input = module.inverse(input, params=_params_geo, extra_args=extra_args)
+
         elif isinstance(module, kornia.augmentation.ImageSequential):
             temp = module.apply_inverse_func
             module.apply_inverse_func = InputApplyInverse
+            if param is None:
+                _params = None
+            elif isinstance(param, ParamItem) and isinstance(param.data, list):
+                _params = param.data
+            else:
+                raise TypeError(f'Expected param (ParamItem.data) be a list. Gotcha {type(param.data)}')
+
             if isinstance(module, kornia.augmentation.AugmentationSequential):
-                input = cast(Tensor, module.inverse(input, params=None if param is None else cast(List, param.data)))
+                _ret = module.inverse(input, params=_params, data_keys=[cls.data_key])
+                if isinstance(_ret, Tensor):
+                    input = _ret
+                else:
+                    raise TypeError(
+                        f'The return of the method inverse from {module} should be a Tensor. Gotcha {type(_ret)}'
+                    )
             else:
-                input = module.inverse(
-                    input, params=None if param is None else cast(List, param.data), extra_args=extra_args
-                )
+                input = module.inverse(input, params=_params, extra_args=extra_args)
+
             module.apply_inverse_func = temp
         return input
 
 
 class MaskApplyInverse(ApplyInverseImpl):
     """Apply and inverse transformations for mask tensors."""
 
     data_key = DataKey.MASK
 
     @classmethod
-    def make_input_only_sequential(cls, module: "kornia.augmentation.ImageSequential") -> Callable:
+    def make_input_only_sequential(cls, module: 'kornia.augmentation.ImageSequential') -> Callable[..., torch.Tensor]:
         """Disable all other additional inputs (e.g. ) for ImageSequential."""
 
         def f(*args, **kwargs):
             if_return_label = module.return_label
             module.return_label = False
             out = module(*args, **kwargs)
             module.return_label = if_return_label
@@ -260,84 +260,122 @@
         return f
 
     @classmethod
     def apply_trans(
         cls,
         input: Tensor,
         label: Optional[Tensor],
-        module: nn.Module,
+        module: Module,
         param: Optional[ParamItem] = None,
         extra_args: Dict[str, Any] = {},
     ) -> Tuple[Tensor, Optional[Tensor]]:
         """Apply a transformation with respect to the parameters.
 
         Args:
             input: the input tensor.
             label: the optional label tensor.
             module: any torch Module but only kornia augmentation modules will count
                 to apply transformations.
             param: the corresponding parameters to the module.
         """
-        if param is not None:
-            _param = param.data
-        else:
-            _param = None  # type: ignore
-
         if isinstance(module, (GeometricAugmentationBase2D, RandomErasing)):
-            _param = cast(Dict[str, Tensor], _param).copy()
-            # TODO: Parametrize value to pad with across the board for different keys
-            if 'values' in _param:
-                _param['values'] = torch.zeros_like(_param['values'])  # Always pad with zeros
+            if isinstance(param, ParamItem) and isinstance(param.data, dict):
+                _param = param.data.copy()
+                # TODO: Parametrize value to pad with across the board for different keys
+                if 'values' in _param:
+                    _param['values'] = torch.zeros_like(_param['values'])  # Always pad with zeros
+            elif param is None:
+                _param = None
+            else:
+                raise TypeError(f'Expected param be None or ParamItem.data as a dict. Gotcha {type(_param)}')
 
             input = module(input, params=_param, **extra_args)
+
         elif isinstance(module, kornia.augmentation.ImageSequential) and not module.is_intensity_only():
-            _param = cast(List[ParamItem], _param)
+            if param is None:
+                geo_param = None
+            elif isinstance(param, ParamItem) and isinstance(param.data, list):
+                geo_param = _get_geometric_only_param(module, param.data)
+            else:
+                raise TypeError(f'Expected param be None or ParamItem.data as a list. Gotcha {type(geo_param)}')
+
             temp = module.apply_inverse_func
             module.apply_inverse_func = MaskApplyInverse
-            geo_param: List[ParamItem] = _get_geometric_only_param(module, _param)
-            input = cls.make_input_only_sequential(module)(input, label=None, params=geo_param)
+            if isinstance(module, kornia.augmentation.AugmentationSequential):
+                input = cls.make_input_only_sequential(module)(
+                    input, label=None, params=geo_param, data_keys=[cls.data_key]
+                )
+            else:
+                input = cls.make_input_only_sequential(module)(
+                    input, label=None, params=geo_param, extra_args=extra_args
+                )
             module.apply_inverse_func = temp
         else:
             pass  # No need to update anything
         return input, label
 
     @classmethod
     def inverse(
-        cls, input: Tensor, module: nn.Module, param: Optional[ParamItem] = None, extra_args: Dict[str, Any] = {}
+        cls, input: Tensor, module: Module, param: Optional[ParamItem] = None, extra_args: Dict[str, Any] = {}
     ) -> Tensor:
         """Inverse a transformation with respect to the parameters.
 
         Args:
             input: the input tensor.
             module: any torch Module but only kornia augmentation modules will count
                 to apply transformations.
             param: the corresponding parameters to the module.
         """
 
         if isinstance(module, GeometricAugmentationBase2D):
-            input = module.inverse(input, params=None if param is None else cast(Dict, param.data), **extra_args)
+            if param is None:
+                _params_geo = None
+            elif isinstance(param, ParamItem) and isinstance(param.data, dict):
+                _params_geo = param.data
+            else:
+                raise TypeError(f'Expected param (ParamItem.data) be a dict. Gotcha {type(param.data)}')
+
+            input = module.inverse(input, params=_params_geo, **extra_args)
+
         elif isinstance(module, kornia.augmentation.ImageSequential):
             temp = module.apply_inverse_func
             module.apply_inverse_func = MaskApplyInverse
-            input = module.inverse(input, params=None if param is None else cast(List, param.data))
+
+            if param is None:
+                _params = None
+            elif isinstance(param, ParamItem) and isinstance(param.data, list):
+                _params = param.data
+            else:
+                raise TypeError(f'Expected param (ParamItem.data) be a list. Gotcha {type(param.data)}')
+
+            if isinstance(module, kornia.augmentation.AugmentationSequential):
+                _ret = module.inverse(input, params=_params, data_keys=[cls.data_key])
+                if isinstance(_ret, Tensor):
+                    input = _ret
+                else:
+                    raise TypeError(
+                        f'The return of the method inverse from {module} should be a Tensor. Gotcha {type(_ret)}'
+                    )
+            else:
+                input = module.inverse(input, params=_params, extra_args=extra_args)
+
             module.apply_inverse_func = temp
         return input
 
 
 class BBoxApplyInverse(ApplyInverseImpl):
     """Apply and inverse transformations for bounding box tensors.
 
     This is for transform boxes in the format (B, N, 4, 2).
     """
 
     @classmethod
-    def _get_padding_size(cls, module: nn.Module, param: Optional[ParamItem]) -> Optional[Tensor]:
-        if isinstance(module, RandomCrop):
-            _param = cast(Dict[str, Tensor], param.data)  # type: ignore
-            return _param.get("padding_size")
+    def _get_padding_size(cls, module: Module, param: Optional[ParamItem]) -> Optional[Tensor]:
+        if isinstance(module, RandomCrop) and param is not None and isinstance(param.data, dict):
+            return param.data["padding_size"]
         return None
 
     @classmethod
     def pad(cls, input: Tensor, padding_size: Tensor) -> Tensor:
         """
         Args:
             input: (B, N, 4, 2)
@@ -390,20 +428,15 @@
 
         return _input
 
     apply_func = partial(transform_bbox, mode="xyxy", restore_coordinates=True)
 
     @classmethod
     def apply_trans(
-        cls,
-        input: Tensor,
-        label: Optional[Tensor],
-        module: nn.Module,
-        param: ParamItem,
-        extra_args: Dict[str, Any] = {},
+        cls, input: Tensor, label: Optional[Tensor], module: Module, param: ParamItem, extra_args: Dict[str, Any] = {}
     ) -> Tuple[Tensor, Optional[Tensor]]:
         """Apply a transformation with respect to the parameters.
 
         Args:
             input: the input tensor, (B, N, 4, 2) or (B, 4, 2).
             label: the optional label tensor.
             module: any torch Module but only kornia augmentation modules will count
@@ -420,15 +453,15 @@
 
         # TODO: Filter/crop boxes outside crop (with negative or larger than crop size coords)?
 
         return _input, label
 
     @classmethod
     def inverse(
-        cls, input: Tensor, module: nn.Module, param: Optional[ParamItem] = None, extra_args: Dict[str, Any] = {}
+        cls, input: Tensor, module: Module, param: Optional[ParamItem] = None, extra_args: Dict[str, Any] = {}
     ) -> Tensor:
         """Inverse a transformation with respect to the parameters.
 
         Args:
             input: the input tensor.
             module: any torch Module but only kornia augmentation modules will count
                 to apply transformations.
@@ -467,27 +500,22 @@
         for i in range(len(_padding_size)):
             input[i, :, 0::2] -= _padding_size[i][0]  # left padding
             input[i, :, 1::2] -= _padding_size[i][2]  # top padding
         return input
 
     @classmethod
     def apply_trans(
-        cls,
-        input: Tensor,
-        label: Optional[Tensor],
-        module: nn.Module,
-        param: ParamItem,
-        extra_args: Dict[str, Any] = {},
+        cls, input: Tensor, label: Optional[Tensor], module: Module, param: ParamItem, extra_args: Dict[str, Any] = {}
     ) -> Tuple[Tensor, Optional[Tensor]]:
         warnings.warn("BBoxXYXYApplyInverse is no longer maintained. Please use BBoxApplyInverse instead.")
         return super().apply_trans(input, label=label, module=module, param=param, extra_args=extra_args)
 
     @classmethod
     def inverse(
-        cls, input: Tensor, module: nn.Module, param: Optional[ParamItem] = None, extra_args: Dict[str, Any] = {}
+        cls, input: Tensor, module: Module, param: Optional[ParamItem] = None, extra_args: Dict[str, Any] = {}
     ) -> Tensor:
         warnings.warn("BBoxXYXYApplyInverse is no longer maintained. Please use BBoxApplyInverse instead.")
         return super().inverse(input, module=module, param=param, extra_args=extra_args)
 
 
 class BBoxXYWHApplyInverse(BBoxXYXYApplyInverse):
     """Apply and inverse transformations for bounding box tensors.
@@ -523,15 +551,14 @@
     """
 
     # Hot fix for the typing mismatching
     apply_func = partial(transform_points)
 
     @classmethod
     def pad(cls, input: Tensor, padding_size: Tensor) -> Tensor:
-
         if len(input.shape) not in (2, 3):
             raise AssertionError(input.shape)
 
         if len(padding_size.shape) != 2:
             raise AssertionError(padding_size.shape)
 
         _input = input.clone()
@@ -546,15 +573,14 @@
         if input.dim() == 2:
             _input = _input[:, 0]  # squeeze back
 
         return _input
 
     @classmethod
     def unpad(cls, input: Tensor, padding_size: Tensor) -> Tensor:
-
         if len(input.shape) not in (2, 3):
             raise AssertionError(input.shape)
         if len(padding_size.shape) != 2:
             raise AssertionError(padding_size.shape)
 
         _input = input.clone()
 
@@ -589,15 +615,15 @@
         raise NotImplementedError(f"input type of {dcate} is not implemented.")
 
     @classmethod
     def apply_by_key(
         cls,
         input: Tensor,
         label: Optional[Tensor],
-        module: nn.Module,
+        module: Module,
         param: ParamItem,
         dcate: Union[str, int, DataKey] = DataKey.INPUT,
         extra_args: Dict[str, Any] = {},
     ) -> Tuple[Tensor, Optional[Tensor]]:
         """Apply a transformation with respect to the parameters.
 
         Args:
@@ -609,22 +635,22 @@
             dcate: data category. 'input', 'mask', 'bbox', 'bbox_xyxy', 'bbox_xyhw', 'keypoints'.
                 By default, it is set to 'input'.
         """
         func: Type[ApplyInverseInterface] = cls._get_func_by_key(dcate)
 
         if isinstance(input, (tuple,)):
             # If the input is a tuple with (input, mat) or something else
-            return (func.apply_trans(input[0], label, module, param, extra_args), *input[1:])  # type: ignore
+            return (func.apply_trans(input[0], label, module, param, extra_args), *input[1:])
         return func.apply_trans(input, label, module=module, param=param, extra_args=extra_args)
 
     @classmethod
     def inverse_by_key(
         cls,
         input: Tensor,
-        module: nn.Module,
+        module: Module,
         param: Optional[ParamItem] = None,
         dcate: Union[str, int, DataKey] = DataKey.INPUT,
         extra_args: Dict[str, Any] = {},
     ) -> Tensor:
         """Inverse a transformation with respect to the parameters.
 
         Args:
```

### Comparing `kornia-0.6.8/kornia/augmentation/container/video.py` & `kornia-0.6.9/kornia/augmentation/container/video.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,19 +1,18 @@
 from typing import Any, Dict, List, Optional, Tuple, Union, cast
 
 import torch
-import torch.nn as nn
 
 import kornia
-from kornia.augmentation import RandomCrop
-from kornia.augmentation._2d.mix.base import MixAugmentationBase, MixAugmentationBaseV2
+from kornia.augmentation import MixAugmentationBaseV2, RandomCrop
 from kornia.augmentation.base import _AugmentationBase
 from kornia.augmentation.container.base import SequentialBase
 from kornia.augmentation.container.image import ImageSequential, ParamItem, _get_new_batch_shape
 from kornia.augmentation.container.utils import InputApplyInverse, MaskApplyInverse
+from kornia.core import Module, Tensor
 
 __all__ = ["VideoSequential"]
 
 
 class VideoSequential(ImageSequential):
     r"""VideoSequential for processing 5-dim video data like (B, T, C, H, W) and (B, C, T, H, W).
 
@@ -58,20 +57,20 @@
         tensor(True)
 
         If set `same_on_frame` to False:
 
         >>> aug_list = VideoSequential(
         ...     kornia.augmentation.ColorJiggle(0.1, 0.1, 0.1, 0.1, p=1.0),
         ...     kornia.augmentation.RandomAffine(360, p=1.0),
-        ...     kornia.augmentation.RandomMixUp(p=1.0),
+        ...     kornia.augmentation.RandomMixUpV2(p=1.0),
         ... data_format="BCTHW",
         ... same_on_frame=False)
         >>> output, lab = aug_list(input)
         >>> output.shape, lab.shape
-        (torch.Size([2, 3, 4, 5, 6]), torch.Size([2, 4, 3]))
+        (torch.Size([3, 4, 5, 6]), torch.Size([3, 4, 5, 6]))
         >>> (output[0, :, 0] == output[0, :, 1]).all()
         tensor(False)
 
         Reproduce with provided params.
         >>> out2, lab2 = aug_list(input, label, params=aug_list._params)
         >>> torch.equal(output, out2)
         True
@@ -79,28 +78,28 @@
     Perform ``OneOf`` transformation with ``random_apply=1`` and ``random_apply_weights`` in ``VideoSequential``.
 
         >>> import kornia
         >>> input, label = torch.randn(2, 3, 1, 5, 6).repeat(1, 1, 4, 1, 1), torch.tensor([0, 1])
         >>> aug_list = VideoSequential(
         ...     kornia.augmentation.ColorJiggle(0.1, 0.1, 0.1, 0.1, p=1.0),
         ...     kornia.augmentation.RandomAffine(360, p=1.0),
-        ...     kornia.augmentation.RandomMixUp(p=1.0),
+        ...     kornia.augmentation.RandomMixUpV2(p=1.0),
         ... data_format="BCTHW",
         ... same_on_frame=False,
         ... random_apply=1,
         ... random_apply_weights=[0.5, 0.3, 0.8]
         ... )
         >>> out= aug_list(input, label)
         >>> out[0].shape
         torch.Size([2, 3, 4, 5, 6])
     """
 
     def __init__(
         self,
-        *args: nn.Module,
+        *args: Module,
         data_format: str = "BTCHW",
         same_on_frame: bool = True,
         random_apply: Union[int, bool, Tuple[int, int]] = False,
         random_apply_weights: Optional[List[float]] = None,
     ) -> None:
         super().__init__(
             *args,
@@ -119,29 +118,29 @@
         elif self.data_format == "BTCHW":
             self._temporal_channel = 1
 
     def __infer_channel_exclusive_batch_shape__(self, batch_shape: torch.Size, chennel_index: int) -> torch.Size:
         # Fix mypy complains: error: Incompatible return value type (got "Tuple[int, ...]", expected "Size")
         return cast(torch.Size, batch_shape[:chennel_index] + batch_shape[chennel_index + 1 :])
 
-    def __repeat_param_across_channels__(self, param: torch.Tensor, frame_num: int) -> torch.Tensor:
+    def __repeat_param_across_channels__(self, param: Tensor, frame_num: int) -> Tensor:
         """Repeat parameters across channels.
 
         The input is shaped as (B, ...), while to output (B * same_on_frame, ...), which
         to guarantee that the same transformation would happen for each frame.
 
         (B1, B2, ..., Bn) => (B1, ... B1, B2, ..., B2, ..., Bn, ..., Bn)
                               | ch_size | | ch_size |  ..., | ch_size |
         """
         repeated = param[:, None, ...].repeat(1, frame_num, *([1] * len(param.shape[1:])))
-        return repeated.reshape(-1, *list(param.shape[1:]))  # type: ignore
+        return repeated.reshape(-1, *list(param.shape[1:]))
 
     def _input_shape_convert_in(
-        self, input: torch.Tensor, label: Optional[torch.Tensor], frame_num: int
-    ) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:
+        self, input: Tensor, label: Optional[Tensor], frame_num: int
+    ) -> Tuple[Tensor, Optional[Tensor]]:
         # Convert any shape to (B, T, C, H, W)
         if self.data_format == "BCTHW":
             # Convert (B, C, T, H, W) to (B, T, C, H, W)
             input = input.transpose(1, 2)
         if self.data_format == "BTCHW":
             pass
 
@@ -156,16 +155,16 @@
                 pass
             else:
                 raise NotImplementedError(f"Invalid label shape of {label.shape}.")
         input = input.reshape(-1, *input.shape[2:])
         return input, label
 
     def _input_shape_convert_back(
-        self, input: torch.Tensor, label: Optional[torch.Tensor], frame_num: int
-    ) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:
+        self, input: Tensor, label: Optional[Tensor], frame_num: int
+    ) -> Tuple[Tensor, Optional[Tensor]]:
         input = input.view(-1, frame_num, *input.shape[1:])
         if self.data_format == "BCTHW":
             input = input.transpose(1, 2)
         if self.data_format == "BTCHW":
             pass
 
         if label is not None:
@@ -191,15 +190,15 @@
                     mod_param["dst"] = mod_param["dst"].repeat(frame_num, 1, 1)
                 param = ParamItem(name, mod_param)
             elif isinstance(module, (SequentialBase,)):
                 seq_param = module.forward_parameters(batch_shape)
                 if self.same_on_frame:
                     raise ValueError("Sequential is currently unsupported for ``same_on_frame``.")
                 param = ParamItem(name, seq_param)
-            elif isinstance(module, (_AugmentationBase, MixAugmentationBase, MixAugmentationBaseV2)):
+            elif isinstance(module, (_AugmentationBase, MixAugmentationBaseV2)):
                 mod_param = module.forward_parameters(batch_shape)
                 if self.same_on_frame:
                     for k, v in mod_param.items():
                         # TODO: revise ColorJiggle and ColorJitter order param in the future to align the standard.
                         if k == "order" and (
                             isinstance(module, kornia.augmentation.ColorJiggle)
                             or isinstance(module, kornia.augmentation.ColorJitter)
@@ -213,16 +212,16 @@
             else:
                 param = ParamItem(name, None)
             batch_shape = _get_new_batch_shape(param, batch_shape)
             params.append(param)
         return params
 
     def inverse(
-        self, input: torch.Tensor, params: Optional[List[ParamItem]] = None, extra_args: Dict[str, Any] = {}
-    ) -> torch.Tensor:
+        self, input: Tensor, params: Optional[List[ParamItem]] = None, extra_args: Dict[str, Any] = {}
+    ) -> Tensor:
         """Inverse transformation.
 
         Used to inverse a tensor according to the performed transformation by a forward pass, or with respect to
         provided parameters.
         """
         if self.apply_inverse_func in (InputApplyInverse, MaskApplyInverse):
             frame_num: int = input.size(self._temporal_channel)
@@ -235,21 +234,21 @@
         if self.apply_inverse_func in (InputApplyInverse, MaskApplyInverse):
             input, _ = self._input_shape_convert_back(input, None, frame_num)
         else:
             input = input.view(batch_size, -1, *input.shape[1:])
 
         return input
 
-    def forward(  # type: ignore
+    def forward(
         self,
-        input: torch.Tensor,
-        label: Optional[torch.Tensor] = None,
+        input: Tensor,
+        label: Optional[Tensor] = None,
         params: Optional[List[ParamItem]] = None,
         extra_args: Dict[str, Any] = {},
-    ) -> Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:
+    ) -> Union[Tensor, Tuple[Tensor, Optional[Tensor]]]:
         """Define the video computation performed."""
         if len(input.shape) != 5:
             raise AssertionError(f"Input must be a 5-dim tensor. Got {input.shape}.")
 
         if params is None:
             params = self.forward_parameters(input.shape)
 
@@ -259,19 +258,19 @@
             input, label = self._input_shape_convert_in(input, label, frame_num)
         else:
             if label is not None:
                 raise ValueError(f"Invalid label value. Got {label}")
             batch_size: int = input.size(0)
             input = input.view(-1, *input.shape[2:])
 
-        out = super().forward(input, label, params, extra_args=extra_args)  # type: ignore
+        out = super().forward(input, label, params, extra_args=extra_args)
         if self.return_label:
-            output, label = cast(Tuple[torch.Tensor, torch.Tensor], out)
+            output, label = cast(Tuple[Tensor, Tensor], out)
         else:
-            output = cast(torch.Tensor, out)
+            output = cast(Tensor, out)
 
         if isinstance(output, (tuple, list)):
             if self.apply_inverse_func in (InputApplyInverse, MaskApplyInverse):
                 _out, label = self._input_shape_convert_back(output[0], label, frame_num)
                 output = (_out, output[1])
             else:
                 if label is not None:
```

### Comparing `kornia-0.6.8/kornia/augmentation/random_generator/_2d/__init__.py` & `kornia-0.6.9/kornia/augmentation/random_generator/_2d/__init__.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,13 +1,12 @@
 from kornia.augmentation.random_generator._2d.affine import *
 from kornia.augmentation.random_generator._2d.color_jiggle import *
 from kornia.augmentation.random_generator._2d.color_jitter import *
 from kornia.augmentation.random_generator._2d.crop import *
 from kornia.augmentation.random_generator._2d.cutmix import *
-from kornia.augmentation.random_generator._2d.func import *
 from kornia.augmentation.random_generator._2d.jigsaw import *
 from kornia.augmentation.random_generator._2d.mixup import *
 from kornia.augmentation.random_generator._2d.mosaic import *
 from kornia.augmentation.random_generator._2d.motion_blur import *
 from kornia.augmentation.random_generator._2d.perspective import *
 from kornia.augmentation.random_generator._2d.plain_uniform import *
 from kornia.augmentation.random_generator._2d.planckian_jitter import *
```

### Comparing `kornia-0.6.8/kornia/augmentation/random_generator/_2d/affine.py` & `kornia-0.6.9/kornia/augmentation/random_generator/_3d/affine.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,288 +1,219 @@
-from typing import Dict, Optional, Tuple, Union, cast
+from typing import Dict, Optional, Tuple, Union
 
 import torch
 from torch.distributions import Uniform
 
 from kornia.augmentation.random_generator.base import RandomGeneratorBase
-from kornia.augmentation.utils import (
-    _adapted_rsampling,
-    _adapted_uniform,
-    _common_param_check,
-    _joint_range_check,
-    _range_bound,
-)
-from kornia.utils.helpers import _deprecated, _extract_device_dtype
+from kornia.augmentation.utils import _adapted_rsampling, _singular_range_check, _tuple_range_reader
+from kornia.utils.helpers import _extract_device_dtype
 
 
-class AffineGenerator(RandomGeneratorBase):
-    r"""Get parameters for ``affine`` for a random affine transform.
+class AffineGenerator3D(RandomGeneratorBase):
+    r"""Get parameters for ```3d affine``` transformation random affine transform.
 
     Args:
-        degrees: Range of degrees to select from like (min, max).
-        translate: tuple of maximum absolute fraction for horizontal
-            and vertical translations. For example translate=(a, b), then horizontal shift
-            is randomly sampled in the range -img_width * a < dx < img_width * a and vertical shift is
-            randomly sampled in the range -img_height * b < dy < img_height * b. Will not translate by default.
-        scale: scaling factor interval, e.g (a, b), then scale is
-            randomly sampled from the range a <= scale <= b. Will keep original scale by default.
-        shear: Range of degrees to select from.
-            If float, a shear parallel to the x axis in the range (-shear, +shear) will be applied.
-            If (a, b), a shear parallel to the x axis in the range (-shear, +shear) will be applied.
-            If (a, b, c, d), then x-axis shear in (shear[0], shear[1]) and y-axis shear in (shear[2], shear[3])
-            will be applied. Will not apply shear by default.
-            If tensor, shear is a 2x2 tensor, a x-axis shear in (shear[0][0], shear[0][1]) and y-axis shear in
-            (shear[1][0], shear[1][1]) will be applied. Will not apply shear by default.
+        degrees: Range of yaw (x-axis), pitch (y-axis), roll (z-axis) to select from.
+            If degrees is a number, then yaw, pitch, roll will be generated from the range of (-degrees, +degrees).
+            If degrees is a tuple of (min, max), then yaw, pitch, roll will be generated from the range of (min, max).
+            If degrees is a list of floats [a, b, c], then yaw, pitch, roll will be generated from (-a, a), (-b, b)
+            and (-c, c).
+            If degrees is a list of tuple ((a, b), (m, n), (x, y)), then yaw, pitch, roll will be generated from
+            (a, b), (m, n) and (x, y).
+            Set to 0 to deactivate rotations.
+        translate: tuple of maximum absolute fraction for horizontal, vertical and
+            depthical translations (dx,dy,dz). For example translate=(a, b, c), then
+            horizontal shift will be randomly sampled in the range -img_width * a < dx < img_width * a
+            vertical shift will be randomly sampled in the range -img_height * b < dy < img_height * b.
+            depthical shift will be randomly sampled in the range -img_depth * c < dz < img_depth * c.
+            Will not translate by default.
+        scale: scaling factor interval.
+            If (a, b) represents isotropic scaling, the scale is randomly sampled from the range a <= scale <= b.
+            If ((a, b), (c, d), (e, f)), the scale is randomly sampled from the range a <= scale_x <= b,
+            c <= scale_y <= d, e <= scale_z <= f. Will keep original scale by default.
+        shears: Range of degrees to select from.
+            If shear is a number, a shear to the 6 facets in the range (-shear, +shear) will be applied.
+            If shear is a tuple of 2 values, a shear to the 6 facets in the range (shear[0], shear[1]) will be applied.
+            If shear is a tuple of 6 values, a shear to the i-th facet in the range (-shear[i], shear[i])
+            will be applied.
+            If shear is a tuple of 6 tuples, a shear to the i-th facet in the range (-shear[i, 0], shear[i, 1])
+            will be applied.
 
     Returns:
         A dict of parameters to be passed for transformation.
-            - translations (torch.Tensor): element-wise translations with a shape of (B, 2).
-            - center (torch.Tensor): element-wise center with a shape of (B, 2).
-            - scale (torch.Tensor): element-wise scales with a shape of (B, 2).
-            - angle (torch.Tensor): element-wise rotation angles with a shape of (B,).
-            - sx (torch.Tensor): element-wise x-axis shears with a shape of (B,).
-            - sy (torch.Tensor): element-wise y-axis shears with a shape of (B,).
+            - translations (torch.Tensor): element-wise translations with a shape of (B, 3).
+            - center (torch.Tensor): element-wise center with a shape of (B, 3).
+            - scale (torch.Tensor): element-wise scales with a shape of (B, 3).
+            - angle (torch.Tensor): element-wise rotation angles with a shape of (B, 3).
+            - sxy (torch.Tensor): element-wise x-y-facet shears with a shape of (B,).
+            - sxz (torch.Tensor): element-wise x-z-facet shears with a shape of (B,).
+            - syx (torch.Tensor): element-wise y-x-facet shears with a shape of (B,).
+            - syz (torch.Tensor): element-wise y-z-facet shears with a shape of (B,).
+            - szx (torch.Tensor): element-wise z-x-facet shears with a shape of (B,).
+            - szy (torch.Tensor): element-wise z-y-facet shears with a shape of (B,).
 
     Note:
         The generated random numbers are not reproducible across different devices and dtypes. By default,
         the parameters will be generated on CPU in float32. This can be changed by calling
         ``self.set_rng_device_and_dtype(device="cuda", dtype=torch.float64)``.
     """
 
     def __init__(
         self,
-        degrees: Union[torch.Tensor, float, Tuple[float, float]],
-        translate: Optional[Union[torch.Tensor, Tuple[float, float]]] = None,
-        scale: Optional[Union[torch.Tensor, Tuple[float, float], Tuple[float, float, float, float]]] = None,
-        shear: Optional[Union[torch.Tensor, float, Tuple[float, float]]] = None,
+        degrees: Union[
+            torch.Tensor,
+            float,
+            Tuple[float, float],
+            Tuple[float, float, float],
+            Tuple[Tuple[float, float], Tuple[float, float], Tuple[float, float]],
+        ],
+        translate: Optional[Union[torch.Tensor, Tuple[float, float, float]]] = None,
+        scale: Optional[
+            Union[
+                torch.Tensor, Tuple[float, float], Tuple[Tuple[float, float], Tuple[float, float], Tuple[float, float]]
+            ]
+        ] = None,
+        shears: Union[
+            None,
+            torch.Tensor,
+            float,
+            Tuple[float, float],
+            Tuple[float, float, float, float, float, float],
+            Tuple[
+                Tuple[float, float],
+                Tuple[float, float],
+                Tuple[float, float],
+                Tuple[float, float],
+                Tuple[float, float],
+                Tuple[float, float],
+            ],
+        ] = None,
     ) -> None:
         super().__init__()
         self.degrees = degrees
+        self.shears = shears
         self.translate = translate
         self.scale = scale
-        self.shear = shear
 
     def __repr__(self) -> str:
-        repr = f"degrees={self.degrees}, translate={self.translate}, scale={self.scale}, shear={self.shear}"
+        repr = f"degrees={self.degrees}, shears={self.shears}, translate={self.translate}, scale={self.scale}"
         return repr
 
     def make_samplers(self, device: torch.device, dtype: torch.dtype) -> None:
-        _degrees = _range_bound(self.degrees, 'degrees', 0, (-360, 360)).to(device=device, dtype=dtype)
-        _translate = (
-            self.translate
-            if self.translate is None
-            else _range_bound(self.translate, 'translate', bounds=(0, 1), check='singular').to(
-                device=device, dtype=dtype
-            )
-        )
-        _scale: Optional[torch.Tensor] = None
+        degrees = _tuple_range_reader(self.degrees, 3, device, dtype)
+        shear: Optional[torch.Tensor] = None
+        if self.shears is not None:
+            shear = _tuple_range_reader(self.shears, 6, device, dtype)
+            self.sxy_sampler = Uniform(shear[0, 0], shear[0, 1], validate_args=False)
+            self.sxz_sampler = Uniform(shear[1, 0], shear[1, 1], validate_args=False)
+            self.syx_sampler = Uniform(shear[2, 0], shear[2, 1], validate_args=False)
+            self.syz_sampler = Uniform(shear[3, 0], shear[3, 1], validate_args=False)
+            self.szx_sampler = Uniform(shear[4, 0], shear[4, 1], validate_args=False)
+            self.szy_sampler = Uniform(shear[5, 0], shear[5, 1], validate_args=False)
+
+        # check translation range
+        self._translate: Optional[torch.Tensor] = None
+        if self.translate is not None:
+            self._translate = torch.as_tensor(self.translate, device=device, dtype=dtype)
+            _singular_range_check(self._translate, 'translate', bounds=(0, 1), mode='3d')
+
+        # check scale range
+        self._scale: Optional[torch.Tensor] = None
         if self.scale is not None:
-            if len(self.scale) == 2:
-                _scale = _range_bound(self.scale[:2], 'scale', bounds=(0, float('inf')), check='singular').to(
-                    device=device, dtype=dtype
-                )
-            elif len(self.scale) == 4:
-                _scale = torch.cat(
-                    [
-                        _range_bound(self.scale[:2], 'scale_x', bounds=(0, float('inf')), check='singular'),
-                        _range_bound(
-                            self.scale[2:], 'scale_y', bounds=(0, float('inf')), check='singular'  # type:ignore
-                        ),
-                    ]
-                ).to(device=device, dtype=dtype)
-            else:
-                raise ValueError(f"'scale' expected to be either 2 or 4 elements. Got {self.scale}")
-        _shear: Optional[torch.Tensor] = None
-        if self.shear is not None:
-            shear = torch.as_tensor(self.shear, device=device, dtype=dtype)
-            if shear.shape == torch.Size([2, 2]):
-                _shear = shear
-            else:
-                _shear = torch.stack(
-                    [
-                        _range_bound(shear if shear.dim() == 0 else shear[:2], 'shear-x', 0, (-360, 360)),
-                        torch.tensor([0, 0], device=device, dtype=dtype)
-                        if shear.dim() == 0 or len(shear) == 2
-                        else _range_bound(shear[2:], 'shear-y', 0, (-360, 360)),
-                    ]
-                )
-
-        translate_x_sampler: Optional[Uniform] = None
-        translate_y_sampler: Optional[Uniform] = None
-        scale_2_sampler: Optional[Uniform] = None
-        scale_4_sampler: Optional[Uniform] = None
-        shear_x_sampler: Optional[Uniform] = None
-        shear_y_sampler: Optional[Uniform] = None
-
-        if _translate is not None:
-            translate_x_sampler = Uniform(-_translate[0], _translate[0], validate_args=False)
-            translate_y_sampler = Uniform(-_translate[1], _translate[1], validate_args=False)
-        if _scale is not None:
-            if len(_scale) == 2:
-                scale_2_sampler = Uniform(_scale[0], _scale[1], validate_args=False)
-            elif len(_scale) == 4:
-                scale_2_sampler = Uniform(_scale[0], _scale[1], validate_args=False)
-                scale_4_sampler = Uniform(_scale[2], _scale[3], validate_args=False)
+            _scale = torch.as_tensor(self.scale, device=device, dtype=dtype)
+            if _scale.shape == torch.Size([2]):
+                self._scale = _scale.unsqueeze(0).repeat(3, 1)
+            elif _scale.shape != torch.Size([3, 2]):
+                raise ValueError(f"'scale' shall be either shape (2) or (3, 2). Got {self.scale}.")
             else:
-                raise ValueError(f"'scale' expected to be either 2 or 4 elements. Got {self.scale}")
-        if _shear is not None:
-            _joint_range_check(cast(torch.Tensor, _shear)[0], "shear")
-            _joint_range_check(cast(torch.Tensor, _shear)[1], "shear")
-            shear_x_sampler = Uniform(_shear[0][0], _shear[0][1], validate_args=False)
-            shear_y_sampler = Uniform(_shear[1][0], _shear[1][1], validate_args=False)
-
-        self.degree_sampler = Uniform(_degrees[0], _degrees[1], validate_args=False)
-        self.translate_x_sampler = translate_x_sampler
-        self.translate_y_sampler = translate_y_sampler
-        self.scale_2_sampler = scale_2_sampler
-        self.scale_4_sampler = scale_4_sampler
-        self.shear_x_sampler = shear_x_sampler
-        self.shear_y_sampler = shear_y_sampler
+                self._scale = _scale
+            _singular_range_check(self._scale[0], 'scale-x', bounds=(0, float('inf')), mode='2d')
+            _singular_range_check(self._scale[1], 'scale-y', bounds=(0, float('inf')), mode='2d')
+            _singular_range_check(self._scale[2], 'scale-z', bounds=(0, float('inf')), mode='2d')
+            self.scale_1_sampler = Uniform(self._scale[0, 0], self._scale[0, 1], validate_args=False)
+            self.scale_2_sampler = Uniform(self._scale[1, 0], self._scale[1, 1], validate_args=False)
+            self.scale_3_sampler = Uniform(self._scale[2, 0], self._scale[2, 1], validate_args=False)
+
+        self.yaw_sampler = Uniform(degrees[0][0], degrees[0][1], validate_args=False)
+        self.pitch_sampler = Uniform(degrees[1][0], degrees[1][1], validate_args=False)
+        self.roll_sampler = Uniform(degrees[2][0], degrees[2][1], validate_args=False)
+
+        self.uniform_sampler = Uniform(
+            torch.tensor(0, device=device, dtype=dtype),
+            torch.tensor(1, device=device, dtype=dtype),
+            validate_args=False,
+        )
 
-    def forward(self, batch_shape: torch.Size, same_on_batch: bool = False) -> Dict[str, torch.Tensor]:  # type: ignore
+    def forward(self, batch_shape: torch.Size, same_on_batch: bool = False) -> Dict[str, torch.Tensor]:
         batch_size = batch_shape[0]
+        depth = batch_shape[-3]
         height = batch_shape[-2]
         width = batch_shape[-1]
 
-        _device, _dtype = _extract_device_dtype([self.degrees, self.translate, self.scale, self.shear])
-        _common_param_check(batch_size, same_on_batch)
-        if not (isinstance(width, (int,)) and isinstance(height, (int,)) and width > 0 and height > 0):
-            raise AssertionError(f"`width` and `height` must be positive integers. Got {width}, {height}.")
-
-        angle = _adapted_rsampling((batch_size,), self.degree_sampler, same_on_batch).to(device=_device, dtype=_dtype)
+        if not (
+            type(depth) is int and depth > 0 and type(height) is int and height > 0 and type(width) is int and width > 0
+        ):
+            raise AssertionError(f"'depth', 'height' and 'width' must be integers. Got {depth}, {height}, {width}.")
+
+        _device, _dtype = _extract_device_dtype([self.degrees, self.translate, self.scale, self.shears])
+
+        # degrees = degrees.to(device=device, dtype=dtype)
+        yaw = _adapted_rsampling((batch_size,), self.yaw_sampler, same_on_batch)
+        pitch = _adapted_rsampling((batch_size,), self.pitch_sampler, same_on_batch)
+        roll = _adapted_rsampling((batch_size,), self.roll_sampler, same_on_batch)
+        angles = torch.stack([yaw, pitch, roll], dim=1)
 
         # compute tensor ranges
-        if self.scale_2_sampler is not None:
-            _scale = _adapted_rsampling((batch_size,), self.scale_2_sampler, same_on_batch).unsqueeze(1).repeat(1, 2)
-            if self.scale_4_sampler is not None:
-                _scale[:, 1] = _adapted_rsampling((batch_size,), self.scale_4_sampler, same_on_batch)
-            _scale = _scale.to(device=_device, dtype=_dtype)
+        if self._scale is not None:
+            scale = torch.stack(
+                [
+                    _adapted_rsampling((batch_size,), self.scale_1_sampler, same_on_batch),
+                    _adapted_rsampling((batch_size,), self.scale_2_sampler, same_on_batch),
+                    _adapted_rsampling((batch_size,), self.scale_3_sampler, same_on_batch),
+                ],
+                dim=1,
+            )
         else:
-            _scale = torch.ones((batch_size, 2), device=_device, dtype=_dtype)
+            scale = torch.ones(batch_size, device=_device, dtype=_dtype).reshape(batch_size, 1).repeat(1, 3)
 
-        if self.translate_x_sampler is not None and self.translate_y_sampler is not None:
+        if self._translate is not None:
+            max_dx: torch.Tensor = self._translate[0] * width
+            max_dy: torch.Tensor = self._translate[1] * height
+            max_dz: torch.Tensor = self._translate[2] * depth
+            # translations should be in x,y,z
             translations = torch.stack(
                 [
-                    _adapted_rsampling((batch_size,), self.translate_x_sampler, same_on_batch) * width,
-                    _adapted_rsampling((batch_size,), self.translate_y_sampler, same_on_batch) * height,
+                    (_adapted_rsampling((batch_size,), self.uniform_sampler, same_on_batch) - 0.5) * max_dx * 2,
+                    (_adapted_rsampling((batch_size,), self.uniform_sampler, same_on_batch) - 0.5) * max_dy * 2,
+                    (_adapted_rsampling((batch_size,), self.uniform_sampler, same_on_batch) - 0.5) * max_dz * 2,
                 ],
-                dim=-1,
+                dim=1,
             )
-            translations = translations.to(device=_device, dtype=_dtype)
         else:
-            translations = torch.zeros((batch_size, 2), device=_device, dtype=_dtype)
+            translations = torch.zeros((batch_size, 3), device=_device, dtype=_dtype)
 
-        center: torch.Tensor = torch.tensor([width, height], device=_device, dtype=_dtype).view(1, 2) / 2.0 - 0.5
+        # center should be in x,y,z
+        center: torch.Tensor = torch.tensor([width, height, depth], device=_device, dtype=_dtype).view(1, 3) / 2.0 - 0.5
         center = center.expand(batch_size, -1)
 
-        if self.shear_x_sampler is not None and self.shear_y_sampler is not None:
-            sx = _adapted_rsampling((batch_size,), self.shear_x_sampler, same_on_batch)
-            sy = _adapted_rsampling((batch_size,), self.shear_y_sampler, same_on_batch)
-            sx = sx.to(device=_device, dtype=_dtype)
-            sy = sy.to(device=_device, dtype=_dtype)
+        if self.shears is not None:
+            sxy = _adapted_rsampling((batch_size,), self.sxy_sampler, same_on_batch)
+            sxz = _adapted_rsampling((batch_size,), self.sxz_sampler, same_on_batch)
+            syx = _adapted_rsampling((batch_size,), self.syx_sampler, same_on_batch)
+            syz = _adapted_rsampling((batch_size,), self.syz_sampler, same_on_batch)
+            szx = _adapted_rsampling((batch_size,), self.szx_sampler, same_on_batch)
+            szy = _adapted_rsampling((batch_size,), self.szy_sampler, same_on_batch)
         else:
-            sx = sy = torch.tensor([0] * batch_size, device=_device, dtype=_dtype)
+            sxy = sxz = syx = syz = szx = szy = torch.tensor([0] * batch_size, device=_device, dtype=_dtype)
 
-        return dict(translations=translations, center=center, scale=_scale, angle=angle, sx=sx, sy=sy)
-
-
-@_deprecated(replace_with=AffineGenerator.__name__)
-def random_affine_generator(
-    batch_size: int,
-    height: int,
-    width: int,
-    degrees: torch.Tensor,
-    translate: Optional[torch.Tensor] = None,
-    scale: Optional[torch.Tensor] = None,
-    shear: Optional[torch.Tensor] = None,
-    same_on_batch: bool = False,
-    device: torch.device = torch.device('cpu'),
-    dtype: torch.dtype = torch.float32,
-) -> Dict[str, torch.Tensor]:
-    r"""Get parameters for ``affine`` for a random affine transform.
-
-    Args:
-        batch_size (int): the tensor batch size.
-        height (int) : height of the image.
-        width (int): width of the image.
-        degrees (torch.Tensor): Range of degrees to select from like (min, max).
-        translate (tensor, optional): tuple of maximum absolute fraction for horizontal
-            and vertical translations. For example translate=(a, b), then horizontal shift
-            is randomly sampled in the range -img_width * a < dx < img_width * a and vertical shift is
-            randomly sampled in the range -img_height * b < dy < img_height * b. Will not translate by default.
-        scale (tensor, optional): scaling factor interval, e.g (a, b), then scale is
-            randomly sampled from the range a <= scale <= b. Will keep original scale by default.
-        shear (tensor, optional): Range of degrees to select from.
-            Shear is a 2x2 tensor, a x-axis shear in (shear[0][0], shear[0][1]) and y-axis shear in
-            (shear[1][0], shear[1][1]) will be applied. Will not apply shear by default.
-        same_on_batch (bool): apply the same transformation across the batch. Default: False.
-        device (torch.device): the device on which the random numbers will be generated. Default: cpu.
-        dtype (torch.dtype): the data type of the generated random numbers. Default: float32.
-
-    Returns:
-        params Dict[str, torch.Tensor]: parameters to be passed for transformation.
-            - translations (torch.Tensor): element-wise translations with a shape of (B, 2).
-            - center (torch.Tensor): element-wise center with a shape of (B, 2).
-            - scale (torch.Tensor): element-wise scales with a shape of (B, 2).
-            - angle (torch.Tensor): element-wise rotation angles with a shape of (B,).
-            - sx (torch.Tensor): element-wise x-axis shears with a shape of (B,).
-            - sy (torch.Tensor): element-wise y-axis shears with a shape of (B,).
-
-    Note:
-        The generated random numbers are not reproducible across different devices and dtypes.
-    """
-    _common_param_check(batch_size, same_on_batch)
-    _joint_range_check(degrees, "degrees")
-    if not (isinstance(width, (int,)) and isinstance(height, (int,)) and width > 0 and height > 0):
-        raise AssertionError(f"`width` and `height` must be positive integers. Got {width}, {height}.")
-
-    _device, _dtype = _extract_device_dtype([degrees, translate, scale, shear])
-    degrees = degrees.to(device=device, dtype=dtype)
-    angle = _adapted_uniform((batch_size,), degrees[0], degrees[1], same_on_batch)
-    angle = angle.to(device=_device, dtype=_dtype)
-
-    # compute tensor ranges
-    if scale is not None:
-        scale = scale.to(device=device, dtype=dtype)
-        if not (len(scale.shape) == 1 and len(scale) in (2, 4)):
-            raise AssertionError(f"`scale` shall have 2 or 4 elements. Got {scale}.")
-        _joint_range_check(cast(torch.Tensor, scale[:2]), "scale")
-        _scale = _adapted_uniform((batch_size,), scale[0], scale[1], same_on_batch).unsqueeze(1).repeat(1, 2)
-        if len(scale) == 4:
-            _joint_range_check(cast(torch.Tensor, scale[2:]), "scale_y")
-            _scale[:, 1] = _adapted_uniform((batch_size,), scale[2], scale[3], same_on_batch)
-        _scale = _scale.to(device=_device, dtype=_dtype)
-    else:
-        _scale = torch.ones((batch_size, 2), device=_device, dtype=_dtype)
-
-    if translate is not None:
-        translate = translate.to(device=device, dtype=dtype)
-        if not (0.0 <= translate[0] <= 1.0 and 0.0 <= translate[1] <= 1.0 and translate.shape == torch.Size([2])):
-            raise AssertionError(f"Expect translate contains two elements and ranges are in [0, 1]. Got {translate}.")
-        max_dx: torch.Tensor = translate[0] * width
-        max_dy: torch.Tensor = translate[1] * height
-        translations = torch.stack(
-            [
-                _adapted_uniform((batch_size,), -max_dx, max_dx, same_on_batch),
-                _adapted_uniform((batch_size,), -max_dy, max_dy, same_on_batch),
-            ],
-            dim=-1,
+        return dict(
+            translations=torch.as_tensor(translations, device=_device, dtype=_dtype),
+            center=torch.as_tensor(center, device=_device, dtype=_dtype),
+            scale=torch.as_tensor(scale, device=_device, dtype=_dtype),
+            angles=torch.as_tensor(angles, device=_device, dtype=_dtype),
+            sxy=torch.as_tensor(sxy, device=_device, dtype=_dtype),
+            sxz=torch.as_tensor(sxz, device=_device, dtype=_dtype),
+            syx=torch.as_tensor(syx, device=_device, dtype=_dtype),
+            syz=torch.as_tensor(syz, device=_device, dtype=_dtype),
+            szx=torch.as_tensor(szx, device=_device, dtype=_dtype),
+            szy=torch.as_tensor(szy, device=_device, dtype=_dtype),
         )
-        translations = translations.to(device=_device, dtype=_dtype)
-    else:
-        translations = torch.zeros((batch_size, 2), device=_device, dtype=_dtype)
-
-    center: torch.Tensor = torch.tensor([width, height], device=_device, dtype=_dtype).view(1, 2) / 2.0 - 0.5
-    center = center.expand(batch_size, -1)
-
-    if shear is not None:
-        shear = shear.to(device=device, dtype=dtype)
-        _joint_range_check(cast(torch.Tensor, shear)[0], "shear")
-        _joint_range_check(cast(torch.Tensor, shear)[1], "shear")
-        sx = _adapted_uniform((batch_size,), shear[0][0], shear[0][1], same_on_batch)
-        sy = _adapted_uniform((batch_size,), shear[1][0], shear[1][1], same_on_batch)
-        sx = sx.to(device=_device, dtype=_dtype)
-        sy = sy.to(device=_device, dtype=_dtype)
-    else:
-        sx = sy = torch.tensor([0] * batch_size, device=_device, dtype=_dtype)
-
-    return dict(translations=translations, center=center, scale=_scale, angle=angle, sx=sx, sy=sy)
```

### Comparing `kornia-0.6.8/kornia/augmentation/random_generator/_2d/color_jiggle.py` & `kornia-0.6.9/kornia/augmentation/random_generator/_2d/color_jiggle.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,16 +1,16 @@
 from functools import partial
 from typing import Dict, List, Tuple, Union
 
 import torch
-from torch import Tensor
 from torch.distributions import Uniform
 
 from kornia.augmentation.random_generator.base import RandomGeneratorBase
 from kornia.augmentation.utils import _adapted_rsampling, _common_param_check, _joint_range_check, _range_bound
+from kornia.core import Tensor
 from kornia.utils.helpers import _extract_device_dtype
 
 
 class ColorJiggleGenerator(RandomGeneratorBase):
     r"""Generate random color jiter parameters for a batch of images following OpenCV.
 
     Args:
@@ -67,15 +67,15 @@
 
         self.brightness_sampler = Uniform(brightness[0], brightness[1], validate_args=False)
         self.contrast_sampler = Uniform(contrast[0], contrast[1], validate_args=False)
         self.hue_sampler = Uniform(hue[0], hue[1], validate_args=False)
         self.saturation_sampler = Uniform(saturation[0], saturation[1], validate_args=False)
         self.randperm = partial(torch.randperm, device=device, dtype=dtype)
 
-    def forward(self, batch_shape: torch.Size, same_on_batch: bool = False) -> Dict[str, Tensor]:  # type:ignore
+    def forward(self, batch_shape: torch.Size, same_on_batch: bool = False) -> Dict[str, Tensor]:
         batch_size = batch_shape[0]
         _common_param_check(batch_size, same_on_batch)
         _device, _dtype = _extract_device_dtype([self.brightness, self.contrast, self.hue, self.saturation])
         brightness_factor = _adapted_rsampling((batch_size,), self.brightness_sampler, same_on_batch)
         contrast_factor = _adapted_rsampling((batch_size,), self.contrast_sampler, same_on_batch)
         hue_factor = _adapted_rsampling((batch_size,), self.hue_sampler, same_on_batch)
         saturation_factor = _adapted_rsampling((batch_size,), self.saturation_sampler, same_on_batch)
```

### Comparing `kornia-0.6.8/kornia/augmentation/random_generator/_2d/color_jitter.py` & `kornia-0.6.9/kornia/augmentation/random_generator/_2d/color_jitter.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,16 +1,16 @@
 from functools import partial
 from typing import Dict, List, Tuple, Union
 
 import torch
-from torch import Tensor
 from torch.distributions import Uniform
 
 from kornia.augmentation.random_generator.base import RandomGeneratorBase
 from kornia.augmentation.utils import _adapted_rsampling, _common_param_check, _joint_range_check, _range_bound
+from kornia.core import Tensor
 from kornia.utils.helpers import _extract_device_dtype
 
 
 class ColorJitterGenerator(RandomGeneratorBase):
     r"""Generate random color jiter parameters for a batch of images following Pil.
 
     This implementation is for maintaining compatibility with torchvision. It does not
@@ -71,15 +71,15 @@
 
         self.brightness_sampler = Uniform(brightness[0], brightness[1], validate_args=False)
         self.contrast_sampler = Uniform(contrast[0], contrast[1], validate_args=False)
         self.hue_sampler = Uniform(hue[0], hue[1], validate_args=False)
         self.saturation_sampler = Uniform(saturation[0], saturation[1], validate_args=False)
         self.randperm = partial(torch.randperm, device=device, dtype=dtype)
 
-    def forward(self, batch_shape: torch.Size, same_on_batch: bool = False) -> Dict[str, Tensor]:  # type:ignore
+    def forward(self, batch_shape: torch.Size, same_on_batch: bool = False) -> Dict[str, Tensor]:
         batch_size = batch_shape[0]
         _common_param_check(batch_size, same_on_batch)
         _device, _dtype = _extract_device_dtype([self.brightness, self.contrast, self.hue, self.saturation])
         brightness_factor = _adapted_rsampling((batch_size,), self.brightness_sampler, same_on_batch)
         contrast_factor = _adapted_rsampling((batch_size,), self.contrast_sampler, same_on_batch)
         hue_factor = _adapted_rsampling((batch_size,), self.hue_sampler, same_on_batch)
         saturation_factor = _adapted_rsampling((batch_size,), self.saturation_sampler, same_on_batch)
```

### Comparing `kornia-0.6.8/kornia/augmentation/random_generator/_2d/cutmix.py` & `kornia-0.6.9/kornia/augmentation/random_generator/_2d/cutmix.py`

 * *Files identical despite different names*

### Comparing `kornia-0.6.8/kornia/augmentation/random_generator/_2d/jigsaw.py` & `kornia-0.6.9/kornia/augmentation/random_generator/_2d/jigsaw.py`

 * *Files identical despite different names*

### Comparing `kornia-0.6.8/kornia/augmentation/random_generator/_2d/mixup.py` & `kornia-0.6.9/kornia/augmentation/random_generator/_2d/mixup.py`

 * *Files identical despite different names*

### Comparing `kornia-0.6.8/kornia/augmentation/random_generator/_2d/mosaic.py` & `kornia-0.6.9/kornia/augmentation/random_generator/_2d/mosaic.py`

 * *Files identical despite different names*

### Comparing `kornia-0.6.8/kornia/augmentation/random_generator/_2d/motion_blur.py` & `kornia-0.6.9/kornia/augmentation/random_generator/_3d/motion_blur.py`

 * *Files 21% similar despite different names*

```diff
@@ -1,24 +1,19 @@
 from typing import Dict, Tuple, Union
 
 import torch
 from torch.distributions import Uniform
 
 from kornia.augmentation.random_generator.base import RandomGeneratorBase
-from kornia.augmentation.utils import (
-    _adapted_rsampling,
-    _adapted_uniform,
-    _common_param_check,
-    _joint_range_check,
-    _range_bound,
-)
-from kornia.utils.helpers import _deprecated, _extract_device_dtype
+from kornia.augmentation.utils import _adapted_rsampling, _common_param_check, _range_bound, _tuple_range_reader
+from kornia.core import Tensor, stack
+from kornia.utils.helpers import _extract_device_dtype
 
 
-class MotionBlurGenerator(RandomGeneratorBase):
+class MotionBlurGenerator3D(RandomGeneratorBase):
     r"""Get parameters for motion blur.
 
     Args:
         kernel_size: motion kernel size (odd and positive).
             If int, the kernel will have a fixed size.
             If Tuple[int, int], it will randomly generate the value from the range batch-wisely.
         angle: angle of the motion blur in degrees (anti-clockwise rotation).
@@ -28,135 +23,75 @@
             while higher values towards 1.0 will point the motion blur forward. A value of 0.0 leads to a
             uniformly (but still angled) motion blur.
             If float, it will generate the value from (-direction, direction).
             If Tuple[int, int], it will randomly generate the value from the range.
 
     Returns:
         A dict of parameters to be passed for transformation.
-            - ksize_factor (torch.Tensor): element-wise kernel size factors with a shape of (B,).
-            - angle_factor (torch.Tensor): element-wise angle factors with a shape of (B,).
-            - direction_factor (torch.Tensor): element-wise direction factors with a shape of (B,).
+            - ksize_factor (Tensor): element-wise kernel size factors with a shape of (B,).
+            - angle_factor (Tensor): element-wise angle factors with a shape of (B,).
+            - direction_factor (Tensor): element-wise direction factors with a shape of (B,).
 
     Note:
         The generated random numbers are not reproducible across different devices and dtypes. By default,
         the parameters will be generated on CPU in float32. This can be changed by calling
         ``self.set_rng_device_and_dtype(device="cuda", dtype=torch.float64)``.
     """
 
     def __init__(
         self,
         kernel_size: Union[int, Tuple[int, int]],
-        angle: Union[torch.Tensor, float, Tuple[float, float]],
-        direction: Union[torch.Tensor, float, Tuple[float, float]],
+        angle: Union[
+            Tensor,
+            float,
+            Tuple[float, float, float],
+            Tuple[Tuple[float, float], Tuple[float, float], Tuple[float, float]],
+        ],
+        direction: Union[Tensor, float, Tuple[float, float]],
     ) -> None:
         super().__init__()
         self.kernel_size = kernel_size
         self.angle = angle
         self.direction = direction
 
     def __repr__(self) -> str:
         repr = f"kernel_size={self.kernel_size}, angle={self.angle}, direction={self.direction}"
         return repr
 
     def make_samplers(self, device: torch.device, dtype: torch.dtype) -> None:
-        angle = _range_bound(self.angle, 'angle', center=0.0, bounds=(-360, 360)).to(device=device, dtype=dtype)
+        angle: Tensor = _tuple_range_reader(self.angle, 3, device=device, dtype=dtype)
         direction = _range_bound(self.direction, 'direction', center=0.0, bounds=(-1, 1)).to(device=device, dtype=dtype)
         if isinstance(self.kernel_size, int):
             if not (self.kernel_size >= 3 and self.kernel_size % 2 == 1):
                 raise AssertionError(f"`kernel_size` must be odd and greater than 3. Got {self.kernel_size}.")
             self.ksize_sampler = Uniform(self.kernel_size // 2, self.kernel_size // 2, validate_args=False)
         elif isinstance(self.kernel_size, tuple):
             # kernel_size is fixed across the batch
             if len(self.kernel_size) != 2:
                 raise AssertionError(f"`kernel_size` must be (2,) if it is a tuple. Got {self.kernel_size}.")
             self.ksize_sampler = Uniform(self.kernel_size[0] // 2, self.kernel_size[1] // 2, validate_args=False)
         else:
             raise TypeError(f"Unsupported type: {type(self.kernel_size)}")
 
-        self.angle_sampler = Uniform(angle[0], angle[1], validate_args=False)
+        self.yaw_sampler = Uniform(angle[0][0], angle[0][1], validate_args=False)
+        self.pitch_sampler = Uniform(angle[1][0], angle[1][1], validate_args=False)
+        self.roll_sampler = Uniform(angle[2][0], angle[2][1], validate_args=False)
         self.direction_sampler = Uniform(direction[0], direction[1], validate_args=False)
 
-    def forward(self, batch_shape: torch.Size, same_on_batch: bool = False) -> Dict[str, torch.Tensor]:  # type:ignore
+    def forward(self, batch_shape: torch.Size, same_on_batch: bool = False) -> Dict[str, Tensor]:
         batch_size = batch_shape[0]
         _common_param_check(batch_size, same_on_batch)
         # self.ksize_factor.expand((batch_size, -1))
         _device, _dtype = _extract_device_dtype([self.angle, self.direction])
-        angle_factor = _adapted_rsampling((batch_size,), self.angle_sampler, same_on_batch)
+        yaw_factor = _adapted_rsampling((batch_size,), self.yaw_sampler, same_on_batch)
+        pitch_factor = _adapted_rsampling((batch_size,), self.pitch_sampler, same_on_batch)
+        roll_factor = _adapted_rsampling((batch_size,), self.roll_sampler, same_on_batch)
+        angle_factor = stack([yaw_factor, pitch_factor, roll_factor], 1)
+
         direction_factor = _adapted_rsampling((batch_size,), self.direction_sampler, same_on_batch)
         ksize_factor = _adapted_rsampling((batch_size,), self.ksize_sampler, same_on_batch).int() * 2 + 1
 
         return dict(
             ksize_factor=ksize_factor.to(device=_device, dtype=torch.int32),
             angle_factor=angle_factor.to(device=_device, dtype=_dtype),
             direction_factor=direction_factor.to(device=_device, dtype=_dtype),
         )
-
-
-@_deprecated(replace_with=MotionBlurGenerator.__name__)
-def random_motion_blur_generator(
-    batch_size: int,
-    kernel_size: Union[int, Tuple[int, int]],
-    angle: torch.Tensor,
-    direction: torch.Tensor,
-    same_on_batch: bool = False,
-    device: torch.device = torch.device('cpu'),
-    dtype: torch.dtype = torch.float32,
-) -> Dict[str, torch.Tensor]:
-    r"""Get parameters for motion blur.
-
-    Args:
-        batch_size (int): the tensor batch size.
-        kernel_size (int or (int, int)): motion kernel size (odd and positive) or range.
-        angle (torch.Tensor): angle of the motion blur in degrees (anti-clockwise rotation).
-        direction (torch.Tensor): forward/backward direction of the motion blur.
-            Lower values towards -1.0 will point the motion blur towards the back (with
-            angle provided via angle), while higher values towards 1.0 will point the motion
-            blur forward. A value of 0.0 leads to a uniformly (but still angled) motion blur.
-        same_on_batch (bool): apply the same transformation across the batch. Default: False.
-        device (torch.device): the device on which the random numbers will be generated. Default: cpu.
-        dtype (torch.dtype): the data type of the generated random numbers. Default: float32.
-
-    Returns:
-        params Dict[str, torch.Tensor]: parameters to be passed for transformation.
-            - ksize_factor (torch.Tensor): element-wise kernel size factors with a shape of (B,).
-            - angle_factor (torch.Tensor): element-wise angle factors with a shape of (B,).
-            - direction_factor (torch.Tensor): element-wise direction factors with a shape of (B,).
-
-    Note:
-        The generated random numbers are not reproducible across different devices and dtypes.
-    """
-    _common_param_check(batch_size, same_on_batch)
-    _joint_range_check(angle, 'angle')
-    _joint_range_check(direction, 'direction', (-1, 1))
-
-    _device, _dtype = _extract_device_dtype([angle, direction])
-
-    if isinstance(kernel_size, int):
-        if not (kernel_size >= 3 and kernel_size % 2 == 1):
-            raise AssertionError(f"`kernel_size` must be odd and greater than 3. Got {kernel_size}.")
-        ksize_factor = torch.tensor([kernel_size] * batch_size, device=device, dtype=dtype)
-    elif isinstance(kernel_size, tuple):
-        # kernel_size is fixed across the batch
-        if len(kernel_size) != 2:
-            raise AssertionError(f"`kernel_size` must be (2,) if it is a tuple. Got {kernel_size}.")
-        ksize_factor = (
-            _adapted_uniform((batch_size,), kernel_size[0] // 2, kernel_size[1] // 2, same_on_batch=True).int() * 2 + 1
-        )
-    else:
-        raise TypeError(f"Unsupported type: {type(kernel_size)}")
-
-    angle_factor = _adapted_uniform(
-        (batch_size,), angle[0].to(device=device, dtype=dtype), angle[1].to(device=device, dtype=dtype), same_on_batch
-    )
-
-    direction_factor = _adapted_uniform(
-        (batch_size,),
-        direction[0].to(device=device, dtype=dtype),
-        direction[1].to(device=device, dtype=dtype),
-        same_on_batch,
-    )
-
-    return dict(
-        ksize_factor=ksize_factor.to(device=_device, dtype=torch.int32),
-        angle_factor=angle_factor.to(device=_device, dtype=_dtype),
-        direction_factor=direction_factor.to(device=_device, dtype=_dtype),
-    )
```

### Comparing `kornia-0.6.8/kornia/augmentation/random_generator/_2d/plain_uniform.py` & `kornia-0.6.9/kornia/augmentation/random_generator/_2d/plain_uniform.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,14 +1,15 @@
 from typing import Any, Dict, Optional, Tuple
 
 import torch
 from torch.distributions import Distribution, Uniform
 
 from kornia.augmentation.random_generator.base import RandomGeneratorBase
 from kornia.augmentation.utils import _adapted_rsampling, _common_param_check, _range_bound
+from kornia.core import Tensor, as_tensor
 from kornia.utils.helpers import _extract_device_dtype
 
 # factor, name, center, range
 ParameterBound = Tuple[Any, str, Optional[float], Optional[Tuple[float, float]]]
 
 
 class PlainUniformGenerator(RandomGeneratorBase):
@@ -45,33 +46,33 @@
         names = []
         for factor, name, _, _ in samplers:
             if name in names:
                 raise RuntimeError(f"factor name `{name}` has already been registered. Please check the duplication.")
             names.append(name)
             if isinstance(factor, torch.nn.Parameter):
                 self.register_parameter(name, factor)
-            elif isinstance(factor, torch.Tensor):
+            elif isinstance(factor, Tensor):
                 self.register_buffer(name, factor)
 
     def __repr__(self) -> str:
         repr = ", ".join([f"{name}={factor}" for factor, name, _, _ in self.samplers])
         return repr
 
     def make_samplers(self, device: torch.device, dtype: torch.dtype) -> None:
         self.sampler_dict: Dict[str, Distribution] = {}
         for factor, name, center, bound in self.samplers:
             if center is None and bound is None:
-                factor = torch.as_tensor(factor, device=device, dtype=dtype)
+                factor = as_tensor(factor, device=device, dtype=dtype)
             elif center is None or bound is None:
                 raise ValueError(f"`center` and `bound` should be both None or provided. Got {center} and {bound}.")
             else:
                 factor = _range_bound(factor, name, center=center, bounds=bound, device=device, dtype=dtype)
             self.sampler_dict.update({name: Uniform(factor[0], factor[1], validate_args=False)})
 
-    def forward(self, batch_shape: torch.Size, same_on_batch: bool = False) -> Dict[str, torch.Tensor]:  # type:ignore
+    def forward(self, batch_shape: torch.Size, same_on_batch: bool = False) -> Dict[str, Tensor]:
         batch_size = batch_shape[0]
         _common_param_check(batch_size, same_on_batch)
         _device, _dtype = _extract_device_dtype([t for t, _, _, _ in self.samplers])
 
         return {
             name: _adapted_rsampling((batch_size,), dist, same_on_batch).to(device=_device, dtype=_dtype)
             for name, dist in self.sampler_dict.items()
```

### Comparing `kornia-0.6.8/kornia/augmentation/random_generator/_2d/planckian_jitter.py` & `kornia-0.6.9/kornia/augmentation/random_generator/_2d/planckian_jitter.py`

 * *Files 0% similar despite different names*

```diff
@@ -11,15 +11,14 @@
     r"""Generate random planckian jitter parameters for a batch of images."""
 
     def __init__(self, domain: List[float]) -> None:
         super().__init__()
         self.domain = domain
 
     def make_samplers(self, device: torch.device, dtype: torch.dtype) -> None:
-
         idx_range = _range_bound(self.domain, 'idx_range', device=device, dtype=dtype)
 
         _joint_range_check(idx_range, 'idx_range', (0, self.domain[1]))
         self.pl_idx_dist = Uniform(idx_range[0], idx_range[1], validate_args=False)
 
     def forward(self, batch_shape: torch.Size, same_on_batch: bool = False) -> Dict[str, torch.Tensor]:
         batch_size = batch_shape[0]
```

### Comparing `kornia-0.6.8/kornia/augmentation/random_generator/_2d/probability.py` & `kornia-0.6.9/kornia/augmentation/random_generator/_2d/probability.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,25 +1,26 @@
 from typing import Dict
 
 import torch
 from torch.distributions import Bernoulli
 
 from kornia.augmentation.random_generator.base import RandomGeneratorBase
 from kornia.augmentation.utils import _adapted_sampling, _common_param_check
+from kornia.core import Tensor, tensor
 
 
 class ProbabilityGenerator(RandomGeneratorBase):
     r"""Generate random probabilities for a batch of inputs.
 
     Args:
         p: probability to generate an 1-d binary mask. Default value is 0.5.
 
     Returns:
         A dict of parameters to be passed for transformation.
-            - probs (torch.Tensor): element-wise probabilities with a shape of (B,).
+            - probs (Tensor): element-wise probabilities with a shape of (B,).
 
     Note:
         The generated random numbers are not reproducible across different devices and dtypes. By default,
         the parameters will be generated on CPU in float32. This can be changed by calling
         ``self.set_rng_device_and_dtype(device="cuda", dtype=torch.float64)``.
     """
 
@@ -31,44 +32,44 @@
         repr = f"p={self.p}"
         return repr
 
     def make_samplers(self, device: torch.device, dtype: torch.dtype) -> None:
         p = torch.tensor(float(self.p), device=device, dtype=dtype)
         self.sampler = Bernoulli(p)
 
-    def forward(self, batch_shape: torch.Size, same_on_batch: bool = False) -> Dict[str, torch.Tensor]:  # type:ignore
+    def forward(self, batch_shape: torch.Size, same_on_batch: bool = False) -> Dict[str, Tensor]:
         batch_size = batch_shape[0]
-        probs_mask: torch.Tensor = _adapted_sampling((batch_size,), self.sampler, same_on_batch).bool()
+        probs_mask: Tensor = _adapted_sampling((batch_size,), self.sampler, same_on_batch).bool()
         return dict(probs=probs_mask)
 
 
 def random_prob_generator(
     batch_size: int,
     p: float = 0.5,
     same_on_batch: bool = False,
     device: torch.device = torch.device('cpu'),
     dtype: torch.dtype = torch.float32,
-) -> torch.Tensor:
+) -> Tensor:
     r"""Generate random probabilities for a batch of inputs.
 
     Args:
         batch_size (int): the number of images.
         p (float): probability to generate an 1-d binary mask. Default value is 0.5.
         same_on_batch (bool): apply the same transformation across the batch. Default: False.
         device (torch.device): the device on which the random numbers will be generated. Default: cpu.
         dtype (torch.dtype): the data type of the generated random numbers. Default: float32.
 
     Returns:
-        torch.Tensor: parameters to be passed for transformation.
-            - probs (torch.Tensor): element-wise probabilities with a shape of (B,).
+        Tensor: parameters to be passed for transformation.
+            - probs (Tensor): element-wise probabilities with a shape of (B,).
 
     Note:
         The generated random numbers are not reproducible across different devices and dtypes.
     """
     _common_param_check(batch_size, same_on_batch)
     if not isinstance(p, (int, float)) or p > 1 or p < 0:
         raise TypeError(f"The probability should be a float number within [0, 1]. Got {type(p)}.")
 
-    _bernoulli = Bernoulli(torch.tensor(float(p), device=device, dtype=dtype))
-    probs_mask: torch.Tensor = _adapted_sampling((batch_size,), _bernoulli, same_on_batch).bool()
+    _bernoulli = Bernoulli(tensor(float(p), device=device, dtype=dtype))
+    probs_mask: Tensor = _adapted_sampling((batch_size,), _bernoulli, same_on_batch).bool()
 
     return probs_mask
```

### Comparing `kornia-0.6.8/kornia/augmentation/random_generator/_2d/resize.py` & `kornia-0.6.9/kornia/augmentation/random_generator/_2d/resize.py`

 * *Files 11% similar despite different names*

```diff
@@ -1,72 +1,71 @@
-from typing import Dict, Optional, Tuple, Union
+from typing import Dict, Tuple, Union
 
 import torch
 
 from kornia.augmentation.random_generator.base import RandomGeneratorBase
 from kornia.augmentation.utils import _common_param_check
+from kornia.core import Device, Tensor, tensor
 from kornia.geometry.bbox import bbox_generator
 from kornia.geometry.transform.affwarp import _side_to_image_size
 
 
 class ResizeGenerator(RandomGeneratorBase):
     r"""Get parameters for ```resize``` transformation for resize transform.
 
     Args:
         resize_to: Desired output size of the crop, like (h, w).
         side: Which side to resize if `resize_to` is only of type int.
 
     Returns:
         parameters to be passed for transformation.
-            - src (torch.Tensor): cropping bounding boxes with a shape of (B, 4, 2).
-            - dst (torch.Tensor): output bounding boxes with a shape (B, 4, 2).
-            - input_size (torch.Tensor): (h, w) from batch input.
+            - src (Tensor): cropping bounding boxes with a shape of (B, 4, 2).
+            - dst (Tensor): output bounding boxes with a shape (B, 4, 2).
+            - input_size (Tensor): (h, w) from batch input.
             - resize_to (tuple): new (h, w) for batch input.
 
     Note:
         The generated random numbers are not reproducible across different devices and dtypes. By default,
         the parameters will be generated on CPU in float32. This can be changed by calling
         ``self.set_rng_device_and_dtype(device="cuda", dtype=torch.float64)``.
     """
 
     def __init__(self, resize_to: Union[int, Tuple[int, int]], side: str = "short") -> None:
         super().__init__()
         self.output_size = resize_to
         self.side = side
-        self.device: Optional[torch.device] = None
-        self.dtype: Optional[torch.dtype] = None
 
     def __repr__(self) -> str:
         repr = f"output_size={self.output_size}"
         return repr
 
-    def make_samplers(self, device: torch.device, dtype: torch.dtype) -> None:
+    def make_samplers(self, device: Device, dtype: torch.dtype) -> None:
         self.device = device
         self.dtype = dtype
         pass
 
-    def forward(self, batch_shape: torch.Size, same_on_batch: bool = False) -> Dict[str, torch.Tensor]:
+    def forward(self, batch_shape: torch.Size, same_on_batch: bool = False) -> Dict[str, Tensor]:
         batch_size = batch_shape[0]
         _common_param_check(batch_size, same_on_batch)
         _device = self.device
         _dtype = self.dtype
 
         if batch_size == 0:
             return dict(
                 src=torch.zeros([0, 4, 2], device=_device, dtype=_dtype),
                 dst=torch.zeros([0, 4, 2], device=_device, dtype=_dtype),
             )
 
         input_size = h, w = (batch_shape[-2], batch_shape[-1])
 
         src = bbox_generator(
-            torch.tensor(0, device=_device, dtype=_dtype),
-            torch.tensor(0, device=_device, dtype=_dtype),
-            torch.tensor(input_size[1], device=_device, dtype=_dtype),
-            torch.tensor(input_size[0], device=_device, dtype=_dtype),
+            tensor(0, device=_device, dtype=_dtype),
+            tensor(0, device=_device, dtype=_dtype),
+            tensor(input_size[1], device=_device, dtype=_dtype),
+            tensor(input_size[0], device=_device, dtype=_dtype),
         ).repeat(batch_size, 1, 1)
 
         if isinstance(self.output_size, int):
             aspect_ratio = w / h
             self.output_size = _side_to_image_size(self.output_size, aspect_ratio, self.side)
 
         if not (
@@ -75,17 +74,17 @@
             and isinstance(self.output_size[1], (int,))
             and self.output_size[0] > 0
             and self.output_size[1] > 0
         ):
             raise AssertionError(f"`resize_to` must be a tuple of 2 positive integers. Got {self.output_size}.")
 
         dst = bbox_generator(
-            torch.tensor(0, device=_device, dtype=_dtype),
-            torch.tensor(0, device=_device, dtype=_dtype),
-            torch.tensor(self.output_size[1], device=_device, dtype=_dtype),
-            torch.tensor(self.output_size[0], device=_device, dtype=_dtype),
+            tensor(0, device=_device, dtype=_dtype),
+            tensor(0, device=_device, dtype=_dtype),
+            tensor(self.output_size[1], device=_device, dtype=_dtype),
+            tensor(self.output_size[0], device=_device, dtype=_dtype),
         ).repeat(batch_size, 1, 1)
 
-        _input_size = torch.tensor(input_size, device=_device, dtype=torch.long).expand(batch_size, -1)
-        _output_size = torch.tensor(self.output_size, device=_device, dtype=torch.long).expand(batch_size, -1)
+        _input_size = tensor(input_size, device=_device, dtype=torch.long).expand(batch_size, -1)
+        _output_size = tensor(self.output_size, device=_device, dtype=torch.long).expand(batch_size, -1)
 
         return dict(src=src, dst=dst, input_size=_input_size, output_size=_output_size)
```

### Comparing `kornia-0.6.8/kornia/augmentation/random_generator/_3d/crop.py` & `kornia-0.6.9/kornia/augmentation/random_generator/_2d/crop.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,360 +1,310 @@
 from typing import Dict, Optional, Tuple, Union
 
 import torch
 from torch.distributions import Uniform
 
 from kornia.augmentation.random_generator.base import RandomGeneratorBase
-from kornia.augmentation.utils import _adapted_rsampling, _adapted_uniform, _common_param_check
-from kornia.geometry.bbox import bbox_generator3d
-from kornia.utils.helpers import _deprecated, _extract_device_dtype
+from kornia.augmentation.utils import _adapted_rsampling, _common_param_check, _joint_range_check
+from kornia.core import Device, Tensor, tensor, where, zeros
+from kornia.geometry.bbox import bbox_generator
+from kornia.utils.helpers import _extract_device_dtype
 
 
-class CropGenerator3D(RandomGeneratorBase):
+class CropGenerator(RandomGeneratorBase):
     r"""Get parameters for ```crop``` transformation for crop transform.
 
     Args:
-        size (tuple): Desired size of the crop operation, like (d, h, w).
-            If tensor, it must be (B, 3).
-        resize_to (tuple): Desired output size of the crop, like (d, h, w). If None, no resize will be performed.
+        size (tuple): Desired size of the crop operation, like (h, w).
+            If tensor, it must be (B, 2).
+        resize_to (tuple): Desired output size of the crop, like (h, w). If None, no resize will be performed.
 
     Returns:
-        A dict of parameters to be passed for transformation.
-            - src (torch.Tensor): cropping bounding boxes with a shape of (B, 8, 3).
-            - dst (torch.Tensor): output bounding boxes with a shape (B, 8, 3).
+        params Dict[str, Tensor]: parameters to be passed for transformation.
+            - src (Tensor): cropping bounding boxes with a shape of (B, 4, 2).
+            - dst (Tensor): output bounding boxes with a shape (B, 4, 2).
 
     Note:
         The generated random numbers are not reproducible across different devices and dtypes. By default,
         the parameters will be generated on CPU in float32. This can be changed by calling
         ``self.set_rng_device_and_dtype(device="cuda", dtype=torch.float64)``.
     """
 
-    def __init__(
-        self, size: Union[Tuple[int, int, int], torch.Tensor], resize_to: Optional[Tuple[int, int, int]] = None
-    ) -> None:
+    def __init__(self, size: Union[Tuple[int, int], Tensor], resize_to: Optional[Tuple[int, int]] = None) -> None:
         super().__init__()
         self.size = size
         self.resize_to = resize_to
 
     def __repr__(self) -> str:
         repr = f"crop_size={self.size}"
         if self.resize_to is not None:
             repr += f", resize_to={self.resize_to}"
         return repr
 
     def make_samplers(self, device: torch.device, dtype: torch.dtype) -> None:
-        self.rand_sampler = Uniform(
-            torch.tensor(0.0, device=device, dtype=dtype), torch.tensor(1.0, device=device, dtype=dtype)
-        )
+        self.rand_sampler = Uniform(tensor(0.0, device=device, dtype=dtype), tensor(1.0, device=device, dtype=dtype))
 
-    def forward(self, batch_shape: torch.Size, same_on_batch: bool = False) -> Dict[str, torch.Tensor]:  # type:ignore
-        batch_size, _, depth, height, width = batch_shape
+    def forward(self, batch_shape: torch.Size, same_on_batch: bool = False) -> Dict[str, Tensor]:
+        batch_size = batch_shape[0]
         _common_param_check(batch_size, same_on_batch)
-        _device, _dtype = _extract_device_dtype([self.size if isinstance(self.size, torch.Tensor) else None])
-
-        if not isinstance(self.size, torch.Tensor):
-            size = torch.tensor(self.size, device=_device, dtype=_dtype).repeat(batch_size, 1)
-        else:
-            size = self.size.to(device=_device, dtype=_dtype)
-        if size.shape != torch.Size([batch_size, 3]):
-            raise AssertionError(
-                "If `size` is a tensor, it must be shaped as (B, 3). "
-                f"Got {size.shape} while expecting {torch.Size([batch_size, 3])}."
-            )
-        if not (
-            isinstance(depth, (int,))
-            and isinstance(height, (int,))
-            and isinstance(width, (int,))
-            and depth > 0
-            and height > 0
-            and width > 0
-        ):
-            raise AssertionError(f"`batch_shape` should not contain negative values. Got {(batch_shape)}.")
-
-        x_diff = width - size[:, 2] + 1
-        y_diff = height - size[:, 1] + 1
-        z_diff = depth - size[:, 0] + 1
-
-        if (x_diff < 0).any() or (y_diff < 0).any() or (z_diff < 0).any():
-            raise ValueError(
-                f"input_size {(depth, height, width)} cannot be smaller than crop size {str(size)} in any dimension."
-            )
+        _device, _dtype = _extract_device_dtype([self.size if isinstance(self.size, Tensor) else None])
 
         if batch_size == 0:
             return dict(
-                src=torch.zeros([0, 8, 3], device=_device, dtype=_dtype),
-                dst=torch.zeros([0, 8, 3], device=_device, dtype=_dtype),
+                src=zeros([0, 4, 2], device=_device, dtype=_dtype), dst=zeros([0, 4, 2], device=_device, dtype=_dtype)
             )
 
-        x_start = _adapted_rsampling((batch_size,), self.rand_sampler, same_on_batch).to(device=_device, dtype=_dtype)
-        y_start = _adapted_rsampling((batch_size,), self.rand_sampler, same_on_batch).to(device=_device, dtype=_dtype)
-        z_start = _adapted_rsampling((batch_size,), self.rand_sampler, same_on_batch).to(device=_device, dtype=_dtype)
-
-        x_start = (x_start * x_diff).floor()
-        y_start = (y_start * y_diff).floor()
-        z_start = (z_start * z_diff).floor()
-
-        crop_src = bbox_generator3d(
-            x_start.view(-1), y_start.view(-1), z_start.view(-1), size[:, 2] - 1, size[:, 1] - 1, size[:, 0] - 1
+        input_size = (batch_shape[-2], batch_shape[-1])
+        if not isinstance(self.size, Tensor):
+            size = tensor(self.size, device=_device, dtype=_dtype).repeat(batch_size, 1)
+        else:
+            size = self.size.to(device=_device, dtype=_dtype)
+        if size.shape != torch.Size([batch_size, 2]):
+            raise AssertionError(
+                "If `size` is a tensor, it must be shaped as (B, 2). "
+                f"Got {size.shape} while expecting {torch.Size([batch_size, 2])}."
+            )
+        if not (input_size[0] > 0 and input_size[1] > 0 and (size > 0).all()):
+            raise AssertionError(f"Got non-positive input size or size. {input_size}, {size}.")
+        size = size.floor()
+
+        x_diff = input_size[1] - size[:, 1] + 1
+        y_diff = input_size[0] - size[:, 0] + 1
+
+        # Start point will be 0 if diff < 0
+        x_diff = x_diff.clamp(0)
+        y_diff = y_diff.clamp(0)
+
+        if same_on_batch:
+            # If same_on_batch, select the first then repeat.
+            x_start = (
+                _adapted_rsampling((batch_size,), self.rand_sampler, same_on_batch).to(x_diff) * x_diff[0]
+            ).floor()
+            y_start = (
+                _adapted_rsampling((batch_size,), self.rand_sampler, same_on_batch).to(y_diff) * y_diff[0]
+            ).floor()
+        else:
+            x_start = (_adapted_rsampling((batch_size,), self.rand_sampler, same_on_batch).to(x_diff) * x_diff).floor()
+            y_start = (_adapted_rsampling((batch_size,), self.rand_sampler, same_on_batch).to(y_diff) * y_diff).floor()
+        crop_src = bbox_generator(
+            x_start.view(-1).to(device=_device, dtype=_dtype),
+            y_start.view(-1).to(device=_device, dtype=_dtype),
+            where(size[:, 1] == 0, tensor(input_size[1], device=_device, dtype=_dtype), size[:, 1]),
+            where(size[:, 0] == 0, tensor(input_size[0], device=_device, dtype=_dtype), size[:, 0]),
         )
 
         if self.resize_to is None:
-            crop_dst = bbox_generator3d(
-                torch.tensor([0] * batch_size, device=_device, dtype=_dtype),
-                torch.tensor([0] * batch_size, device=_device, dtype=_dtype),
-                torch.tensor([0] * batch_size, device=_device, dtype=_dtype),
-                size[:, 2] - 1,
-                size[:, 1] - 1,
-                size[:, 0] - 1,
+            crop_dst = bbox_generator(
+                tensor([0] * batch_size, device=_device, dtype=_dtype),
+                tensor([0] * batch_size, device=_device, dtype=_dtype),
+                size[:, 1],
+                size[:, 0],
             )
+            _output_size = size.to(dtype=torch.long)
         else:
             if not (
-                len(self.resize_to) == 3
+                len(self.resize_to) == 2
                 and isinstance(self.resize_to[0], (int,))
                 and isinstance(self.resize_to[1], (int,))
-                and isinstance(self.resize_to[2], (int,))
                 and self.resize_to[0] > 0
                 and self.resize_to[1] > 0
-                and self.resize_to[2] > 0
             ):
-                raise AssertionError(f"`resize_to` must be a tuple of 3 positive integers. Got {self.resize_to}.")
-            crop_dst = torch.tensor(
+                raise AssertionError(f"`resize_to` must be a tuple of 2 positive integers. Got {self.resize_to}.")
+            crop_dst = tensor(
                 [
                     [
-                        [0, 0, 0],
-                        [self.resize_to[-1] - 1, 0, 0],
-                        [self.resize_to[-1] - 1, self.resize_to[-2] - 1, 0],
-                        [0, self.resize_to[-2] - 1, 0],
-                        [0, 0, self.resize_to[-3] - 1],
-                        [self.resize_to[-1] - 1, 0, self.resize_to[-3] - 1],
-                        [self.resize_to[-1] - 1, self.resize_to[-2] - 1, self.resize_to[-3] - 1],
-                        [0, self.resize_to[-2] - 1, self.resize_to[-3] - 1],
+                        [0, 0],
+                        [self.resize_to[1] - 1, 0],
+                        [self.resize_to[1] - 1, self.resize_to[0] - 1],
+                        [0, self.resize_to[0] - 1],
                     ]
                 ],
                 device=_device,
                 dtype=_dtype,
             ).repeat(batch_size, 1, 1)
+            _output_size = tensor(self.resize_to, device=_device, dtype=torch.long).expand(batch_size, -1)
+
+        _input_size = tensor(input_size, device=_device, dtype=torch.long).expand(batch_size, -1)
+
+        return dict(src=crop_src, dst=crop_dst, input_size=_input_size, output_size=_output_size)
+
+
+class ResizedCropGenerator(CropGenerator):
+    r"""Get cropping heights and widths for ```crop``` transformation for resized crop transform.
+
+    Args:
+        output_size (Tuple[int, int]): expected output size of each edge.
+        scale (Tensor): range of size of the origin size cropped with (2,) shape.
+        ratio (Tensor): range of aspect ratio of the origin aspect ratio cropped with (2,) shape.
+
+    Returns:
+        params Dict[str, Tensor]: parameters to be passed for transformation.
+            - size (Tensor): element-wise cropping sizes with a shape of (B, 2).
+
+    Note:
+        The generated random numbers are not reproducible across different devices and dtypes.
+
+    Examples:
+        >>> _ = torch.manual_seed(42)
+        >>> rcg = ResizedCropGenerator((30, 30), scale=torch.tensor([.7, 1.3]), ratio=torch.tensor([.9, 1.]))
+        >>> out = rcg(torch.Size([1, 3, 3]))
+        >>> out["src"]
+        tensor([[[0., 0.],
+                 [2., 0.],
+                 [2., 2.],
+                 [0., 2.]]])
+        >>> out["dst"]
+        tensor([[[ 0.,  0.],
+                 [29.,  0.],
+                 [29., 29.],
+                 [ 0., 29.]]])
+        >>> out["input_size"]
+        tensor([[3, 3]])
+        >>> out["output_size"]
+        tensor([[30, 30]])
+    """
+
+    def __init__(
+        self,
+        output_size: Tuple[int, int],
+        scale: Union[Tensor, Tuple[float, float]],
+        ratio: Union[Tensor, Tuple[float, float]],
+    ) -> None:
+        if not (
+            len(output_size) == 2
+            and isinstance(output_size[0], (int,))
+            and isinstance(output_size[1], (int,))
+            and output_size[0] > 0
+            and output_size[1] > 0
+        ):
+            raise AssertionError(f"`output_size` must be a tuple of 2 positive integers. Got {output_size}.")
+        super().__init__(size=output_size, resize_to=output_size)  # fake an intermedia crop size
+        self.scale = scale
+        self.ratio = ratio
+        self.output_size = output_size
+
+    def __repr__(self) -> str:
+        repr = f"scale={self.scale}, resize_to={self.ratio}, output_size={self.output_size}"
+        return repr
+
+    def make_samplers(self, device: torch.device, dtype: torch.dtype) -> None:
+        scale = torch.as_tensor(self.scale, device=device, dtype=dtype)
+        ratio = torch.as_tensor(self.ratio, device=device, dtype=dtype)
+        _joint_range_check(scale, "scale")
+        _joint_range_check(ratio, "ratio")
+        self.rand_sampler = Uniform(tensor(0.0, device=device, dtype=dtype), tensor(1.0, device=device, dtype=dtype))
+        self.log_ratio_sampler = Uniform(torch.log(ratio[0]), torch.log(ratio[1]), validate_args=False)
+
+    def forward(self, batch_shape: torch.Size, same_on_batch: bool = False) -> Dict[str, Tensor]:
+        batch_size = batch_shape[0]
+        size = (batch_shape[-2], batch_shape[-1])
+        _device, _dtype = _extract_device_dtype([self.scale, self.ratio])
 
-        return dict(src=crop_src.to(device=_device), dst=crop_dst.to(device=_device))
+        if batch_size == 0:
+            return dict(
+                src=zeros([0, 4, 2], device=_device, dtype=_dtype),
+                dst=zeros([0, 4, 2], device=_device, dtype=_dtype),
+                size=zeros([0, 2], device=_device, dtype=_dtype),
+            )
 
+        rand = _adapted_rsampling((batch_size, 10), self.rand_sampler, same_on_batch).to(device=_device, dtype=_dtype)
+        area = (rand * (self.scale[1] - self.scale[0]) + self.scale[0]) * size[0] * size[1]
+        log_ratio = _adapted_rsampling((batch_size, 10), self.log_ratio_sampler, same_on_batch).to(
+            device=_device, dtype=_dtype
+        )
+        aspect_ratio = torch.exp(log_ratio)
 
-def center_crop_generator3d(
-    batch_size: int,
-    depth: int,
-    height: int,
-    width: int,
-    size: Tuple[int, int, int],
-    device: torch.device = torch.device('cpu'),
-) -> Dict[str, torch.Tensor]:
-    r"""Get parameters for ```center_crop3d``` transformation for center crop transform.
+        w = torch.sqrt(area * aspect_ratio).round().floor()
+        h = torch.sqrt(area / aspect_ratio).round().floor()
+        # Element-wise w, h condition
+        cond = ((0 < w) * (w < size[0]) * (0 < h) * (h < size[1])).int()
+
+        # torch.argmax is not reproducible across devices: https://github.com/pytorch/pytorch/issues/17738
+        # Here, we will select the first occurrence of the duplicated elements.
+        cond_bool, argmax_dim1 = ((cond.cumsum(1) == 1) & cond.bool()).max(1)
+        h_out = w[torch.arange(0, batch_size, device=_device, dtype=torch.long), argmax_dim1]
+        w_out = h[torch.arange(0, batch_size, device=_device, dtype=torch.long), argmax_dim1]
+
+        if not cond_bool.all():
+            # Fallback to center crop
+            in_ratio = float(size[0]) / float(size[1])
+            _min = float(self.ratio.min()) if isinstance(self.ratio, Tensor) else min(self.ratio)
+            if in_ratio < _min:
+                h_ct = tensor(size[0], device=_device, dtype=_dtype)
+                w_ct = torch.round(h_ct / _min)
+            elif in_ratio > _min:
+                w_ct = tensor(size[1], device=_device, dtype=_dtype)
+                h_ct = torch.round(w_ct * _min)
+            else:  # whole image
+                h_ct = tensor(size[0], device=_device, dtype=_dtype)
+                w_ct = tensor(size[1], device=_device, dtype=_dtype)
+            h_ct = h_ct.floor()
+            w_ct = w_ct.floor()
+
+            h_out = h_out.where(cond_bool, h_ct)
+            w_out = w_out.where(cond_bool, w_ct)
+
+        # Update the crop size.
+        self.size = torch.stack([h_out, w_out], dim=1)
+        return super().forward(batch_shape, same_on_batch)
+
+
+def center_crop_generator(
+    batch_size: int, height: int, width: int, size: Tuple[int, int], device: Device = torch.device('cpu')
+) -> Dict[str, Tensor]:
+    r"""Get parameters for ```center_crop``` transformation for center crop transform.
 
     Args:
         batch_size (int): the tensor batch size.
-        depth (int) : depth of the image.
         height (int) : height of the image.
         width (int): width of the image.
-        size (tuple): Desired output size of the crop, like (d, h, w).
-        device (torch.device): the device on which the random numbers will be generated. Default: cpu.
+        size (tuple): Desired output size of the crop, like (h, w).
+        device (Device): the device on which the random numbers will be generated. Default: cpu.
 
     Returns:
-        params Dict[str, torch.Tensor]: parameters to be passed for transformation.
-            - src (torch.Tensor): cropping bounding boxes with a shape of (B, 8, 3).
-            - dst (torch.Tensor): output bounding boxes with a shape (B, 8, 3).
+        params Dict[str, Tensor]: parameters to be passed for transformation.
+            - src (Tensor): cropping bounding boxes with a shape of (B, 4, 2).
+            - dst (Tensor): output bounding boxes with a shape (B, 4, 2).
 
     Note:
         No random number will be generated.
     """
-    if not isinstance(size, (tuple, list)) and len(size) == 3:
-        raise ValueError(f"Input size must be a tuple/list of length 3. Got {size}")
-    if not (
-        type(depth) is int and depth > 0 and type(height) is int and height > 0 and type(width) is int and width > 0
-    ):
-        raise AssertionError(f"'depth', 'height' and 'width' must be integers. Got {depth}, {height}, {width}.")
-    if not (depth >= size[0] and height >= size[1] and width >= size[2]):
-        raise AssertionError(f"Crop size must be smaller than input size. Got ({depth}, {height}, {width}) and {size}.")
+    _common_param_check(batch_size)
+    if not isinstance(size, (tuple, list)) and len(size) == 2:
+        raise ValueError(f"Input size must be a tuple/list of length 2. Got {size}")
+    if not (type(height) is int and height > 0 and type(width) is int and width > 0):
+        raise AssertionError(f"'height' and 'width' must be integers. Got {height}, {width}.")
+    if not (height >= size[0] and width >= size[1]):
+        raise AssertionError(f"Crop size must be smaller than input size. Got ({height}, {width}) and {size}.")
 
-    if batch_size == 0:
-        return dict(src=torch.zeros([0, 8, 3]), dst=torch.zeros([0, 8, 3]))
     # unpack input sizes
-    dst_d, dst_h, dst_w = size
-    src_d, src_h, src_w = (depth, height, width)
+    dst_h, dst_w = size
+    src_h, src_w = height, width
 
     # compute start/end offsets
-    dst_d_half = dst_d / 2
     dst_h_half = dst_h / 2
     dst_w_half = dst_w / 2
-    src_d_half = src_d / 2
     src_h_half = src_h / 2
     src_w_half = src_w / 2
 
-    start_x = src_w_half - dst_w_half
-    start_y = src_h_half - dst_h_half
-    start_z = src_d_half - dst_d_half
+    start_x = int(src_w_half - dst_w_half)
+    start_y = int(src_h_half - dst_h_half)
 
     end_x = start_x + dst_w - 1
     end_y = start_y + dst_h - 1
-    end_z = start_z + dst_d - 1
-    # [x, y, z] origin
-    # top-left-front, top-right-front, bottom-right-front, bottom-left-front
-    # top-left-back, top-right-back, bottom-right-back, bottom-left-back
-    # Note: DeprecationWarning: an integer is required (got type float).
-    # Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.
-    points_src: torch.Tensor = torch.tensor(
-        [
-            [
-                [int(start_x), int(start_y), int(start_z)],
-                [int(end_x), int(start_y), int(start_z)],
-                [int(end_x), int(end_y), int(start_z)],
-                [int(start_x), int(end_y), int(start_z)],
-                [int(start_x), int(start_y), int(end_z)],
-                [int(end_x), int(start_y), int(end_z)],
-                [int(end_x), int(end_y), int(end_z)],
-                [int(start_x), int(end_y), int(end_z)],
-            ]
-        ],
-        device=device,
-        dtype=torch.long,
-    ).expand(batch_size, -1, -1)
 
-    # [x, y, z] destination
-    # top-left-front, top-right-front, bottom-right-front, bottom-left-front
-    # top-left-back, top-right-back, bottom-right-back, bottom-left-back
-    points_dst: torch.Tensor = torch.tensor(
-        [
-            [
-                [0, 0, 0],
-                [dst_w - 1, 0, 0],
-                [dst_w - 1, dst_h - 1, 0],
-                [0, dst_h - 1, 0],
-                [0, 0, dst_d - 1],
-                [dst_w - 1, 0, dst_d - 1],
-                [dst_w - 1, dst_h - 1, dst_d - 1],
-                [0, dst_h - 1, dst_d - 1],
-            ]
-        ],
-        device=device,
-        dtype=torch.long,
+    # [y, x] origin
+    # top-left, top-right, bottom-right, bottom-left
+    points_src: Tensor = tensor(
+        [[[start_x, start_y], [end_x, start_y], [end_x, end_y], [start_x, end_y]]], device=device, dtype=torch.long
     ).expand(batch_size, -1, -1)
-    return dict(src=points_src, dst=points_dst)
-
-
-@_deprecated(replace_with=CropGenerator3D.__name__)
-def random_crop_generator3d(
-    batch_size: int,
-    input_size: Tuple[int, int, int],
-    size: Union[Tuple[int, int, int], torch.Tensor],
-    resize_to: Optional[Tuple[int, int, int]] = None,
-    same_on_batch: bool = False,
-    device: torch.device = torch.device('cpu'),
-    dtype: torch.dtype = torch.float32,
-) -> Dict[str, torch.Tensor]:
-    r"""Get parameters for ```crop``` transformation for crop transform.
-
-    Args:
-        batch_size (int): the tensor batch size.
-        input_size (tuple): Input image shape, like (d, h, w).
-        size (tuple): Desired size of the crop operation, like (d, h, w).
-            If tensor, it must be (B, 3).
-        resize_to (tuple): Desired output size of the crop, like (d, h, w). If None, no resize will be performed.
-        same_on_batch (bool): apply the same transformation across the batch. Default: False.
-        device (torch.device): the device on which the random numbers will be generated. Default: cpu.
-        dtype (torch.dtype): the data type of the generated random numbers. Default: float32.
 
-    Returns:
-        params Dict[str, torch.Tensor]: parameters to be passed for transformation.
-            - src (torch.Tensor): cropping bounding boxes with a shape of (B, 8, 3).
-            - dst (torch.Tensor): output bounding boxes with a shape (B, 8, 3).
-
-    Note:
-        The generated random numbers are not reproducible across different devices and dtypes.
-    """
-    _device, _dtype = _extract_device_dtype([size if isinstance(size, torch.Tensor) else None])
-    if not isinstance(size, torch.Tensor):
-        size = torch.tensor(size, device=device, dtype=dtype).repeat(batch_size, 1)
-    else:
-        size = size.to(device=device, dtype=dtype)
-    if size.shape != torch.Size([batch_size, 3]):
-        raise AssertionError(
-            "If `size` is a tensor, it must be shaped as (B, 3). "
-            f"Got {size.shape} while expecting {torch.Size([batch_size, 3])}."
-        )
-    if not (
-        len(input_size) == 3
-        and isinstance(input_size[0], (int,))
-        and isinstance(input_size[1], (int,))
-        and isinstance(input_size[2], (int,))
-        and input_size[0] > 0
-        and input_size[1] > 0
-        and input_size[2] > 0
-    ):
-        raise AssertionError(f"`input_size` must be a tuple of 3 positive integers. Got {input_size}.")
-
-    x_diff = input_size[2] - size[:, 2] + 1
-    y_diff = input_size[1] - size[:, 1] + 1
-    z_diff = input_size[0] - size[:, 0] + 1
-
-    if (x_diff < 0).any() or (y_diff < 0).any() or (z_diff < 0).any():
-        raise ValueError(f"input_size {str(input_size)} cannot be smaller than crop size {str(size)} in any dimension.")
-
-    if batch_size == 0:
-        return dict(
-            src=torch.zeros([0, 8, 3], device=_device, dtype=_dtype),
-            dst=torch.zeros([0, 8, 3], device=_device, dtype=_dtype),
-        )
+    # [y, x] destination
+    # top-left, top-right, bottom-right, bottom-left
+    points_dst: Tensor = tensor(
+        [[[0, 0], [dst_w - 1, 0], [dst_w - 1, dst_h - 1], [0, dst_h - 1]]], device=device, dtype=torch.long
+    ).expand(batch_size, -1, -1)
 
-    if same_on_batch:
-        # If same_on_batch, select the first then repeat.
-        x_start = _adapted_uniform((batch_size,), 0, x_diff[0], same_on_batch).floor()
-        y_start = _adapted_uniform((batch_size,), 0, y_diff[0], same_on_batch).floor()
-        z_start = _adapted_uniform((batch_size,), 0, z_diff[0], same_on_batch).floor()
-    else:
-        x_start = _adapted_uniform((1,), 0, x_diff, same_on_batch).floor()
-        y_start = _adapted_uniform((1,), 0, y_diff, same_on_batch).floor()
-        z_start = _adapted_uniform((1,), 0, z_diff, same_on_batch).floor()
-
-    crop_src = bbox_generator3d(
-        x_start.to(device=_device, dtype=_dtype).view(-1),
-        y_start.to(device=_device, dtype=_dtype).view(-1),
-        z_start.to(device=_device, dtype=_dtype).view(-1),
-        size[:, 2].to(device=_device, dtype=_dtype) - 1,
-        size[:, 1].to(device=_device, dtype=_dtype) - 1,
-        size[:, 0].to(device=_device, dtype=_dtype) - 1,
-    )
-
-    if resize_to is None:
-        crop_dst = bbox_generator3d(
-            torch.tensor([0] * batch_size, device=_device, dtype=_dtype),
-            torch.tensor([0] * batch_size, device=_device, dtype=_dtype),
-            torch.tensor([0] * batch_size, device=_device, dtype=_dtype),
-            size[:, 2].to(device=_device, dtype=_dtype) - 1,
-            size[:, 1].to(device=_device, dtype=_dtype) - 1,
-            size[:, 0].to(device=_device, dtype=_dtype) - 1,
-        )
-    else:
-        if not (
-            len(resize_to) == 3
-            and isinstance(resize_to[0], (int,))
-            and isinstance(resize_to[1], (int,))
-            and isinstance(resize_to[2], (int,))
-            and resize_to[0] > 0
-            and resize_to[1] > 0
-            and resize_to[2] > 0
-        ):
-            raise AssertionError(f"`resize_to` must be a tuple of 3 positive integers. Got {resize_to}.")
-        crop_dst = torch.tensor(
-            [
-                [
-                    [0, 0, 0],
-                    [resize_to[-1] - 1, 0, 0],
-                    [resize_to[-1] - 1, resize_to[-2] - 1, 0],
-                    [0, resize_to[-2] - 1, 0],
-                    [0, 0, resize_to[-3] - 1],
-                    [resize_to[-1] - 1, 0, resize_to[-3] - 1],
-                    [resize_to[-1] - 1, resize_to[-2] - 1, resize_to[-3] - 1],
-                    [0, resize_to[-2] - 1, resize_to[-3] - 1],
-                ]
-            ],
-            device=_device,
-            dtype=_dtype,
-        ).repeat(batch_size, 1, 1)
+    _input_size = tensor((height, width), device=device, dtype=torch.long).expand(batch_size, -1)
+    _output_size = tensor(size, device=device, dtype=torch.long).expand(batch_size, -1)
 
-    return dict(src=crop_src.to(device=_device), dst=crop_dst.to(device=_device))
+    return dict(src=points_src, dst=points_dst, input_size=_input_size, output_size=_output_size)
```

### Comparing `kornia-0.6.8/kornia/augmentation/random_generator/_3d/motion_blur.py` & `kornia-0.6.9/kornia/filters/motion.py`

 * *Files 27% similar despite different names*

```diff
@@ -1,170 +1,195 @@
-from typing import Dict, Tuple, Union
+from typing import Tuple, Union
 
 import torch
-from torch.distributions import Uniform
+import torch.nn as nn
 
-from kornia.augmentation.random_generator.base import RandomGeneratorBase
-from kornia.augmentation.utils import (
-    _adapted_rsampling,
-    _adapted_uniform,
-    _common_param_check,
-    _joint_range_check,
-    _range_bound,
-    _tuple_range_reader,
-)
-from kornia.utils.helpers import _deprecated, _extract_device_dtype
+from .filter import filter2d, filter3d
+from .kernels_geometry import get_motion_kernel2d, get_motion_kernel3d
 
 
-class MotionBlurGenerator3D(RandomGeneratorBase):
-    r"""Get parameters for motion blur.
+class MotionBlur(nn.Module):
+    r"""Blur 2D images (4D tensor) using the motion filter.
 
     Args:
-        kernel_size: motion kernel size (odd and positive).
-            If int, the kernel will have a fixed size.
-            If Tuple[int, int], it will randomly generate the value from the range batch-wisely.
+        kernel_size: motion kernel width and height. It should be odd and positive.
         angle: angle of the motion blur in degrees (anti-clockwise rotation).
-            If float, it will generate the value from (-angle, angle).
         direction: forward/backward direction of the motion blur.
             Lower values towards -1.0 will point the motion blur towards the back (with angle provided via angle),
             while higher values towards 1.0 will point the motion blur forward. A value of 0.0 leads to a
             uniformly (but still angled) motion blur.
-            If float, it will generate the value from (-direction, direction).
-            If Tuple[int, int], it will randomly generate the value from the range.
+        border_type: the padding mode to be applied before convolving. The expected modes are:
+            ``'constant'``, ``'reflect'``, ``'replicate'`` or ``'circular'``.
 
     Returns:
-        A dict of parameters to be passed for transformation.
-            - ksize_factor (torch.Tensor): element-wise kernel size factors with a shape of (B,).
-            - angle_factor (torch.Tensor): element-wise angle factors with a shape of (B,).
-            - direction_factor (torch.Tensor): element-wise direction factors with a shape of (B,).
-
-    Note:
-        The generated random numbers are not reproducible across different devices and dtypes. By default,
-        the parameters will be generated on CPU in float32. This can be changed by calling
-        ``self.set_rng_device_and_dtype(device="cuda", dtype=torch.float64)``.
+        the blurred input tensor.
+
+    Shape:
+        - Input: :math:`(B, C, H, W)`
+        - Output: :math:`(B, C, H, W)`
+
+    Examples:
+        >>> input = torch.rand(2, 4, 5, 7)
+        >>> motion_blur = MotionBlur(3, 35., 0.5)
+        >>> output = motion_blur(input)  # 2x4x5x7
+    """
+
+    def __init__(self, kernel_size: int, angle: float, direction: float, border_type: str = 'constant') -> None:
+        super().__init__()
+        self.kernel_size = kernel_size
+        self.angle: float = angle
+        self.direction: float = direction
+        self.border_type: str = border_type
+
+    def __repr__(self) -> str:
+        return (
+            f'{self.__class__.__name__} (kernel_size={self.kernel_size}, '
+            f'angle={self.angle}, direction={self.direction}, border_type={self.border_type})'
+        )
+
+    def forward(self, x: torch.Tensor):
+        return motion_blur(x, self.kernel_size, self.angle, self.direction, self.border_type)
+
+
+class MotionBlur3D(nn.Module):
+    r"""Blur 3D volumes (5D tensor) using the motion filter.
+
+    Args:
+        kernel_size: motion kernel width and height. It should be odd and positive.
+        angle: Range of yaw (x-axis), pitch (y-axis), roll (z-axis) to select from.
+        direction: forward/backward direction of the motion blur.
+            Lower values towards -1.0 will point the motion blur towards the back (with angle provided via angle),
+            while higher values towards 1.0 will point the motion blur forward. A value of 0.0 leads to a
+            uniformly (but still angled) motion blur.
+        border_type: the padding mode to be applied before convolving. The expected modes are:
+            ``'constant'``, ``'reflect'``, ``'replicate'`` or ``'circular'``.
+
+    Returns:
+        the blurred input tensor.
+
+    Shape:
+        - Input: :math:`(B, C, D, H, W)`
+        - Output: :math:`(B, C, D, H, W)`
+
+    Examples:
+        >>> input = torch.rand(2, 4, 5, 7, 9)
+        >>> motion_blur = MotionBlur3D(3, 35., 0.5)
+        >>> output = motion_blur(input)  # 2x4x5x7x9
     """
 
     def __init__(
         self,
-        kernel_size: Union[int, Tuple[int, int]],
-        angle: Union[
-            torch.Tensor,
-            float,
-            Tuple[float, float, float],
-            Tuple[Tuple[float, float], Tuple[float, float], Tuple[float, float]],
-        ],
-        direction: Union[torch.Tensor, float, Tuple[float, float]],
+        kernel_size: int,
+        angle: Union[float, Tuple[float, float, float]],
+        direction: float,
+        border_type: str = 'constant',
     ) -> None:
         super().__init__()
         self.kernel_size = kernel_size
-        self.angle = angle
-        self.direction = direction
+        self.angle: Tuple[float, float, float]
+        if isinstance(angle, float):
+            self.angle = (angle, angle, angle)
+        elif isinstance(angle, (tuple, list)) and len(angle) == 3:
+            self.angle = angle
+        else:
+            raise ValueError(f"Expect angle to be either a float or a tuple of floats. Got {angle}.")
+        self.direction: float = direction
+        self.border_type: str = border_type
 
     def __repr__(self) -> str:
-        repr = f"kernel_size={self.kernel_size}, angle={self.angle}, direction={self.direction}"
-        return repr
+        return (
+            f'{self.__class__.__name__} (kernel_size={self.kernel_size}, '
+            f'angle={self.angle}, direction={self.direction}, border_type={self.border_type})'
+        )
 
-    def make_samplers(self, device: torch.device, dtype: torch.dtype) -> None:
-        angle: torch.Tensor = _tuple_range_reader(self.angle, 3, device=device, dtype=dtype)
-        direction = _range_bound(self.direction, 'direction', center=0.0, bounds=(-1, 1)).to(device=device, dtype=dtype)
-        if isinstance(self.kernel_size, int):
-            if not (self.kernel_size >= 3 and self.kernel_size % 2 == 1):
-                raise AssertionError(f"`kernel_size` must be odd and greater than 3. Got {self.kernel_size}.")
-            self.ksize_sampler = Uniform(self.kernel_size // 2, self.kernel_size // 2, validate_args=False)
-        elif isinstance(self.kernel_size, tuple):
-            # kernel_size is fixed across the batch
-            if len(self.kernel_size) != 2:
-                raise AssertionError(f"`kernel_size` must be (2,) if it is a tuple. Got {self.kernel_size}.")
-            self.ksize_sampler = Uniform(self.kernel_size[0] // 2, self.kernel_size[1] // 2, validate_args=False)
-        else:
-            raise TypeError(f"Unsupported type: {type(self.kernel_size)}")
+    def forward(self, x: torch.Tensor):
+        return motion_blur3d(x, self.kernel_size, self.angle, self.direction, self.border_type)
 
-        self.yaw_sampler = Uniform(angle[0][0], angle[0][1], validate_args=False)
-        self.pitch_sampler = Uniform(angle[1][0], angle[1][1], validate_args=False)
-        self.roll_sampler = Uniform(angle[2][0], angle[2][1], validate_args=False)
-        self.direction_sampler = Uniform(direction[0], direction[1], validate_args=False)
-
-    def forward(self, batch_shape: torch.Size, same_on_batch: bool = False) -> Dict[str, torch.Tensor]:  # type:ignore
-        batch_size = batch_shape[0]
-        _common_param_check(batch_size, same_on_batch)
-        # self.ksize_factor.expand((batch_size, -1))
-        _device, _dtype = _extract_device_dtype([self.angle, self.direction])
-        yaw_factor = _adapted_rsampling((batch_size,), self.yaw_sampler, same_on_batch)
-        pitch_factor = _adapted_rsampling((batch_size,), self.pitch_sampler, same_on_batch)
-        roll_factor = _adapted_rsampling((batch_size,), self.roll_sampler, same_on_batch)
-        angle_factor = torch.stack([yaw_factor, pitch_factor, roll_factor], dim=1)
-
-        direction_factor = _adapted_rsampling((batch_size,), self.direction_sampler, same_on_batch)
-        ksize_factor = _adapted_rsampling((batch_size,), self.ksize_sampler, same_on_batch).int() * 2 + 1
-
-        return dict(
-            ksize_factor=ksize_factor.to(device=_device, dtype=torch.int32),
-            angle_factor=angle_factor.to(device=_device, dtype=_dtype),
-            direction_factor=direction_factor.to(device=_device, dtype=_dtype),
-        )
 
+def motion_blur(
+    input: torch.Tensor,
+    kernel_size: int,
+    angle: Union[float, torch.Tensor],
+    direction: Union[float, torch.Tensor],
+    border_type: str = 'constant',
+    mode: str = 'nearest',
+) -> torch.Tensor:
+    r"""Perform motion blur on tensor images.
 
-@_deprecated(replace_with=MotionBlurGenerator3D.__name__)
-def random_motion_blur_generator3d(
-    batch_size: int,
-    kernel_size: Union[int, Tuple[int, int]],
-    angle: torch.Tensor,
-    direction: torch.Tensor,
-    same_on_batch: bool = False,
-    device: torch.device = torch.device('cpu'),
-    dtype: torch.dtype = torch.float32,
-) -> Dict[str, torch.Tensor]:
-    r"""Get parameters for motion blur.
+    .. image:: _static/img/motion_blur.png
 
     Args:
-        batch_size (int): the tensor batch size.
-        kernel_size (int or (int, int)): motion kernel size (odd and positive) or range.
-        angle (torch.Tensor): yaw, pitch and roll range of the motion blur in degrees :math:`(3, 2)`.
-        direction (torch.Tensor): forward/backward direction of the motion blur.
-            Lower values towards -1.0 will point the motion blur towards the back (with
-            angle provided via angle), while higher values towards 1.0 will point the motion
-            blur forward. A value of 0.0 leads to a uniformly (but still angled) motion blur.
-        same_on_batch (bool): apply the same transformation across the batch. Default: False.
-        device (torch.device): the device on which the random numbers will be generated. Default: cpu.
-        dtype (torch.dtype): the data type of the generated random numbers. Default: float32.
+        input: the input tensor with shape :math:`(B, C, H, W)`.
+        kernel_size: motion kernel width and height. It should be odd and positive.
+        angle (Union[torch.Tensor, float]): angle of the motion blur in degrees (anti-clockwise rotation).
+            If tensor, it must be :math:`(B,)`.
+        direction : forward/backward direction of the motion blur.
+            Lower values towards -1.0 will point the motion blur towards the back (with angle provided via angle),
+            while higher values towards 1.0 will point the motion blur forward. A value of 0.0 leads to a
+            uniformly (but still angled) motion blur.
+            If tensor, it must be :math:`(B,)`.
+        border_type: the padding mode to be applied before convolving. The expected modes are:
+            ``'constant'``, ``'reflect'``, ``'replicate'`` or ``'circular'``. Default: ``'constant'``.
+        mode: interpolation mode for rotating the kernel. ``'bilinear'`` or ``'nearest'``.
+
+    Return:
+        the blurred image with shape :math:`(B, C, H, W)`.
+
+    Example:
+        >>> input = torch.randn(1, 3, 80, 90).repeat(2, 1, 1, 1)
+        >>> # perform exact motion blur across the batch
+        >>> out_1 = motion_blur(input, 5, 90., 1)
+        >>> torch.allclose(out_1[0], out_1[1])
+        True
+        >>> # perform element-wise motion blur across the batch
+        >>> out_1 = motion_blur(input, 5, torch.tensor([90., 180,]), torch.tensor([1., -1.]))
+        >>> torch.allclose(out_1[0], out_1[1])
+        False
+    """
+    if border_type not in ["constant", "reflect", "replicate", "circular"]:
+        raise AssertionError
+    kernel: torch.Tensor = get_motion_kernel2d(kernel_size, angle, direction, mode)
+    return filter2d(input, kernel, border_type)
 
-    Returns:
-        params Dict[str, torch.Tensor]: parameters to be passed for transformation.
-            - ksize_factor (torch.Tensor): element-wise kernel size factors with a shape of (B,).
-            - angle_factor (torch.Tensor): element-wise center with a shape of (B,).
-            - direction_factor (torch.Tensor): element-wise scales with a shape of (B,).
 
-    Note:
-        The generated random numbers are not reproducible across different devices and dtypes.
-    """
-    _device, _dtype = _extract_device_dtype([angle, direction])
-    _joint_range_check(direction, 'direction', (-1, 1))
-    if isinstance(kernel_size, int):
-        if not (kernel_size >= 3 and kernel_size % 2 == 1):
-            raise AssertionError(f"`kernel_size` must be odd and greater than 3. Got {kernel_size}.")
-        ksize_factor = torch.tensor([kernel_size] * batch_size, device=device, dtype=dtype).int()
-    elif isinstance(kernel_size, tuple):
-        if not (len(kernel_size) == 2 and kernel_size[0] >= 3 and kernel_size[0] <= kernel_size[1]):
-            raise AssertionError(f"`kernel_size` must be greater than 3. Got range {kernel_size}.")
-        # kernel_size is fixed across the batch
-        ksize_factor = (
-            _adapted_uniform((batch_size,), kernel_size[0] // 2, kernel_size[1] // 2, same_on_batch=True).int() * 2 + 1
-        )
-    else:
-        raise TypeError(f"Unsupported type: {type(kernel_size)}")
+def motion_blur3d(
+    input: torch.Tensor,
+    kernel_size: int,
+    angle: Union[Tuple[float, float, float], torch.Tensor],
+    direction: Union[float, torch.Tensor],
+    border_type: str = 'constant',
+    mode: str = 'nearest',
+) -> torch.Tensor:
+    r"""Perform motion blur on 3D volumes (5D tensor).
 
-    if angle.shape != torch.Size([3, 2]):
-        raise AssertionError(f"'angle' must be the shape of (3, 2). Got {angle.shape}.")
-    angle = angle.to(device=device, dtype=dtype)
-    yaw = _adapted_uniform((batch_size,), angle[0][0], angle[0][1], same_on_batch)
-    pitch = _adapted_uniform((batch_size,), angle[1][0], angle[1][1], same_on_batch)
-    roll = _adapted_uniform((batch_size,), angle[2][0], angle[2][1], same_on_batch)
-    angle_factor = torch.stack([yaw, pitch, roll], dim=1)
-
-    direction = direction.to(device=device, dtype=dtype)
-    direction_factor = _adapted_uniform((batch_size,), direction[0], direction[1], same_on_batch)
-
-    return dict(
-        ksize_factor=ksize_factor.to(device=_device),
-        angle_factor=angle_factor.to(device=_device, dtype=_dtype),
-        direction_factor=direction_factor.to(device=_device, dtype=_dtype),
-    )
+    Args:
+        input: the input tensor with shape :math:`(B, C, D, H, W)`.
+        kernel_size: motion kernel width, height and depth. It should be odd and positive.
+        angle: Range of yaw (x-axis), pitch (y-axis), roll (z-axis) to select from.
+            If tensor, it must be :math:`(B, 3)`.
+        direction: forward/backward direction of the motion blur.
+            Lower values towards -1.0 will point the motion blur towards the back (with angle provided via angle),
+            while higher values towards 1.0 will point the motion blur forward. A value of 0.0 leads to a
+            uniformly (but still angled) motion blur.
+            If tensor, it must be :math:`(B,)`.
+        border_type: the padding mode to be applied before convolving. The expected modes are:
+            ``'constant'``, ``'reflect'``, ``'replicate'`` or ``'circular'``. Default: ``'constant'``.
+        mode: interpolation mode for rotating the kernel. ``'bilinear'`` or ``'nearest'``.
+
+    Return:
+        the blurred image with shape :math:`(B, C, D, H, W)`.
+
+    Example:
+        >>> input = torch.randn(1, 3, 120, 80, 90).repeat(2, 1, 1, 1, 1)
+        >>> # perform exact motion blur across the batch
+        >>> out_1 = motion_blur3d(input, 5, (0., 90., 90.), 1)
+        >>> torch.allclose(out_1[0], out_1[1])
+        True
+        >>> # perform element-wise motion blur across the batch
+        >>> out_1 = motion_blur3d(input, 5, torch.tensor([[0., 90., 90.], [90., 180., 0.]]), torch.tensor([1., -1.]))
+        >>> torch.allclose(out_1[0], out_1[1])
+        False
+    """
+    if border_type not in ["constant", "reflect", "replicate", "circular"]:
+        raise AssertionError
+    kernel: torch.Tensor = get_motion_kernel3d(kernel_size, angle, direction, mode)
+    return filter3d(input, kernel, border_type)
```

### Comparing `kornia-0.6.8/kornia/augmentation/random_generator/base.py` & `kornia-0.6.9/kornia/augmentation/random_generator/base.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,30 +1,32 @@
 from typing import Callable, Dict, Optional
 
 import torch
-import torch.nn as nn
 from torch.distributions import Distribution
 
+from kornia.core import Device, Module, Tensor
+
 
 class _PostInitInjectionMetaClass(type):
     """To inject the ``__post_init__`` function after the creation of each instance."""
 
     def __call__(cls, *args, **kwargs):
         obj = type.__call__(cls, *args, **kwargs)
         obj.__post_init__()
         return obj
 
 
-class RandomGeneratorBase(nn.Module, metaclass=_PostInitInjectionMetaClass):
+class RandomGeneratorBase(Module, metaclass=_PostInitInjectionMetaClass):
     """Base class for generating random augmentation parameters."""
 
+    device: Optional[Device] = None
+    dtype: torch.dtype
+
     def __init__(self) -> None:
         super().__init__()
-        self.device = None
-        self.dtype = None
 
     def __post_init__(self) -> None:
         self.set_rng_device_and_dtype()
 
     def set_rng_device_and_dtype(
         self, device: torch.device = torch.device('cpu'), dtype: torch.dtype = torch.float32
     ) -> None:
@@ -43,15 +45,15 @@
         device, dtype, non_blocking, convert_to_format = torch._C._nn._parse_to(*args, **kwargs)
         self.set_rng_device_and_dtype(device=device, dtype=dtype)
         return self
 
     def make_samplers(self, device: torch.device, dtype: torch.dtype) -> None:
         raise NotImplementedError
 
-    def forward(self, batch_shape: torch.Size, same_on_batch: bool = False) -> Dict[str, torch.Tensor]:  # type:ignore
+    def forward(self, batch_shape: torch.Size, same_on_batch: bool = False) -> Dict[str, Tensor]:
         raise NotImplementedError
 
 
 class DistributionWithMapper(Distribution):
     """Wraps a distribution with a value mapper function.
 
     This is used to restrict the output values of a given distribution by a value mapper function.
@@ -72,31 +74,31 @@
         >>> # with sigmoid mapper
         >>> dist = DistributionWithMapper(Normal(0., 1.,), map_fn=nn.Sigmoid())
         >>> _ = torch.manual_seed(0)
         >>> dist.rsample((8,))
         tensor([0.8236, 0.4272, 0.1017, 0.6384, 0.2527, 0.1980, 0.5995, 0.6980])
     """
 
-    def __init__(self, dist: Distribution, map_fn: Optional[Callable] = None) -> None:
+    def __init__(self, dist: Distribution, map_fn: Optional[Callable[[Tensor], Tensor]] = None) -> None:
         self.dist = dist
         self.map_fn = map_fn
 
-    def rsample(self, sample_shape: torch.Size) -> torch.Tensor:  # type:ignore
+    def rsample(self, sample_shape: torch.Size) -> Tensor:  # type: ignore[override]
         out = self.dist.rsample(sample_shape)
         if self.map_fn is not None:
             out = self.map_fn(out)
         return out
 
-    def sample(self, sample_shape: torch.Size) -> torch.Tensor:  # type:ignore
+    def sample(self, sample_shape: torch.Size) -> Tensor:  # type: ignore[override]
         out = self.dist.sample(sample_shape)
         if self.map_fn is not None:
             out = self.map_fn(out)
         return out
 
-    def sample_n(self, n) -> torch.Tensor:
+    def sample_n(self, n) -> Tensor:
         out = self.dist.sample_n(n)
         if self.map_fn is not None:
             out = self.map_fn(out)
         return out
 
     def __getattr__(self, attr):
         try:
```

### Comparing `kornia-0.6.8/kornia/augmentation/utils/__init__.py` & `kornia-0.6.9/kornia/augmentation/utils/__init__.py`

 * *Files identical despite different names*

### Comparing `kornia-0.6.8/kornia/augmentation/utils/helpers.py` & `kornia-0.6.9/kornia/augmentation/utils/helpers.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,18 +1,18 @@
 from functools import wraps
-from typing import Any, Callable, Dict, List, Optional, Tuple, Union, cast
+from typing import Any, Callable, Dict, List, Optional, Tuple, Union
 
 import torch
-from torch import Tensor
 from torch.distributions import Beta, Uniform
 
+from kornia.core import Tensor, as_tensor
 from kornia.utils import _extract_device_dtype
 
 
-def _validate_input(f: Callable) -> Callable:
+def _validate_input(f: Callable[..., Any]) -> Callable[..., Any]:
     r"""Validate the 2D input of the wrapped function.
 
     Args:
         f: a function that takes the first argument as tensor.
 
     Returns:
         the wrapped function after input is validated.
@@ -27,15 +27,15 @@
         _validate_input_dtype(input, accepted_dtypes=[torch.float16, torch.float32, torch.float64])
 
         return f(input, *args, **kwargs)
 
     return wrapper
 
 
-def _validate_input3d(f: Callable) -> Callable:
+def _validate_input3d(f: Callable[..., Any]) -> Callable[..., Any]:
     r"""Validate the 3D input of the wrapped function.
 
     Args:
         f: a function that takes the first argument as tensor.
 
     Returns:
         the wrapped function after input is validated.
@@ -124,45 +124,44 @@
 
     if len(input.shape) == 4:
         input = input.unsqueeze(0)
 
     return input
 
 
-def _validate_input_dtype(input: Tensor, accepted_dtypes: List) -> None:
+def _validate_input_dtype(input: Tensor, accepted_dtypes: List[torch.dtype]) -> None:
     r"""Check if the dtype of the input tensor is in the range of accepted_dtypes
     Args:
         input: Tensor
         accepted_dtypes: List. e.g. [torch.float32, torch.float64]
     """
     if input.dtype not in accepted_dtypes:
         raise TypeError(f"Expected input of {accepted_dtypes}. Got {input.dtype}")
 
 
-def _transform_output_shape(output: Tensor, shape: Tuple) -> Tensor:
+def _transform_output_shape(output: Tensor, shape: Tuple[int, ...]) -> Tensor:
     r"""Collapse the broadcasted batch dimensions an input tensor to be the specified shape.
     Args:
         input: Tensor
         shape: List/tuple of int
 
     Returns:
         Tensor
     """
-    out_tensor: Tensor
-    out_tensor = cast(Tensor, output)
+    out_tensor = output.clone()
 
     for dim in range(len(out_tensor.shape) - len(shape)):
         if out_tensor.shape[0] != 1:
             raise AssertionError(f'Dimension {dim} of input is ' f'expected to be 1, got {out_tensor.shape[0]}')
         out_tensor = out_tensor.squeeze(0)
 
-    return out_tensor  # type: ignore
+    return out_tensor
 
 
-def _validate_shape(shape: Union[Tuple, torch.Size], required_shapes: Tuple[str, ...] = ("BCHW",)) -> None:
+def _validate_shape(shape: Union[Tuple[int, ...], torch.Size], required_shapes: Tuple[str, ...] = ("BCHW",)) -> None:
     r"""Check if the dtype of the input tensor is in the range of accepted_dtypes
     Args:
         shape: tensor shape
         required_shapes: List. e.g. ["BCHW", "BCDHW"]
     """
     passed = False
     for required_shape in required_shapes:
@@ -186,41 +185,47 @@
     Returns:
         bool
     """
     return input.shape[channel_index] == number
 
 
 def _adapted_rsampling(
-    shape: Union[Tuple, torch.Size], dist: torch.distributions.Distribution, same_on_batch=False
+    shape: Union[Tuple[int, ...], torch.Size], dist: torch.distributions.Distribution, same_on_batch=False
 ) -> Tensor:
     r"""The uniform reparameterized sampling function that accepts 'same_on_batch'.
 
     If same_on_batch is True, all values generated will be exactly same given a batch_size (shape[0]). By default,
     same_on_batch is set to False.
     """
+    if isinstance(shape, tuple):
+        shape = torch.Size(shape)
+
     if same_on_batch:
-        return dist.rsample((1, *shape[1:])).repeat(shape[0], *[1] * (len(shape) - 1))
+        return dist.rsample(torch.Size((1, *shape[1:]))).repeat(shape[0], *[1] * (len(shape) - 1))
     return dist.rsample(shape)
 
 
 def _adapted_sampling(
-    shape: Union[Tuple, torch.Size], dist: torch.distributions.Distribution, same_on_batch=False
+    shape: Union[Tuple[int, ...], torch.Size], dist: torch.distributions.Distribution, same_on_batch=False
 ) -> Tensor:
     r"""The uniform sampling function that accepts 'same_on_batch'.
 
     If same_on_batch is True, all values generated will be exactly same given a batch_size (shape[0]). By default,
     same_on_batch is set to False.
     """
+    if isinstance(shape, tuple):
+        shape = torch.Size(shape)
+
     if same_on_batch:
-        return dist.sample((1, *shape[1:])).repeat(shape[0], *[1] * (len(shape) - 1))
+        return dist.sample(torch.Size((1, *shape[1:]))).repeat(shape[0], *[1] * (len(shape) - 1))
     return dist.sample(shape)
 
 
 def _adapted_uniform(
-    shape: Union[Tuple, torch.Size],
+    shape: Union[Tuple[int, ...], torch.Size],
     low: Union[float, int, Tensor],
     high: Union[float, int, Tensor],
     same_on_batch: bool = False,
 ) -> Tensor:
     r"""The uniform sampling function that accepts 'same_on_batch'.
 
     If same_on_batch is True, all values generated will be exactly same given a batch_size (shape[0]).
@@ -228,44 +233,44 @@
 
     By default, sampling happens on the default device and dtype. If low/high is a tensor, sampling will happen
     in the same device/dtype as low/high tensor.
     """
     device, dtype = _extract_device_dtype(
         [low if isinstance(low, Tensor) else None, high if isinstance(high, Tensor) else None]
     )
-    low = torch.as_tensor(low, device=device, dtype=dtype)
-    high = torch.as_tensor(high, device=device, dtype=dtype)
+    low = as_tensor(low, device=device, dtype=dtype)
+    high = as_tensor(high, device=device, dtype=dtype)
     # validate_args=False to fix pytorch 1.7.1 error:
     #     ValueError: Uniform is not defined when low>= high.
     dist = Uniform(low, high, validate_args=False)
     return _adapted_rsampling(shape, dist, same_on_batch)
 
 
 def _adapted_beta(
-    shape: Union[Tuple, torch.Size],
+    shape: Union[Tuple[int, ...], torch.Size],
     a: Union[float, int, Tensor],
     b: Union[float, int, Tensor],
     same_on_batch: bool = False,
 ) -> Tensor:
     r"""The beta sampling function that accepts 'same_on_batch'.
 
     If same_on_batch is True, all values generated will be exactly same given a batch_size (shape[0]).
     By default, same_on_batch is set to False.
 
     By default, sampling happens on the default device and dtype. If a/b is a tensor, sampling will happen
     in the same device/dtype as a/b tensor.
     """
     device, dtype = _extract_device_dtype([a if isinstance(a, Tensor) else None, b if isinstance(b, Tensor) else None])
-    a = torch.as_tensor(a, device=device, dtype=dtype)
-    b = torch.as_tensor(b, device=device, dtype=dtype)
+    a = as_tensor(a, device=device, dtype=dtype)
+    b = as_tensor(b, device=device, dtype=dtype)
     dist = Beta(a, b, validate_args=False)
     return _adapted_rsampling(shape, dist, same_on_batch)
 
 
-def _shape_validation(param: Tensor, shape: Union[tuple, list], name: str) -> None:
+def _shape_validation(param: Tensor, shape: Union[Tuple[int, ...], List[int]], name: str) -> None:
     if param.shape != torch.Size(shape):
         raise AssertionError(f"Invalid shape for {name}. Expected {shape}. Got {param.shape}")
 
 
 def deepcopy_dict(params: Dict[str, Any]) -> Dict[str, Any]:
     """Perform deep copy on any dict.
```

### Comparing `kornia-0.6.8/kornia/augmentation/utils/param_validation.py` & `kornia-0.6.9/kornia/augmentation/utils/param_validation.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,57 +1,59 @@
-from typing import List, Optional, Tuple, Union, cast
+from typing import Any, List, Optional, Tuple, Union
 
 import torch
 
+from kornia.core import Tensor, as_tensor, tensor
+
 
 def _common_param_check(batch_size: int, same_on_batch: Optional[bool] = None):
     """Valid batch_size and same_on_batch params."""
     if not (type(batch_size) is int and batch_size >= 0):
         raise AssertionError(f"`batch_size` shall be a positive integer. Got {batch_size}.")
     if same_on_batch is not None and type(same_on_batch) is not bool:
         raise AssertionError(f"`same_on_batch` shall be boolean. Got {same_on_batch}.")
 
 
 def _range_bound(
-    factor: Union[torch.Tensor, float, Tuple[float, float], List[float]],
+    factor: Union[Tensor, float, Tuple[float, float], List[float]],
     name: str,
     center: float = 0.0,
     bounds: Tuple[float, float] = (0, float('inf')),
     check: Optional[str] = 'joint',
     device: torch.device = torch.device('cpu'),
     dtype: torch.dtype = torch.get_default_dtype(),
-) -> torch.Tensor:
+) -> Tensor:
     r"""Check inputs and compute the corresponding factor bounds."""
-    if not isinstance(factor, (torch.Tensor)):
-        factor = torch.tensor(factor, device=device, dtype=dtype)
-    factor_bound: torch.Tensor
+    if not isinstance(factor, (Tensor)):
+        factor = tensor(factor, device=device, dtype=dtype)
+    factor_bound: Tensor
 
     if factor.dim() == 0:
         if factor < 0:
             raise ValueError(f"If {name} is a single number number, it must be non negative. Got {factor}")
         # Should be something other than clamp
         # Currently, single value factor will not out of scope as long as the user provided it.
         # Note: I personally think throw an error will be better than a coarse clamp.
-        factor_bound = factor.repeat(2) * torch.tensor([-1.0, 1.0], device=factor.device, dtype=factor.dtype) + center
+        factor_bound = factor.repeat(2) * tensor([-1.0, 1.0], device=factor.device, dtype=factor.dtype) + center
         factor_bound = factor_bound.clamp(bounds[0], bounds[1]).to(device=device, dtype=dtype)
     else:
-        factor_bound = torch.as_tensor(factor, device=device, dtype=dtype)
+        factor_bound = as_tensor(factor, device=device, dtype=dtype)
 
     if check is not None:
         if check == 'joint':
             _joint_range_check(factor_bound, name, bounds)
         elif check == 'singular':
             _singular_range_check(factor_bound, name, bounds)
         else:
             raise NotImplementedError(f"methods '{check}' not implemented.")
 
     return factor_bound
 
 
-def _joint_range_check(ranged_factor: torch.Tensor, name: str, bounds: Optional[Tuple[float, float]] = None) -> None:
+def _joint_range_check(ranged_factor: Tensor, name: str, bounds: Optional[Tuple[float, float]] = None) -> None:
     """Check if bounds[0] <= ranged_factor[0] <= ranged_factor[1] <= bounds[1]"""
     if bounds is None:
         bounds = (float('-inf'), float('inf'))
     if ranged_factor.dim() == 1 and len(ranged_factor) == 2:
         if not bounds[0] <= ranged_factor[0] or not bounds[1] >= ranged_factor[1]:
             raise ValueError(f"{name} out of bounds. Expected inside {bounds}, got {ranged_factor}.")
 
@@ -60,15 +62,15 @@
     else:
         raise TypeError(
             f"{name} should be a tensor with length 2 whose values between {bounds}. " f"Got {ranged_factor}."
         )
 
 
 def _singular_range_check(
-    ranged_factor: torch.Tensor,
+    ranged_factor: Tensor,
     name: str,
     bounds: Optional[Tuple[float, float]] = None,
     skip_none: bool = False,
     mode: str = '2d',
 ) -> None:
     """Check if bounds[0] <= ranged_factor[0] <= bounds[1] and bounds[0] <= ranged_factor[1] <= bounds[1]"""
     if mode == '2d':
@@ -90,87 +92,83 @@
         raise TypeError(
             f"{name} should be a float number or a tuple with length {dim_size} whose values between {bounds}."
             f"Got {ranged_factor}"
         )
 
 
 def _tuple_range_reader(
-    input_range: Union[torch.Tensor, float, tuple],
+    input_range: Union[Tensor, float, Tuple[Any, ...]],
     target_size: int,
     device: Optional[torch.device] = None,
     dtype: Optional[torch.dtype] = None,
-) -> torch.Tensor:
+) -> Tensor:
     """Given target_size, it will generate the corresponding (target_size, 2) range tensor for element-wise params.
 
     Example:
-    >>> degree = torch.tensor([0.2, 0.3])
+    >>> degree = tensor([0.2, 0.3])
     >>> _tuple_range_reader(degree, 3)  # read degree for yaw, pitch and roll.
     tensor([[0.2000, 0.3000],
             [0.2000, 0.3000],
             [0.2000, 0.3000]])
     """
     target_shape = torch.Size([target_size, 2])
-    if not torch.is_tensor(input_range):
-        if isinstance(input_range, (float, int)):
+
+    if isinstance(input_range, Tensor):
+        if (len(input_range.shape) == 0) or (len(input_range.shape) == 1 and len(input_range) == 1):
             if input_range < 0:
                 raise ValueError(f"If input_range is only one number it must be a positive number. Got{input_range}")
-            input_range_tmp = torch.tensor([-input_range, input_range], device=device, dtype=dtype).repeat(
-                target_shape[0], 1
+            input_range_tmp = input_range.repeat(2).to(device=device, dtype=dtype) * tensor(
+                [-1, 1], device=device, dtype=dtype
             )
+            input_range_tmp = input_range_tmp.repeat(target_shape[0], 1)
+
+        elif len(input_range.shape) == 1 and len(input_range) == 2:
+            input_range_tmp = input_range.repeat(target_shape[0], 1).to(device=device, dtype=dtype)
+
+        elif len(input_range.shape) == 1 and len(input_range) == target_shape[0]:
+            input_range_tmp = input_range.unsqueeze(1).repeat(1, 2).to(device=device, dtype=dtype) * tensor(
+                [-1, 1], device=device, dtype=dtype
+            )
+
+        elif input_range.shape == target_shape:
+            input_range_tmp = input_range.to(device=device, dtype=dtype)
+
+        else:
+            raise ValueError(
+                f"Degrees must be a {list(target_shape)} tensor for the degree range for independent operation."
+                f"Got {input_range}"
+            )
+    else:
+        if isinstance(input_range, (float, int)):
+            if input_range < 0:
+                raise ValueError(f"If input_range is only one number it must be a positive number. Got{input_range}")
+            input_range_tmp = tensor([-input_range, input_range], device=device, dtype=dtype).repeat(target_shape[0], 1)
 
         elif (
             isinstance(input_range, (tuple, list))
             and len(input_range) == 2
             and isinstance(input_range[0], (float, int))
             and isinstance(input_range[1], (float, int))
         ):
-            input_range_tmp = torch.tensor(input_range, device=device, dtype=dtype).repeat(target_shape[0], 1)
+            input_range_tmp = tensor(input_range, device=device, dtype=dtype).repeat(target_shape[0], 1)
 
         elif (
             isinstance(input_range, (tuple, list))
             and len(input_range) == target_shape[0]
             and all(isinstance(x, (float, int)) for x in input_range)
         ):
-            input_range_tmp = torch.tensor([(-s, s) for s in input_range], device=device, dtype=dtype)
+            input_range_tmp = tensor([(-s, s) for s in input_range], device=device, dtype=dtype)
 
         elif (
             isinstance(input_range, (tuple, list))
             and len(input_range) == target_shape[0]
             and all(isinstance(x, (tuple, list)) for x in input_range)
         ):
-            input_range_tmp = torch.tensor(input_range, device=device, dtype=dtype)
+            input_range_tmp = tensor(input_range, device=device, dtype=dtype)
 
         else:
             raise TypeError(
                 "If not pass a tensor, it must be float, (float, float) for isotropic operation or a tuple of "
                 f"{target_size} floats or {target_size} (float, float) for independent operation. Got {input_range}."
             )
 
-    else:
-        # https://mypy.readthedocs.io/en/latest/casts.html cast to please mypy gods
-        input_range = cast(torch.Tensor, input_range)
-        if (len(input_range.shape) == 0) or (len(input_range.shape) == 1 and len(input_range) == 1):
-            if input_range < 0:
-                raise ValueError(f"If input_range is only one number it must be a positive number. Got{input_range}")
-            input_range_tmp = input_range.repeat(2) * torch.tensor(
-                [-1.0, 1.0], device=input_range.device, dtype=input_range.dtype
-            )
-            input_range_tmp = input_range_tmp.repeat(target_shape[0], 1)
-
-        elif len(input_range.shape) == 1 and len(input_range) == 2:
-            input_range_tmp = input_range.repeat(target_shape[0], 1)
-
-        elif len(input_range.shape) == 1 and len(input_range) == target_shape[0]:
-            input_range_tmp = input_range.unsqueeze(1).repeat(1, 2) * torch.tensor(
-                [-1, 1], device=input_range.device, dtype=input_range.dtype
-            )
-
-        elif input_range.shape == target_shape:
-            input_range_tmp = input_range
-
-        else:
-            raise ValueError(
-                f"Degrees must be a {list(target_shape)} tensor for the degree range for independent operation."
-                f"Got {input_range}"
-            )
-
     return input_range_tmp
```

### Comparing `kornia-0.6.8/kornia/color/__init__.py` & `kornia-0.6.9/kornia/color/__init__.py`

 * *Files 6% similar despite different names*

```diff
@@ -18,14 +18,15 @@
     linear_rgb_to_rgb,
     rgb_to_bgr,
     rgb_to_linear_rgb,
     rgb_to_rgba,
     rgba_to_bgr,
     rgba_to_rgb,
 )
+from .sepia import Sepia, sepia_from_rgb
 from .xyz import RgbToXyz, XyzToRgb, rgb_to_xyz, xyz_to_rgb
 from .ycbcr import RgbToYcbcr, YcbcrToRgb, rgb_to_y, rgb_to_ycbcr, ycbcr_to_rgb
 from .yuv import (
     RgbToYuv,
     RgbToYuv420,
     RgbToYuv422,
     Yuv420ToRgb,
@@ -100,8 +101,12 @@
     "LinearRgbToRgb",
     "rgb_to_linear_rgb",
     "rgba_to_rgb",
     "rgba_to_bgr",
     "RgbaToRgb",
     "RgbaToBgr",
     "RgbToLinearRgb",
+    "Sepia",
+    "sepia",
 ]
+
+sepia = sepia_from_rgb
```

### Comparing `kornia-0.6.8/kornia/color/gray.py` & `kornia-0.6.9/kornia/color/gray.py`

 * *Files identical despite different names*

### Comparing `kornia-0.6.8/kornia/color/hls.py` & `kornia-0.6.9/kornia/color/hls.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,14 +1,16 @@
 import math
+from typing import Tuple
 
 import torch
-import torch.nn as nn
 
+from kornia.core import Module, Tensor, stack, tensor, where
 
-def rgb_to_hls(image: torch.Tensor, eps: float = 1e-8) -> torch.Tensor:
+
+def rgb_to_hls(image: Tensor, eps: float = 1e-8) -> Tensor:
     r"""Convert a RGB image to HLS.
 
     .. image:: _static/img/rgb_to_hls.png
 
     The image data is assumed to be in the range of (0, 1).
 
     NOTE: this method cannot be compiled with JIT in pytohrch < 1.7.0
@@ -20,38 +22,27 @@
     Returns:
         HLS version of the image with shape :math:`(*, 3, H, W)`.
 
     Example:
         >>> input = torch.rand(2, 3, 4, 5)
         >>> output = rgb_to_hls(input)  # 2x3x4x5
     """
-    if not isinstance(image, torch.Tensor):
-        raise TypeError(f"Input type is not a torch.Tensor. Got {type(image)}")
+    if not isinstance(image, Tensor):
+        raise TypeError(f"Input type is not a Tensor. Got {type(image)}")
 
     if len(image.shape) < 3 or image.shape[-3] != 3:
         raise ValueError(f"Input size must have a shape of (*, 3, H, W). Got {image.shape}")
 
-    if not torch.jit.is_scripting():
-        # weird way to use globals compiling with JIT even in the code not used by JIT...
-        # __setattr__ can be removed if pytorch version is > 1.6.0 and then use:
-        # rgb_to_hls.RGB2HSL_IDX = hls_to_rgb.RGB2HSL_IDX.to(image.device)
-        rgb_to_hls.__setattr__('RGB2HSL_IDX', rgb_to_hls.RGB2HSL_IDX.to(image))  # type: ignore
-        _RGB2HSL_IDX: torch.Tensor = rgb_to_hls.RGB2HSL_IDX  # type: ignore
-    else:
-        _RGB2HSL_IDX = torch.tensor([[[0.0]], [[1.0]], [[2.0]]], device=image.device, dtype=image.dtype)  # 3x1x1
+    _RGB2HSL_IDX = tensor([[[0.0]], [[1.0]], [[2.0]]], device=image.device, dtype=image.dtype)  # 3x1x1
+
+    _img_max: Tuple[Tensor, Tensor] = image.max(-3)
+    maxc = _img_max[0]
+    imax = _img_max[1]
+    minc: Tensor = image.min(-3)[0]
 
-    # maxc: torch.Tensor  # not supported by JIT
-    # imax: torch.Tensor  # not supported by JIT
-    maxc, imax = image.max(-3)
-    minc: torch.Tensor = image.min(-3)[0]
-
-    # h: torch.Tensor  # not supported by JIT
-    # l: torch.Tensor  # not supported by JIT
-    # s: torch.Tensor  # not supported by JIT
-    # image_hls: torch.Tensor  # not supported by JIT
     if image.requires_grad:
         l_ = maxc + minc
         s = maxc - minc
         # weird behaviour with undefined vars in JIT...
         # scripting requires image_hls be defined even if it is not used :S
         h = l_  # assign to any tensor...
         image_hls = l_  # assign to any tensor...
@@ -63,96 +54,83 @@
         h = torch.select(image_hls, -3, 0)
         l_ = torch.select(image_hls, -3, 1)
         s = torch.select(image_hls, -3, 2)
         torch.add(maxc, minc, out=l_)  # l = max + min
         torch.sub(maxc, minc, out=s)  # s = max - min
 
     # precompute image / (max - min)
-    im: torch.Tensor = image / (s + eps).unsqueeze(-3)
+    im = image / (s + eps).unsqueeze(-3)
 
     # epsilon cannot be inside the torch.where to avoid precision issues
-    s /= torch.where(l_ < 1.0, l_, 2.0 - l_) + eps  # saturation
+    s /= where(l_ < 1.0, l_, 2.0 - l_) + eps  # saturation
     l_ /= 2  # luminance
 
     # note that r,g and b were previously div by (max - min)
-    r: torch.Tensor = torch.select(im, -3, 0)
-    g: torch.Tensor = torch.select(im, -3, 1)
-    b: torch.Tensor = torch.select(im, -3, 2)
+    r = torch.select(im, -3, 0)
+    g = torch.select(im, -3, 1)
+    b = torch.select(im, -3, 2)
     # h[imax == 0] = (((g - b) / (max - min)) % 6)[imax == 0]
     # h[imax == 1] = (((b - r) / (max - min)) + 2)[imax == 1]
     # h[imax == 2] = (((r - g) / (max - min)) + 4)[imax == 2]
-    cond: torch.Tensor = imax.unsqueeze(-3) == _RGB2HSL_IDX
+    cond = imax.unsqueeze(-3) == _RGB2HSL_IDX
     if image.requires_grad:
         h = torch.mul((g - b) % 6, torch.select(cond, -3, 0))
     else:
         torch.mul((g - b).remainder(6), torch.select(cond, -3, 0), out=h)
     h += torch.add(b - r, 2) * torch.select(cond, -3, 1)
     h += torch.add(r - g, 4) * torch.select(cond, -3, 2)
     # h = 2.0 * math.pi * (60.0 * h) / 360.0
     h *= math.pi / 3.0  # hue [0, 2*pi]
 
     if image.requires_grad:
-        return torch.stack([h, l_, s], dim=-3)
+        return stack([h, l_, s], -3)
     return image_hls
 
 
-def hls_to_rgb(image: torch.Tensor) -> torch.Tensor:
+def hls_to_rgb(image: Tensor) -> Tensor:
     r"""Convert a HLS image to RGB.
 
     The image data is assumed to be in the range of (0, 1).
 
     Args:
         image: HLS image to be converted to RGB with shape :math:`(*, 3, H, W)`.
 
     Returns:
         RGB version of the image with shape :math:`(*, 3, H, W)`.
 
     Example:
         >>> input = torch.rand(2, 3, 4, 5)
         >>> output = hls_to_rgb(input)  # 2x3x4x5
     """
-    if not isinstance(image, torch.Tensor):
-        raise TypeError(f"Input type is not a torch.Tensor. Got {type(image)}")
+    if not isinstance(image, Tensor):
+        raise TypeError(f"Input type is not a Tensor. Got {type(image)}")
 
     if len(image.shape) < 3 or image.shape[-3] != 3:
         raise ValueError(f"Input size must have a shape of (*, 3, H, W). Got {image.shape}")
 
-    if not torch.jit.is_scripting():
-        # weird way to use globals compiling with JIT even in the code not used by JIT...
-        # __setattr__ can be removed if pytorch version is > 1.6.0 and then use:
-        # hls_to_rgb.HLS2RGB = hls_to_rgb.HLS2RGB.to(image.device)
-        hls_to_rgb.__setattr__('HLS2RGB', hls_to_rgb.HLS2RGB.to(image))  # type: ignore
-        _HLS2RGB: torch.Tensor = hls_to_rgb.HLS2RGB  # type: ignore
-    else:
-        _HLS2RGB = torch.tensor([[[0.0]], [[8.0]], [[4.0]]], device=image.device, dtype=image.dtype)  # 3x1x1
+    _HLS2RGB = tensor([[[0.0]], [[8.0]], [[4.0]]], device=image.device, dtype=image.dtype)  # 3x1x1
 
-    im: torch.Tensor = image.unsqueeze(-4)
-    h: torch.Tensor = torch.select(im, -3, 0)
-    l: torch.Tensor = torch.select(im, -3, 1)
-    s: torch.Tensor = torch.select(im, -3, 2)
+    im: Tensor = image.unsqueeze(-4)
+    h: Tensor = torch.select(im, -3, 0)
+    l: Tensor = torch.select(im, -3, 1)
+    s: Tensor = torch.select(im, -3, 2)
     h = h * (6 / math.pi)  # h * 360 / (2 * math.pi) / 30
     a = s * torch.min(l, 1.0 - l)
 
     # kr = (0 + h) % 12
     # kg = (8 + h) % 12
     # kb = (4 + h) % 12
-    k: torch.Tensor = (h + _HLS2RGB) % 12
+    k: Tensor = (h + _HLS2RGB) % 12
 
     # l - a * max(min(min(k - 3.0, 9.0 - k), 1), -1)
     mink = torch.min(k - 3.0, 9.0 - k)
     return torch.addcmul(l, a, mink.clamp_(min=-1.0, max=1.0), value=-1)
 
 
-# tricks to speed up a little bit the conversions by presetting small tensors
-# (in the functions they are moved to the proper device)
-hls_to_rgb.__setattr__('HLS2RGB', torch.tensor([[[0.0]], [[8.0]], [[4.0]]]))  # 3x1x1
-rgb_to_hls.__setattr__('RGB2HSL_IDX', torch.tensor([[[0.0]], [[1.0]], [[2.0]]]))  # 3x1x1
-
-
-class RgbToHls(nn.Module):
+class RgbToHls(Module):
     r"""Convert an image from RGB to HLS.
 
     The image data is assumed to be in the range of (0, 1).
 
     Returns:
         HLS version of the image.
 
@@ -162,19 +140,19 @@
 
     Examples:
         >>> input = torch.rand(2, 3, 4, 5)
         >>> hls = RgbToHls()
         >>> output = hls(input)  # 2x3x4x5
     """
 
-    def forward(self, image: torch.Tensor) -> torch.Tensor:
+    def forward(self, image: Tensor) -> Tensor:
         return rgb_to_hls(image)
 
 
-class HlsToRgb(nn.Module):
+class HlsToRgb(Module):
     r"""Convert an image from HLS to RGB.
 
     The image data is assumed to be in the range of (0, 1).
 
     Returns:
         RGB version of the image.
 
@@ -187,9 +165,9 @@
 
     Examples:
         >>> input = torch.rand(2, 3, 4, 5)
         >>> rgb = HlsToRgb()
         >>> output = rgb(input)  # 2x3x4x5
     """
 
-    def forward(self, image: torch.Tensor) -> torch.Tensor:
+    def forward(self, image: Tensor) -> Tensor:
         return hls_to_rgb(image)
```

### Comparing `kornia-0.6.8/kornia/color/hsv.py` & `kornia-0.6.9/kornia/color/hsv.py`

 * *Files identical despite different names*

### Comparing `kornia-0.6.8/kornia/color/lab.py` & `kornia-0.6.9/kornia/color/lab.py`

 * *Files identical despite different names*

### Comparing `kornia-0.6.8/kornia/color/luv.py` & `kornia-0.6.9/kornia/color/luv.py`

 * *Files identical despite different names*

### Comparing `kornia-0.6.8/kornia/color/raw.py` & `kornia-0.6.9/kornia/color/raw.py`

 * *Files identical despite different names*

### Comparing `kornia-0.6.8/kornia/color/rgb.py` & `kornia-0.6.9/kornia/color/rgb.py`

 * *Files identical despite different names*

### Comparing `kornia-0.6.8/kornia/color/xyz.py` & `kornia-0.6.9/kornia/color/xyz.py`

 * *Files identical despite different names*

### Comparing `kornia-0.6.8/kornia/color/ycbcr.py` & `kornia-0.6.9/kornia/color/ycbcr.py`

 * *Files identical despite different names*

### Comparing `kornia-0.6.8/kornia/color/yuv.py` & `kornia-0.6.9/kornia/color/yuv.py`

 * *Files identical despite different names*

### Comparing `kornia-0.6.8/kornia/contrib/__init__.py` & `kornia-0.6.9/kornia/contrib/__init__.py`

 * *Files identical despite different names*

### Comparing `kornia-0.6.8/kornia/contrib/classification.py` & `kornia-0.6.9/kornia/contrib/classification.py`

 * *Files identical despite different names*

### Comparing `kornia-0.6.8/kornia/contrib/connected_components.py` & `kornia-0.6.9/kornia/contrib/connected_components.py`

 * *Files identical despite different names*

### Comparing `kornia-0.6.8/kornia/contrib/diamond_square.py` & `kornia-0.6.9/kornia/contrib/diamond_square.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,28 +1,28 @@
 # based on: https://github.com/anguelos/tormentor/blob/e8050ac235b0c7ad3c7d931cfa47c308a305c486/diamond_square/diamond_square.py  # noqa: E501
 import math
 from typing import Callable, List, Optional, Tuple, Union
 
 import torch
-from torch import Tensor
 
+from kornia.core import Tensor
 from kornia.enhance import normalize_min_max
 from kornia.filters import filter2d
 from kornia.testing import KORNIA_CHECK, KORNIA_CHECK_IS_TENSOR, KORNIA_CHECK_SHAPE
 
 # the default kernels for the diamond square
 default_diamond_kernel: List[List[float]] = [[0.25, 0.0, 0.25], [0.0, 0.0, 0.0], [0.25, 0.0, 0.25]]
 default_square_kernel: List[List[float]] = [[0.0, 0.25, 0.0], [0.25, 0.0, 0.25], [0.0, 0.25, 0.0]]
 
 
 def _diamond_square_seed(
     replicates: int,
     width: int,
     height: int,
-    random_fn: Callable,
+    random_fn: Callable[..., Tensor],
     device: Optional[torch.device] = None,
     dtype: Optional[torch.dtype] = None,
 ) -> Tensor:
     """Helper function to generate the diamond square image seee.
 
     Args:
         replicates: the num of batched replicas for the image.
@@ -69,15 +69,15 @@
         res = res.transpose(2, 3)
     return res
 
 
 def _one_diamond_one_square(
     img: Tensor,
     random_scale: Union[float, Tensor],
-    random_fn: Callable = torch.rand,
+    random_fn: Callable[..., Tensor] = torch.rand,
     diamond_kernel: Optional[Tensor] = None,
     square_kernel: Optional[Tensor] = None,
 ) -> Tensor:
     """Doubles the image resolution by applying a single diamond square steps.
 
     Recursive application of this method creates plasma fractals.
 
@@ -137,15 +137,15 @@
     return new_img
 
 
 def diamond_square(
     output_size: Tuple[int, int, int, int],
     roughness: Union[float, Tensor] = 0.5,
     random_scale: Union[float, Tensor] = 1.0,
-    random_fn: Callable = torch.rand,
+    random_fn: Callable[..., Tensor] = torch.rand,
     normalize_range: Optional[Tuple[int, int]] = None,
     device: Optional[torch.device] = None,
     dtype: Optional[torch.dtype] = None,
 ) -> Tensor:
     """Generates Plasma Fractal Images using the diamond square algorithm.
 
     See: https://en.wikipedia.org/wiki/Diamond-square_algorithm
```

### Comparing `kornia-0.6.8/kornia/contrib/distance_transform.py` & `kornia-0.6.9/kornia/contrib/distance_transform.py`

 * *Files identical despite different names*

### Comparing `kornia-0.6.8/kornia/contrib/edge_detection.py` & `kornia-0.6.9/kornia/contrib/edge_detection.py`

 * *Files identical despite different names*

### Comparing `kornia-0.6.8/kornia/contrib/extract_patches.py` & `kornia-0.6.9/kornia/contrib/extract_patches.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,14 +1,14 @@
 from typing import Optional, Tuple, Union, cast
 
 import torch
-import torch.nn as nn
-import torch.nn.functional as F
 from torch.nn.modules.utils import _pair
 
+from kornia.core import Module, Tensor, concatenate, pad
+
 PadType = Union[Tuple[int, int], Tuple[int, int, int, int]]
 
 
 def compute_padding(
     original_size: Union[int, Tuple[int, int]], window_size: Union[int, Tuple[int, int]]
 ) -> Tuple[int, int, int, int]:
     r"""Compute required padding to ensure chaining of :func:`extract_tensor_patches` and
@@ -53,15 +53,15 @@
 
     padt, padb = paddim(original_size[0], window_size[0])
     padl, padr = paddim(original_size[1], window_size[1])
 
     return (padt, padb, padl, padr)
 
 
-class ExtractTensorPatches(nn.Module):
+class ExtractTensorPatches(Module):
     r"""Module that extract patches from tensors and stack them.
 
     In the simplest case, the output value of the operator with input size
     :math:`(B, C, H, W)` is :math:`(B, N, C, H_{out}, W_{out})`.
 
     where
       - :math:`B` is the batch size.
@@ -122,19 +122,19 @@
         padding: Optional[Union[int, PadType]] = 0,
     ) -> None:
         super().__init__()
         self.window_size: Tuple[int, int] = _pair(window_size)
         self.stride: Tuple[int, int] = _pair(stride)
         self.padding: PadType = _pair(padding)
 
-    def forward(self, input: torch.Tensor) -> torch.Tensor:  # type: ignore
+    def forward(self, input: Tensor) -> Tensor:
         return extract_tensor_patches(input, self.window_size, stride=self.stride, padding=self.padding)
 
 
-class CombineTensorPatches(nn.Module):
+class CombineTensorPatches(Module):
     r"""Module that combine patches from tensors.
 
     In the simplest case, the output value of the operator with input size
     :math:`(B, N, C, H_{out}, W_{out})` is :math:`(B, C, H, W)`.
 
     where
       - :math:`B` is the batch size.
@@ -193,27 +193,27 @@
         unpadding: Union[int, PadType] = 0,
     ) -> None:
         super().__init__()
         self.original_size: Tuple[int, int] = _pair(original_size)
         self.window_size: Tuple[int, int] = _pair(window_size)
         self.unpadding: PadType = _pair(unpadding)
 
-    def forward(self, input: torch.Tensor) -> torch.Tensor:
+    def forward(self, input: Tensor) -> Tensor:
         return combine_tensor_patches(
             input, self.original_size, self.window_size, stride=self.window_size, unpadding=self.unpadding
         )
 
 
 def combine_tensor_patches(
-    patches: torch.Tensor,
+    patches: Tensor,
     original_size: Union[int, Tuple[int, int]],
     window_size: Union[int, Tuple[int, int]],
     stride: Union[int, Tuple[int, int]],
     unpadding: Union[int, PadType] = 0,
-) -> torch.Tensor:
+) -> Tensor:
     r"""Restore input from patches.
 
     See :class:`~kornia.contrib.CombineTensorPatches` for details.
 
     Args:
         patches: patched tensor with shape :math:`(B, N, C, H_{out}, W_{out})`.
         original_size: the size of the original tensor and the output patch size.
@@ -271,40 +271,38 @@
 
         window_size = (
             (original_size[0] + (unpadding[2] + unpadding[3])) // window_size[0],
             (original_size[1] + (unpadding[0] + unpadding[1])) // window_size[1],
         )
 
     patches_tensor = patches.view(-1, window_size[0], window_size[1], *patches.shape[-3:])
-    restored_tensor = torch.cat(torch.chunk(patches_tensor, window_size[0], dim=1), -2).squeeze(1)
-    restored_tensor = torch.cat(torch.chunk(restored_tensor, window_size[1], dim=1), -1).squeeze(1)
+    restored_tensor = concatenate(torch.chunk(patches_tensor, window_size[0], 1), -2).squeeze(1)
+    restored_tensor = concatenate(torch.chunk(restored_tensor, window_size[1], 1), -1).squeeze(1)
 
     if unpadding:
         unpadding = cast(Tuple[int, int, int, int], unpadding)
-        restored_tensor = F.pad(restored_tensor, [-i for i in unpadding])
+        restored_tensor = pad(restored_tensor, [-i for i in unpadding])
     return restored_tensor
 
 
-def _extract_tensor_patchesnd(
-    input: torch.Tensor, window_sizes: Tuple[int, ...], strides: Tuple[int, ...]
-) -> torch.Tensor:
+def _extract_tensor_patchesnd(input: Tensor, window_sizes: Tuple[int, ...], strides: Tuple[int, ...]) -> Tensor:
     batch_size, num_channels = input.size()[:2]
     dims = range(2, input.dim())
     for dim, patch_size, stride in zip(dims, window_sizes, strides):
         input = input.unfold(dim, patch_size, stride)
     input = input.permute(0, *dims, 1, *(dim + len(dims) for dim in dims)).contiguous()
     return input.view(batch_size, -1, num_channels, *window_sizes)
 
 
 def extract_tensor_patches(
-    input: torch.Tensor,
+    input: Tensor,
     window_size: Union[int, Tuple[int, int]],
     stride: Union[int, Tuple[int, int]] = 1,
     padding: Union[int, PadType] = 0,
-) -> torch.Tensor:
+) -> Tensor:
     r"""Function that extract patches from tensors and stack them.
 
     See :class:`~kornia.contrib.ExtractTensorPatches` for details.
 
     Args:
         input: tensor image where to extract the patches with shape :math:`(B, C, H, W)`.
         window_size: the size of the sliding window and the output patch size.
@@ -322,15 +320,15 @@
                   [3., 4., 5.],
                   [6., 7., 8.]]]])
         >>> patches[:, -1]
         tensor([[[[3., 4., 5.],
                   [6., 7., 8.]]]])
     """
     if not torch.is_tensor(input):
-        raise TypeError(f"Input input type is not a torch.Tensor. Got {type(input)}")
+        raise TypeError(f"Input input type is not a Tensor. Got {type(input)}")
 
     if len(input.shape) != 4:
         raise ValueError(f"Invalid input shape, we expect BxCxHxW. Got: {input.shape}")
 
     if padding:
         padding = cast(PadType, _pair(padding))
 
@@ -340,10 +338,10 @@
         if len(padding) == 2:
             pad_vert = _pair(padding[0])
             pad_horz = _pair(padding[1])
         else:
             pad_vert = padding[:2]
             pad_horz = padding[2:]
         padding = cast(Tuple[int, int, int, int], pad_horz + pad_vert)
-        input = F.pad(input, padding)
+        input = pad(input, padding)
 
     return _extract_tensor_patchesnd(input, _pair(window_size), _pair(stride))
```

### Comparing `kornia-0.6.8/kornia/contrib/face_detection.py` & `kornia-0.6.9/kornia/contrib/face_detection.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,17 +1,18 @@
 # based on: https://github.com/ShiqiYu/libfacedetection.train/blob/74f3aa77c63234dd954d21286e9a60703b8d0868/tasks/task1/yufacedetectnet.py  # noqa
 import math
 from enum import Enum
-from typing import Callable, Dict, List, Optional, Tuple
+from typing import Dict, List, Optional, Tuple
 
 import torch
 import torch.nn as nn
 import torch.nn.functional as F
 
 from kornia.geometry.bbox import nms as nms_kornia
+from kornia.utils.helpers import map_location_to_cpu
 
 __all__ = ["FaceDetector", "FaceDetectorResult", "FaceKeypoint"]
 
 
 url: str = "https://github.com/kornia/data/raw/main/yunet_final.pth"
 
 
@@ -132,15 +133,15 @@
     Args:
         top_k: the maximum number of detections to return before the nms.
         confidence_threshold: the threshold used to discard detections.
         nms_threshold: the threshold used by the nms for iou.
         keep_top_k: the maximum number of detections to return after the nms.
 
     Return:
-        A tensor of shape :math:`(N,15)` to be used with :py:class:`kornia.contrib.FaceDetectorResult`.
+        A list of B tensors with shape :math:`(N,15)` to be used with :py:class:`kornia.contrib.FaceDetectorResult`.
 
     Example:
         >>> img = torch.rand(1, 3, 320, 320)
         >>> detect = FaceDetector()
         >>> res = detect(img)
     """
 
@@ -155,62 +156,73 @@
         self.config = {
             'name': 'YuFaceDetectNet',
             'min_sizes': [[10, 16, 24], [32, 48], [64, 96], [128, 192, 256]],
             'steps': [8, 16, 32, 64],
             'variance': [0.1, 0.2],
             'clip': False,
         }
-        self.min_sizes: List[List[int]] = [[10, 16, 24], [32, 48], [64, 96], [128, 192, 256]]
-        self.steps: List[int] = [8, 16, 32, 64]
-        self.variance: List[float] = [0.1, 0.2]
-        self.clip: bool = False
+        self.min_sizes = [[10, 16, 24], [32, 48], [64, 96], [128, 192, 256]]
+        self.steps = [8, 16, 32, 64]
+        self.variance = [0.1, 0.2]
+        self.clip = False
         self.model = YuFaceDetectNet('test', pretrained=True)
-        self.nms: Callable = nms_kornia
+        self.nms = nms_kornia
 
     def preprocess(self, image: torch.Tensor) -> torch.Tensor:
         return image
 
-    def postprocess(self, data: Dict[str, torch.Tensor], height: int, width: int) -> torch.Tensor:
+    def postprocess(self, data: Dict[str, torch.Tensor], height: int, width: int) -> List[torch.Tensor]:
         loc, conf, iou = data['loc'], data['conf'], data['iou']
 
         scale = torch.tensor(
             [width, height, width, height, width, height, width, height, width, height, width, height, width, height],
             device=loc.device,
             dtype=loc.dtype,
         )  # 14
 
         priors = _PriorBox(self.min_sizes, self.steps, self.clip, image_size=(height, width))
         priors = priors.to(loc.device, loc.dtype)
 
-        boxes = _decode(loc, priors(), self.variance)  # Nx14
-        boxes = boxes * scale
+        batched_dets: List[torch.Tensor] = []
+        for batch_elem in range(loc.shape[0]):
+            boxes = _decode(loc[batch_elem], priors(), self.variance)  # Nx14
+            boxes = boxes * scale
+
+            # clamp here for the compatibility for ONNX
+            cls_scores, iou_scores = conf[batch_elem, :, 1], iou[batch_elem, :, 0]
+            scores = (cls_scores * iou_scores.clamp(0.0, 1.0)).sqrt()
+
+            # ignore low scores
+            inds = scores > self.confidence_threshold
+            boxes, scores = boxes[inds], scores[inds]
+
+            # keep top-K before NMS
+            order = scores.sort(descending=True)[1][: self.top_k]
+            boxes, scores = boxes[order], scores[order]
+
+            # performd NMS
+            # NOTE: nms need to be revise since does not export well to onnx
+            dets = torch.cat((boxes, scores[:, None]), dim=-1)  # Nx15
+            keep = self.nms(boxes[:, :4], scores, self.nms_threshold)
+            if len(keep) > 0:
+                dets = dets[keep, :]
+
+            # keep top-K faster NMS
+            batched_dets.append(dets[: self.keep_top_k])
+        return batched_dets
 
-        # clamp here for the compatibility for ONNX
-        cls_scores, iou_scores = conf[:, 1], iou[:, 0]
-        scores = (cls_scores * iou_scores.clamp(0.0, 1.0)).sqrt()
-
-        # ignore low scores
-        inds = scores > self.confidence_threshold
-        boxes, scores = boxes[inds], scores[inds]
-
-        # keep top-K before NMS
-        order = scores.sort(descending=True)[1][: self.top_k]
-        boxes, scores = boxes[order], scores[order]
-
-        # performd NMS
-        # NOTE: nms need to be revise since does not export well to onnx
-        dets = torch.cat((boxes, scores[:, None]), dim=-1)  # Nx15
-        keep = self.nms(boxes[:, :4], scores, self.nms_threshold)
-        if len(keep) > 0:
-            dets = dets[keep, :]
+    def forward(self, image: torch.Tensor) -> List[torch.Tensor]:
+        r"""Detect faces in a given batch of images.
 
-        # keep top-K faster NMS
-        return dets[: self.keep_top_k]
+        Args:
+            image: batch of images :math:`(B,3,H,W)`
 
-    def forward(self, image: torch.Tensor) -> torch.Tensor:
+        Return:
+            List[torch.Tensor]: list with the boxes found on each image. :math:`Bx(N,15)`.
+        """
         img = self.preprocess(image)
         out = self.model(img)
         return self.postprocess(out, img.shape[-2], img.shape[-1])
 
 
 # utils for the network
 
@@ -272,16 +284,15 @@
                         m.weight.data.normal_(0, 0.01)
                 elif isinstance(m, nn.BatchNorm2d):
                     m.weight.data.fill_(1)
                     m.bias.data.zero_()
 
         # use torch.hub to load pretrained model
         if pretrained:
-            storage_fcn: Callable = lambda storage, loc: storage
-            pretrained_dict = torch.hub.load_state_dict_from_url(url, map_location=storage_fcn)
+            pretrained_dict = torch.hub.load_state_dict_from_url(url, map_location=map_location_to_cpu)
             self.load_state_dict(pretrained_dict, strict=True)
         self.eval()
 
     def forward(self, x: torch.Tensor) -> Dict[str, torch.Tensor]:
         detection_sources, head_list = [], []
 
         x = self.model0(x)
@@ -310,17 +321,15 @@
 
         head_data = torch.cat([o.view(o.size(0), -1) for o in head_list], 1)
         head_data = head_data.view(head_data.size(0), -1, 17)
 
         loc_data, conf_data, iou_data = head_data.split((14, 2, 1), dim=-1)
 
         if self.phase == "test":
-            loc_data = loc_data.view(-1, 14)
-            conf_data = torch.softmax(conf_data.view(-1, self.num_classes), dim=-1)
-            iou_data = iou_data.view(-1, 1)
+            conf_data = torch.softmax(conf_data, dim=-1)
         else:
             loc_data = loc_data.view(loc_data.size(0), -1, 14)
             conf_data = conf_data.view(conf_data.size(0), -1, self.num_classes)
             iou_data = iou_data.view(iou_data.size(0), -1, 1)
 
         return {"loc": loc_data, "conf": conf_data, "iou": iou_data}
```

### Comparing `kornia-0.6.8/kornia/contrib/histogram_matching.py` & `kornia-0.6.9/kornia/contrib/histogram_matching.py`

 * *Files identical despite different names*

### Comparing `kornia-0.6.8/kornia/contrib/image_stitching.py` & `kornia-0.6.9/kornia/contrib/image_stitching.py`

 * *Files 15% similar despite different names*

```diff
@@ -1,20 +1,20 @@
 from typing import Dict, List, Optional, Tuple
 
 import torch
-import torch.nn as nn
 
 from kornia.color import rgb_to_grayscale
+from kornia.core import Module, Tensor, concatenate, where, zeros_like
 from kornia.feature import LocalFeatureMatcher, LoFTR
 from kornia.geometry.homography import find_homography_dlt_iterated
 from kornia.geometry.ransac import RANSAC
 from kornia.geometry.transform import warp_perspective
 
 
-class ImageStitcher(nn.Module):
+class ImageStitcher(Module):
     """Stitch two images with overlapping fields of view.
 
     Args:
         matcher: image feature matching module.
         estimator: method to compute homography, either "vanilla" or "ransac".
             "ransac" is slower with a better accuracy.
         blending_method: method to blend two images together.
@@ -29,109 +29,109 @@
         # Compute the stitched result with less GPU memory cost.
         with torch.inference_mode():
             out = IS(img_left, img_right)
         # Show the result
         plt.imshow(K.tensor_to_image(out))
     """
 
-    def __init__(self, matcher: nn.Module, estimator: str = 'ransac', blending_method: str = "naive") -> None:
+    def __init__(self, matcher: Module, estimator: str = 'ransac', blending_method: str = "naive") -> None:
         super().__init__()
         self.matcher = matcher
         self.estimator = estimator
         self.blending_method = blending_method
         if estimator not in ['ransac', 'vanilla']:
             raise NotImplementedError(f"Unsupported estimator {estimator}. Use ransac or vanilla instead.")
         if estimator == "ransac":
             self.ransac = RANSAC('homography')
 
-    def _estimate_homography(self, keypoints1: torch.Tensor, keypoints2: torch.Tensor) -> torch.Tensor:
+    def _estimate_homography(self, keypoints1: Tensor, keypoints2: Tensor) -> Tensor:
         """Estimate homography by the matched keypoints.
 
         Args:
             keypoints1: matched keypoint set from an image, shaped as :math:`(N, 2)`.
             keypoints2: matched keypoint set from the other image, shaped as :math:`(N, 2)`.
         """
-        homo: torch.Tensor
+        homo: Tensor
         if self.estimator == "vanilla":
             homo = find_homography_dlt_iterated(
                 keypoints2[None], keypoints1[None], torch.ones_like(keypoints1[None, :, 0])
             )
         elif self.estimator == "ransac":
             homo, _ = self.ransac(keypoints2, keypoints1)
             homo = homo[None]
         else:
             raise NotImplementedError(f"Unsupported estimator {self.estimator}. Use ransac or vanilla instead.")
         return homo
 
-    def estimate_transform(self, **kwargs) -> torch.Tensor:
+    def estimate_transform(self, **kwargs) -> Tensor:
         """Compute the corresponding homography."""
-        homos: List[torch.Tensor] = []
+        homos: List[Tensor] = []
         kp1, kp2, idx = kwargs['keypoints0'], kwargs['keypoints1'], kwargs['batch_indexes']
         for i in range(len(idx.unique())):
             homos.append(self._estimate_homography(kp1[idx == i], kp2[idx == i]))
         if len(homos) == 0:
             raise RuntimeError("Compute homography failed. No matched keypoints found.")
-        return torch.cat(homos)
+        return concatenate(homos)
 
-    def blend_image(self, src_img: torch.Tensor, dst_img: torch.Tensor, mask: torch.Tensor) -> torch.Tensor:
+    def blend_image(self, src_img: Tensor, dst_img: Tensor, mask: Tensor) -> Tensor:
         """Blend two images together."""
-        out: torch.Tensor
+        out: Tensor
         if self.blending_method == "naive":
-            out = torch.where(mask == 1, src_img, dst_img)
+            out = where(mask == 1, src_img, dst_img)
         else:
             raise NotImplementedError(f"Unsupported blending method {self.blending_method}. Use naive.")
         return out
 
-    def preprocess(self, image_1: torch.Tensor, image_2: torch.Tensor) -> Dict[str, torch.Tensor]:
+    def preprocess(self, image_1: Tensor, image_2: Tensor) -> Dict[str, Tensor]:
         """Preprocess input to the required format."""
         # TODO: probably perform histogram matching here.
         if isinstance(self.matcher, LoFTR) or isinstance(self.matcher, LocalFeatureMatcher):
-            input_dict: Dict[str, torch.Tensor] = {  # LofTR works on grayscale images only
+            input_dict: Dict[str, Tensor] = {  # LofTR works on grayscale images only
                 "image0": rgb_to_grayscale(image_1),
                 "image1": rgb_to_grayscale(image_2),
             }
         else:
             raise NotImplementedError(f"The preprocessor for {self.matcher} has not been implemented.")
         return input_dict
 
-    def postprocess(self, image: torch.Tensor, mask: torch.Tensor) -> torch.Tensor:
+    def postprocess(self, image: Tensor, mask: Tensor) -> Tensor:
         # NOTE: assumes no batch mode. This method keeps all valid regions after stitching.
-        mask_: torch.Tensor = mask.sum((0, 1))
-        index: int = int(mask_.bool().any(0).long().argmin().item())
+        mask_ = mask.sum((0, 1))
+        index = int(mask_.bool().any(0).long().argmin().item())
         if index == 0:  # If no redundant space
             return image
         return image[..., :index]
 
-    def on_matcher(self, data) -> dict:
+    def on_matcher(self, data) -> Dict[str, Tensor]:
         return self.matcher(data)
 
     def stitch_pair(
         self,
-        images_left: torch.Tensor,
-        images_right: torch.Tensor,
-        mask_left: Optional[torch.Tensor] = None,
-        mask_right: Optional[torch.Tensor] = None,
-    ) -> Tuple[torch.Tensor, torch.Tensor]:
+        images_left: Tensor,
+        images_right: Tensor,
+        mask_left: Optional[Tensor] = None,
+        mask_right: Optional[Tensor] = None,
+    ) -> Tuple[Tensor, Tensor]:
         # Compute the transformed images
-        input_dict: Dict[str, torch.Tensor] = self.preprocess(images_left, images_right)
-        out_shape: Tuple[int, int] = (images_left.shape[-2], images_left.shape[-1] + images_right.shape[-1])
-        correspondences: dict = self.on_matcher(input_dict)
-        homo: torch.Tensor = self.estimate_transform(**correspondences)
+        input_dict = self.preprocess(images_left, images_right)
+        out_shape = (images_left.shape[-2], images_left.shape[-1] + images_right.shape[-1])
+        correspondences = self.on_matcher(input_dict)
+        homo: Tensor = self.estimate_transform(**correspondences)
         src_img = warp_perspective(images_right, homo, out_shape)
-        dst_img = torch.cat([images_left, torch.zeros_like(images_right)], dim=-1)
+        dst_img = concatenate([images_left, zeros_like(images_right)], -1)
 
         # Compute the transformed masks
         if mask_left is None:
             mask_left = torch.ones_like(images_left)
         if mask_right is None:
             mask_right = torch.ones_like(images_right)
         # 'nearest' to ensure no floating points in the mask
         src_mask = warp_perspective(mask_right, homo, out_shape, mode='nearest')
-        dst_mask = torch.cat([mask_left, torch.zeros_like(mask_right)], dim=-1)
+        dst_mask = concatenate([mask_left, zeros_like(mask_right)], -1)
         return self.blend_image(src_img, dst_img, src_mask), (dst_mask + src_mask).bool().to(src_mask.dtype)
 
-    def forward(self, *imgs: torch.Tensor) -> torch.Tensor:
+    def forward(self, *imgs: Tensor) -> Tensor:
         img_out = imgs[0]
         mask_left = torch.ones_like(img_out)
         for i in range(len(imgs) - 1):
             img_out, mask_left = self.stitch_pair(img_out, imgs[i + 1], mask_left)
         return self.postprocess(img_out, mask_left)
```

### Comparing `kornia-0.6.8/kornia/contrib/lambda_module.py` & `kornia-0.6.9/kornia/contrib/lambda_module.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,14 +1,13 @@
 from typing import Callable
 
-import torch
-import torch.nn as nn
+from kornia.core import Module, Tensor
 
 
-class Lambda(nn.Module):
+class Lambda(Module):
     """Applies user-defined lambda as a transform.
 
     Args:
         func: Callable function.
 
     Returns:
         The output of the user-defined lambda.
@@ -17,16 +16,16 @@
         >>> import kornia
         >>> x = torch.rand(1, 3, 5, 5)
         >>> f = Lambda(lambda x: kornia.color.rgb_to_grayscale(x))
         >>> f(x).shape
         torch.Size([1, 1, 5, 5])
     """
 
-    def __init__(self, func: Callable) -> None:
+    def __init__(self, func: Callable[..., Tensor]) -> None:
         super().__init__()
         if not callable(func):
             raise TypeError(f"Argument lambd should be callable, got {repr(type(func).__name__)}")
 
         self.func = func
 
-    def forward(self, img: torch.Tensor, *args, **kwargs) -> torch.Tensor:
+    def forward(self, img: Tensor, *args, **kwargs) -> Tensor:
         return self.func(img, *args, **kwargs)
```

### Comparing `kornia-0.6.8/kornia/contrib/vit.py` & `kornia-0.6.9/kornia/contrib/vit.py`

 * *Files identical despite different names*

### Comparing `kornia-0.6.8/kornia/contrib/vit_mobile.py` & `kornia-0.6.9/kornia/contrib/vit_mobile.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,31 +1,19 @@
 from typing import Tuple
 
 import torch
 from torch import nn
 
 
-class SiLU(nn.Module):
-    """Module SiLU (Sigmoid Linear Units)
-
-    This implementation is to support pytorch < 1.8, and will be deprecated after 1.8.
-
-    Paper: https://arxiv.org/abs/1702.03118
-    """
-
-    def forward(self, x: torch.Tensor) -> torch.Tensor:
-        return x * torch.sigmoid(x)
-
-
 def conv_1x1_bn(inp: int, oup: int) -> nn.Module:
-    return nn.Sequential(nn.Conv2d(inp, oup, 1, 1, 0, bias=False), nn.BatchNorm2d(oup), SiLU())
+    return nn.Sequential(nn.Conv2d(inp, oup, 1, 1, 0, bias=False), nn.BatchNorm2d(oup), nn.SiLU())
 
 
 def conv_nxn_bn(inp: int, oup: int, kernal_size: int = 3, stride: int = 1) -> nn.Module:
-    return nn.Sequential(nn.Conv2d(inp, oup, kernal_size, stride, 1, bias=False), nn.BatchNorm2d(oup), SiLU())
+    return nn.Sequential(nn.Conv2d(inp, oup, kernal_size, stride, 1, bias=False), nn.BatchNorm2d(oup), nn.SiLU())
 
 
 class PreNorm(nn.Module):
     def __init__(self, dim: int, fn: nn.Module) -> None:
         super().__init__()
         self.norm = nn.LayerNorm(dim)
         self.fn = fn
@@ -34,15 +22,15 @@
         return self.fn(self.norm(x), **kwargs)
 
 
 class FeedForward(nn.Module):
     def __init__(self, dim: int, hidden_dim: int, dropout: float = 0.0) -> None:
         super().__init__()
         self.net = nn.Sequential(
-            nn.Linear(dim, hidden_dim), SiLU(), nn.Dropout(dropout), nn.Linear(hidden_dim, dim), nn.Dropout(dropout)
+            nn.Linear(dim, hidden_dim), nn.SiLU(), nn.Dropout(dropout), nn.Linear(hidden_dim, dim), nn.Dropout(dropout)
         )
 
     def forward(self, x: torch.Tensor) -> torch.Tensor:
         return self.net(x)
 
 
 class Attention(nn.Module):
@@ -129,29 +117,29 @@
         self.use_res_connect = self.stride == 1 and inp == oup
 
         if expansion == 1:
             self.conv = nn.Sequential(
                 # depthwise
                 nn.Conv2d(hidden_dim, hidden_dim, 3, stride, 1, groups=hidden_dim, bias=False),
                 nn.BatchNorm2d(hidden_dim),
-                SiLU(),
+                nn.SiLU(),
                 # pointwise
                 nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),
                 nn.BatchNorm2d(oup),
             )
         else:
             self.conv = nn.Sequential(
                 # pointwise
                 nn.Conv2d(inp, hidden_dim, 1, 1, 0, bias=False),
                 nn.BatchNorm2d(hidden_dim),
-                SiLU(),
+                nn.SiLU(),
                 # depthwise
                 nn.Conv2d(hidden_dim, hidden_dim, 3, stride, 1, groups=hidden_dim, bias=False),
                 nn.BatchNorm2d(hidden_dim),
-                SiLU(),
+                nn.SiLU(),
                 # pointwise
                 nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),
                 nn.BatchNorm2d(oup),
             )
 
     def forward(self, x: torch.Tensor) -> torch.Tensor:
         if self.use_res_connect:
```

### Comparing `kornia-0.6.8/kornia/enhance/__init__.py` & `kornia-0.6.9/kornia/enhance/__init__.py`

 * *Files identical despite different names*

### Comparing `kornia-0.6.8/kornia/enhance/adjust.py` & `kornia-0.6.9/kornia/enhance/adjust.py`

 * *Files identical despite different names*

### Comparing `kornia-0.6.8/kornia/enhance/core.py` & `kornia-0.6.9/kornia/enhance/core.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,12 +1,16 @@
-from kornia.core import Module, Tensor
+from typing import Union
+
+from kornia.core import Module, Tensor, tensor
 from kornia.testing import KORNIA_CHECK, KORNIA_CHECK_IS_TENSOR
 
 
-def add_weighted(src1: Tensor, alpha, src2: Tensor, beta, gamma) -> Tensor:
+def add_weighted(
+    src1: Tensor, alpha: Union[float, Tensor], src2: Tensor, beta: Union[float, Tensor], gamma: Union[float, Tensor]
+) -> Tensor:
     r"""Calculate the weighted sum of two Tensors.
 
     .. image:: _static/img/add_weighted.png
 
     The function calculates the weighted sum of two Tensors as follows:
 
     .. math::
@@ -31,20 +35,29 @@
 
     Notes:
         Tensor alpha/beta/gamma have to be with shape broadcastable to src1 and src2 shapes.
     """
     KORNIA_CHECK_IS_TENSOR(src1)
     KORNIA_CHECK_IS_TENSOR(src2)
     KORNIA_CHECK(src1.shape == src2.shape, f"src1 and src2 have different shapes. Got {src1.shape} and {src2.shape}")
+
     if isinstance(alpha, Tensor):
         KORNIA_CHECK(src1.shape == alpha.shape, "alpha has a different shape than src.")
+    else:
+        alpha = tensor(alpha, dtype=src1.dtype, device=src1.device)
+
     if isinstance(beta, Tensor):
         KORNIA_CHECK(src1.shape == beta.shape, "beta has a different shape than src.")
+    else:
+        beta = tensor(beta, dtype=src1.dtype, device=src1.device)
+
     if isinstance(gamma, Tensor):
         KORNIA_CHECK(src1.shape == gamma.shape, "gamma has a different shape than src.")
+    else:
+        gamma = tensor(gamma, dtype=src1.dtype, device=src1.device)
 
     return src1 * alpha + src2 * beta + gamma
 
 
 class AddWeighted(Module):
     r"""Calculate the weighted sum of two Tensors.
 
@@ -76,9 +89,9 @@
 
     def __init__(self, alpha, beta, gamma) -> None:
         super().__init__()
         self.alpha = alpha
         self.beta = beta
         self.gamma = gamma
 
-    def forward(self, src1: Tensor, src2: Tensor) -> Tensor:  # type: ignore
+    def forward(self, src1: Tensor, src2: Tensor) -> Tensor:
         return add_weighted(src1, self.alpha, src2, self.beta, self.gamma)
```

### Comparing `kornia-0.6.8/kornia/enhance/equalization.py` & `kornia-0.6.9/kornia/enhance/equalization.py`

 * *Files identical despite different names*

### Comparing `kornia-0.6.8/kornia/enhance/histogram.py` & `kornia-0.6.9/kornia/enhance/histogram.py`

 * *Files identical despite different names*

### Comparing `kornia-0.6.8/kornia/enhance/normalize.py` & `kornia-0.6.9/kornia/enhance/normalize.py`

 * *Files 0% similar despite different names*

```diff
@@ -153,15 +153,14 @@
         >>> std = 255. * torch.ones(1, 4)
         >>> out = Denormalize(mean, std)(x)
         >>> out.shape
         torch.Size([1, 4, 3, 3, 3])
     """
 
     def __init__(self, mean: Union[torch.Tensor, float], std: Union[torch.Tensor, float]) -> None:
-
         super().__init__()
 
         self.mean = mean
         self.std = std
 
     def forward(self, input: torch.Tensor) -> torch.Tensor:
         return denormalize(input, self.mean, self.std)
```

### Comparing `kornia-0.6.8/kornia/enhance/shift_rgb.py` & `kornia-0.6.9/kornia/enhance/shift_rgb.py`

 * *Files identical despite different names*

### Comparing `kornia-0.6.8/kornia/enhance/zca.py` & `kornia-0.6.9/kornia/enhance/zca.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,16 +1,17 @@
 from typing import List, Optional, Tuple
 
 import torch
-import torch.nn as nn
+
+from kornia.core import Module, Tensor, concatenate, tensor
 
 __all__ = ["zca_mean", "zca_whiten", "linear_transform", "ZCAWhitening"]
 
 
-class ZCAWhitening(nn.Module):
+class ZCAWhitening(Module):
     r"""Compute the ZCA whitening matrix transform and the mean vector and applies the transform to the data.
 
     The data tensor is flattened, and the mean :math:`\mathbf{\mu}`
     and covariance matrix :math:`\mathbf{\Sigma}` are computed from
     the flattened data :math:`\mathbf{X} \in \mathbb{R}^{N \times D}`, where
     :math:`N` is the sample size and :math:`D` is flattened dimensionality
     (e.g. for a tensor with size 5x3x2x2 :math:`N = 5` and :math:`D = 12`). The ZCA whitening
@@ -76,19 +77,19 @@
         self.eps = eps
         self.unbiased = unbiased
         self.detach_transforms = detach_transforms
         self.compute_inv = compute_inv
 
         self.fitted = False
 
-        self.mean_vector: torch.Tensor
-        self.transform_matrix: torch.Tensor
-        self.transform_inv: Optional[torch.Tensor]
+        self.mean_vector: Tensor
+        self.transform_matrix: Tensor
+        self.transform_inv: Optional[Tensor]
 
-    def fit(self, x: torch.Tensor):
+    def fit(self, x: Tensor):
         r"""Fit ZCA whitening matrices to the data.
 
         Args:
 
             x: Input data.
 
         returns:
@@ -109,15 +110,15 @@
             self.transform_matrix = self.transform_matrix.detach()
             self.transform_inv = self.transform_inv.detach()
 
         self.fitted = True
 
         return self
 
-    def forward(self, x: torch.Tensor, include_fit: bool = False) -> torch.Tensor:
+    def forward(self, x: Tensor, include_fit: bool = False) -> Tensor:
         r"""Apply the whitening transform to the data.
 
         Args:
             x: Input data.
             include_fit: Indicates whether to fit the data as part of the forward pass.
 
         Returns:
@@ -130,15 +131,15 @@
         if not self.fitted:
             raise RuntimeError("Needs to be fitted first before running. Please call fit or set include_fit to True.")
 
         x_whiten = linear_transform(x, self.transform_matrix, self.mean_vector, self.dim)
 
         return x_whiten
 
-    def inverse_transform(self, x: torch.Tensor) -> torch.Tensor:
+    def inverse_transform(self, x: Tensor) -> Tensor:
         r"""Apply the inverse transform to the whitened data.
 
         Args:
             x: Whitened data.
 
         Returns:
             Original data.
@@ -146,24 +147,27 @@
 
         if not self.fitted:
             raise RuntimeError("Needs to be fitted first before running. Please call fit or set include_fit to True.")
 
         if not self.compute_inv:
             raise RuntimeError("Did not compute inverse ZCA. Please set compute_inv to True")
 
-        mean_inv: torch.Tensor = -self.mean_vector.mm(self.transform_matrix)  # type: ignore
+        if self.transform_inv is None:
+            raise TypeError('The transform inverse should be a Tensor. Gotcha None.')
+
+        mean_inv: Tensor = -self.mean_vector.mm(self.transform_matrix)
 
-        y = linear_transform(x, self.transform_inv, mean_inv)  # type: ignore
+        y = linear_transform(x, self.transform_inv, mean_inv)
 
         return y
 
 
 def zca_mean(
-    inp: torch.Tensor, dim: int = 0, unbiased: bool = True, eps: float = 1e-6, return_inverse: bool = False
-) -> Tuple[torch.Tensor, torch.Tensor, Optional[torch.Tensor]]:
+    inp: Tensor, dim: int = 0, unbiased: bool = True, eps: float = 1e-6, return_inverse: bool = False
+) -> Tuple[Tensor, Tensor, Optional[Tensor]]:
     r"""Compute the ZCA whitening matrix and mean vector.
 
     The output can be used with :py:meth:`~kornia.color.linear_transform`.
     See :class:`~kornia.color.ZCAWhitening` for details.
 
     Args:
         inp: input data tensor.
@@ -190,16 +194,16 @@
         >>> x = torch.tensor([[0,1],[1,0],[-1,0],[0,-1]], dtype = torch.float32)
         >>> transform_matrix, mean_vector,_ = zca_mean(x) # Returns transformation matrix and data mean
         >>> x = torch.rand(3,20,2,2)
         >>> transform_matrix, mean_vector, inv_transform = zca_mean(x, dim = 1, return_inverse = True)
         >>> # transform_matrix.size() equals (12,12) and the mean vector.size equal (1,12)
     """
 
-    if not isinstance(inp, torch.Tensor):
-        raise TypeError(f"Input type is not a torch.Tensor. Got {type(inp)}")
+    if not isinstance(inp, Tensor):
+        raise TypeError(f"Input type is not a Tensor. Got {type(inp)}")
 
     if not isinstance(eps, float):
         raise TypeError(f"eps type is not a float. Got{type(eps)}")
 
     if not isinstance(unbiased, bool):
         raise TypeError(f"unbiased type is not bool. Got{type(unbiased)}")
 
@@ -217,51 +221,51 @@
                 -len(inp_size), len(inp_size) - 1, dim
             )
         )
 
     if dim < 0:
         dim = len(inp_size) + dim
 
-    feat_dims = torch.cat([torch.arange(0, dim), torch.arange(dim + 1, len(inp_size))])
+    feat_dims = concatenate([torch.arange(0, dim), torch.arange(dim + 1, len(inp_size))])
 
-    new_order: List[int] = torch.cat([torch.tensor([dim]), feat_dims]).tolist()
+    new_order: List[int] = concatenate([tensor([dim]), feat_dims]).tolist()
 
     inp_permute = inp.permute(new_order)
 
     N = inp_size[dim]
-    feature_sizes = torch.tensor(inp_size[0:dim] + inp_size[dim + 1 : :])
+    feature_sizes = tensor(inp_size[0:dim] + inp_size[dim + 1 : :])
     num_features: int = int(torch.prod(feature_sizes).item())
 
-    mean: torch.Tensor = torch.mean(inp_permute, dim=0, keepdim=True)
+    mean: Tensor = torch.mean(inp_permute, dim=0, keepdim=True)
 
     mean = mean.reshape((1, num_features))
 
-    inp_center_flat: torch.Tensor = inp_permute.reshape((N, num_features)) - mean
+    inp_center_flat: Tensor = inp_permute.reshape((N, num_features)) - mean
 
     cov = inp_center_flat.t().mm(inp_center_flat)
 
     if unbiased:
         cov = cov / float(N - 1)
     else:
         cov = cov / float(N)
 
     U, S, _ = torch.linalg.svd(cov)
 
     S = S.reshape(-1, 1)
-    S_inv_root: torch.Tensor = torch.rsqrt(S + eps)
-    T: torch.Tensor = (U).mm(S_inv_root * U.t())
+    S_inv_root: Tensor = torch.rsqrt(S + eps)
+    T: Tensor = (U).mm(S_inv_root * U.t())
 
-    T_inv: Optional[torch.Tensor] = None
+    T_inv: Optional[Tensor] = None
     if return_inverse:
         T_inv = (U).mm(torch.sqrt(S + eps) * U.t())
 
     return T, mean, T_inv
 
 
-def zca_whiten(inp: torch.Tensor, dim: int = 0, unbiased: bool = True, eps: float = 1e-6) -> torch.Tensor:
+def zca_whiten(inp: Tensor, dim: int = 0, unbiased: bool = True, eps: float = 1e-6) -> Tensor:
     r"""Apply ZCA whitening transform.
 
     See :class:`~kornia.color.ZCAWhitening` for details.
 
     Args:
         inp: input data tensor.
         dim: Specifies the dimension that serves as the samples dimension.
@@ -279,16 +283,16 @@
         >>> x = torch.tensor([[0,1],[1,0],[-1,0]], dtype = torch.float32)
         >>> zca_whiten(x)
         tensor([[ 0.0000,  1.1547],
                 [ 1.0000, -0.5773],
                 [-1.0000, -0.5773]])
     """
 
-    if not isinstance(inp, torch.Tensor):
-        raise TypeError(f"Input type is not a torch.Tensor. Got {type(inp)}")
+    if not isinstance(inp, Tensor):
+        raise TypeError(f"Input type is not a Tensor. Got {type(inp)}")
 
     if not isinstance(eps, float):
         raise TypeError(f"eps type is not a float. Got{type(eps)}")
 
     if not isinstance(unbiased, bool):
         raise TypeError(f"unbiased type is not bool. Got{type(unbiased)}")
 
@@ -298,17 +302,15 @@
     transform, mean, _ = zca_mean(inp, dim, unbiased, eps, False)
 
     inp_whiten = linear_transform(inp, transform, mean, dim)
 
     return inp_whiten
 
 
-def linear_transform(
-    inp: torch.Tensor, transform_matrix: torch.Tensor, mean_vector: torch.Tensor, dim: int = 0
-) -> torch.Tensor:
+def linear_transform(inp: Tensor, transform_matrix: Tensor, mean_vector: Tensor, dim: int = 0) -> Tensor:
     r"""Given a transformation matrix and a mean vector, this function will flatten the input tensor along the given
     dimension and subtract the mean vector from it. Then the dot product with the transformation matrix will be
     computed and then the resulting tensor is reshaped to the original input shape.
 
     .. math::
 
         \mathbf{X}_{T} = (\mathbf{X - \mu})(T)
@@ -353,23 +355,23 @@
                 -len(inp_size), len(inp_size) - 1, dim
             )
         )
 
     if dim < 0:
         dim = len(inp_size) + dim
 
-    feat_dims = torch.cat([torch.arange(0, dim), torch.arange(dim + 1, len(inp_size))])
+    feat_dims = concatenate([torch.arange(0, dim), torch.arange(dim + 1, len(inp_size))])
 
-    perm = torch.cat([torch.tensor([dim]), feat_dims])
+    perm = concatenate([tensor([dim]), feat_dims])
     perm_inv = torch.argsort(perm)
 
     new_order: List[int] = perm.tolist()
     inv_order: List[int] = perm_inv.tolist()
 
-    feature_sizes = torch.tensor(inp_size[0:dim] + inp_size[dim + 1 : :])
+    feature_sizes = tensor(inp_size[0:dim] + inp_size[dim + 1 : :])
     num_features: int = int(torch.prod(feature_sizes).item())
 
     inp_permute = inp.permute(new_order)
     inp_flat = inp_permute.reshape((-1, num_features))
 
     inp_center = inp_flat - mean_vector
     inp_transformed = inp_center.mm(transform_matrix)
```

### Comparing `kornia-0.6.8/kornia/feature/__init__.py` & `kornia-0.6.9/kornia/feature/__init__.py`

 * *Files identical despite different names*

### Comparing `kornia-0.6.8/kornia/feature/adalam/adalam.py` & `kornia-0.6.9/kornia/feature/adalam/adalam.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,48 +1,45 @@
 # Integrated from original AdaLAM repo
 # https://github.com/cavalli1234/AdaLAM
 # Copyright (c) 2020, Luca Cavalli
 
-from typing import Dict, Optional, Tuple, Union
+from typing import Optional, Tuple, Union
 
 import torch
-from torch import Tensor
 
+from kornia.core import Tensor
 from kornia.feature.laf import get_laf_center, get_laf_orientation, get_laf_scale
 from kornia.testing import KORNIA_CHECK_LAF, KORNIA_CHECK_SHAPE
-from kornia.utils.helpers import get_cuda_device_if_available
 
-from .core import _no_match, adalam_core
+from .core import AdalamConfig, _no_match, adalam_core
 from .utils import dist_matrix
 
 
-def get_adalam_default_config():
-    DEFAULT_CONFIG = {
-        'area_ratio': 100,  # Ratio between seed circle area and image area. Higher values produce more seeds with smaller neighborhoods.    # noqa: E501
-        'search_expansion': 4,  # Expansion factor of the seed circle radius for the purpose of collecting neighborhoods. Increases neighborhood radius without changing seed distribution    # noqa: E501
-        'ransac_iters': 128,  # Fixed number of inner GPU-RANSAC iterations
-        'min_inliers': 6,  # Minimum number of inliers required to accept inliers coming from a neighborhood    # noqa: E501
-        'min_confidence': 200,  # Threshold used by the confidence-based GPU-RANSAC
-        'orientation_difference_threshold': 30,  # Maximum difference in orientations for a point to be accepted in a neighborhood. Set to None to disable the use of keypoint orientations.   # noqa: E501
-        'scale_rate_threshold': 1.5,  # Maximum difference (ratio) in scales for a point to be accepted in a neighborhood. Set to None to disable the use of keypoint scales.   # noqa: E501
-        'detected_scale_rate_threshold': 5,  # Prior on maximum possible scale change detectable in image couples. Affinities with higher scale changes are regarded as outliers.   # noqa: E501
-        'refit': True,  # Whether to perform refitting at the end of the RANSACs. Generally improves accuracy at the cost of runtime.   # noqa: E501
-        'force_seed_mnn': True,  # Whether to consider only MNN for the purpose of selecting seeds. Generally improves accuracy at the cost of runtime.    # noqa: E501
-        # You can provide a MNN mask in input to skip MNN computation and still get the improvement.
-        'device': get_cuda_device_if_available(),  # Device to be used for running AdaLAM. Use GPU if available.   # noqa: E501
-    }
-    return DEFAULT_CONFIG
+def get_adalam_default_config() -> AdalamConfig:
+    return AdalamConfig(
+        area_ratio=100,
+        search_expansion=4,
+        ransac_iters=128,
+        min_inliers=6,
+        min_confidence=200,
+        orientation_difference_threshold=30,
+        scale_rate_threshold=1.5,
+        detected_scale_rate_threshold=5,
+        refit=True,
+        force_seed_mnn=True,
+        device=torch.device('cpu'),
+    )
 
 
 def match_adalam(
     desc1: Tensor,
     desc2: Tensor,
     lafs1: Tensor,
     lafs2: Tensor,
-    config: Optional[Dict] = None,
+    config: Optional[AdalamConfig] = None,
     hw1: Optional[Tensor] = None,
     hw2: Optional[Tensor] = None,
     dm: Optional[Tensor] = None,
 ) -> Tuple[Tensor, Tensor]:
     """Function, which performs descriptor matching, followed by AdaLAM filtering (see :cite:`AdaLAM2020` for more
     details)
 
@@ -85,83 +82,77 @@
         get_laf_scale(lafs2).reshape(-1),
         return_dist=True,
     )
     return quality, idxs
 
 
 class AdalamFilter:
-    DEFAULT_CONFIG = get_adalam_default_config()
-
-    def __init__(self, custom_config: Optional[dict] = None):
+    def __init__(self, custom_config: Optional[AdalamConfig] = None):
         """This class acts as a wrapper to the method AdaLAM for outlier filtering.
 
         init args:
             custom_config: dictionary overriding the default configuration. Missing parameters are kept as default.
                            See documentation of DEFAULT_CONFIG for specific explanations on the accepted parameters.
         """
-        self.config = AdalamFilter.DEFAULT_CONFIG.copy()
-
         if custom_config is not None:
-            for key, val in custom_config.items():
-                if key not in self.config.keys():
-                    print(
-                        f"WARNING: custom configuration contains a key which is not recognized ({key}). "
-                        f"Known configurations are {list(self.config.keys())}."
-                    )
-                    continue
-                self.config[key] = val
+            self.config = custom_config
+        else:
+            self.config = get_adalam_default_config()
 
     def filter_matches(
         self,
-        k1: torch.Tensor,
-        k2: torch.Tensor,
-        putative_matches: torch.Tensor,
-        scores: torch.Tensor,
-        mnn: Optional[torch.Tensor] = None,
-        im1shape: Optional[tuple] = None,
-        im2shape: Optional[tuple] = None,
-        o1: Optional[torch.Tensor] = None,
-        o2: Optional[torch.Tensor] = None,
-        s1: Optional[torch.Tensor] = None,
-        s2: Optional[torch.Tensor] = None,
+        k1: Tensor,
+        k2: Tensor,
+        putative_matches: Tensor,
+        scores: Tensor,
+        mnn: Optional[Tensor] = None,
+        im1shape: Optional[Tuple[int, int]] = None,
+        im2shape: Optional[Tuple[int, int]] = None,
+        o1: Optional[Tensor] = None,
+        o2: Optional[Tensor] = None,
+        s1: Optional[Tensor] = None,
+        s2: Optional[Tensor] = None,
         return_dist: bool = False,
     ) -> Union[Tuple[Tensor, Tensor], Tensor]:
         """Call the core functionality of AdaLAM, i.e. just outlier filtering. No sanity check is performed on the
         inputs.
 
         Inputs:
             k1: keypoint locations in the source image, in pixel coordinates.
                 Expected a float32 tensor with shape (num_keypoints_in_source_image, 2).
             k2: keypoint locations in the destination image, in pixel coordinates.
                 Expected a float32 tensor with shape (num_keypoints_in_destination_image, 2).
             putative_matches: Initial set of putative matches to be filtered.
                               The current implementation assumes that these are unfiltered nearest neighbor matches,
-                              so it requires this to be a list of indices a_i such that the source keypoint i is associated to the destination keypoint a_i.
-                              For now to use AdaLAM on different inputs a workaround on the input format is required.
+                              so it requires this to be a list of indices a_i such that the source keypoint i is
+                              associated to the destination keypoint a_i. For now to use AdaLAM on different inputs a
+                              workaround on the input format is required.
                               Expected a long tensor with shape (num_keypoints_in_source_image,).
             scores: Confidence scores on the putative_matches. Usually holds Lowe's ratio scores.
-            mnn: A mask indicating which putative matches are also mutual nearest neighbors. See documentation on 'force_seed_mnn' in the DEFAULT_CONFIG.
-                 If None, it disables the mutual nearest neighbor filtering on seed point selection.
-                 Expected a bool tensor with shape (num_keypoints_in_source_image,)
-            im1shape: Shape of the source image. If None, it is inferred from keypoints max and min, at the cost of wasted runtime. So please provide it.
-                      Expected a tuple with (width, height) or (height, width) of source image
-            im2shape: Shape of the destination image. If None, it is inferred from keypoints max and min, at the cost of wasted runtime. So please provide it.
-                      Expected a tuple with (width, height) or (height, width) of destination image
-            o1/o2: keypoint orientations in degrees. They can be None if 'orientation_difference_threshold' in config is set to None.
-                   See documentation on 'orientation_difference_threshold' in the DEFAULT_CONFIG.
+            mnn: A mask indicating which putative matches are also mutual nearest neighbors. See documentation on
+                 'force_seed_mnn' in the DEFAULT_CONFIG. If None, it disables the mutual nearest neighbor filtering on
+                 seed point selection. Expected a bool tensor with shape (num_keypoints_in_source_image,)
+            im1shape: Shape of the source image. If None, it is inferred from keypoints max and min, at the cost of
+                      wasted runtime. So please provide it. Expected a tuple with (width, height) or (height, width)
+                      of source image
+            im2shape: Shape of the destination image. If None, it is inferred from keypoints max and min, at the cost
+                      of wasted runtime. So please provide it. Expected a tuple with (width, height) or (height, width)
+                      of destination image
+            o1/o2: keypoint orientations in degrees. They can be None if 'orientation_difference_threshold' in config
+                   is set to None. See documentation on 'orientation_difference_threshold' in the DEFAULT_CONFIG.
                    Expected a float32 tensor with shape (num_keypoints_in_source/destination_image,)
             s1/s2: keypoint scales. They can be None if 'scale_rate_threshold' in config is set to None.
                    See documentation on 'scale_rate_threshold' in the DEFAULT_CONFIG.
                    Expected a float32 tensor with shape (num_keypoints_in_source/destination_image,)
             return_dist: if True, inverse confidence value is also outputted.
 
         Returns:
             Filtered putative matches.
             A long tensor with shape (num_filtered_matches, 2) with indices of corresponding keypoints in k1 and k2.
-        """  # noqa: E501
+        """
         with torch.no_grad():
             return adalam_core(
                 k1,
                 k2,
                 fnn12=putative_matches,
                 scores1=scores,
                 mnn=mnn,
```

### Comparing `kornia-0.6.8/kornia/feature/adalam/core.py` & `kornia-0.6.9/kornia/feature/adalam/core.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,19 +1,56 @@
 import math
 from typing import Optional, Tuple, Union
 
 import torch
-from torch import tensor
+from typing_extensions import NotRequired, TypedDict
 
-from kornia.core import Tensor
+from kornia.core import Device, Tensor, concatenate, tensor, where
 
 from .ransac import ransac
 from .utils import dist_matrix, orientation_diff
 
 
+class AdalamConfig(TypedDict):
+    """
+    area_ratio: Ratio between seed circle area and image area. Higher values produce more seeds with smaller
+        neighborhoods
+    search_expansion: Expansion factor of the seed circle radius for the purpose of collecting neighborhoods.
+        Increases neighborhood radius without changing seed distribution
+    ransac_iters: Fixed number of inner GPU-RANSAC iterations
+    min_inliers: Minimum number of inliers required to accept inliers coming from a neighborhood
+    min_confidence: Threshold used by the confidence-based GPU-RANSAC
+    orientation_difference_threshold: Maximum difference in orientations for a point to be accepted in a
+        neighborhood. Set to None to disable the use of keypoint orientations
+    scale_rate_threshold: Maximum difference (ratio) in scales for a point to be accepted in a neighborhood. Set
+        to None to disable the use of keypoint scales
+    detected_scale_rate_threshold: Prior on maximum possible scale change detectable in image couples. Affinities
+        with higher scale changes are regarded as outliers
+    refit: Whether to perform refitting at the end of the RANSACs. Generally improves accuracy at the cost of
+        runtime
+    force_seed_mnn: Whether to consider only MNN for the purpose of selecting seeds. Generally improves accuracy
+        at the cost of runtime
+    device: Device to be used for running AdaLAM. Use GPU if available.
+    mnn: Default None. You can provide a MNN mask in input to skip MNN computation and still get the improvement.
+    """
+
+    area_ratio: NotRequired[int]
+    search_expansion: NotRequired[int]
+    ransac_iters: NotRequired[int]
+    min_inliers: NotRequired[int]
+    min_confidence: NotRequired[int]
+    orientation_difference_threshold: NotRequired[int]
+    scale_rate_threshold: NotRequired[float]
+    detected_scale_rate_threshold: NotRequired[int]
+    refit: NotRequired[bool]
+    force_seed_mnn: NotRequired[bool]
+    device: NotRequired[Device]
+    mnn: NotRequired[Tensor]
+
+
 def _no_match(dm: Tensor):
     """Helper function, which output empty tensors.
 
     Returns:
             - Descriptor distance of matching descriptors, shape of :math:`(0, 1)`.
             - Long tensor indexes of matching descriptors in desc1 and desc2, shape of :math:`(0, 2)`.
     """
@@ -48,15 +85,15 @@
         im1bs = (
             (~torch.any(im1neighmap & im1scorescomp & mnn.unsqueeze(0), dim=1)) & mnn & (scores1 < 0.8**2)
         )  # (n1,)
     else:
         im1bs = (~torch.any(im1neighmap & im1scorescomp, dim=1)) & (scores1 < 0.8**2)
 
     # collect all seeds in both images and the 1NN of the seeds of the other image
-    im1seeds = torch.where(im1bs)[0]  # (n1bs) index format
+    im1seeds = where(im1bs)[0]  # (n1bs) index format
     im2seeds = fnn12[im1bs]  # (n1bs) index format
     return im1seeds, im2seeds
 
 
 def extract_neighborhood_sets(
     o1: Optional[Tensor],
     o2: Optional[Tensor],
@@ -174,15 +211,15 @@
                  This allows to distinguish inputs belonging to the same problem.
         tokp1: Index of the original keypoint in image I_1 for each RANSAC sample.
         tokp2: Index of the original keypoint in image I_2 for each RANSAC sample.
     """
     # first get an indexing representation of the assignments:
     # - ransidx holds the index of the seed for each assignment
     # - tokp1 holds the index of the keypoint in image I_1 for each assignment
-    ransidx, tokp1 = torch.where(fnn_to_seed_local_consistency_map_corr)
+    ransidx, tokp1 = where(fnn_to_seed_local_consistency_map_corr)
     # - and of course tokp2 holds the index of the corresponding keypoint in image I_2
     tokp2 = fnn12[tokp1]
 
     # Now take the locations in the image of each considered keypoint ...
     im1abspattern = k1[tokp1]
     im2abspattern = k2[tokp2]
 
@@ -205,18 +242,18 @@
 
 
 def adalam_core(
     k1: Tensor,
     k2: Tensor,
     fnn12: Tensor,
     scores1: Tensor,
-    config: dict,
+    config: AdalamConfig,
     mnn: Optional[Tensor] = None,
-    im1shape: Optional[Tuple] = None,
-    im2shape: Optional[Tuple] = None,
+    im1shape: Optional[Tuple[int, int]] = None,
+    im2shape: Optional[Tuple[int, int]] = None,
     o1: Optional[Tensor] = None,
     o2: Optional[Tensor] = None,
     s1: Optional[Tensor] = None,
     s2: Optional[Tensor] = None,
     return_dist: bool = False,
 ) -> Union[Tuple[Tensor, Tensor], Tensor]:
     """Call the core functionality of AdaLAM, i.e. just outlier filtering. No sanity check is performed on the
@@ -257,26 +294,31 @@
     RANSAC_ITERS = config['ransac_iters']
     MIN_INLIERS = config['min_inliers']
     MIN_CONF = config['min_confidence']
     ORIENTATION_THR = config['orientation_difference_threshold']
     SCALE_RATE_THR = config['scale_rate_threshold']
     REFIT = config['refit']
 
-    if im1shape is None:
-        k1mins, _ = torch.min(k1, dim=0)
-        k1maxs, _ = torch.max(k1, dim=0)
-        im1shape = k1maxs - k1mins
-    if im2shape is None:
-        k2mins, _ = torch.min(k2, dim=0)
-        k2maxs, _ = torch.max(k2, dim=0)
-        im2shape = k2maxs - k2mins
+    if isinstance(im1shape, tuple):
+        _im1shape = tensor(im1shape, device=k1.device, dtype=k1.dtype)
+    else:
+        k1mins = k1.min(dim=0).values
+        k1maxs = k1.max(dim=0).values
+        _im1shape = k1maxs - k1mins
+
+    if isinstance(im2shape, tuple):
+        _im2shape = tensor(im2shape, device=k2.device, dtype=k2.dtype)
+    else:
+        k2mins = k2.min(dim=0).values
+        k2maxs = k2.max(dim=0).values
+        _im2shape = k2maxs - k2mins
 
     # Compute seed selection radii to be invariant to image rescaling
-    R1 = torch.sqrt(torch.prod(tensor(im1shape[:2])) / AREA_RATIO / math.pi)
-    R2 = torch.sqrt(torch.prod(tensor(im2shape[:2])) / AREA_RATIO / math.pi)
+    R1 = torch.sqrt(torch.prod(_im1shape[:2]) / AREA_RATIO / math.pi)
+    R2 = torch.sqrt(torch.prod(_im2shape[:2]) / AREA_RATIO / math.pi)
 
     # Precompute the inner distances of keypoints in image I_1
     dist1 = dist_matrix(k1, k1)
 
     # Select seeds
     im1seeds, im2seeds = select_seeds(dist1, R1, scores1, fnn12, mnn)
 
@@ -299,15 +341,15 @@
         SEARCH_EXP,
         MIN_INLIERS,
     )
 
     if rdims.shape[0] == 0:
         # No seed point survived. Just output ratio-test matches. This should happen very rarely.
         score_mask = scores1 <= 0.95
-        absolute_im1idx = torch.where(score_mask)[0]
+        absolute_im1idx = where(score_mask)[0]
         if len(absolute_im1idx) > 0:
             absolute_im2idx = fnn12[absolute_im1idx]
             out_scores = scores1[score_mask].reshape(-1, 1)
             idxs = torch.stack([absolute_im1idx, absolute_im2idx], dim=1)
         else:
             idxs, out_scores = _no_match(scores1)
         if return_dist:
@@ -339,13 +381,13 @@
 
     final_matches = torch.stack([absolute_im1idx, absolute_im2idx], dim=1)
     if final_matches.shape[0] > 1:
         # https://stackoverflow.com/a/72005790
         final_matches, idxs, counts = torch.unique(final_matches, dim=0, return_inverse=True, return_counts=True)
         _, ind_sorted = torch.sort(idxs)
         cum_sum = counts.cumsum(0)
-        cum_sum = torch.cat((torch.tensor([0], dtype=cum_sum.dtype, device=cum_sum.device), cum_sum[:-1]))
+        cum_sum = concatenate((torch.tensor([0], dtype=cum_sum.dtype, device=cum_sum.device), cum_sum[:-1]))
         first_indicies = ind_sorted[cum_sum]
         accepted_dist = accepted_dist[first_indicies]
     if return_dist:
         return final_matches, accepted_dist.reshape(-1, 1)
     return final_matches
```

### Comparing `kornia-0.6.8/kornia/feature/adalam/ransac.py` & `kornia-0.6.9/kornia/feature/adalam/ransac.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,9 +1,13 @@
+from typing import Tuple
+
 import torch
 
+from kornia.core import Tensor
+
 from .utils import arange_sequence, batch_2x2_ellipse, batch_2x2_inv, draw_first_k_couples, piecewise_arange
 
 
 def stable_sort_residuals(residuals, ransidx):
     logres = torch.log(residuals + 1e-10)
     minlogres = torch.min(logres)
     maxlogres = torch.max(logres)
@@ -69,30 +73,38 @@
     highest_accepted_sqr_residuals = sorted_res_sqr[inl_iters, idxoffsets + inl_counts - 1]
     expected_extra_inl = (
         balanced_rdims[inl_iters, torch.arange(numransacs, device=dv)].float() * highest_accepted_sqr_residuals
     )
     return inl_ransidx, inl_sampleidx, inl_counts, inl_iters, inl_counts.float() / expected_extra_inl
 
 
-def sample_padded_inliers(xsamples, ysamples, inlier_counts, inl_ransidx, inl_sampleidx, numransacs, dv):
-    maxinliers = torch.max(inlier_counts).item()
+def sample_padded_inliers(
+    xsamples: Tensor,
+    ysamples: Tensor,
+    inlier_counts: Tensor,
+    inl_ransidx: Tensor,
+    inl_sampleidx: Tensor,
+    numransacs: int,
+    dv: torch.device,
+) -> Tuple[Tensor, Tensor]:
+    maxinliers = int(torch.max(inlier_counts).item())
     dtype = xsamples.dtype
     padded_inlier_x = torch.zeros(size=(numransacs, maxinliers, 2), device=dv, dtype=dtype)
     padded_inlier_y = torch.zeros(size=(numransacs, maxinliers, 2), device=dv, dtype=dtype)
 
     padded_inlier_x[inl_ransidx, piecewise_arange(inl_ransidx)] = xsamples[inl_sampleidx]
     padded_inlier_y[inl_ransidx, piecewise_arange(inl_ransidx)] = ysamples[inl_sampleidx]
 
     return padded_inlier_x, padded_inlier_y
 
 
-def ransac(xsamples, ysamples, rdims, config, iters=128, refit=True):
+def ransac(xsamples, ysamples, rdims: Tensor, config, iters=128, refit=True):
     DET_THR = config['detected_scale_rate_threshold']
     MIN_CONFIDENCE = config['min_confidence']
-    dv = config['device']
+    dv: torch.device = config['device']
 
     numransacs = rdims.shape[0]
     ransidx = torch.arange(numransacs, device=dv).repeat_interleave(rdims)
     idxoffsets = torch.cat([torch.tensor([0], device=dv), torch.cumsum(rdims[:-1], dim=0)], dim=0)
 
     rand_samples_rel = draw_first_k_couples(iters, rdims, dv)
     rand_samples_abs = rand_samples_rel + idxoffsets
```

### Comparing `kornia-0.6.8/kornia/feature/adalam/utils.py` & `kornia-0.6.9/kornia/feature/adalam/utils.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,11 +1,13 @@
 import math
 
 import torch
 
+from kornia.core import Tensor
+
 
 def arange_sequence(ranges):
     """
     returns a sequence of the ranges specified by the argument.
     Example:
     [2, 5, 1, 2] -> [0, 1, 0, 1, 2, 3, 4, 0, 0, 1]
     """
@@ -30,25 +32,26 @@
 def orientation_diff(o1, o2):
     diff = o2 - o1
     diff[diff < -180] += 360
     diff[diff >= 180] -= 360
     return diff
 
 
-def piecewise_arange(piecewise_idxer):
+def piecewise_arange(piecewise_idxer: Tensor):
     """
     count repeated indices
     Example:
     [0, 0, 0, 3, 3, 3, 3, 1, 1, 2] -> [0, 1, 2, 0, 1, 2, 3, 0, 1, 0]
     """
     dv = piecewise_idxer.device
     # print(piecewise_idxer)
+    uni: Tensor
     uni, counts = torch.unique_consecutive(piecewise_idxer, return_counts=True)
     # print(counts)
-    maxcnt = torch.max(counts).item()
+    maxcnt = int(torch.max(counts).item())
     numuni = uni.shape[0]
     tmp = torch.zeros(size=(numuni, maxcnt), device=dv).bool()
     ranges = torch.arange(maxcnt, device=dv).unsqueeze(0).expand(numuni, -1)
     tmp[ranges < counts.unsqueeze(-1)] = True
     return ranges[tmp]
 
 
@@ -104,15 +107,15 @@
     eigenvecs = torch.stack([b.unsqueeze(-1) / dens, torch.ones_like(dens)], dim=-2)
     eigenvecs = eigenvecs / torch.norm(eigenvecs, dim=-2, keepdim=True)
 
     # err = eigenvecs @ torch.diag_embed(eigenvals) @ eigenvecs.transpose(-2, -1) - q
     return eigenvals, eigenvecs
 
 
-def draw_first_k_couples(k: int, rdims: int, dv: torch.device):
+def draw_first_k_couples(k: int, rdims: Tensor, dv: torch.device):
     # exhaustive search over the first n samples:
     # n(n+1)/2 = n2/2 + n/2 couples
     # max n for which we can exhaustively sample with k couples:
     # n2/2 + n/2 = k
     # n = sqrt(1/4 + 2k)-1/2 = (sqrt(8k+1)-1)/2
     max_exhaustive_search = int(math.sqrt(2 * k + 0.25) - 0.5)
     residual_search = k - max_exhaustive_search * (max_exhaustive_search + 1) / 2
```

### Comparing `kornia-0.6.8/kornia/feature/affine_shape.py` & `kornia-0.6.9/kornia/feature/affine_shape.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,17 +1,18 @@
 import math
 import warnings
-from typing import Callable, Dict, Optional
+from typing import Dict, Optional
 
 import torch
 import torch.nn as nn
 
 from kornia.filters.kernels import get_gaussian_kernel2d
 from kornia.filters.sobel import SpatialGradient
 from kornia.testing import KORNIA_CHECK_LAF, KORNIA_CHECK_SHAPE
+from kornia.utils.helpers import map_location_to_cpu
 
 from .laf import (
     ellipse_to_laf,
     extract_patches_from_pyramid,
     get_laf_orientation,
     get_laf_scale,
     make_upright,
@@ -185,16 +186,15 @@
             nn.Conv2d(64, 3, kernel_size=8, stride=1, padding=0, bias=True),
             nn.Tanh(),
             nn.AdaptiveAvgPool2d(1),
         )
         self.patch_size = 32
         # use torch.hub to load pretrained model
         if pretrained:
-            storage_fcn: Callable = lambda storage, loc: storage
-            pretrained_dict = torch.hub.load_state_dict_from_url(urls['affnet'], map_location=storage_fcn)
+            pretrained_dict = torch.hub.load_state_dict_from_url(urls['affnet'], map_location=map_location_to_cpu)
             self.load_state_dict(pretrained_dict['state_dict'], strict=False)
         self.preserve_orientation = preserve_orientation
         if preserve_orientation:
             warnings.warn(
                 "`LAFAffNetShapeEstimator` default behaviour is changed "
                 "and now it does preserve original LAF orientation. "
                 "Make sure your code accounts for this.",
```

### Comparing `kornia-0.6.8/kornia/feature/defmo.py` & `kornia-0.6.9/kornia/feature/defmo.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,12 +1,15 @@
 from typing import Callable, Dict, List, Optional, Type
 
 import torch
 import torch.nn as nn
 
+from kornia.core import Module, Tensor, concatenate, stack
+from kornia.utils.helpers import map_location_to_cpu
+
 urls: Dict[str, str] = {}
 urls["defmo_encoder"] = "http://ptak.felk.cvut.cz/personal/rozumden/defmo_saved_models/encoder_best.pt"
 urls["defmo_rendering"] = "http://ptak.felk.cvut.cz/personal/rozumden/defmo_saved_models/rendering_best.pt"
 
 
 # conv1x1, conv3x3, Bottleneck, ResNet are taken from:
 # https://github.com/pytorch/vision/blob/master/torchvision/models/resnet.py
@@ -27,33 +30,33 @@
         padding=dilation,
         groups=groups,
         bias=False,
         dilation=dilation,
     )
 
 
-class Bottleneck(nn.Module):
+class Bottleneck(Module):
     # Bottleneck in torchvision places the stride for downsampling at 3x3 convolution(self.conv2)
     # while original implementation places the stride at the first 1x1 convolution(self.conv1)
     # according to "Deep residual learning for image recognition"https://arxiv.org/abs/1512.03385.
     # This variant is also known as ResNet V1.5 and improves accuracy according to
     # https://ngc.nvidia.com/catalog/model-scripts/nvidia:resnet_50_v1_5_for_pytorch.
 
     expansion: int = 4
 
     def __init__(
         self,
         inplanes: int,
         planes: int,
         stride: int = 1,
-        downsample: Optional[nn.Module] = None,
+        downsample: Optional[Module] = None,
         groups: int = 1,
         base_width: int = 64,
         dilation: int = 1,
-        norm_layer: Optional[Callable[..., nn.Module]] = None,
+        norm_layer: Optional[Callable[..., Module]] = None,
     ) -> None:
         super().__init__()
         if norm_layer is None:
             norm_layer = nn.BatchNorm2d
         width = int(planes * (base_width / 64.0)) * groups
         # Both self.conv2 and self.downsample layers downsample the input when stride != 1
         self.conv1 = conv1x1(inplanes, width)
@@ -62,15 +65,15 @@
         self.bn2 = norm_layer(width)
         self.conv3 = conv1x1(width, planes * self.expansion)
         self.bn3 = norm_layer(planes * self.expansion)
         self.relu = nn.ReLU(inplace=True)
         self.downsample = downsample
         self.stride = stride
 
-    def forward(self, x: torch.Tensor) -> torch.Tensor:
+    def forward(self, x: Tensor) -> Tensor:
         identity = x
 
         out = self.conv1(x)
         out = self.bn1(out)
         out = self.relu(out)
 
         out = self.conv2(out)
@@ -85,25 +88,25 @@
 
         out += identity
         out = self.relu(out)
 
         return out
 
 
-class ResNet(nn.Module):
+class ResNet(Module):
     def __init__(
         self,
         block: Type[Bottleneck],
         layers: List[int],
         num_classes: int = 1000,
         zero_init_residual: bool = False,
         groups: int = 1,
         width_per_group: int = 64,
         replace_stride_with_dilation: Optional[List[bool]] = None,
-        norm_layer: Optional[Callable[..., nn.Module]] = None,
+        norm_layer: Optional[Callable[..., Module]] = None,
     ) -> None:
         super().__init__()
         if norm_layer is None:
             norm_layer = nn.BatchNorm2d
         self._norm_layer = norm_layer
 
         self.inplanes = 64
@@ -138,16 +141,16 @@
                 nn.init.constant_(m.bias, 0)
 
         # Zero-initialize the last BN in each residual branch,
         # so that the residual branch starts with zeros, and each residual block behaves like an identity.
         # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677
         if zero_init_residual:
             for m in self.modules():
-                if isinstance(m, Bottleneck):
-                    nn.init.constant_(m.bn3.weight, 0)  # type: ignore[arg-type]
+                if isinstance(m, Bottleneck) and isinstance(m.bn3.weight, Tensor):
+                    nn.init.constant_(m.bn3.weight, 0)
 
     def _make_layer(
         self, block: Type[Bottleneck], planes: int, blocks: int, stride: int = 1, dilate: bool = False
     ) -> nn.Sequential:
         norm_layer = self._norm_layer
         downsample = None
         previous_dilation = self.dilation
@@ -176,15 +179,15 @@
                     dilation=self.dilation,
                     norm_layer=norm_layer,
                 )
             )
 
         return nn.Sequential(*layers)
 
-    def _forward_impl(self, x: torch.Tensor) -> torch.Tensor:
+    def _forward_impl(self, x: Tensor) -> Tensor:
         # See note [TorchScript super()]
         x = self.conv1(x)
         x = self.bn1(x)
         x = self.relu(x)
         x = self.maxpool(x)
 
         x = self.layer1(x)
@@ -194,32 +197,32 @@
 
         x = self.avgpool(x)
         x = torch.flatten(x, 1)
         x = self.fc(x)
 
         return x
 
-    def forward(self, x: torch.Tensor) -> torch.Tensor:
+    def forward(self, x: Tensor) -> Tensor:
         return self._forward_impl(x)
 
 
-class EncoderDeFMO(nn.Module):
+class EncoderDeFMO(Module):
     def __init__(self):
         super().__init__()
         model = ResNet(Bottleneck, [3, 4, 6, 3])  # ResNet50
         modelc1 = nn.Sequential(*list(model.children())[:3])
         modelc2 = nn.Sequential(*list(model.children())[4:8])
         modelc1[0] = nn.Conv2d(6, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
         self.net = nn.Sequential(modelc1, modelc2)
 
-    def forward(self, input_data: torch.Tensor) -> torch.Tensor:
+    def forward(self, input_data: Tensor) -> Tensor:
         return self.net(input_data)
 
 
-class RenderingDeFMO(nn.Module):
+class RenderingDeFMO(Module):
     def __init__(self):
         super().__init__()
         self.tsr_steps: int = 24
         model = nn.Sequential(
             nn.Conv2d(2049, 1024, kernel_size=3, stride=1, padding=1, bias=False),
             nn.BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),
             nn.ReLU(inplace=True),
@@ -234,36 +237,36 @@
             nn.Conv2d(4, 4, kernel_size=3, stride=1, padding=1, bias=True),
             nn.ReLU(inplace=True),
             nn.Conv2d(4, 4, kernel_size=3, stride=1, padding=1, bias=True),
         )
         self.net = model
         self.times = torch.linspace(0, 1, self.tsr_steps)
 
-    def forward(self, latent: torch.Tensor) -> torch.Tensor:
+    def forward(self, latent: Tensor) -> Tensor:
         times = self.times.to(latent.device).unsqueeze(0).repeat(latent.shape[0], 1)
         renders = []
         for ki in range(times.shape[1]):
             t_tensor = (
                 # TODO: replace by after deprecate pytorch 1.6
                 # times[list(range(times.shape[0])), ki]
                 times[[x for x in range(times.shape[0])], ki]  # skipcq: PYL-R1721
                 .unsqueeze(-1)
                 .unsqueeze(-1)
                 .unsqueeze(-1)
                 .repeat(1, 1, latent.shape[2], latent.shape[3])
             )
-            latenti = torch.cat((t_tensor, latent), 1)
+            latenti = concatenate((t_tensor, latent), 1)
             result = self.net(latenti)
             renders.append(result)
-        renders_stacked = torch.stack(renders, 1).contiguous()
+        renders_stacked = stack(renders, 1).contiguous()
         renders_stacked[:, :, :4] = torch.sigmoid(renders_stacked[:, :, :4])
         return renders_stacked
 
 
-class DeFMO(nn.Module):
+class DeFMO(Module):
     """Module that disentangle a fast-moving object from the background and performs deblurring.
 
     This is based on the original code from paper "DeFMO: Deblurring and Shape Recovery
         of Fast Moving Objects". See :cite:`DeFMO2021` for more details.
 
     Args:
         pretrained: Download and set pretrained weights to the model. Default: false.
@@ -283,18 +286,21 @@
     def __init__(self, pretrained: bool = False) -> None:
         super().__init__()
         self.encoder = EncoderDeFMO()
         self.rendering = RenderingDeFMO()
 
         # use torch.hub to load pretrained model
         if pretrained:
-            storage_fcn: Callable = lambda storage, loc: storage
-            pretrained_dict = torch.hub.load_state_dict_from_url(urls['defmo_encoder'], map_location=storage_fcn)
+            pretrained_dict = torch.hub.load_state_dict_from_url(
+                urls['defmo_encoder'], map_location=map_location_to_cpu
+            )
             self.encoder.load_state_dict(pretrained_dict, strict=True)
-            pretrained_dict_ren = torch.hub.load_state_dict_from_url(urls['defmo_rendering'], map_location=storage_fcn)
+            pretrained_dict_ren = torch.hub.load_state_dict_from_url(
+                urls['defmo_rendering'], map_location=map_location_to_cpu
+            )
             self.rendering.load_state_dict(pretrained_dict_ren, strict=True)
         self.eval()
 
-    def forward(self, input_data: torch.Tensor) -> torch.Tensor:
+    def forward(self, input_data: Tensor) -> Tensor:
         latent = self.encoder(input_data)
         x_out = self.rendering(latent)
         return x_out
```

### Comparing `kornia-0.6.8/kornia/feature/hardnet.py` & `kornia-0.6.9/kornia/feature/hardnet.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,21 +1,22 @@
-from typing import Callable, Dict
+from typing import Dict
 
 import torch
 import torch.nn as nn
 import torch.nn.functional as F
 
 from kornia.testing import KORNIA_CHECK_SHAPE, is_mps_tensor_safe
+from kornia.utils.helpers import map_location_to_cpu
 
 urls: Dict[str, str] = {}
 urls["hardnet++"] = "https://github.com/DagnyT/hardnet/raw/master/pretrained/pretrained_all_datasets/HardNet++.pth"
 urls[
     "liberty_aug"
-] = "https://github.com/DagnyT/hardnet/raw/master/pretrained/train_liberty_with_aug/checkpoint_liberty_with_aug.pth"  # noqa pylint: disable
-urls["hardnet8v2"] = "http://cmp.felk.cvut.cz/~mishkdmy/hardnet8v2.pt"  # pylint: disable
+] = "https://github.com/DagnyT/hardnet/raw/master/pretrained/train_liberty_with_aug/checkpoint_liberty_with_aug.pth"
+urls["hardnet8v2"] = "http://cmp.felk.cvut.cz/~mishkdmy/hardnet8v2.pt"
 
 
 class HardNet(nn.Module):
     r"""Module, which computes HardNet descriptors of given grayscale patches of 32x32.
 
     This is based on the original code from paper "Working hard to know your neighbor's
     margins: Local descriptor learning loss". See :cite:`HardNet2017` for more details.
@@ -61,16 +62,15 @@
             nn.Dropout(0.3),
             nn.Conv2d(128, 128, kernel_size=8, bias=False),
             nn.BatchNorm2d(128, affine=False),
         )
 
         # use torch.hub to load pretrained model
         if pretrained:
-            storage_fcn: Callable = lambda storage, loc: storage
-            pretrained_dict = torch.hub.load_state_dict_from_url(urls['liberty_aug'], map_location=storage_fcn)
+            pretrained_dict = torch.hub.load_state_dict_from_url(urls['liberty_aug'], map_location=map_location_to_cpu)
             self.load_state_dict(pretrained_dict['state_dict'], strict=True)
         self.eval()
 
     @staticmethod
     def _normalize_input(x: torch.Tensor, eps: float = 1e-6) -> torch.Tensor:
         """Utility function that normalizes the input by batch."""
         if not is_mps_tensor_safe(x):
@@ -144,16 +144,15 @@
         )
         self.features.apply(self.weights_init)
         self.register_buffer('components', torch.ones(512, 128, dtype=torch.float))
         self.register_buffer('mean', torch.zeros(512, dtype=torch.float))
 
         # use torch.hub to load pretrained model
         if pretrained:
-            storage_fcn: Callable = lambda storage, loc: storage
-            pretrained_dict = torch.hub.load_state_dict_from_url(urls['hardnet8v2'], map_location=storage_fcn)
+            pretrained_dict = torch.hub.load_state_dict_from_url(urls['hardnet8v2'], map_location=map_location_to_cpu)
             self.load_state_dict(pretrained_dict, strict=True)
         self.eval()
 
     @staticmethod
     def weights_init(m):
         if isinstance(m, nn.Conv2d):
             nn.init.orthogonal_(m.weight.data, gain=0.6)
```

### Comparing `kornia-0.6.8/kornia/feature/hynet.py` & `kornia-0.6.9/kornia/feature/hynet.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,25 +1,28 @@
-from typing import Callable, Dict
+from typing import Dict
 
 import torch
 import torch.nn as nn
 
+from kornia.core import Module, Parameter, Tensor, tensor, zeros
+from kornia.utils.helpers import map_location_to_cpu
+
 urls: Dict[str, str] = {}
 urls[
     "liberty"
 ] = "https://github.com/ducha-aiki/Key.Net-Pytorch/raw/main/model/HyNet/weights/HyNet_LIB.pth"  # pylint: disable
 urls[
     "notredame"
 ] = "https://github.com/ducha-aiki/Key.Net-Pytorch/raw/main/model/HyNet/weights/HyNet_ND.pth"  # pylint: disable
 urls[
     "yosemite"
 ] = "https://github.com/ducha-aiki/Key.Net-Pytorch/raw/main/model/HyNet/weights/HyNet_YOS.pth"  # pylint: disable
 
 
-class FilterResponseNorm2d(nn.Module):
+class FilterResponseNorm2d(Module):
     r"""Feature Response Normalization layer from 'Filter Response Normalization Layer: Eliminating Batch Dependence
     in the Training of Deep Neural Networks', see :cite:`FRN2019` for more details.
 
     .. math::
         y =  \gamma \times \frac{x}{\sqrt{\mathrm{E}[x^2]} + |\epsilon|} + \beta
 
 
@@ -52,47 +55,47 @@
 
         self.num_features = num_features
         self.init_eps = eps
         self.is_eps_leanable = is_eps_leanable
         self.is_bias = is_bias
         self.is_scale = is_scale
 
-        self.weight = nn.parameter.Parameter(torch.ones(1, num_features, 1, 1), requires_grad=True)
-        self.bias = nn.parameter.Parameter(torch.zeros(1, num_features, 1, 1), requires_grad=True)
+        self.weight = Parameter(torch.ones(1, num_features, 1, 1), requires_grad=True)
+        self.bias = Parameter(zeros(1, num_features, 1, 1), requires_grad=True)
         if is_eps_leanable:
-            self.eps = nn.parameter.Parameter(torch.tensor(1), requires_grad=True)
+            self.eps = Parameter(tensor(1), requires_grad=True)
         else:
-            self.register_buffer('eps', torch.tensor([eps]))
+            self.register_buffer('eps', tensor([eps]))
         self.reset_parameters()
 
     def reset_parameters(self):
         nn.init.ones_(self.weight)
         nn.init.zeros_(self.bias)
         if self.is_eps_leanable:
             nn.init.constant_(self.eps, self.init_eps)
 
     def extra_repr(self):
         return 'num_features={num_features}, eps={init_eps}'.format(**self.__dict__)
 
-    def forward(self, x: torch.Tensor) -> torch.Tensor:
+    def forward(self, x: Tensor) -> Tensor:
         # Compute the mean norm of activations per channel.
         nu2 = x.pow(2).mean(dim=[2, 3], keepdim=True)
 
         # Perform FRN.
         x = x * torch.rsqrt(nu2 + self.eps.abs())
 
         # Scale and Bias
         if self.is_scale:
             x = self.weight * x
         if self.is_bias:
             x = x + self.bias
         return x
 
 
-class TLU(nn.Module):
+class TLU(Module):
     r"""TLU layer from 'Filter Response Normalization Layer: Eliminating Batch Dependence in the Training of Deep
     Neural Networks, see :cite:`FRN2019` for more details. :math:`{\tau}` is learnable per channel.
 
     .. math::
         y = \max(x, {\tau})
 
     Args:
@@ -106,29 +109,29 @@
         - Output: :math:`(B, \text{num_features}, H, W)`
     """
 
     def __init__(self, num_features: int):
         """max(y, tau) = max(y - tau, 0) + tau = ReLU(y - tau) + tau"""
         super().__init__()
         self.num_features = num_features
-        self.tau = nn.parameter.Parameter(-torch.ones(1, num_features, 1, 1), requires_grad=True)
+        self.tau = Parameter(-torch.ones(1, num_features, 1, 1), requires_grad=True)
         self.reset_parameters()
 
     def reset_parameters(self):
         # nn.init.zeros_(self.tau)
         nn.init.constant_(self.tau, -1)
 
     def extra_repr(self):
         return 'num_features={num_features}'.format(**self.__dict__)
 
-    def forward(self, x: torch.Tensor) -> torch.Tensor:
+    def forward(self, x: Tensor) -> Tensor:
         return torch.max(x, self.tau)
 
 
-class HyNet(nn.Module):
+class HyNet(Module):
     r"""Module, which computes HyNet descriptors of given grayscale patches of 32x32.
 
     This is based on the original code from paper
     "HyNet: Learning Local Descriptor with Hybrid Similarity Measure and Triplet Loss".
     See :cite:`hynet2020` for more details.
 
     Args:
@@ -208,21 +211,20 @@
             nn.Conv2d(128, self.dim_desc, kernel_size=8, bias=False),
             nn.BatchNorm2d(self.dim_desc, affine=False),
         )
 
         self.desc_norm = nn.LocalResponseNorm(2 * self.dim_desc, 2.0 * self.dim_desc, 0.5, 0.0)
         # use torch.hub to load pretrained model
         if pretrained:
-            storage_fcn: Callable = lambda storage, loc: storage
-            pretrained_dict = torch.hub.load_state_dict_from_url(urls['liberty'], map_location=storage_fcn)
+            pretrained_dict = torch.hub.load_state_dict_from_url(urls['liberty'], map_location=map_location_to_cpu)
             self.load_state_dict(pretrained_dict, strict=True)
         self.eval()
         return
 
-    def forward(self, x: torch.Tensor) -> torch.Tensor:
+    def forward(self, x: Tensor) -> Tensor:
         x = self.layer1(x)
         x = self.layer2(x)
         x = self.layer3(x)
         x = self.layer4(x)
         x = self.layer5(x)
         x = self.layer6(x)
         x = self.layer7(x)
```

### Comparing `kornia-0.6.8/kornia/feature/integrated.py` & `kornia-0.6.9/kornia/feature/integrated.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,14 +1,14 @@
 import warnings
 from typing import Dict, List, Optional, Tuple
 
 import torch
-import torch.nn as nn
 
 from kornia.color import rgb_to_grayscale
+from kornia.core import Device, Module, Tensor, concatenate
 from kornia.geometry.subpix import ConvQuadInterp3d
 from kornia.geometry.transform import ScalePyramid
 from kornia.testing import KORNIA_CHECK_LAF
 
 from .affine_shape import LAFAffNetShapeEstimator
 from .hardnet import HardNet
 from .keynet import KeyNetDetector
@@ -16,20 +16,16 @@
 from .orientation import LAFOrienter, OriNet, PassLAF
 from .responses import BlobDoG, CornerGFTT
 from .scale_space_detector import ScaleSpaceDetector
 from .siftdesc import SIFTDescriptor
 
 
 def get_laf_descriptors(
-    img: torch.Tensor,
-    lafs: torch.Tensor,
-    patch_descriptor: nn.Module,
-    patch_size: int = 32,
-    grayscale_descriptor: bool = True,
-) -> torch.Tensor:
+    img: Tensor, lafs: Tensor, patch_descriptor: Module, patch_size: int = 32, grayscale_descriptor: bool = True
+) -> Tensor:
     r"""Function to get local descriptors, corresponding to LAFs (keypoints).
 
     Args:
         img: image features with shape :math:`(B,C,H,W)`.
         lafs: local affine frames :math:`(B,N,2,3)`.
         patch_descriptor: patch descriptor module, e.g. :class:`~kornia.feature.SIFTDescriptor`
             or :class:`~kornia.feature.HardNet`.
@@ -39,45 +35,42 @@
     Returns:
         Local descriptors of shape :math:`(B,N,D)` where :math:`D` is descriptor size.
     """
     KORNIA_CHECK_LAF(lafs)
     patch_descriptor = patch_descriptor.to(img)
     patch_descriptor.eval()
 
-    timg: torch.Tensor = img
+    timg: Tensor = img
     if lafs.shape[1] == 0:
         warnings.warn(f"LAF contains no keypoints {lafs.shape}, returning empty tensor")
         return torch.empty(lafs.shape[0], lafs.shape[1], 128)
     if grayscale_descriptor and img.size(1) == 3:
         timg = rgb_to_grayscale(img)
 
-    patches: torch.Tensor = extract_patches_from_pyramid(timg, lafs, patch_size)
+    patches: Tensor = extract_patches_from_pyramid(timg, lafs, patch_size)
     # Descriptor accepts standard tensor [B, CH, H, W], while patches are [B, N, CH, H, W] shape
     # So we need to reshape a bit :)
     B, N, CH, H, W = patches.size()
     return patch_descriptor(patches.view(B * N, CH, H, W)).view(B, N, -1)
 
 
-class LAFDescriptor(nn.Module):
+class LAFDescriptor(Module):
     r"""Module to get local descriptors, corresponding to LAFs (keypoints).
 
     Internally uses :func:`~kornia.feature.get_laf_descriptors`.
 
     Args:
         patch_descriptor_module: patch descriptor module, e.g. :class:`~kornia.feature.SIFTDescriptor`
             or :class:`~kornia.feature.HardNet`. Default: :class:`~kornia.feature.HardNet`.
         patch_size: patch size in pixels, which descriptor expects.
         grayscale_descriptor: ``True`` if patch_descriptor expects single-channel image.
     """
 
     def __init__(
-        self,
-        patch_descriptor_module: Optional[nn.Module] = None,
-        patch_size: int = 32,
-        grayscale_descriptor: bool = True,
+        self, patch_descriptor_module: Optional[Module] = None, patch_size: int = 32, grayscale_descriptor: bool = True
     ) -> None:
         super().__init__()
         if patch_descriptor_module is None:
             patch_descriptor_module = HardNet(True)
         self.descriptor = patch_descriptor_module
         self.patch_size = patch_size
         self.grayscale_descriptor = grayscale_descriptor
@@ -93,15 +86,15 @@
             + str(self.patch_size)
             + ', '
             + 'grayscale_descriptor='
             + str(self.grayscale_descriptor)
             + ')'
         )
 
-    def forward(self, img: torch.Tensor, lafs: torch.Tensor) -> torch.Tensor:
+    def forward(self, img: Tensor, lafs: Tensor) -> Tensor:
         r"""Three stage local feature detection.
 
         First the location and scale of interest points are determined by
         detect function. Then affine shape and orientation.
 
         Args:
             img: image features with shape :math:`(B,C,H,W)`.
@@ -109,34 +102,32 @@
 
         Returns:
             Local descriptors of shape :math:`(B,N,D)` where :math:`D` is descriptor size.
         """
         return get_laf_descriptors(img, lafs, self.descriptor, self.patch_size, self.grayscale_descriptor)
 
 
-class LocalFeature(nn.Module):
+class LocalFeature(Module):
     """Module, which combines local feature detector and descriptor.
 
     Args:
         detector: the detection module.
         descriptor: the descriptor module.
         scaling_coef: multiplier for change default detector scale (e.g. it is too small for KeyNet by default)
     """
 
-    def __init__(self, detector: nn.Module, descriptor: LAFDescriptor, scaling_coef: float = 1.0) -> None:
+    def __init__(self, detector: Module, descriptor: LAFDescriptor, scaling_coef: float = 1.0) -> None:
         super().__init__()
         self.detector = detector
         self.descriptor = descriptor
         if scaling_coef <= 0:
             raise ValueError(f"Scaling coef should be >= 0, got {scaling_coef}")
         self.scaling_coef = scaling_coef
 
-    def forward(
-        self, img: torch.Tensor, mask: Optional[torch.Tensor] = None
-    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:  # type: ignore
+    def forward(self, img: Tensor, mask: Optional[Tensor] = None) -> Tuple[Tensor, Tensor, Tensor]:
         """
         Args:
             img: image to extract features with shape :math:`(B,C,H,W)`.
             mask: a mask with weights where to apply the response function.
                 The shape must be the same as the input image.
 
         Returns:
@@ -157,15 +148,15 @@
     """
 
     def __init__(
         self,
         num_features: int = 8000,
         upright: bool = False,
         rootsift: bool = True,
-        device: torch.device = torch.device('cpu'),
+        device: Device = torch.device('cpu'),
     ):
         patch_size: int = 41
         detector = ScaleSpaceDetector(
             num_features,
             resp_module=BlobDoG(),
             nms_module=ConvQuadInterp3d(10),
             scale_pyr_module=ScalePyramid(3, 1.6, 32, double_image=True),
@@ -179,15 +170,15 @@
         ).to(device)
         super().__init__(detector, descriptor)
 
 
 class GFTTAffNetHardNet(LocalFeature):
     """Convenience module, which implements GFTT detector + AffNet-HardNet descriptor."""
 
-    def __init__(self, num_features: int = 8000, upright: bool = False, device: torch.device = torch.device('cpu')):
+    def __init__(self, num_features: int = 8000, upright: bool = False, device: Device = torch.device('cpu')):
         detector = ScaleSpaceDetector(
             num_features,
             resp_module=CornerGFTT(),
             nms_module=ConvQuadInterp3d(10, 1e-5),
             scale_pyr_module=ScalePyramid(3, 1.6, 32, double_image=False),
             ori_module=PassLAF() if upright else LAFOrienter(19),
             aff_module=LAFAffNetShapeEstimator(True).eval(),
@@ -200,15 +191,15 @@
 class KeyNetHardNet(LocalFeature):
     """Convenience module, which implements KeyNet detector + HardNet descriptor."""
 
     def __init__(
         self,
         num_features: int = 8000,
         upright: bool = False,
-        device: torch.device = torch.device('cpu'),
+        device: Device = torch.device('cpu'),
         scale_laf: float = 1.0,
     ):
         ori_module = PassLAF() if upright else LAFOrienter(angle_detector=OriNet(True))
         detector = KeyNetDetector(True, num_features=num_features, ori_module=ori_module).to(device)
         descriptor = LAFDescriptor(None, patch_size=32, grayscale_descriptor=True).to(device)
         super().__init__(detector, descriptor, scale_laf)
 
@@ -216,67 +207,67 @@
 class KeyNetAffNetHardNet(LocalFeature):
     """Convenience module, which implements KeyNet detector + AffNet + HardNet descriptor."""
 
     def __init__(
         self,
         num_features: int = 8000,
         upright: bool = False,
-        device: torch.device = torch.device('cpu'),
+        device: Device = torch.device('cpu'),
         scale_laf: float = 1.0,
     ):
         ori_module = PassLAF() if upright else LAFOrienter(angle_detector=OriNet(True))
         detector = KeyNetDetector(
             True, num_features=num_features, ori_module=ori_module, aff_module=LAFAffNetShapeEstimator(True).eval()
         ).to(device)
         descriptor = LAFDescriptor(None, patch_size=32, grayscale_descriptor=True).to(device)
         super().__init__(detector, descriptor, scale_laf)
 
 
-class LocalFeatureMatcher(nn.Module):
+class LocalFeatureMatcher(Module):
     r"""Module, which finds correspondences between two images based on local features.
 
     Args:
         local_feature: Local feature detector. See :class:`~kornia.feature.GFTTAffNetHardNet`.
         matcher: Descriptor matcher, see :class:`~kornia.feature.DescriptorMatcher`.
 
     Returns:
-        Dict[str, torch.Tensor]: Dictionary with image correspondences and confidence scores.
+        Dict[str, Tensor]: Dictionary with image correspondences and confidence scores.
 
     Example:
         >>> img1 = torch.rand(1, 1, 320, 200)
         >>> img2 = torch.rand(1, 1, 128, 128)
         >>> input = {"image0": img1, "image1": img2}
         >>> gftt_hardnet_matcher = LocalFeatureMatcher(
         ...     GFTTAffNetHardNet(10), kornia.feature.DescriptorMatcher('snn', 0.8)
         ... )
         >>> out = gftt_hardnet_matcher(input)
     """
 
-    def __init__(self, local_feature: nn.Module, matcher: nn.Module) -> None:
+    def __init__(self, local_feature: Module, matcher: Module) -> None:
         super().__init__()
         self.local_feature = local_feature
         self.matcher = matcher
         self.eval()
 
-    def extract_features(self, image: torch.Tensor, mask: Optional[torch.Tensor] = None) -> Dict[str, torch.Tensor]:
+    def extract_features(self, image: Tensor, mask: Optional[Tensor] = None) -> Dict[str, Tensor]:
         """Function for feature extraction from simple image."""
         lafs0, resps0, descs0 = self.local_feature(image, mask)
         return {"lafs": lafs0, "responses": resps0, "descriptors": descs0}
 
-    def no_match_output(self, device: torch.device, dtype: torch.dtype) -> dict:
+    def no_match_output(self, device: Device, dtype: torch.dtype) -> Dict[str, Tensor]:
         return {
             'keypoints0': torch.empty(0, 2, device=device, dtype=dtype),
             'keypoints1': torch.empty(0, 2, device=device, dtype=dtype),
             'lafs0': torch.empty(0, 0, 2, 3, device=device, dtype=dtype),
             'lafs1': torch.empty(0, 0, 2, 3, device=device, dtype=dtype),
             'confidence': torch.empty(0, device=device, dtype=dtype),
             'batch_indexes': torch.empty(0, device=device, dtype=torch.long),
         }
 
-    def forward(self, data: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:
+    def forward(self, data: Dict[str, Tensor]) -> Dict[str, Tensor]:
         """
         Args:
             data: dictionary containing the input data in the following format:
 
         Keyword Args:
             image0: left image with shape :math:`(N, 1, H1, W1)`.
             image1: right image with shape :math:`(N, 1, H2, W2)`.
@@ -291,34 +282,34 @@
             - ``lafs1``, matching LAFs from image1 :math:`(1, NC, 2, 3)`.
             - ``batch_indexes``, batch indexes for the keypoints and lafs :math:`(NC)`.
         """
         num_image_pairs: int = data['image0'].shape[0]
 
         if ('lafs0' not in data.keys()) or ('descriptors0' not in data.keys()):
             # One can supply pre-extracted local features
-            feats_dict0: Dict[str, torch.Tensor] = self.extract_features(data['image0'])
+            feats_dict0: Dict[str, Tensor] = self.extract_features(data['image0'])
             lafs0, descs0 = feats_dict0['lafs'], feats_dict0['descriptors']
         else:
             lafs0, descs0 = data['lafs0'], data['descriptors0']
 
         if ('lafs1' not in data.keys()) or ('descriptors1' not in data.keys()):
-            feats_dict1: Dict[str, torch.Tensor] = self.extract_features(data['image1'])
+            feats_dict1: Dict[str, Tensor] = self.extract_features(data['image1'])
             lafs1, descs1 = feats_dict1['lafs'], feats_dict1['descriptors']
         else:
             lafs1, descs1 = data['lafs1'], data['descriptors1']
 
-        keypoints0: torch.Tensor = get_laf_center(lafs0)
-        keypoints1: torch.Tensor = get_laf_center(lafs1)
+        keypoints0: Tensor = get_laf_center(lafs0)
+        keypoints1: Tensor = get_laf_center(lafs1)
 
-        out_keypoints0: List[torch.Tensor] = []
-        out_keypoints1: List[torch.Tensor] = []
-        out_confidence: List[torch.Tensor] = []
-        out_batch_indexes: List[torch.Tensor] = []
-        out_lafs0: List[torch.Tensor] = []
-        out_lafs1: List[torch.Tensor] = []
+        out_keypoints0: List[Tensor] = []
+        out_keypoints1: List[Tensor] = []
+        out_confidence: List[Tensor] = []
+        out_batch_indexes: List[Tensor] = []
+        out_lafs0: List[Tensor] = []
+        out_lafs1: List[Tensor] = []
 
         for batch_idx in range(num_image_pairs):
             dists, idxs = self.matcher(descs0[batch_idx], descs1[batch_idx])
             if len(idxs) == 0:
                 continue
 
             current_keypoints_0 = keypoints0[batch_idx, idxs[:, 0]]
@@ -334,14 +325,14 @@
             out_lafs1.append(current_lafs_1)
             out_batch_indexes.append(batch_idxs)
 
         if len(out_batch_indexes) == 0:
             return self.no_match_output(data['image0'].device, data['image0'].dtype)
 
         return {
-            'keypoints0': torch.cat(out_keypoints0, dim=0).view(-1, 2),
-            'keypoints1': torch.cat(out_keypoints1, dim=0).view(-1, 2),
-            'lafs0': torch.cat(out_lafs0, dim=0).view(1, -1, 2, 3),
-            'lafs1': torch.cat(out_lafs1, dim=0).view(1, -1, 2, 3),
-            'confidence': torch.cat(out_confidence, dim=0).view(-1),
-            'batch_indexes': torch.cat(out_batch_indexes, dim=0).view(-1),
+            'keypoints0': concatenate(out_keypoints0, dim=0).view(-1, 2),
+            'keypoints1': concatenate(out_keypoints1, dim=0).view(-1, 2),
+            'lafs0': concatenate(out_lafs0, dim=0).view(1, -1, 2, 3),
+            'lafs1': concatenate(out_lafs1, dim=0).view(1, -1, 2, 3),
+            'confidence': concatenate(out_confidence, dim=0).view(-1),
+            'batch_indexes': concatenate(out_batch_indexes, dim=0).view(-1),
         }
```

### Comparing `kornia-0.6.8/kornia/feature/keynet.py` & `kornia-0.6.9/kornia/feature/keynet.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,42 +1,53 @@
 import math
-from typing import Callable, Dict, List, Optional, Tuple
+from typing import List, Optional, Tuple
 
 import torch
 import torch.nn as nn
 import torch.nn.functional as F
-from torch import Tensor
+from typing_extensions import TypedDict
 
+from kornia.core import Module, Tensor, concatenate, tensor, where, zeros
 from kornia.filters import SpatialGradient
 from kornia.geometry.subpix import NonMaximaSuppression2d
 from kornia.geometry.transform import pyrdown
+from kornia.utils.helpers import map_location_to_cpu
 
 from .laf import laf_from_center_scale_ori
 from .orientation import PassLAF
 
-keynet_config = {
-    'KeyNet_default_config': {
-        # Key.Net Model
-        'num_filters': 8,
-        'num_levels': 3,
-        'kernel_size': 5,
-        # Extraction Parameters
-        'nms_size': 15,
-        'pyramid_levels': 4,
-        'up_levels': 1,
-        'scale_factor_levels': math.sqrt(2),
-        's_mult': 22,
-    }
+
+class KeyNet_conf(TypedDict):
+    num_filters: int
+    num_levels: int
+    kernel_size: int
+    nms_size: int
+    pyramid_levels: int
+    up_levels: int
+    scale_factor_levels: float
+    s_mult: float
+
+
+keynet_default_config: KeyNet_conf = {
+    # Key.Net Model
+    'num_filters': 8,
+    'num_levels': 3,
+    'kernel_size': 5,
+    # Extraction Parameters
+    'nms_size': 15,
+    'pyramid_levels': 4,
+    'up_levels': 1,
+    'scale_factor_levels': math.sqrt(2),
+    's_mult': 22.0,
 }
 
-urls: Dict[str, str] = {}
-urls["keynet"] = "https://github.com/axelBarroso/Key.Net-Pytorch/raw/main/model/weights/keynet_pytorch.pth"
+KeyNet_URL = "https://github.com/axelBarroso/Key.Net-Pytorch/raw/main/model/weights/keynet_pytorch.pth"
 
 
-class _FeatureExtractor(nn.Module):
+class _FeatureExtractor(Module):
     """Helper class for KeyNet.
 
     It loads both, the handcrafted and learnable blocks
     """
 
     def __init__(self):
         super().__init__()
@@ -46,33 +57,32 @@
 
     def forward(self, x: Tensor) -> Tensor:
         x_hc = self.hc_block(x)
         x_lb = self.lb_block(x_hc)
         return x_lb
 
 
-class _HandcraftedBlock(nn.Module):
+class _HandcraftedBlock(Module):
     """Helper class for KeyNet, it defines the handcrafted filters within the Key.Net handcrafted block."""
 
     def __init__(self):
         super().__init__()
         self.spatial_gradient = SpatialGradient('sobel', 1)
 
     def forward(self, x: Tensor) -> Tensor:
-
         sobel = self.spatial_gradient(x)
         dx, dy = sobel[:, :, 0, :, :], sobel[:, :, 1, :, :]
 
         sobel_dx = self.spatial_gradient(dx)
         dxx, dxy = sobel_dx[:, :, 0, :, :], sobel_dx[:, :, 1, :, :]
 
         sobel_dy = self.spatial_gradient(dy)
         dyy = sobel_dy[:, :, 1, :, :]
 
-        hc_feats = torch.cat([dx, dy, dx**2.0, dy**2.0, dx * dy, dxy, dxy**2.0, dxx, dyy, dxx * dyy], dim=1)
+        hc_feats = concatenate([dx, dy, dx**2.0, dy**2.0, dx * dy, dxy, dxy**2.0, dxx, dyy, dxx * dyy], 1)
 
         return hc_feats
 
 
 class _LearnableBlock(nn.Sequential):
     """Helper class for KeyNet.
 
@@ -106,15 +116,15 @@
     return nn.Sequential(
         nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, dilation),
         nn.BatchNorm2d(out_channels),
         nn.ReLU(inplace=True),
     )
 
 
-class KeyNet(nn.Module):
+class KeyNet(Module):
     """Key.Net model definition -- local feature detector (response function). This is based on the original code
     from paper "Key.Net: Keypoint Detection by Handcrafted and Learned CNN Filters". See :cite:`KeyNet2019` for
     more details.
 
     Args:
         pretrained: Download and set pretrained weights to the model.
         keynet_conf: Dict with initiliazation parameters. Do not pass it, unless you know what you are doing`.
@@ -123,52 +133,51 @@
         KeyNet response score.
 
     Shape:
         - Input: :math:`(B, 1, H, W)`
         - Output: :math:`(B, 1, H, W)`
     """
 
-    def __init__(self, pretrained: bool = False, keynet_conf: Dict = keynet_config['KeyNet_default_config']):
+    def __init__(self, pretrained: bool = False, keynet_conf: KeyNet_conf = keynet_default_config):
         super().__init__()
 
         num_filters = keynet_conf['num_filters']
-        self.num_levels: int = keynet_conf['num_levels']
-        kernel_size: int = keynet_conf['kernel_size']
-        padding: int = kernel_size // 2
+        self.num_levels = keynet_conf['num_levels']
+        kernel_size = keynet_conf['kernel_size']
+        padding = kernel_size // 2
 
         self.feature_extractor = _FeatureExtractor()
         self.last_conv = nn.Sequential(
             nn.Conv2d(
                 in_channels=num_filters * self.num_levels, out_channels=1, kernel_size=kernel_size, padding=padding
             ),
             nn.ReLU(inplace=True),
         )
         # use torch.hub to load pretrained model
         if pretrained:
-            storage_fcn: Callable = lambda storage, loc: storage
-            pretrained_dict = torch.hub.load_state_dict_from_url(urls['keynet'], map_location=storage_fcn)
+            pretrained_dict = torch.hub.load_state_dict_from_url(KeyNet_URL, map_location=map_location_to_cpu)
             self.load_state_dict(pretrained_dict['state_dict'], strict=True)
         self.eval()
 
     def forward(self, x: Tensor) -> Tensor:
         """
         x - input image
         """
         shape_im = x.shape
         feats: List[Tensor] = [self.feature_extractor(x)]
         for i in range(1, self.num_levels):
             x = pyrdown(x, factor=1.2)
             feats_i = self.feature_extractor(x)
             feats_i = F.interpolate(feats_i, size=(shape_im[2], shape_im[3]), mode='bilinear')
             feats.append(feats_i)
-        scores = self.last_conv(torch.cat(feats, dim=1))
+        scores = self.last_conv(concatenate(feats, 1))
         return scores
 
 
-class KeyNetDetector(nn.Module):
+class KeyNetDetector(Module):
     """Multi-scale feature detector based on KeyNet.
 
     This is based on the original code from paper
     "Key.Net: Keypoint Detection by Handcrafted and Learned CNN Filters".
     See :cite:`KeyNet2019` for more details.
 
     Args:
@@ -181,30 +190,38 @@
             which does nothing. See :class:`~kornia.feature.LAFAffineShapeEstimator` for details.
     """
 
     def __init__(
         self,
         pretrained: bool = False,
         num_features: int = 2048,
-        keynet_conf: Dict = keynet_config['KeyNet_default_config'],
-        ori_module: nn.Module = PassLAF(),
-        aff_module: nn.Module = PassLAF(),
+        keynet_conf: KeyNet_conf = keynet_default_config,
+        ori_module: Optional[Module] = None,
+        aff_module: Optional[Module] = None,
     ):
         super().__init__()
         self.model = KeyNet(pretrained, keynet_conf)
         # Load extraction configuration
-        self.num_pyramid_levels: int = keynet_conf['pyramid_levels']
-        self.num_upscale_levels: int = keynet_conf['up_levels']
-        self.scale_factor_levels: float = keynet_conf['scale_factor_levels']
-        self.mr_size: float = keynet_conf['s_mult']
-        self.nms_size: int = keynet_conf['nms_size']
+        self.num_pyramid_levels = keynet_conf['pyramid_levels']
+        self.num_upscale_levels = keynet_conf['up_levels']
+        self.scale_factor_levels = keynet_conf['scale_factor_levels']
+        self.mr_size = keynet_conf['s_mult']
+        self.nms_size = keynet_conf['nms_size']
         self.nms = NonMaximaSuppression2d((self.nms_size, self.nms_size))
         self.num_features = num_features
-        self.ori = ori_module
-        self.aff = aff_module
+
+        if ori_module is None:
+            self.ori: Module = PassLAF()
+        else:
+            self.ori = ori_module
+
+        if aff_module is None:
+            self.aff: Module = PassLAF()
+        else:
+            self.aff = aff_module
 
     def remove_borders(self, score_map, borders: int = 15):
         """It removes the borders of the image to avoid detections on the corners."""
         mask = torch.zeros_like(score_map)
         mask[:, :, borders:-borders, borders:-borders] = 1
         return mask * score_map
 
@@ -215,28 +232,24 @@
         device = level_img.device
         dtype = level_img.dtype
         yx = det_map.nonzero()[:, 2:].t()
         scores = det_map[0, 0, yx[0], yx[1]]  # keynet supports only non-batched images
 
         scores_sorted, indices = torch.sort(scores, descending=True)
 
-        indices = indices[torch.where(scores_sorted > 0.0)]
+        indices = indices[where(scores_sorted > 0.0)]
         yx = yx[:, indices[:num_kp]].t()
         current_kp_num = len(yx)
-        xy_projected = yx.view(1, current_kp_num, 2).flip(2) * torch.tensor(factor, device=device, dtype=dtype)
+        xy_projected = yx.view(1, current_kp_num, 2).flip(2) * tensor(factor, device=device, dtype=dtype)
         scale_factor = 0.5 * (factor[0] + factor[1])
         scale = scale_factor * self.mr_size * torch.ones(1, current_kp_num, 1, 1, device=device, dtype=dtype)
-        lafs = laf_from_center_scale_ori(
-            xy_projected, scale, torch.zeros(1, current_kp_num, 1, device=device, dtype=dtype)
-        )
+        lafs = laf_from_center_scale_ori(xy_projected, scale, zeros(1, current_kp_num, 1, device=device, dtype=dtype))
         return scores_sorted[:num_kp], lafs
 
-    def detect(  # type: ignore
-        self, img: Tensor, mask: Optional[Tensor] = None  # type: ignore
-    ) -> Tuple[Tensor, Tensor]:
+    def detect(self, img: Tensor, mask: Optional[Tensor] = None) -> Tuple[Tensor, Tensor]:
         # Compute points per level
         num_features_per_level: List[float] = []
         tmp = 0.0
         factor_points = self.scale_factor_levels**2
         levels = self.num_pyramid_levels + self.num_upscale_levels + 1
         for idx_level in range(levels):
             tmp += factor_points ** (-1 * (idx_level - self.num_upscale_levels))
@@ -279,24 +292,22 @@
                 nf2 = [num_features_per_level[a] for a in range(0, idx_level + 1 + self.num_upscale_levels)]
                 res_points = Tensor(nf2).sum().item()
                 num_points_level = int(res_points)
 
             cur_scores, cur_lafs = self.detect_features_on_single_level(cur_img, num_points_level, factor)
             all_responses.append(cur_scores.view(1, -1))
             all_lafs.append(cur_lafs)
-        responses: Tensor = torch.cat(all_responses, dim=1)
-        lafs: Tensor = torch.cat(all_lafs, dim=1)
+        responses: Tensor = concatenate(all_responses, 1)
+        lafs: Tensor = concatenate(all_lafs, 1)
         if lafs.shape[1] > self.num_features:
             responses, idxs = torch.topk(responses, k=self.num_features, dim=1)
             lafs = torch.gather(lafs, 1, idxs.unsqueeze(-1).unsqueeze(-1).repeat(1, 1, 2, 3))
         return responses, lafs
 
-    def forward(  # type: ignore
-        self, img: Tensor, mask: Optional[Tensor] = None  # type: ignore
-    ) -> Tuple[Tensor, Tensor]:
+    def forward(self, img: Tensor, mask: Optional[Tensor] = None) -> Tuple[Tensor, Tensor]:
         """Three stage local feature detection. First the location and scale of interest points are determined by
         detect function. Then affine shape and orientation.
 
         Args:
             img: image to extract features with shape [1xCxHxW]. KeyNetDetector does not support batch processing,
         because the number of detections is different on each image.
             mask: a mask with weights where to apply the response function. The shape must be the same as
```

### Comparing `kornia-0.6.8/kornia/feature/laf.py` & `kornia-0.6.9/kornia/feature/laf.py`

 * *Files 17% similar despite different names*

```diff
@@ -1,20 +1,21 @@
 import math
 from typing import Optional, Union
 
 import torch
 import torch.nn.functional as F
 
+from kornia.core import Tensor, concatenate, stack, tensor, zeros
 from kornia.geometry.conversions import angle_to_rotation_matrix, convert_points_from_homogeneous, rad2deg
 from kornia.geometry.linalg import transform_points
 from kornia.geometry.transform import pyrdown
 from kornia.testing import KORNIA_CHECK_LAF, KORNIA_CHECK_SHAPE
 
 
-def get_laf_scale(LAF: torch.Tensor) -> torch.Tensor:
+def get_laf_scale(LAF: Tensor) -> Tensor:
     """Return a scale of the LAFs.
 
     Args:
         LAF: tensor [BxNx2x3] or [BxNx2x2].
 
     Returns:
         tensor  BxNx1x1.
@@ -29,15 +30,15 @@
     """
     KORNIA_CHECK_LAF(LAF)
     eps = 1e-10
     out = LAF[..., 0:1, 0:1] * LAF[..., 1:2, 1:2] - LAF[..., 1:2, 0:1] * LAF[..., 0:1, 1:2] + eps
     return out.abs().sqrt()
 
 
-def get_laf_center(LAF: torch.Tensor) -> torch.Tensor:
+def get_laf_center(LAF: Tensor) -> Tensor:
     """Return a center (keypoint) of the LAFs.
 
     Args:
         LAF: tensor [BxNx2x3].
 
     Returns:
         tensor  BxNx2.
@@ -47,41 +48,41 @@
         - Output: :math: `(B, N, 2)`
 
     Example:
         >>> input = torch.ones(1, 5, 2, 3)  # BxNx2x3
         >>> output = get_laf_center(input)  # BxNx2
     """
     KORNIA_CHECK_LAF(LAF)
-    out: torch.Tensor = LAF[..., 2]
+    out = LAF[..., 2]
     return out
 
 
-def get_laf_orientation(LAF: torch.Tensor) -> torch.Tensor:
+def get_laf_orientation(LAF: Tensor) -> Tensor:
     """Return orientation of the LAFs, in degrees.
 
     Args:
-        LAF: (torch.Tensor): tensor [BxNx2x3].
+        LAF: (Tensor): tensor [BxNx2x3].
 
     Returns:
-        torch.Tensor: tensor  BxNx1 .
+        Tensor: tensor  BxNx1 .
 
     Shape:
         - Input: :math: `(B, N, 2, 3)`
         - Output: :math: `(B, N, 1)`
 
     Example:
         >>> input = torch.ones(1, 5, 2, 3)  # BxNx2x3
         >>> output = get_laf_orientation(input)  # BxNx1
     """
     KORNIA_CHECK_LAF(LAF)
-    angle_rad: torch.Tensor = torch.atan2(LAF[..., 0, 1], LAF[..., 0, 0])
+    angle_rad = torch.atan2(LAF[..., 0, 1], LAF[..., 0, 0])
     return rad2deg(angle_rad).unsqueeze(-1)
 
 
-def set_laf_orientation(LAF: torch.Tensor, angles_degrees: torch.Tensor) -> torch.Tensor:
+def set_laf_orientation(LAF: Tensor, angles_degrees: Tensor) -> Tensor:
     """Change the orientation of the LAFs.
 
     Args:
         LAF: tensor [BxNx2x3].
         angles: tensor BxNx1, in degrees.
 
     Returns:
@@ -89,24 +90,22 @@
 
     Shape:
         - Input: :math: `(B, N, 2, 3)`, `(B, N, 1)`
         - Output: :math: `(B, N, 2, 3)`
     """
     KORNIA_CHECK_LAF(LAF)
     B, N = LAF.shape[:2]
-    rotmat: torch.Tensor = angle_to_rotation_matrix(angles_degrees).view(B * N, 2, 2)
-    laf_out: torch.Tensor = torch.cat(
+    rotmat = angle_to_rotation_matrix(angles_degrees).view(B * N, 2, 2)
+    laf_out = concatenate(
         [torch.bmm(make_upright(LAF).view(B * N, 2, 3)[:, :2, :2], rotmat), LAF.view(B * N, 2, 3)[:, :2, 2:]], dim=2
     ).view(B, N, 2, 3)
     return laf_out
 
 
-def laf_from_center_scale_ori(
-    xy: torch.Tensor, scale: Optional[torch.Tensor] = None, ori: Optional[torch.Tensor] = None
-) -> torch.Tensor:
+def laf_from_center_scale_ori(xy: Tensor, scale: Optional[Tensor] = None, ori: Optional[Tensor] = None) -> Tensor:
     """Return orientation of the LAFs, in radians. Useful to create kornia LAFs from OpenCV keypoints.
 
     Args:
         xy: tensor [BxNx2].
         scale: tensor [BxNx1x1]. If not provided, scale = 1 is assumed
         ori: tensor [BxNx1]. If not provided orientation = 0 is assumed
 
@@ -116,23 +115,23 @@
     KORNIA_CHECK_SHAPE(xy, ["B", "N", "2"])
     device = xy.device
     dtype = xy.dtype
     B, N = xy.shape[:2]
     if scale is None:
         scale = torch.ones(B, N, 1, 1, device=device, dtype=dtype)
     if ori is None:
-        ori = torch.zeros(B, N, 1, device=device, dtype=dtype)
+        ori = zeros(B, N, 1, device=device, dtype=dtype)
     KORNIA_CHECK_SHAPE(scale, ["B", "N", "1", "1"])
     KORNIA_CHECK_SHAPE(ori, ["B", "N", "1"])
-    unscaled_laf: torch.Tensor = torch.cat([angle_to_rotation_matrix(ori.squeeze(-1)), xy.unsqueeze(-1)], dim=-1)
-    laf: torch.Tensor = scale_laf(unscaled_laf, scale)
+    unscaled_laf = concatenate([angle_to_rotation_matrix(ori.squeeze(-1)), xy.unsqueeze(-1)], dim=-1)
+    laf = scale_laf(unscaled_laf, scale)
     return laf
 
 
-def scale_laf(laf: torch.Tensor, scale_coef: Union[float, torch.Tensor]) -> torch.Tensor:
+def scale_laf(laf: Tensor, scale_coef: Union[float, Tensor]) -> Tensor:
     """Multiplies region part of LAF ([:, :, :2, :2]) by a scale_coefficient.
 
     So the center, shape and orientation of the local feature stays the same, but the region area changes.
 
     Args:
         laf: tensor [BxNx2x3] or [BxNx2x2].
         scale_coef: broadcastable tensor or float.
@@ -146,22 +145,22 @@
         - Output: :math:`(B, N, 1, 1)`
 
     Example:
         >>> input = torch.ones(1, 5, 2, 3)  # BxNx2x3
         >>> scale = 0.5
         >>> output = scale_laf(input, scale)  # BxNx2x3
     """
-    if (type(scale_coef) is not float) and (type(scale_coef) is not torch.Tensor):
-        raise TypeError("scale_coef should be float or torch.Tensor " "Got {}".format(type(scale_coef)))
+    if (type(scale_coef) is not float) and (type(scale_coef) is not Tensor):
+        raise TypeError("scale_coef should be float or Tensor " "Got {}".format(type(scale_coef)))
     KORNIA_CHECK_LAF(laf)
-    centerless_laf: torch.Tensor = laf[:, :, :2, :2]
-    return torch.cat([scale_coef * centerless_laf, laf[:, :, :, 2:]], dim=3)
+    centerless_laf = laf[:, :, :2, :2]
+    return concatenate([scale_coef * centerless_laf, laf[:, :, :, 2:]], dim=3)
 
 
-def make_upright(laf: torch.Tensor, eps: float = 1e-9) -> torch.Tensor:
+def make_upright(laf: Tensor, eps: float = 1e-9) -> Tensor:
     """Rectify the affine matrix, so that it becomes upright.
 
     Args:
         laf: tensor of LAFs.
         eps : for safe division.
 
     Returns:
@@ -177,27 +176,27 @@
     """
     KORNIA_CHECK_LAF(laf)
     det = get_laf_scale(laf)
     scale = det
     # The function is equivalent to doing 2x2 SVD and resetting rotation
     # matrix to an identity: U, S, V = svd(LAF); LAF_upright = U * S.
     b2a2 = torch.sqrt(laf[..., 0:1, 1:2] ** 2 + laf[..., 0:1, 0:1] ** 2) + eps
-    laf1_ell = torch.cat([(b2a2 / det).contiguous(), torch.zeros_like(det)], dim=3)
-    laf2_ell = torch.cat(  # type: ignore
+    laf1_ell = concatenate([(b2a2 / det).contiguous(), torch.zeros_like(det)], dim=3)
+    laf2_ell = concatenate(
         [
             ((laf[..., 1:2, 1:2] * laf[..., 0:1, 1:2] + laf[..., 1:2, 0:1] * laf[..., 0:1, 0:1]) / (b2a2 * det)),
             (det / b2a2).contiguous(),
         ],
         dim=3,
     )
-    laf_unit_scale = torch.cat([torch.cat([laf1_ell, laf2_ell], dim=2), laf[..., :, 2:3]], dim=3)
+    laf_unit_scale = concatenate([concatenate([laf1_ell, laf2_ell], dim=2), laf[..., :, 2:3]], dim=3)
     return scale_laf(laf_unit_scale, scale)
 
 
-def ellipse_to_laf(ells: torch.Tensor) -> torch.Tensor:
+def ellipse_to_laf(ells: Tensor) -> Tensor:
     """Convert ellipse regions to LAF format.
 
     Ellipse (a, b, c) and upright covariance matrix [a11 a12; 0 a22] are connected
     by inverse matrix square root: A = invsqrt([a b; b c]).
 
     See also https://github.com/vlfeat/vlfeat/blob/master/toolbox/sift/vl_frame2oell.m
 
@@ -218,35 +217,35 @@
     n_dims = len(ells.size())
     if n_dims != 3:
         raise TypeError("ellipse shape should be must be [BxNx5]. " "Got {}".format(ells.size()))
     B, N, dim = ells.size()
     if dim != 5:
         raise TypeError("ellipse shape should be must be [BxNx5]. " "Got {}".format(ells.size()))
     # Previous implementation was incorrectly using Cholesky decomp as matrix sqrt
-    # ell_shape = torch.cat([torch.cat([ells[..., 2:3], ells[..., 3:4]], dim=2).unsqueeze(2),
-    #                       torch.cat([ells[..., 3:4], ells[..., 4:5]], dim=2).unsqueeze(2)], dim=2).view(-1, 2, 2)
+    # ell_shape = concatenate([concatenate([ells[..., 2:3], ells[..., 3:4]], dim=2).unsqueeze(2),
+    #                       concatenate([ells[..., 3:4], ells[..., 4:5]], dim=2).unsqueeze(2)], dim=2).view(-1, 2, 2)
     # out = torch.matrix_power(torch.cholesky(ell_shape, False), -1).view(B, N, 2, 2)
 
     # We will calculate 2x2 matrix square root via special case formula
     # https://en.wikipedia.org/wiki/Square_root_of_a_matrix
     # "The Cholesky factorization provides another particular example of square root
     #  which should not be confused with the unique non-negative square root."
     # https://en.wikipedia.org/wiki/Square_root_of_a_2_by_2_matrix
     # M = (A 0; C D)
     # R = (sqrt(A) 0; C / (sqrt(A)+sqrt(D)) sqrt(D))
     a11 = ells[..., 2:3].abs().sqrt()
     a12 = torch.zeros_like(a11)
     a22 = ells[..., 4:5].abs().sqrt()
     a21 = ells[..., 3:4] / (a11 + a22).clamp(1e-9)
-    A = torch.stack([a11, a12, a21, a22], dim=-1).view(B, N, 2, 2).inverse()
-    out = torch.cat([A, ells[..., :2].view(B, N, 2, 1)], dim=3)
+    A = stack([a11, a12, a21, a22], dim=-1).view(B, N, 2, 2).inverse()
+    out = concatenate([A, ells[..., :2].view(B, N, 2, 1)], dim=3)
     return out
 
 
-def laf_to_boundary_points(LAF: torch.Tensor, n_pts: int = 50) -> torch.Tensor:
+def laf_to_boundary_points(LAF: Tensor, n_pts: int = 50) -> Tensor:
     """Convert LAFs to boundary points of the regions + center.
 
     Used for local features visualization, see visualize_laf function.
 
     Args:
         LAF:
         n_pts: number of points to output.
@@ -256,32 +255,32 @@
 
     Shape:
         - Input: :math:`(B, N, 2, 3)`
         - Output:  :math:`(B, N, n_pts, 2)`
     """
     KORNIA_CHECK_LAF(LAF)
     B, N, _, _ = LAF.size()
-    pts = torch.cat(
+    pts = concatenate(
         [
             torch.sin(torch.linspace(0, 2 * math.pi, n_pts - 1)).unsqueeze(-1),
             torch.cos(torch.linspace(0, 2 * math.pi, n_pts - 1)).unsqueeze(-1),
             torch.ones(n_pts - 1, 1),
         ],
         dim=1,
     )
     # Add origin to draw also the orientation
-    pts = torch.cat([torch.tensor([0.0, 0.0, 1.0]).view(1, 3), pts], dim=0).unsqueeze(0).expand(B * N, n_pts, 3)
+    pts = concatenate([tensor([0.0, 0.0, 1.0]).view(1, 3), pts], dim=0).unsqueeze(0).expand(B * N, n_pts, 3)
     pts = pts.to(LAF.device).to(LAF.dtype)
-    aux = torch.tensor([0.0, 0.0, 1.0]).view(1, 1, 3).expand(B * N, 1, 3)
-    HLAF = torch.cat([LAF.view(-1, 2, 3), aux.to(LAF.device).to(LAF.dtype)], dim=1)
+    aux = tensor([0.0, 0.0, 1.0]).view(1, 1, 3).expand(B * N, 1, 3)
+    HLAF = concatenate([LAF.view(-1, 2, 3), aux.to(LAF.device).to(LAF.dtype)], dim=1)
     pts_h = torch.bmm(HLAF, pts.permute(0, 2, 1)).permute(0, 2, 1)
     return convert_points_from_homogeneous(pts_h.view(B, N, n_pts, 3))
 
 
-def get_laf_pts_to_draw(LAF: torch.Tensor, img_idx: int = 0):
+def get_laf_pts_to_draw(LAF: Tensor, img_idx: int = 0):
     """Return numpy array for drawing LAFs (local features).
 
     Args:
         LAF:
         n_pts: number of boundary points to output.
 
     Returns:
@@ -301,15 +300,15 @@
     # TODO: Refactor doctest
     KORNIA_CHECK_LAF(LAF)
     pts = laf_to_boundary_points(LAF[img_idx : img_idx + 1])[0]
     pts_np = pts.detach().permute(1, 0, 2).cpu().numpy()
     return (pts_np[..., 0], pts_np[..., 1])
 
 
-def denormalize_laf(LAF: torch.Tensor, images: torch.Tensor) -> torch.Tensor:
+def denormalize_laf(LAF: Tensor, images: Tensor) -> Tensor:
     """De-normalize LAFs from scale to image scale.
 
         B,N,H,W = images.size()
         MIN_SIZE = min(H,W)
         [a11 a21 x]
         [a21 a22 y]
         becomes
@@ -334,47 +333,47 @@
     min_size = min(hf, wf)
     coef = torch.ones(1, 1, 2, 3).to(LAF.dtype).to(LAF.device) * min_size
     coef[0, 0, 0, 2] = wf
     coef[0, 0, 1, 2] = hf
     return coef.expand_as(LAF) * LAF
 
 
-def normalize_laf(LAF: torch.Tensor, images: torch.Tensor) -> torch.Tensor:
+def normalize_laf(LAF: Tensor, images: Tensor) -> Tensor:
     """Normalize LAFs to [0,1] scale from pixel scale. See below:
         B,N,H,W = images.size()
         MIN_SIZE = min(H,W)
         [a11 a21 x]
         [a21 a22 y]
         becomes:
         [a11/MIN_SIZE a21/MIN_SIZE x/W]
         [a21/MIN_SIZE a22/MIN_SIZE y/H]
 
     Args:
-        LAF: (torch.Tensor).
-        images: (torch.Tensor) images, LAFs are detected in
+        LAF: (Tensor).
+        images: (Tensor) images, LAFs are detected in
 
     Returns:
-        LAF: (torch.Tensor).
+        LAF: (Tensor).
 
     Shape:
         - Input: :math:`(B, N, 2, 3)`
         - Output:  :math:`(B, N, 2, 3)`
     """
     KORNIA_CHECK_LAF(LAF)
     _, _, h, w = images.size()
-    wf: float = float(w)
-    hf: float = float(h)
+    wf = float(w)
+    hf = float(h)
     min_size = min(hf, wf)
     coef = torch.ones(1, 1, 2, 3).to(LAF.dtype).to(LAF.device) / min_size
     coef[0, 0, 0, 2] = 1.0 / wf
     coef[0, 0, 1, 2] = 1.0 / hf
     return coef.expand_as(LAF) * LAF
 
 
-def generate_patch_grid_from_normalized_LAF(img: torch.Tensor, LAF: torch.Tensor, PS: int = 32) -> torch.Tensor:
+def generate_patch_grid_from_normalized_LAF(img: Tensor, LAF: Tensor, PS: int = 32) -> Tensor:
     """Helper function for affine grid generation.
 
     Args:
         img: image tensor of shape :math:`(B, CH, H, W)`.
         LAF: laf with shape :math:`(B, N, 2, 3)`.
         PS: patch size to be extracted.
 
@@ -385,23 +384,23 @@
     B, N, _, _ = LAF.size()
     _, ch, h, w = img.size()
 
     # norm, then renorm is needed for allowing detection on one resolution
     # and extraction at arbitrary other
     LAF_renorm = denormalize_laf(LAF, img)
 
-    grid = F.affine_grid(LAF_renorm.view(B * N, 2, 3), [B * N, ch, PS, PS], align_corners=False)  # type: ignore
+    grid = F.affine_grid(LAF_renorm.view(B * N, 2, 3), [B * N, ch, PS, PS], align_corners=False)
     grid[..., :, 0] = 2.0 * grid[..., :, 0].clone() / float(w) - 1.0
     grid[..., :, 1] = 2.0 * grid[..., :, 1].clone() / float(h) - 1.0
     return grid
 
 
 def extract_patches_simple(
-    img: torch.Tensor, laf: torch.Tensor, PS: int = 32, normalize_lafs_before_extraction: bool = True
-) -> torch.Tensor:
+    img: Tensor, laf: Tensor, PS: int = 32, normalize_lafs_before_extraction: bool = True
+) -> Tensor:
     """Extract patches defined by LAFs from image tensor.
 
     No smoothing applied, huge aliasing (better use extract_patches_from_pyramid).
 
     Args:
         img: images, LAFs are detected in.
         laf:
@@ -409,37 +408,34 @@
         normalize_lafs_before_extraction: if True, lafs are normalized to image size.
 
     Returns:
         patches with shape :math:`(B, N, CH, PS,PS)`.
     """
     KORNIA_CHECK_LAF(laf)
     if normalize_lafs_before_extraction:
-        nlaf: torch.Tensor = normalize_laf(laf, img)
+        nlaf = normalize_laf(laf, img)
     else:
         nlaf = laf
     _, ch, h, w = img.size()
     B, N, _, _ = laf.size()
     out = []
     # for loop temporarily, to be refactored
     for i in range(B):
         grid = generate_patch_grid_from_normalized_LAF(img[i : i + 1], nlaf[i : i + 1], PS).to(img.device)
         out.append(
             F.grid_sample(
-                img[i : i + 1].expand(grid.size(0), ch, h, w),
-                grid,  # type: ignore
-                padding_mode="border",
-                align_corners=False,
+                img[i : i + 1].expand(grid.size(0), ch, h, w), grid, padding_mode="border", align_corners=False
             )
         )
-    return torch.cat(out, dim=0).view(B, N, ch, PS, PS)
+    return concatenate(out, dim=0).view(B, N, ch, PS, PS)
 
 
 def extract_patches_from_pyramid(
-    img: torch.Tensor, laf: torch.Tensor, PS: int = 32, normalize_lafs_before_extraction: bool = True
-) -> torch.Tensor:
+    img: Tensor, laf: Tensor, PS: int = 32, normalize_lafs_before_extraction: bool = True
+) -> Tensor:
     """Extract patches defined by LAFs from image tensor.
 
     Patches are extracted from appropriate pyramid level.
 
     Args:
         laf:
         images: images, LAFs are detected in.
@@ -447,101 +443,96 @@
         normalize_lafs_before_extraction: if True, lafs are normalized to image size.
 
     Returns:
         patches with shape :math:`(B, N, CH, PS,PS)`.
     """
     KORNIA_CHECK_LAF(laf)
     if normalize_lafs_before_extraction:
-        nlaf: torch.Tensor = normalize_laf(laf, img)
+        nlaf = normalize_laf(laf, img)
     else:
         nlaf = laf
     B, N, _, _ = laf.size()
     _, ch, h, w = img.size()
     scale = 2.0 * get_laf_scale(denormalize_laf(nlaf, img)) / float(PS)
     pyr_idx = scale.log2().relu().long()
     cur_img = img
     cur_pyr_level = 0
-    out = torch.zeros(B, N, ch, PS, PS).to(nlaf.dtype).to(nlaf.device)
+    out = zeros(B, N, ch, PS, PS).to(nlaf.dtype).to(nlaf.device)
     while min(cur_img.size(2), cur_img.size(3)) >= PS:
         _, ch, h, w = cur_img.size()
         # for loop temporarily, to be refactored
         for i in range(B):
             scale_mask = (pyr_idx[i] == cur_pyr_level).squeeze()
             if (scale_mask.float().sum()) == 0:
                 continue
             scale_mask = (scale_mask > 0).view(-1)
             grid = generate_patch_grid_from_normalized_LAF(cur_img[i : i + 1], nlaf[i : i + 1, scale_mask, :, :], PS)
             patches = F.grid_sample(
-                cur_img[i : i + 1].expand(grid.size(0), ch, h, w),
-                grid,  # type: ignore
-                padding_mode="border",
-                align_corners=False,
+                cur_img[i : i + 1].expand(grid.size(0), ch, h, w), grid, padding_mode="border", align_corners=False
             )
             out[i].masked_scatter_(scale_mask.view(-1, 1, 1, 1), patches)
         cur_img = pyrdown(cur_img)
         cur_pyr_level += 1
     return out
 
 
-def laf_is_inside_image(laf: torch.Tensor, images: torch.Tensor, border: int = 0) -> torch.Tensor:
+def laf_is_inside_image(laf: Tensor, images: Tensor, border: int = 0) -> Tensor:
     """Check if the LAF is touching or partly outside the image boundary.
 
     Returns the mask of LAFs, which are fully inside the image, i.e. valid.
 
     Args:
         laf:  :math:`(B, N, 2, 3)`.
         images: images, lafs are detected in :math:`(B, CH, H, W)`.
         border: additional border.
 
     Returns:
         mask with shape :math:`(B, N)`.
     """
     KORNIA_CHECK_LAF(laf)
     _, _, h, w = images.size()
-    pts: torch.Tensor = laf_to_boundary_points(laf, 12)
-    good_lafs_mask: torch.Tensor = (
+    pts = laf_to_boundary_points(laf, 12)
+    good_lafs_mask = (
         (pts[..., 0] >= border) * (pts[..., 0] <= w - border) * (pts[..., 1] >= border) * (pts[..., 1] <= h - border)
     )
     good_lafs_mask = good_lafs_mask.min(dim=2)[0]
     return good_lafs_mask
 
 
-def laf_to_three_points(laf: torch.Tensor):
+def laf_to_three_points(laf: Tensor):
     """Convert local affine frame(LAF) to alternative representation: coordinates of LAF center, LAF-x unit vector,
     LAF-y unit vector.
 
     Args:
         laf:  :math:`(B, N, 2, 3)`.
 
     Returns:
         threepts :math:`(B, N, 2, 3)`.
     """
     KORNIA_CHECK_LAF(laf)
-    three_pts: torch.Tensor = torch.stack([laf[..., 2] + laf[..., 0], laf[..., 2] + laf[..., 1], laf[..., 2]], dim=-1)
+    three_pts = stack([laf[..., 2] + laf[..., 0], laf[..., 2] + laf[..., 1], laf[..., 2]], dim=-1)
     return three_pts
 
 
-def laf_from_three_points(threepts: torch.Tensor):
+def laf_from_three_points(threepts: Tensor):
     """Convert three points to local affine frame.
 
     Order is (0,0), (0, 1), (1, 0).
 
     Args:
         threepts: :math:`(B, N, 2, 3)`.
 
     Returns:
         laf :math:`(B, N, 2, 3)`.
     """
-    laf: torch.Tensor = torch.stack(
-        [threepts[..., 0] - threepts[..., 2], threepts[..., 1] - threepts[..., 2], threepts[..., 2]], dim=-1
-    )
+    laf = stack([threepts[..., 0] - threepts[..., 2], threepts[..., 1] - threepts[..., 2], threepts[..., 2]], dim=-1)
     return laf
 
 
-def perspective_transform_lafs(trans_01: torch.Tensor, lafs_1: torch.Tensor) -> torch.Tensor:
+def perspective_transform_lafs(trans_01: Tensor, lafs_1: Tensor) -> Tensor:
     r"""Function that applies perspective transformations to a set of local affine frames (LAFs).
 
     Args:
         trans_01: tensor for perspective transformations of shape :math:`(B, 3, 3)`.
         lafs_1: tensor of lafs of shape :math:`(B, N, 2, 3)`.
 
     Returns:
@@ -578,15 +569,15 @@
         >>> trans_01 = torch.eye(3).repeat(2, 1, 1)  # Bx3x3
         >>> trans_01.shape
         torch.Size([2, 3, 3])
         >>> lafs_0 = perspective_transform_lafs(trans_01, lafs_1)  # BxNx2x3
     """
     KORNIA_CHECK_LAF(lafs_1)
     if not torch.is_tensor(trans_01):
-        raise TypeError("Input type is not a torch.Tensor")
+        raise TypeError("Input type is not a Tensor")
 
     if not trans_01.device == lafs_1.device:
         raise TypeError("Tensor must be in the same device")
 
     if not trans_01.shape[0] == lafs_1.shape[0]:
         raise ValueError("Input batch size must be the same for both tensors")
```

### Comparing `kornia-0.6.8/kornia/feature/loftr/backbone/resnet_fpn.py` & `kornia-0.6.9/kornia/feature/loftr/backbone/resnet_fpn.py`

 * *Files identical despite different names*

### Comparing `kornia-0.6.8/kornia/feature/loftr/loftr.py` & `kornia-0.6.9/kornia/feature/loftr/loftr.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,22 +1,24 @@
-from typing import Callable, Dict, Optional
+from typing import Any, Dict, Optional, Union
 
 import torch
-import torch.nn as nn
 
+from kornia.core import Module, Tensor
 from kornia.geometry import resize
+from kornia.utils.helpers import map_location_to_cpu
 
 from .backbone import build_backbone
 from .loftr_module import FinePreprocess, LocalFeatureTransformer
 from .utils.coarse_matching import CoarseMatching
 from .utils.fine_matching import FineMatching
 from .utils.position_encoding import PositionEncodingSine
 
 urls: Dict[str, str] = {}
 urls["outdoor"] = "http://cmp.felk.cvut.cz/~mishkdmy/models/loftr_outdoor.ckpt"
+urls["indoor_new"] = "http://cmp.felk.cvut.cz/~mishkdmy/models/loftr_indoor_ds_new.ckpt"
 urls["indoor"] = "http://cmp.felk.cvut.cz/~mishkdmy/models/loftr_indoor.ckpt"
 
 # Comments: the config below is the one corresponding to the pretrained models
 # Some do not change there anything, unless you want to retrain it.
 
 default_cfg = {
     'backbone_type': 'ResNetFPN',
@@ -43,15 +45,15 @@
         'train_coarse_percent': 0.4,
         'train_pad_num_gt_min': 200,
     },
     'fine': {'d_model': 128, 'd_ffn': 128, 'nhead': 8, 'layer_names': ['self', 'cross'], 'attention': 'linear'},
 }
 
 
-class LoFTR(nn.Module):
+class LoFTR(Module):
     r"""Module, which finds correspondences between two images.
 
     This is based on the original code from paper "LoFTR: Detector-Free Local
     Feature Matching with Transformers". See :cite:`LoFTR2021` for more details.
 
     If the distance matrix dm is not provided, :py:func:`torch.cdist` is used.
 
@@ -68,38 +70,40 @@
         >>> img1 = torch.rand(1, 1, 320, 200)
         >>> img2 = torch.rand(1, 1, 128, 128)
         >>> input = {"image0": img1, "image1": img2}
         >>> loftr = LoFTR('outdoor')
         >>> out = loftr(input)
     """
 
-    def __init__(self, pretrained: Optional[str] = 'outdoor', config: Dict = default_cfg):
+    def __init__(self, pretrained: Optional[str] = 'outdoor', config: Dict[str, Any] = default_cfg):
         super().__init__()
         # Misc
         self.config = config
+        if pretrained == 'indoor_new':
+            self.config['coarse']['temp_bug_fix'] = True
         # Modules
         self.backbone = build_backbone(config)
         self.pos_encoding = PositionEncodingSine(
             config['coarse']['d_model'], temp_bug_fix=config['coarse']['temp_bug_fix']
         )
         self.loftr_coarse = LocalFeatureTransformer(config['coarse'])
         self.coarse_matching = CoarseMatching(config['match_coarse'])
         self.fine_preprocess = FinePreprocess(config)
         self.loftr_fine = LocalFeatureTransformer(config["fine"])
         self.fine_matching = FineMatching()
         self.pretrained = pretrained
         if pretrained is not None:
             if pretrained not in urls.keys():
                 raise ValueError(f"pretrained should be None or one of {urls.keys()}")
-            storage_fcn: Callable = lambda storage, loc: storage
-            pretrained_dict = torch.hub.load_state_dict_from_url(urls[pretrained], map_location=storage_fcn)
+
+            pretrained_dict = torch.hub.load_state_dict_from_url(urls[pretrained], map_location=map_location_to_cpu)
             self.load_state_dict(pretrained_dict['state_dict'])
         self.eval()
 
-    def forward(self, data: dict) -> Dict[str, torch.Tensor]:
+    def forward(self, data: Dict[str, Tensor]) -> Dict[str, Tensor]:
         """
         Args:
             data: dictionary containing the input data in the following format:
 
         Keyword Args:
             image0: left image with shape :math:`(N, 1, H1, W1)`.
             image1: right image with shape :math:`(N, 1, H2, W2)`.
@@ -110,25 +114,27 @@
             - ``keypoints0``, matching keypoints from image0 :math:`(NC, 2)`.
             - ``keypoints1``, matching keypoints from image1 :math:`(NC, 2)`.
             - ``confidence``, confidence score [0, 1] :math:`(NC)`.
             - ``batch_indexes``, batch indexes for the keypoints and lafs :math:`(NC)`.
         """
 
         # 1. Local Feature CNN
-        data.update(
-            {'bs': data['image0'].size(0), 'hw0_i': data['image0'].shape[2:], 'hw1_i': data['image1'].shape[2:]}
-        )
+        _data: Dict[str, Union[Tensor, int, torch.Size]] = {
+            'bs': data['image0'].size(0),
+            'hw0_i': data['image0'].shape[2:],
+            'hw1_i': data['image1'].shape[2:],
+        }
 
-        if data['hw0_i'] == data['hw1_i']:  # faster & better BN convergence
+        if _data['hw0_i'] == _data['hw1_i']:  # faster & better BN convergence
             feats_c, feats_f = self.backbone(torch.cat([data['image0'], data['image1']], dim=0))
-            (feat_c0, feat_c1), (feat_f0, feat_f1) = feats_c.split(data['bs']), feats_f.split(data['bs'])
+            (feat_c0, feat_c1), (feat_f0, feat_f1) = feats_c.split(_data['bs']), feats_f.split(_data['bs'])
         else:  # handle different input shapes
             (feat_c0, feat_f0), (feat_c1, feat_f1) = self.backbone(data['image0']), self.backbone(data['image1'])
 
-        data.update(
+        _data.update(
             {
                 'hw0_c': feat_c0.shape[2:],
                 'hw1_c': feat_c1.shape[2:],
                 'hw0_f': feat_f0.shape[2:],
                 'hw1_f': feat_f1.shape[2:],
             }
         )
@@ -143,40 +149,44 @@
         feat_c0 = feat_c0.reshape(n, -1, c)
 
         feat_c1 = self.pos_encoding(feat_c1).permute(0, 2, 3, 1)
         n1, h1, w1, c1 = feat_c1.shape
         feat_c1 = feat_c1.reshape(n1, -1, c1)
 
         mask_c0 = mask_c1 = None  # mask is useful in training
-        if 'mask0' in data:
-            mask_c0 = resize(data['mask0'], data['hw0_c'], interpolation='nearest').flatten(-2)
-        if 'mask1' in data:
-            mask_c1 = resize(data['mask1'], data['hw1_c'], interpolation='nearest').flatten(-2)
+        if 'mask0' in _data:
+            mask_c0 = resize(data['mask0'], _data['hw0_c'], interpolation='nearest').flatten(-2)
+        if 'mask1' in _data:
+            mask_c1 = resize(data['mask1'], _data['hw1_c'], interpolation='nearest').flatten(-2)
         feat_c0, feat_c1 = self.loftr_coarse(feat_c0, feat_c1, mask_c0, mask_c1)
 
         # 3. match coarse-level
-        self.coarse_matching(feat_c0, feat_c1, data, mask_c0=mask_c0, mask_c1=mask_c1)
+        self.coarse_matching(feat_c0, feat_c1, _data, mask_c0=mask_c0, mask_c1=mask_c1)
 
         # 4. fine-level refinement
-        feat_f0_unfold, feat_f1_unfold = self.fine_preprocess(feat_f0, feat_f1, feat_c0, feat_c1, data)
+        feat_f0_unfold, feat_f1_unfold = self.fine_preprocess(feat_f0, feat_f1, feat_c0, feat_c1, _data)
         if feat_f0_unfold.size(0) != 0:  # at least one coarse level predicted
             feat_f0_unfold, feat_f1_unfold = self.loftr_fine(feat_f0_unfold, feat_f1_unfold)
 
         # 5. match fine-level
-        self.fine_matching(feat_f0_unfold, feat_f1_unfold, data)
+        self.fine_matching(feat_f0_unfold, feat_f1_unfold, _data)
 
         rename_keys: Dict[str, str] = {
             "mkpts0_f": 'keypoints0',
             "mkpts1_f": 'keypoints1',
             "mconf": 'confidence',
             "b_ids": 'batch_indexes',
         }
-        out = {}
+        out: Dict[str, Tensor] = {}
         for k, v in rename_keys.items():
-            out[v] = data[k]
+            _d = _data[k]
+            if isinstance(_d, Tensor):
+                out[v] = _d
+            else:
+                raise TypeError(f'Expected Tensor for item `{k}`. Gotcha {type(_d)}')
         return out
 
     def load_state_dict(self, state_dict, *args, **kwargs):
         for k in list(state_dict.keys()):
             if k.startswith('matcher.'):
                 state_dict[k.replace('matcher.', '', 1)] = state_dict.pop(k)
         return super().load_state_dict(state_dict, *args, **kwargs)
```

### Comparing `kornia-0.6.8/kornia/feature/loftr/loftr_module/fine_preprocess.py` & `kornia-0.6.9/kornia/feature/loftr/loftr_module/fine_preprocess.py`

 * *Files identical despite different names*

### Comparing `kornia-0.6.8/kornia/feature/loftr/loftr_module/linear_attention.py` & `kornia-0.6.9/kornia/feature/loftr/loftr_module/linear_attention.py`

 * *Files identical despite different names*

### Comparing `kornia-0.6.8/kornia/feature/loftr/loftr_module/transformer.py` & `kornia-0.6.9/kornia/feature/loftr/loftr_module/transformer.py`

 * *Files identical despite different names*

### Comparing `kornia-0.6.8/kornia/feature/loftr/utils/coarse_matching.py` & `kornia-0.6.9/kornia/feature/loftr/utils/coarse_matching.py`

 * *Files identical despite different names*

### Comparing `kornia-0.6.8/kornia/feature/loftr/utils/fine_matching.py` & `kornia-0.6.9/kornia/feature/loftr/utils/fine_matching.py`

 * *Files identical despite different names*

### Comparing `kornia-0.6.8/kornia/feature/loftr/utils/geometry.py` & `kornia-0.6.9/kornia/feature/loftr/utils/geometry.py`

 * *Files identical despite different names*

### Comparing `kornia-0.6.8/kornia/feature/loftr/utils/position_encoding.py` & `kornia-0.6.9/kornia/feature/loftr/utils/position_encoding.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,16 +1,19 @@
 import math
 
 import torch
-from torch import nn
 
+from kornia.core import Module, Tensor, zeros
 
-class PositionEncodingSine(nn.Module):
+
+class PositionEncodingSine(Module):
     """This is a sinusoidal position encoding that generalized to 2-dimensional images."""
 
+    pe: Tensor
+
     def __init__(self, d_model, max_shape=(256, 256), temp_bug_fix=True):
         """
         Args:
             max_shape (tuple): for 1/8 featmap, the max length of 256 corresponds to 2048 pixels
             temp_bug_fix (bool): As noted in this [issue](https://github.com/zju3dv/LoFTR/issues/41),
                 the original implementation of LoFTR includes a bug in the pos-enc impl, which has little impact
                 on the final performance. For now, we keep both impls for backward compatibility.
@@ -25,15 +28,15 @@
 
     def _create_position_encoding(self, max_shape):
         """Creates a position encoding from scratch.
 
         For 1/8 feature map (which is standard): If the input image size is H, W (both divisible by 8), the max_shape
         should be (H//8, W//8).
         """
-        pe = torch.zeros((self.d_model, *max_shape))
+        pe = zeros((self.d_model, *max_shape))
         y_position = torch.ones(max_shape).cumsum(0).float().unsqueeze(0)
         x_position = torch.ones(max_shape).cumsum(1).float().unsqueeze(0)
         if self.temp_bug_fix:
             div_term = torch.exp(
                 torch.arange(0, self.d_model // 2, 2).float() * (-math.log(10000.0) / (self.d_model // 2))
             )
         else:  # a buggy implementation (for backward compatibility only)
```

### Comparing `kornia-0.6.8/kornia/feature/loftr/utils/supervision.py` & `kornia-0.6.9/kornia/feature/loftr/utils/supervision.py`

 * *Files 2% similar despite different names*

```diff
@@ -42,17 +42,17 @@
     scale = config['LOFTR']['RESOLUTION'][0]
     scale0 = scale * data['scale0'][:, None] if 'scale0' in data else scale
     scale1 = scale * data['scale1'][:, None] if 'scale0' in data else scale
     h0, w0, h1, w1 = map(lambda x: x // scale, [H0, W0, H1, W1])
 
     # 2. warp grids
     # create kpts in meshgrid and resize them to image resolution
-    grid_pt0_c = create_meshgrid(h0, w0, False, device).reshape(1, h0 * w0, 2).repeat(N, 1, 1)  # [N, hw, 2]
+    grid_pt0_c = create_meshgrid(h0, w0, False, device).reshape(1, h0 * w0, 2).expand(N, h0 * w0, 2)  # [N, hw, 2]
     grid_pt0_i = scale0 * grid_pt0_c
-    grid_pt1_c = create_meshgrid(h1, w1, False, device).reshape(1, h1 * w1, 2).repeat(N, 1, 1)
+    grid_pt1_c = create_meshgrid(h1, w1, False, device).reshape(1, h1 * w1, 2).expand(N, h1 * w1, 2)
     grid_pt1_i = scale1 * grid_pt1_c
 
     # mask padded region to (0, 0), so no need to manually mask conf_matrix_gt
     if 'mask0' in data:
         grid_pt0_i = mask_pts_at_padded_regions(grid_pt0_i, data['mask0'])
         grid_pt1_i = mask_pts_at_padded_regions(grid_pt1_i, data['mask1'])
```

### Comparing `kornia-0.6.8/kornia/feature/matching.py` & `kornia-0.6.9/kornia/feature/matching.py`

 * *Files 1% similar despite different names*

```diff
@@ -185,16 +185,16 @@
     dists2, idx2 = match_snn(desc2, desc1, th, distance_matrix.t())
 
     if len(dists2) > 0 and len(dists1) > 0:
         idx2 = idx2.flip(1)
         if not is_mps_tensor_safe(idx1):
             idxs_dm = torch.cdist(idx1.float(), idx2.float(), p=1.0)
         else:
-            idxs2_rep = idx2.float().repeat_interleave(idx1.size(0), dim=0)
-            idxs_dm = (idx1.float().repeat(idx2.size(0), 1) - idxs2_rep).abs().sum(dim=1)
+            idxs1_rep = idx1.to(desc1).repeat_interleave(idx2.size(0), dim=0)
+            idxs_dm = (idx2.to(desc2).repeat(idx1.size(0), 1) - idxs1_rep).abs().sum(dim=1)
             idxs_dm = idxs_dm.reshape(idx1.size(0), idx2.size(0))
         mutual_idxs1 = idxs_dm.min(dim=1)[0] < 1e-8
         mutual_idxs2 = idxs_dm.min(dim=0)[0] < 1e-8
         good_idxs1 = idx1[mutual_idxs1.view(-1)]
         good_idxs2 = idx2[mutual_idxs2.view(-1)]
         dists1_good = dists1[mutual_idxs1.view(-1)]
         dists2_good = dists2[mutual_idxs2.view(-1)]
@@ -342,15 +342,15 @@
     Args:
         match_mode: type of matching, can be `fginn`.
         th: threshold on distance ratio, or other quality measure.
     """
 
     known_modes = ['fginn', "adalam"]
 
-    def __init__(self, match_mode: str = 'fginn', params: Dict = {}) -> None:
+    def __init__(self, match_mode: str = 'fginn', params: Dict[str, Tensor] = {}) -> None:
         super().__init__()
         _match_mode: str = match_mode.lower()
         if _match_mode not in self.known_modes:
             raise NotImplementedError(f"{match_mode} is not supported. Try one of {self.known_modes}")
         self.match_mode = _match_mode
         self.params = params
```

### Comparing `kornia-0.6.8/kornia/feature/mkd.py` & `kornia-0.6.9/kornia/feature/mkd.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,44 +1,46 @@
-from typing import Callable, Dict, List, Tuple, Union
+from typing import Any, Dict, List, Tuple, Union
 
 import torch
 import torch.nn as nn
 import torch.nn.functional as F
 
 from kornia.constants import pi
+from kornia.core import Tensor, tensor, zeros
 from kornia.filters import GaussianBlur2d, SpatialGradient
 from kornia.geometry.conversions import cart2pol
 from kornia.utils import create_meshgrid
+from kornia.utils.helpers import map_location_to_cpu
 
 # Precomputed coefficients for Von Mises kernel, given N and K(appa).
 sqrt2: float = 1.4142135623730951
 COEFFS_N1_K1: List[float] = [0.38214156, 0.48090413]
 COEFFS_N2_K8: List[float] = [0.14343168, 0.268285, 0.21979234]
 COEFFS_N3_K8: List[float] = [0.14343168, 0.268285, 0.21979234, 0.15838885]
 COEFFS: Dict[str, List[float]] = {'xy': COEFFS_N1_K1, 'rhophi': COEFFS_N2_K8, 'theta': COEFFS_N3_K8}
 
 urls: Dict[str, str] = {
     k: f'https://github.com/manyids2/mkd_pytorch/raw/master/mkd_pytorch/mkd-{k}-64.pth'
     for k in ['cart', 'polar', 'concat']
 }
 
 
-def get_grid_dict(patch_size: int = 32) -> Dict[str, torch.Tensor]:
+def get_grid_dict(patch_size: int = 32) -> Dict[str, Tensor]:
     r"""Get cartesian and polar parametrizations of grid."""
     kgrid = create_meshgrid(height=patch_size, width=patch_size, normalized_coordinates=True)
     x = kgrid[0, :, :, 0]
     y = kgrid[0, :, :, 1]
     rho, phi = cart2pol(x, y)
     grid_dict = {'x': x, 'y': y, 'rho': rho, 'phi': phi}
     return grid_dict
 
 
-def get_kron_order(d1: int, d2: int) -> torch.Tensor:
+def get_kron_order(d1: int, d2: int) -> Tensor:
     r"""Get order for doing kronecker product."""
-    kron_order = torch.zeros([d1 * d2, 2], dtype=torch.int64)
+    kron_order = zeros([d1 * d2, 2], dtype=torch.int64)
     for i in range(d1):
         for j in range(d2):
             kron_order[i * d2 + j, 0] = i
             kron_order[i * d2 + j, 1] = j
     return kron_order
 
 
@@ -67,17 +69,17 @@
 
     def __init__(self) -> None:
         super().__init__()
         self.eps = 1e-8
 
         self.grad = SpatialGradient(mode='diff', order=1, normalized=False)
 
-    def forward(self, x: torch.Tensor) -> torch.Tensor:
-        if not isinstance(x, torch.Tensor):
-            raise TypeError(f"Input type is not a torch.Tensor. Got {type(x)}")
+    def forward(self, x: Tensor) -> Tensor:
+        if not isinstance(x, Tensor):
+            raise TypeError(f"Input type is not a Tensor. Got {type(x)}")
         if not len(x.shape) == 4:
             raise ValueError(f"Invalid input shape, we expect Bx1xHxW. Got: {x.shape}")
         # Modify 'diff' gradient. Before we had lambda function, but it is not jittable
         grads_xy = -self.grad(x)
         gx = grads_xy[:, :, 0, :, :]
         gy = grads_xy[:, :, 1, :, :]
         y = torch.cat(cart2pol(gx, gy, self.eps), dim=1)
@@ -106,47 +108,47 @@
         >>> vm = VonMisesKernel(patch_size=32,
         ...                     coeffs=[0.14343168,
         ...                             0.268285,
         ...                             0.21979234])
         >>> emb = vm(oris) # 23x7x32x32
     """
 
-    def __init__(self, patch_size: int, coeffs: Union[list, tuple]) -> None:
+    def __init__(self, patch_size: int, coeffs: Union[List[Union[float, int]], Tuple[Union[float, int], ...]]) -> None:
         super().__init__()
 
         self.patch_size = patch_size
-        b_coeffs: torch.Tensor = torch.tensor(coeffs)
+        b_coeffs = tensor(coeffs)
         self.register_buffer('coeffs', b_coeffs)
 
         # Compute parameters.
-        n: int = len(coeffs) - 1
-        self.n: int = n
-        self.d: int = 2 * n + 1
+        n = len(coeffs) - 1
+        self.n = n
+        self.d = 2 * n + 1
 
         # Precompute helper variables.
         emb0 = torch.ones([1, 1, patch_size, patch_size])
         frange = torch.arange(n) + 1
         frange = frange.reshape(-1, 1, 1)
-        weights = torch.zeros([2 * n + 1])
+        weights = zeros([2 * n + 1])
         weights[: n + 1] = torch.sqrt(b_coeffs)
         weights[n + 1 :] = torch.sqrt(b_coeffs[1:])
         weights = weights.reshape(-1, 1, 1)
         self.register_buffer('emb0', emb0)
         self.register_buffer('frange', frange)
         self.register_buffer('weights', weights)
 
-    def forward(self, x: torch.Tensor) -> torch.Tensor:
-        if not isinstance(x, torch.Tensor):
-            raise TypeError(f"Input type is not a torch.Tensor. Got {type(x)}")
+    def forward(self, x: Tensor) -> Tensor:
+        if not isinstance(x, Tensor):
+            raise TypeError(f"Input type is not a Tensor. Got {type(x)}")
 
         if not len(x.shape) == 4 or x.shape[1] != 1:
             raise ValueError(f"Invalid input shape, we expect Bx1xHxW. Got: {x.shape}")
 
         # TODO: unify the two lines below when pytorch 1.6 support is dropped
-        emb0: torch.Tensor = torch.jit.annotate(torch.Tensor, self.emb0)
+        emb0: Tensor = torch.jit.annotate(Tensor, self.emb0)
         emb0 = emb0.to(x).repeat(x.size(0), 1, 1, 1)
         frange = self.frange.to(x) * x
         emb1 = torch.cos(frange)
         emb2 = torch.sin(frange)
         embedding = torch.cat([emb0, emb1, emb2], dim=1)
         embedding = self.weights * embedding
         return embedding
@@ -201,22 +203,22 @@
         self.kernel = VonMisesKernel(patch_size=patch_size, coeffs=COEFFS['theta'])
 
         # Relative gradients.
         kgrid = create_meshgrid(height=patch_size, width=patch_size, normalized_coordinates=True)
         _, phi = cart2pol(kgrid[:, :, :, 0], kgrid[:, :, :, 1])
         self.register_buffer('phi', phi)
 
-    def emb_mags(self, mags: torch.Tensor) -> torch.Tensor:
+    def emb_mags(self, mags: Tensor) -> Tensor:
         """Embed square roots of magnitudes with eps for numerical reasons."""
         mags = torch.sqrt(mags + self.eps)
         return mags
 
-    def forward(self, grads: torch.Tensor) -> torch.Tensor:
-        if not isinstance(grads, torch.Tensor):
-            raise TypeError(f"Input type is not a torch.Tensor. Got {type(grads)}")
+    def forward(self, grads: Tensor) -> Tensor:
+        if not isinstance(grads, Tensor):
+            raise TypeError(f"Input type is not a Tensor. Got {type(grads)}")
         if not len(grads.shape) == 4:
             raise ValueError(f"Invalid input shape, we expect Bx2xHxW. Got: {grads.shape}")
         mags = grads[:, :1, :, :]
         oris = grads[:, 1:, :, :]
         if self.relative:
             oris = oris - self.phi.to(oris)
         y = self.kernel(oris) * self.emb_mags(mags)
@@ -231,15 +233,15 @@
             + ', '
             + 'relative='
             + str(self.relative)
             + ')'
         )
 
 
-def spatial_kernel_embedding(kernel_type, grids: dict) -> torch.Tensor:
+def spatial_kernel_embedding(kernel_type, grids: Dict[str, Tensor]) -> Tensor:
     r"""Compute embeddings for cartesian and polar parametrizations."""
     factors = {"phi": 1.0, "rho": pi / sqrt2, "x": pi / 2, "y": pi / 2}
     if kernel_type == 'cart':
         coeffs_ = 'xy'
         params_ = ['x', 'y']
     elif kernel_type == 'polar':
         coeffs_ = 'rhophi'
@@ -329,33 +331,33 @@
         self.odims: int = self.out_dims
 
         # Store kronecker form.
         emb2, idx1 = self.init_kron()
         self.register_buffer('emb2', emb2)
         self.register_buffer('idx1', idx1)
 
-    def get_gmask(self, sigma: float) -> torch.Tensor:
+    def get_gmask(self, sigma: float) -> Tensor:
         """Compute Gaussian mask."""
         norm_rho = self.grid['rho'] / self.grid['rho'].max()
         gmask = torch.exp(-1 * norm_rho**2 / sigma**2)
         return gmask
 
-    def init_kron(self) -> Tuple[torch.Tensor, torch.Tensor]:
+    def init_kron(self) -> Tuple[Tensor, Tensor]:
         """Initialize helper variables to calculate kronecker."""
         kron = get_kron_order(self.in_dims, self.d_emb)
-        _emb = torch.jit.annotate(torch.Tensor, self.emb)
+        _emb = torch.jit.annotate(Tensor, self.emb)
         emb2 = torch.index_select(_emb, 1, kron[:, 1])
         return emb2, kron[:, 0]
 
-    def forward(self, x: torch.Tensor) -> torch.Tensor:
-        if not isinstance(x, torch.Tensor):
-            raise TypeError(f"Input type is not a torch.Tensor. Got {type(x)}")
+    def forward(self, x: Tensor) -> Tensor:
+        if not isinstance(x, Tensor):
+            raise TypeError(f"Input type is not a Tensor. Got {type(x)}")
         if not ((len(x.shape) == 4) | (x.shape[1] == self.in_dims)):
             raise ValueError(f"Invalid input shape, we expect Bx{self.in_dims}xHxW. Got: {x.shape}")
-        idx1 = torch.jit.annotate(torch.Tensor, self.idx1)
+        idx1 = torch.jit.annotate(Tensor, self.idx1)
         emb1 = torch.index_select(x, 1, idx1)
         output = emb1 * self.emb2
         output = output.sum(dim=(2, 3))
         if self.do_l2:
             output = F.normalize(output, dim=1)
         return output
 
@@ -388,15 +390,15 @@
     r"""Module, performs supervised or unsupervised whitening.
 
     This is based on the paper "Understanding and Improving Kernel Local Descriptors".
     See :cite:`mukundan2019understanding` for more details.
 
     Args:
         xform: Variant of whitening to use. None, 'lw', 'pca', 'pcaws', 'pcawt'.
-        whitening_model: Dictionary with keys 'mean', 'eigvecs', 'eigvals' holding torch.Tensors.
+        whitening_model: Dictionary with keys 'mean', 'eigvecs', 'eigvals' holding Tensors.
         in_dims: Dimensionality of input descriptors.
         output_dims: (int) Dimensionality reduction.
         keval: Shrinkage parameter.
         t: Attenuation parameter.
 
     Returns:
         l2-normalized, whitened descriptors.
@@ -418,15 +420,15 @@
         ...                       t=0.7)
         >>> wdescs = whitening(descs) # 23x128
     """
 
     def __init__(
         self,
         xform: str,
-        whitening_model: Union[Dict[str, Dict[str, torch.Tensor]], None],
+        whitening_model: Union[Dict[str, Dict[str, Tensor]], None],
         in_dims: int,
         output_dims: int = 128,
         keval: int = 40,
         t: float = 0.7,
     ) -> None:
         super().__init__()
 
@@ -437,22 +439,22 @@
         self.pval = 1.0
 
         # Compute true output_dims.
         output_dims = min(output_dims, in_dims)
         self.output_dims = output_dims
 
         # Initialize identity transform.
-        self.mean = nn.Parameter(torch.zeros(in_dims), requires_grad=True)
+        self.mean = nn.Parameter(zeros(in_dims), requires_grad=True)
         self.evecs = nn.Parameter(torch.eye(in_dims)[:, :output_dims], requires_grad=True)
         self.evals = nn.Parameter(torch.ones(in_dims)[:output_dims], requires_grad=True)
 
         if whitening_model is not None:
             self.load_whitening_parameters(whitening_model)
 
-    def load_whitening_parameters(self, whitening_model: Dict[str, Dict[str, torch.Tensor]]) -> None:
+    def load_whitening_parameters(self, whitening_model: Dict[str, Dict[str, Tensor]]) -> None:
         algo = 'lw' if self.xform == 'lw' else 'pca'
         wh_model = whitening_model[algo]
         self.mean.data = wh_model['mean']
         self.evecs.data = wh_model['eigvecs'][:, : self.output_dims]
         self.evals.data = wh_model['eigvals'][: self.output_dims]
 
         modifications = {
@@ -479,17 +481,17 @@
         self.evecs.data = self.evecs @ torch.diag(torch.pow(evals, -0.5))
 
     def _modify_pcawt(self) -> None:
         """Attenuation for eigenvalues."""
         m = -0.5 * self.t
         self.evecs.data = self.evecs @ torch.diag(torch.pow(self.evals, m))
 
-    def forward(self, x: torch.Tensor) -> torch.Tensor:
-        if not isinstance(x, torch.Tensor):
-            raise TypeError(f"Input type is not a torch.Tensor. Got {type(x)}")
+    def forward(self, x: Tensor) -> Tensor:
+        if not isinstance(x, Tensor):
+            raise TypeError(f"Input type is not a Tensor. Got {type(x)}")
         if not len(x.shape) == 2:
             raise ValueError(f"Invalid input shape, we expect NxD. Got: {x.shape}")
         x = x - self.mean  # Center the data.
         x = x @ self.evecs  # Apply rotation and/or scaling.
         x = torch.sign(x) * torch.pow(torch.abs(x), self.pval)  # Powerlaw.
         return F.normalize(x, dim=1)
 
@@ -575,26 +577,27 @@
             self.feats[parametrization] = nn.Sequential(gradient_embedding, spatial_encoding)
             self.odims += spatial_encoding.odims
         # Compute true output_dims.
         self.output_dims: int = min(output_dims, self.odims)
 
         # Load supervised(lw)/unsupervised(pca) model trained on training_set.
         if self.whitening is not None:
-            storage_fcn: Callable = lambda storage, loc: storage
-            whitening_models = torch.hub.load_state_dict_from_url(urls[self.kernel_type], map_location=storage_fcn)
+            whitening_models = torch.hub.load_state_dict_from_url(
+                urls[self.kernel_type], map_location=map_location_to_cpu
+            )
             whitening_model = whitening_models[training_set]
             self.whitening_layer = Whitening(
                 whitening, whitening_model, in_dims=self.odims, output_dims=self.output_dims
             )
             self.odims = self.output_dims
         self.eval()
 
-    def forward(self, patches: torch.Tensor) -> torch.Tensor:
-        if not isinstance(patches, torch.Tensor):
-            raise TypeError(f"Input type is not a torch.Tensor. Got {type(patches)}")
+    def forward(self, patches: Tensor) -> Tensor:
+        if not isinstance(patches, Tensor):
+            raise TypeError(f"Input type is not a Tensor. Got {type(patches)}")
         if not len(patches.shape) == 4:
             raise ValueError(f"Invalid input shape, we expect Bx1xHxW. Got: {patches.shape}")
         # Extract gradients.
         g = self.smoothing(patches)
         g = self.gradients(g)
 
         # Extract polar/cart features.
@@ -633,17 +636,16 @@
             + ', '
             + 'output_dims='
             + str(self.output_dims)
             + ')'
         )
 
 
-def load_whitening_model(kernel_type: str, training_set: str) -> Dict:
-    storage_fcn: Callable = lambda storage, loc: storage
-    whitening_models = torch.hub.load_state_dict_from_url(urls[kernel_type], map_location=storage_fcn)
+def load_whitening_model(kernel_type: str, training_set: str) -> Dict[str, Any]:
+    whitening_models = torch.hub.load_state_dict_from_url(urls[kernel_type], map_location=map_location_to_cpu)
     whitening_model = whitening_models[training_set]
     return whitening_model
 
 
 class SimpleKD(nn.Module):
     """Example to write custom Kernel Descriptors."""
 
@@ -667,9 +669,9 @@
         ese = ExplicitSpacialEncoding(kernel_type=kernel_type, fmap_size=patch_size, in_dims=ori.kernel.d)
         wh = Whitening(
             whitening, load_whitening_model(kernel_type, training_set), in_dims=ese.odims, output_dims=output_dims
         )
 
         self.features = nn.Sequential(smoothing, gradients, ori, ese, wh)
 
-    def forward(self, x: torch.Tensor) -> torch.Tensor:
+    def forward(self, x: Tensor) -> Tensor:
         return self.features(x)
```

### Comparing `kornia-0.6.8/kornia/feature/orientation.py` & `kornia-0.6.9/kornia/feature/orientation.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,18 +1,19 @@
 import math
-from typing import Callable, Dict, Optional
+from typing import Dict, Optional
 
 import torch
 import torch.nn as nn
 import torch.nn.functional as F
 
 from kornia.constants import pi
 from kornia.filters import SpatialGradient, get_gaussian_kernel2d
 from kornia.geometry import rad2deg
 from kornia.testing import KORNIA_CHECK_LAF, KORNIA_CHECK_SHAPE
+from kornia.utils.helpers import map_location_to_cpu
 
 from .laf import extract_patches_from_pyramid, get_laf_orientation, set_laf_orientation
 
 urls: Dict[str, str] = {}
 urls["orinet"] = "https://github.com/ducha-aiki/affnet/raw/master/pretrained/OriNet.pth"
 
 
@@ -161,16 +162,15 @@
             nn.Conv2d(64, 2, kernel_size=8, stride=1, padding=1, bias=True),
             nn.Tanh(),
             nn.AdaptiveAvgPool2d(1),
         )
         self.eps = eps
         # use torch.hub to load pretrained model
         if pretrained:
-            storage_fcn: Callable = lambda storage, loc: storage
-            pretrained_dict = torch.hub.load_state_dict_from_url(urls['orinet'], map_location=storage_fcn)
+            pretrained_dict = torch.hub.load_state_dict_from_url(urls['orinet'], map_location=map_location_to_cpu)
             self.load_state_dict(pretrained_dict['state_dict'], strict=False)
         self.eval()
 
     @staticmethod
     def _normalize_input(x: torch.Tensor, eps: float = 1e-6) -> torch.Tensor:
         """Utility function that normalizes the input by batch.""" ""
         sp, mp = torch.std_mean(x, dim=(-3, -2, -1), keepdim=True)
```

### Comparing `kornia-0.6.8/kornia/feature/responses.py` & `kornia-0.6.9/kornia/feature/responses.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,22 +1,19 @@
 from typing import Optional, Union
 
 import torch
-import torch.nn as nn
 
+from kornia.core import Module, Tensor, tensor
 from kornia.filters import gaussian_blur2d, spatial_gradient
 from kornia.testing import KORNIA_CHECK_SHAPE
 
 
 def harris_response(
-    input: torch.Tensor,
-    k: Union[torch.Tensor, float] = 0.04,
-    grads_mode: str = 'sobel',
-    sigmas: Optional[torch.Tensor] = None,
-) -> torch.Tensor:
+    input: Tensor, k: Union[Tensor, float] = 0.04, grads_mode: str = 'sobel', sigmas: Optional[Tensor] = None
+) -> Tensor:
     r"""Compute the Harris cornerness function.
 
     Function does not do any normalization or nms. The response map is computed according the following formulation:
 
     .. math::
         R = max(0, det(M) - k \cdot trace(M)^2)
 
@@ -63,44 +60,42 @@
                   [0.0039, 0.0065, 0.0040, 0.0000, 0.0040, 0.0065, 0.0039],
                   [0.0012, 0.0039, 0.0020, 0.0000, 0.0020, 0.0039, 0.0012]]]])
     """
     # TODO: Recompute doctest
     KORNIA_CHECK_SHAPE(input, ["B", "C", "H", "W"])
 
     if sigmas is not None:
-        if not isinstance(sigmas, torch.Tensor):
-            raise TypeError(f"sigmas type is not a torch.Tensor. Got {type(sigmas)}")
+        if not isinstance(sigmas, Tensor):
+            raise TypeError(f"sigmas type is not a Tensor. Got {type(sigmas)}")
         if (not len(sigmas.shape) == 1) or (sigmas.size(0) != input.size(0)):
             raise ValueError(f"Invalid sigmas shape, we expect B == input.size(0). Got: {sigmas.shape}")
 
-    gradients: torch.Tensor = spatial_gradient(input, grads_mode)
-    dx: torch.Tensor = gradients[:, :, 0]
-    dy: torch.Tensor = gradients[:, :, 1]
+    gradients: Tensor = spatial_gradient(input, grads_mode)
+    dx: Tensor = gradients[:, :, 0]
+    dy: Tensor = gradients[:, :, 1]
 
     # compute the structure tensor M elements
 
-    dx2: torch.Tensor = gaussian_blur2d(dx**2, (7, 7), (1.0, 1.0))
-    dy2: torch.Tensor = gaussian_blur2d(dy**2, (7, 7), (1.0, 1.0))
-    dxy: torch.Tensor = gaussian_blur2d(dx * dy, (7, 7), (1.0, 1.0))
+    dx2: Tensor = gaussian_blur2d(dx**2, (7, 7), (1.0, 1.0))
+    dy2: Tensor = gaussian_blur2d(dy**2, (7, 7), (1.0, 1.0))
+    dxy: Tensor = gaussian_blur2d(dx * dy, (7, 7), (1.0, 1.0))
 
-    det_m: torch.Tensor = dx2 * dy2 - dxy * dxy
-    trace_m: torch.Tensor = dx2 + dy2
+    det_m: Tensor = dx2 * dy2 - dxy * dxy
+    trace_m: Tensor = dx2 + dy2
 
     # compute the response map
-    scores: torch.Tensor = det_m - k * (trace_m**2)
+    scores: Tensor = det_m - k * (trace_m**2)
 
     if sigmas is not None:
         scores = scores * sigmas.pow(4).view(-1, 1, 1, 1)
 
     return scores
 
 
-def gftt_response(
-    input: torch.Tensor, grads_mode: str = 'sobel', sigmas: Optional[torch.Tensor] = None
-) -> torch.Tensor:
+def gftt_response(input: Tensor, grads_mode: str = 'sobel', sigmas: Optional[Tensor] = None) -> Tensor:
     r"""Compute the Shi-Tomasi cornerness function.
 
     Function does not do any normalization or nms. The response map is computed according the following formulation:
 
     .. math::
         R = min(eig(M))
 
@@ -142,39 +137,37 @@
                   [0.0194, 0.0339, 0.0497, 0.0000, 0.0497, 0.0339, 0.0194],
                   [0.0334, 0.0575, 0.0339, 0.0000, 0.0339, 0.0575, 0.0334],
                   [0.0155, 0.0334, 0.0194, 0.0000, 0.0194, 0.0334, 0.0155]]]])
     """
     # TODO: Recompute doctest
     KORNIA_CHECK_SHAPE(input, ["B", "C", "H", "W"])
 
-    gradients: torch.Tensor = spatial_gradient(input, grads_mode)
-    dx: torch.Tensor = gradients[:, :, 0]
-    dy: torch.Tensor = gradients[:, :, 1]
+    gradients: Tensor = spatial_gradient(input, grads_mode)
+    dx: Tensor = gradients[:, :, 0]
+    dy: Tensor = gradients[:, :, 1]
 
-    dx2: torch.Tensor = gaussian_blur2d(dx**2, (7, 7), (1.0, 1.0))
-    dy2: torch.Tensor = gaussian_blur2d(dy**2, (7, 7), (1.0, 1.0))
-    dxy: torch.Tensor = gaussian_blur2d(dx * dy, (7, 7), (1.0, 1.0))
+    dx2: Tensor = gaussian_blur2d(dx**2, (7, 7), (1.0, 1.0))
+    dy2: Tensor = gaussian_blur2d(dy**2, (7, 7), (1.0, 1.0))
+    dxy: Tensor = gaussian_blur2d(dx * dy, (7, 7), (1.0, 1.0))
 
-    det_m: torch.Tensor = dx2 * dy2 - dxy * dxy
-    trace_m: torch.Tensor = dx2 + dy2
+    det_m: Tensor = dx2 * dy2 - dxy * dxy
+    trace_m: Tensor = dx2 + dy2
 
-    e1: torch.Tensor = 0.5 * (trace_m + torch.sqrt((trace_m**2 - 4 * det_m).abs()))
-    e2: torch.Tensor = 0.5 * (trace_m - torch.sqrt((trace_m**2 - 4 * det_m).abs()))
+    e1: Tensor = 0.5 * (trace_m + torch.sqrt((trace_m**2 - 4 * det_m).abs()))
+    e2: Tensor = 0.5 * (trace_m - torch.sqrt((trace_m**2 - 4 * det_m).abs()))
 
-    scores: torch.Tensor = torch.min(e1, e2)
+    scores: Tensor = torch.min(e1, e2)
 
     if sigmas is not None:
         scores = scores * sigmas.pow(4).view(-1, 1, 1, 1)
 
     return scores
 
 
-def hessian_response(
-    input: torch.Tensor, grads_mode: str = 'sobel', sigmas: Optional[torch.Tensor] = None
-) -> torch.Tensor:
+def hessian_response(input: Tensor, grads_mode: str = 'sobel', sigmas: Optional[Tensor] = None) -> Tensor:
     r"""Compute the absolute of determinant of the Hessian matrix.
 
     Function does not do any normalization or nms. The response map is computed according the following formulation:
 
     .. math::
         R = det(H)
 
@@ -221,113 +214,114 @@
                   [0.0334, 0.0575, 0.0339, 0.0000, 0.0339, 0.0575, 0.0334],
                   [0.0155, 0.0334, 0.0194, 0.0000, 0.0194, 0.0334, 0.0155]]]])
     """
     # TODO: Recompute doctest
     KORNIA_CHECK_SHAPE(input, ["B", "C", "H", "W"])
 
     if sigmas is not None:
-        if not isinstance(sigmas, torch.Tensor):
-            raise TypeError(f"sigmas type is not a torch.Tensor. Got {type(sigmas)}")
+        if not isinstance(sigmas, Tensor):
+            raise TypeError(f"sigmas type is not a Tensor. Got {type(sigmas)}")
 
         if (not len(sigmas.shape) == 1) or (sigmas.size(0) != input.size(0)):
             raise ValueError(f"Invalid sigmas shape, we expect B == input.size(0). Got: {sigmas.shape}")
 
-    gradients: torch.Tensor = spatial_gradient(input, grads_mode, 2)
-    dxx: torch.Tensor = gradients[:, :, 0]
-    dxy: torch.Tensor = gradients[:, :, 1]
-    dyy: torch.Tensor = gradients[:, :, 2]
+    gradients: Tensor = spatial_gradient(input, grads_mode, 2)
+    dxx: Tensor = gradients[:, :, 0]
+    dxy: Tensor = gradients[:, :, 1]
+    dyy: Tensor = gradients[:, :, 2]
 
-    scores: torch.Tensor = dxx * dyy - dxy**2
+    scores: Tensor = dxx * dyy - dxy**2
 
     if sigmas is not None:
         scores = scores * sigmas.pow(4).view(-1, 1, 1, 1)
 
     return scores
 
 
-def dog_response(input: torch.Tensor) -> torch.Tensor:
+def dog_response(input: Tensor) -> Tensor:
     r"""Compute the Difference-of-Gaussian response.
 
     Args:
         input: a given the gaussian 5d tensor :math:`(B, C, D, H, W)`.
 
     Return:
         the response map per channel with shape :math:`(B, C, D-1, H, W)`.
     """
     KORNIA_CHECK_SHAPE(input, ["B", "C", "L", "H", "W"])
 
     return input[:, :, 1:] - input[:, :, :-1]
 
 
-class BlobDoG(nn.Module):
+class BlobDoG(Module):
     r"""Module that calculates Difference-of-Gaussians blobs.
 
     See :func:`~kornia.feature.dog_response` for details.
     """
 
     def __init__(self) -> None:
         super().__init__()
         return
 
     def __repr__(self) -> str:
         return self.__class__.__name__
 
-    def forward(self, input: torch.Tensor, sigmas: Optional[torch.Tensor] = None) -> torch.Tensor:  # type: ignore
-        return dog_response(input)  # type: ignore
+    def forward(self, input: Tensor, sigmas: Optional[Tensor] = None) -> Tensor:
+        return dog_response(input)
 
 
-class CornerHarris(nn.Module):
+class CornerHarris(Module):
     r"""Module that calculates Harris corners.
 
     See :func:`~kornia.feature.harris_response` for details.
     """
+    k: Tensor
 
-    def __init__(self, k: Union[float, torch.Tensor], grads_mode='sobel') -> None:
+    def __init__(self, k: Union[float, Tensor], grads_mode='sobel') -> None:
         super().__init__()
-        if type(k) is float:
-            self.register_buffer('k', torch.tensor(k))
+        if isinstance(k, float):
+            self.register_buffer('k', tensor(k))
         else:
-            self.register_buffer('k', k)  # type: ignore
+            self.register_buffer('k', k)
         self.grads_mode: str = grads_mode
         return
 
     def __repr__(self) -> str:
         return self.__class__.__name__ + '(k=' + str(self.k) + ', ' + 'grads_mode=' + self.grads_mode + ')'
 
-    def forward(self, input: torch.Tensor, sigmas: Optional[torch.Tensor] = None) -> torch.Tensor:  # type: ignore
-        return harris_response(input, self.k, self.grads_mode, sigmas)  # type: ignore
+    def forward(self, input: Tensor, sigmas: Optional[Tensor] = None) -> Tensor:
+        return harris_response(input, self.k, self.grads_mode, sigmas)
 
 
-class CornerGFTT(nn.Module):
+class CornerGFTT(Module):
     r"""Module that calculates Shi-Tomasi corners.
 
     See :func:`~kornia.feature.gfft_response` for details.
     """
 
     def __init__(self, grads_mode='sobel') -> None:
         super().__init__()
         self.grads_mode: str = grads_mode
         return
 
     def __repr__(self) -> str:
         return self.__class__.__name__ + 'grads_mode=' + self.grads_mode + ')'
 
-    def forward(self, input: torch.Tensor, sigmas: Optional[torch.Tensor] = None) -> torch.Tensor:  # type: ignore
+    def forward(self, input: Tensor, sigmas: Optional[Tensor] = None) -> Tensor:
         return gftt_response(input, self.grads_mode, sigmas)
 
 
-class BlobHessian(nn.Module):
+class BlobHessian(Module):
     r"""Module that calculates Hessian blobs.
 
     See :func:`~kornia.feature.hessian_response` for details.
     """
 
     def __init__(self, grads_mode='sobel') -> None:
         super().__init__()
         self.grads_mode: str = grads_mode
         return
 
     def __repr__(self) -> str:
         return self.__class__.__name__ + 'grads_mode=' + self.grads_mode + ')'
 
-    def forward(self, input: torch.Tensor, sigmas: Optional[torch.Tensor] = None) -> torch.Tensor:  # type: ignore
+    def forward(self, input: Tensor, sigmas: Optional[Tensor] = None) -> Tensor:
         return hessian_response(input, self.grads_mode, sigmas)
```

### Comparing `kornia-0.6.8/kornia/feature/scale_space_detector.py` & `kornia-0.6.9/kornia/feature/scale_space_detector.py`

 * *Files 15% similar despite different names*

```diff
@@ -1,22 +1,22 @@
 from typing import List, Optional, Tuple
 
 import torch
-import torch.nn as nn
 import torch.nn.functional as F
 
+from kornia.core import Device, Module, Tensor, concatenate, eye
 from kornia.geometry.subpix import ConvSoftArgmax3d
 from kornia.geometry.transform import ScalePyramid
 
 from .laf import denormalize_laf, laf_is_inside_image, normalize_laf
 from .orientation import PassLAF
 from .responses import BlobHessian
 
 
-def _scale_index_to_scale(max_coords: torch.Tensor, sigmas: torch.Tensor, num_levels: int) -> torch.Tensor:
+def _scale_index_to_scale(max_coords: Tensor, sigmas: Tensor, num_levels: int) -> Tensor:
     r"""Auxiliary function for ScaleSpaceDetector. Converts scale level index from ConvSoftArgmax3d to the actual
     scale, using the sigmas from the ScalePyramid output.
 
     Args:
         max_coords: tensor [BxNx3].
         sigmas: tensor [BxNxD], D >= 1
 
@@ -28,28 +28,28 @@
     # Because grid_sample is for 4d input only, we will create fake 2nd dimension
     # ToDo: replace with 3d input, when grid_sample will start to support it
 
     # Reshape for grid shape
     B, N, _ = max_coords.shape
     scale_coords = max_coords[:, :, 0].contiguous().view(-1, 1, 1, 1)
     # Replace the scale_x_y
-    out = torch.cat(
-        [sigmas[0, 0] * torch.pow(2.0, scale_coords / float(num_levels)).view(B, N, 1), max_coords[:, :, 1:]], dim=2
+    out = concatenate(
+        [sigmas[0, 0] * torch.pow(2.0, scale_coords / float(num_levels)).view(B, N, 1), max_coords[:, :, 1:]], 2
     )
     return out
 
 
-def _create_octave_mask(mask: torch.Tensor, octave_shape: List[int]) -> torch.Tensor:
+def _create_octave_mask(mask: Tensor, octave_shape: List[int]) -> Tensor:
     r"""Downsample a mask based on the given octave shape."""
     mask_shape = octave_shape[-2:]
-    mask_octave = F.interpolate(mask, mask_shape, mode='bilinear', align_corners=False)  # type: ignore
+    mask_octave = F.interpolate(mask, mask_shape, mode='bilinear', align_corners=False)
     return mask_octave.unsqueeze(1)
 
 
-class ScaleSpaceDetector(nn.Module):
+class ScaleSpaceDetector(Module):
     r"""Module for differentiable local feature detection, as close as possible to classical local feature detectors
     like Harris, Hessian-Affine or SIFT (DoG).
 
     It has 5 modules inside: scale pyramid generator, response ("cornerness") function,
     soft nms function, affine shape estimator and patch orientation estimator.
     Each of those modules could be replaced with learned custom one, as long, as
     they respect output shape.
@@ -72,21 +72,21 @@
             Useful for symmetric response functions like DoG or Hessian. Default is False
     """
 
     def __init__(
         self,
         num_features: int = 500,
         mr_size: float = 6.0,
-        scale_pyr_module: nn.Module = ScalePyramid(3, 1.6, 15),
-        resp_module: nn.Module = BlobHessian(),
-        nms_module: nn.Module = ConvSoftArgmax3d(
+        scale_pyr_module: Module = ScalePyramid(3, 1.6, 15),
+        resp_module: Module = BlobHessian(),
+        nms_module: Module = ConvSoftArgmax3d(
             (3, 3, 3), (1, 1, 1), (1, 1, 1), normalized_coordinates=False, output_value=True
         ),
-        ori_module: nn.Module = PassLAF(),
-        aff_module: nn.Module = PassLAF(),
+        ori_module: Module = PassLAF(),
+        aff_module: Module = PassLAF(),
         minima_are_also_good: bool = False,
         scale_space_response=False,
     ):
         super().__init__()
         self.mr_size = mr_size
         self.num_features = num_features
         self.scale_pyr = scale_pyr_module
@@ -121,43 +121,44 @@
             + self.ori.__repr__()
             + ', '
             + 'aff='
             + self.aff.__repr__()
             + ')'
         )
 
-    def detect(
-        self, img: torch.Tensor, num_feats: int, mask: Optional[torch.Tensor] = None
-    ) -> Tuple[torch.Tensor, torch.Tensor]:
-        dev: torch.device = img.device
+    def detect(self, img: Tensor, num_feats: int, mask: Optional[Tensor] = None) -> Tuple[Tensor, Tensor]:
+        dev: Device = img.device
         dtype: torch.dtype = img.dtype
+        sigmas: List[Tensor]
         sp, sigmas, _ = self.scale_pyr(img)
-        all_responses: List[torch.Tensor] = []
-        all_lafs: List[torch.Tensor] = []
+        all_responses: List[Tensor] = []
+        all_lafs: List[Tensor] = []
         for oct_idx, octave in enumerate(sp):
             sigmas_oct = sigmas[oct_idx]
             B, CH, L, H, W = octave.size()
             # Run response function
             if self.scale_space_response:
                 oct_resp = self.resp(octave, sigmas_oct.view(-1))
             else:
                 oct_resp = self.resp(octave.permute(0, 2, 1, 3, 4).reshape(B * L, CH, H, W), sigmas_oct.view(-1)).view(
                     B, L, CH, H, W
                 )
                 # We want nms for scale responses, so reorder to (B, CH, L, H, W)
                 oct_resp = oct_resp.permute(0, 2, 1, 3, 4)
                 # 3rd extra level is required for DoG only
-                if self.scale_pyr.extra_levels % 2 != 0:  # type: ignore
+                if isinstance(self.scale_pyr.extra_levels, Tensor) and self.scale_pyr.extra_levels % 2 != 0:
                     oct_resp = oct_resp[:, :, :-1]
 
             if mask is not None:
-                oct_mask: torch.Tensor = _create_octave_mask(mask, oct_resp.shape)
+                oct_mask: Tensor = _create_octave_mask(mask, oct_resp.shape)
                 oct_resp = oct_mask * oct_resp
 
             # Differentiable nms
+            coord_max: Tensor
+            response_max: Tensor
             coord_max, response_max = self.nms(oct_resp)
             if self.minima_are_also_good:
                 coord_min, response_min = self.nms(-oct_resp)
                 take_min_mask = (response_min > response_max).to(response_max.dtype)
                 response_max = response_min * take_min_mask + (1 - take_min_mask) * response_max
                 coord_max = coord_min * take_min_mask.unsqueeze(2) + (1 - take_min_mask.unsqueeze(2)) * coord_max
 
@@ -170,48 +171,55 @@
                 max_coords_best = torch.gather(max_coords_flatten, 1, idxs.unsqueeze(-1).repeat(1, 1, 3))
             else:
                 resp_flat_best = responses_flatten
                 max_coords_best = max_coords_flatten
             B, N = resp_flat_best.size()
 
             # Converts scale level index from ConvSoftArgmax3d to the actual scale, using the sigmas
-            max_coords_best = _scale_index_to_scale(
-                max_coords_best, sigmas_oct, self.scale_pyr.n_levels  # type: ignore
-            )
+
+            if isinstance(self.scale_pyr.n_levels, Tensor):
+                num_levels = int(self.scale_pyr.n_levels.item())
+            elif isinstance(self.scale_pyr.n_levels, int):
+                num_levels = self.scale_pyr.n_levels
+            else:
+                raise TypeError(
+                    'Expected the scale pyramid module to have `n_levels` as a Tensor or int.'
+                    f'Gotcha {type(self.scale_pyr.n_levels)}'
+                )
+
+            max_coords_best = _scale_index_to_scale(max_coords_best, sigmas_oct, num_levels)
 
             # Create local affine frames (LAFs)
-            rotmat = torch.eye(2, dtype=dtype, device=dev).view(1, 1, 2, 2)
-            current_lafs = torch.cat(
+            rotmat = eye(2, dtype=dtype, device=dev).view(1, 1, 2, 2)
+            current_lafs = concatenate(
                 [
                     self.mr_size * max_coords_best[:, :, 0].view(B, N, 1, 1) * rotmat,
                     max_coords_best[:, :, 1:3].view(B, N, 2, 1),
                 ],
-                dim=3,
+                3,
             )
 
             # Zero response lafs, which touch the boundary
             good_mask = laf_is_inside_image(current_lafs, octave[:, 0])
             resp_flat_best = resp_flat_best * good_mask.to(dev, dtype)
 
             # Normalize LAFs
             current_lafs = normalize_laf(current_lafs, octave[:, 0])  # We don`t need # of scale levels, only shape
 
             all_responses.append(resp_flat_best)
             all_lafs.append(current_lafs)
 
         # Sort and keep best n
-        responses: torch.Tensor = torch.cat(all_responses, dim=1)
-        lafs: torch.Tensor = torch.cat(all_lafs, dim=1)
+        responses = concatenate(all_responses, 1)
+        lafs = concatenate(all_lafs, 1)
         responses, idxs = torch.topk(responses, k=num_feats, dim=1)
         lafs = torch.gather(lafs, 1, idxs.unsqueeze(-1).unsqueeze(-1).repeat(1, 1, 2, 3))
         return responses, denormalize_laf(lafs, img)
 
-    def forward(  # type: ignore
-        self, img: torch.Tensor, mask: Optional[torch.Tensor] = None  # type: ignore
-    ) -> Tuple[torch.Tensor, torch.Tensor]:
+    def forward(self, img: Tensor, mask: Optional[Tensor] = None) -> Tuple[Tensor, Tensor]:
         """Three stage local feature detection. First the location and scale of interest points are determined by
         detect function. Then affine shape and orientation.
 
         Args:
             img: image to extract features with shape [BxCxHxW]
             mask: a mask with weights where to apply the response function. The shape must be the same as
               the input image.
```

### Comparing `kornia-0.6.8/kornia/feature/siftdesc.py` & `kornia-0.6.9/kornia/feature/siftdesc.py`

 * *Files 11% similar despite different names*

```diff
@@ -1,42 +1,42 @@
 import math
 from typing import Tuple
 
 import torch
 import torch.nn as nn
-import torch.nn.functional as F
 
+from kornia.core import Module, Tensor, concatenate, eye, normalize
 from kornia.filters import get_gaussian_kernel2d, spatial_gradient
 from kornia.geometry.conversions import pi
 from kornia.testing import KORNIA_CHECK_SHAPE
 
 
-def _get_reshape_kernel(kd: int, ky: int, kx: int) -> torch.Tensor:
+def _get_reshape_kernel(kd: int, ky: int, kx: int) -> Tensor:
     """Utility function, which returns neigh2channels conv kernel."""
     numel: int = kd * ky * kx
-    weight = torch.eye(numel)
+    weight = eye(numel)
     return weight.view(numel, kd, ky, kx)
 
 
-def get_sift_pooling_kernel(ksize: int = 25) -> torch.Tensor:
+def get_sift_pooling_kernel(ksize: int = 25) -> Tensor:
     r"""Return a weighted pooling kernel for SIFT descriptor.
 
     Args:
         ksize: kernel_size.
 
     Returns:
         the pooling kernel with shape :math:`(ksize, ksize)`.
     """
     ks_2: float = float(ksize) / 2.0
-    xc2: torch.Tensor = ks_2 - (torch.arange(ksize).float() + 0.5 - ks_2).abs()  # type: ignore
-    kernel: torch.Tensor = torch.ger(xc2, xc2) / (ks_2**2)
+    xc2 = ks_2 - (torch.arange(ksize).float() + 0.5 - ks_2).abs()
+    kernel = torch.ger(xc2, xc2) / (ks_2**2)
     return kernel
 
 
-def get_sift_bin_ksize_stride_pad(patch_size: int, num_spatial_bins: int) -> Tuple:
+def get_sift_bin_ksize_stride_pad(patch_size: int, num_spatial_bins: int) -> Tuple[int, int, int]:
     r"""Return a tuple with SIFT parameters.
 
     Args:
         patch_size: the given patch size.
         num_spatial_bins: the ggiven number of spatial bins.
 
     Returns:
@@ -52,15 +52,15 @@
             requested number of spatial bins {num_spatial_bins} \
             for SIFT descriptor. Usually it happens when patch size is too small\
             for num_spatial_bins specified"
         )
     return ksize, stride, pad
 
 
-class SIFTDescriptor(nn.Module):
+class SIFTDescriptor(Module):
     r"""Module which computes SIFT descriptors of given patches.
 
     Args:
         patch_size: Input patch size in pixels.
         num_ang_bins: Number of angular bins.
         num_spatial_bins: Number of spatial bins.
         clipval: clipping value to reduce single-bin dominance
@@ -127,89 +127,91 @@
             1,
             1,
             kernel_size=(nw.size(0), nw.size(1)),
             stride=(self.bin_stride, self.bin_stride),
             padding=(self.pad, self.pad),
             bias=False,
         )
-        self.pk.weight.data.copy_(nw.reshape(1, 1, nw.size(0), nw.size(1)))  # type: ignore
+        self.pk.weight.data.copy_(nw.reshape(1, 1, nw.size(0), nw.size(1)))
         return
 
-    def get_pooling_kernel(self) -> torch.Tensor:
+    def get_pooling_kernel(self) -> Tensor:
         return self.pk.weight.detach()
 
-    def get_weighting_kernel(self) -> torch.Tensor:
+    def get_weighting_kernel(self) -> Tensor:
         return self.gk.detach()
 
     def forward(self, input):
         KORNIA_CHECK_SHAPE(input, ["B", "1", f"{self.patch_size}", f"{self.patch_size}"])
         B: int = input.shape[0]
         self.pk = self.pk.to(input.dtype).to(input.device)
 
-        grads: torch.Tensor = spatial_gradient(input, 'diff')
+        grads = spatial_gradient(input, 'diff')
         # unpack the edges
-        gx: torch.Tensor = grads[:, :, 0]
-        gy: torch.Tensor = grads[:, :, 1]
+        gx = grads[:, :, 0]
+        gy = grads[:, :, 1]
 
-        mag: torch.Tensor = torch.sqrt(gx * gx + gy * gy + self.eps)
-        ori: torch.Tensor = torch.atan2(gy, gx + self.eps) + 2.0 * pi
+        mag = torch.sqrt(gx * gx + gy * gy + self.eps)
+        ori = torch.atan2(gy, gx + self.eps) + 2.0 * pi
         mag = mag * self.gk.expand_as(mag).type_as(mag).to(mag.device)
-        o_big: torch.Tensor = float(self.num_ang_bins) * ori / (2.0 * pi)
+        o_big = float(self.num_ang_bins) * ori / (2.0 * pi)
 
-        bo0_big_: torch.Tensor = torch.floor(o_big)
-        wo1_big_: torch.Tensor = o_big - bo0_big_
-        bo0_big: torch.Tensor = bo0_big_ % self.num_ang_bins
-        bo1_big: torch.Tensor = (bo0_big + 1) % self.num_ang_bins
-        wo0_big: torch.Tensor = (1.0 - wo1_big_) * mag  # type: ignore
-        wo1_big: torch.Tensor = wo1_big_ * mag
-
-        ang_bins = []
-        for i in range(0, self.num_ang_bins):
-            out = self.pk((bo0_big == i).to(input.dtype) * wo0_big + (bo1_big == i).to(input.dtype) * wo1_big)
-            ang_bins.append(out)
-        ang_bins = torch.cat(ang_bins, dim=1)
+        bo0_big_ = torch.floor(o_big)
+        wo1_big_ = o_big - bo0_big_
+        bo0_big = bo0_big_ % self.num_ang_bins
+        bo1_big = (bo0_big + 1) % self.num_ang_bins
+        wo0_big = (1.0 - wo1_big_) * mag
+        wo1_big = wo1_big_ * mag
+
+        ang_bins = concatenate(
+            [
+                self.pk((bo0_big == i).to(input.dtype) * wo0_big + (bo1_big == i).to(input.dtype) * wo1_big)
+                for i in range(0, self.num_ang_bins)
+            ],
+            1,
+        )
         ang_bins = ang_bins.view(B, -1)
-        ang_bins = F.normalize(ang_bins, p=2)
+        ang_bins = normalize(ang_bins, p=2)
         ang_bins = torch.clamp(ang_bins, 0.0, float(self.clipval))
-        ang_bins = F.normalize(ang_bins, p=2)
+        ang_bins = normalize(ang_bins, p=2)
         if self.rootsift:
-            ang_bins = torch.sqrt(F.normalize(ang_bins, p=1) + self.eps)
+            ang_bins = torch.sqrt(normalize(ang_bins, p=1) + self.eps)
         return ang_bins
 
 
 def sift_describe(
-    input: torch.Tensor,
+    input: Tensor,
     patch_size: int = 41,
     num_ang_bins: int = 8,
     num_spatial_bins: int = 4,
     rootsift: bool = True,
     clipval: float = 0.2,
-) -> torch.Tensor:
+) -> Tensor:
     r"""Computes the sift descriptor.
 
     See :class:`~kornia.feature.SIFTDescriptor` for details.
     """
     return SIFTDescriptor(patch_size, num_ang_bins, num_spatial_bins, rootsift, clipval)(input)
 
 
-class DenseSIFTDescriptor(nn.Module):
+class DenseSIFTDescriptor(Module):
     """Module, which computes SIFT descriptor densely over the image.
 
     Args:
         num_ang_bins: Number of angular bins. (8 is default)
         num_spatial_bins: Number of spatial bins per descriptor (4 is default).
     You might want to set odd number and relevant padding to keep feature map size
         spatial_bin_size: Size of a spatial bin in pixels (4 is default)
         clipval: clipping value to reduce single-bin dominance
         rootsift: (bool) if True, RootSIFT (Arandjelovi et. al, 2012) is computed
         stride: default 1
         padding: default 0
 
     Returns:
-        torch.Tensor: DenseSIFT descriptor of the image
+        Tensor: DenseSIFT descriptor of the image
 
     Shape:
         - Input: (B, 1, H, W)
         - Output: (B, num_ang_bins * num_spatial_bins ** 2, (H+padding)/stride, (W+padding)/stride)
 
     Examples::
         >>> input =  torch.rand(2, 1, 200, 300)
@@ -279,43 +281,46 @@
             padding=(self.pad, self.pad),
         )
         self.PoolingConv.weight.data.copy_(
             _get_reshape_kernel(num_ang_bins, num_spatial_bins, num_spatial_bins).float()
         )
         return
 
-    def get_pooling_kernel(self) -> torch.Tensor:
+    def get_pooling_kernel(self) -> Tensor:
         return self.bin_pooling_kernel.weight.detach()
 
-    def forward(self, input):
+    def forward(self, input: Tensor) -> Tensor:
         KORNIA_CHECK_SHAPE(input, ["B", "1", "H", "W"])
 
         B, CH, W, H = input.size()
         self.bin_pooling_kernel = self.bin_pooling_kernel.to(input.dtype).to(input.device)
         self.PoolingConv = self.PoolingConv.to(input.dtype).to(input.device)
-        grads: torch.Tensor = spatial_gradient(input, 'diff')
+        grads = spatial_gradient(input, 'diff')
         # unpack the edges
-        gx: torch.Tensor = grads[:, :, 0]
-        gy: torch.Tensor = grads[:, :, 1]
-        mag: torch.Tensor = torch.sqrt(gx * gx + gy * gy + self.eps)
-        ori: torch.Tensor = torch.atan2(gy, gx + self.eps) + 2.0 * pi
-        o_big: torch.Tensor = float(self.num_ang_bins) * ori / (2.0 * pi)
-
-        bo0_big_: torch.Tensor = torch.floor(o_big)
-        wo1_big_: torch.Tensor = o_big - bo0_big_
-        bo0_big: torch.Tensor = bo0_big_ % self.num_ang_bins
-        bo1_big: torch.Tensor = (bo0_big + 1) % self.num_ang_bins
-        wo0_big: torch.Tensor = (1.0 - wo1_big_) * mag  # type: ignore
-        wo1_big: torch.Tensor = wo1_big_ * mag
-        ang_bins = []
-        for i in range(0, self.num_ang_bins):
-            out = self.bin_pooling_kernel(
-                (bo0_big == i).to(input.dtype) * wo0_big + (bo1_big == i).to(input.dtype) * wo1_big
-            )
-            ang_bins.append(out)
-        ang_bins = torch.cat(ang_bins, dim=1)
+        gx = grads[:, :, 0]
+        gy = grads[:, :, 1]
+        mag = torch.sqrt(gx * gx + gy * gy + self.eps)
+        ori = torch.atan2(gy, gx + self.eps) + 2.0 * pi
+        o_big = float(self.num_ang_bins) * ori / (2.0 * pi)
+
+        bo0_big_ = torch.floor(o_big)
+        wo1_big_ = o_big - bo0_big_
+        bo0_big = bo0_big_ % self.num_ang_bins
+        bo1_big = (bo0_big + 1) % self.num_ang_bins
+        wo0_big = (1.0 - wo1_big_) * mag
+        wo1_big = wo1_big_ * mag
+        ang_bins = concatenate(
+            [
+                self.bin_pooling_kernel(
+                    (bo0_big == i).to(input.dtype) * wo0_big + (bo1_big == i).to(input.dtype) * wo1_big
+                )
+                for i in range(0, self.num_ang_bins)
+            ],
+            1,
+        )
+
         out_no_norm = self.PoolingConv(ang_bins)
-        out = F.normalize(out_no_norm, dim=1, p=2).clamp_(0, float(self.clipval))
-        out = F.normalize(out, dim=1, p=2)
+        out = normalize(out_no_norm, dim=1, p=2).clamp_(0, float(self.clipval))
+        out = normalize(out, dim=1, p=2)
         if self.rootsift:
-            out = torch.sqrt(F.normalize(out, p=1) + self.eps)
+            out = torch.sqrt(normalize(out, p=1) + self.eps)
         return out
```

### Comparing `kornia-0.6.8/kornia/feature/sold2/backbones.py` & `kornia-0.6.9/kornia/feature/sold2/backbones.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,19 +1,20 @@
 """Implements several backbone networks."""
-from typing import Dict, List, Optional, Tuple, Union
+from typing import Any, Dict, List, Optional, Tuple, Type, Union
 
 import torch
 import torch.nn as nn
 import torch.nn.functional as F
 from torch.nn.functional import pixel_shuffle, softmax
 
-# [Hourglass backbone classes]
+from kornia.core import Module
 
 
-class HourglassBackbone(nn.Module):
+# [Hourglass backbone classes]
+class HourglassBackbone(Module):
     """Hourglass network, taken from https://github.com/zhou13/lcnn.
 
     Args:
         input_channel: number of input channels.
         depth: number of residual blocks per hourglass module.
         num_stacks: number of hourglass modules stacked together.
         num_blocks: number of layers in each residual block.
@@ -36,15 +37,15 @@
             }
         )
 
     def forward(self, input_images: torch.Tensor) -> torch.Tensor:
         return self.net(input_images)
 
 
-class MultitaskHead(nn.Module):
+class MultitaskHead(Module):
     def __init__(self, input_channels: int):
         super().__init__()
 
         m = int(input_channels / 4)
         head_size = [[2], [1], [2]]
         heads = []
         for output_channels in sum(head_size, []):
@@ -57,21 +58,17 @@
             )
         self.heads = nn.ModuleList(heads)
 
     def forward(self, x: torch.Tensor) -> torch.Tensor:
         return torch.cat([head(x) for head in self.heads], dim=1)
 
 
-class Bottleneck2D(nn.Module):
+class Bottleneck2D(Module):
     def __init__(
-        self,
-        inplanes: int,
-        planes: int,
-        stride: Union[int, Tuple[int, int]] = 1,
-        downsample: Optional[torch.nn.Module] = None,
+        self, inplanes: int, planes: int, stride: Union[int, Tuple[int, int]] = 1, downsample: Optional[Module] = None
     ):
         super().__init__()
 
         self.bn1 = nn.BatchNorm2d(inplanes)
         self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1)
         self.bn2 = nn.BatchNorm2d(planes)
         self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1)
@@ -100,29 +97,29 @@
             residual = self.downsample(x)
 
         out += residual
 
         return out
 
 
-class Hourglass(nn.Module):
-    def __init__(self, block: torch.nn.Module, num_blocks: int, planes: int, depth: int, expansion: int = 2):
+class Hourglass(Module):
+    def __init__(self, block: Type[Bottleneck2D], num_blocks: int, planes: int, depth: int, expansion: int = 2):
         super().__init__()
         self.depth = depth
         self.block = block
         self.expansion = expansion
         self.hg = self._make_hour_glass(block, num_blocks, planes, depth)
 
-    def _make_residual(self, block: torch.nn.Module, num_blocks: int, planes: int) -> torch.nn.Module:
+    def _make_residual(self, block: Type[Bottleneck2D], num_blocks: int, planes: int) -> Module:
         layers = []
         for _ in range(0, num_blocks):
             layers.append(block(planes * self.expansion, planes))
         return nn.Sequential(*layers)
 
-    def _make_hour_glass(self, block: torch.nn.Module, num_blocks: int, planes: int, depth: int) -> torch.nn.ModuleList:
+    def _make_hour_glass(self, block: Type[Bottleneck2D], num_blocks: int, planes: int, depth: int) -> nn.ModuleList:
         hgl = []
         for i in range(depth):
             res = []
             for _ in range(3):
                 res.append(self._make_residual(block, num_blocks, planes))
             if i == 0:
                 res.append(self._make_residual(block, num_blocks, planes))
@@ -143,21 +140,21 @@
         out = up1 + up2
         return out
 
     def forward(self, x: torch.Tensor) -> torch.Tensor:
         return self._hour_glass_forward(self.depth, x)
 
 
-class HourglassNet(nn.Module):
+class HourglassNet(Module):
     """Hourglass model from Newell et al ECCV 2016."""
 
     def __init__(
         self,
-        block: torch.nn.Module,
-        head: torch.nn.Module,
+        block: Type[Bottleneck2D],
+        head: Module,
         depth: int,
         num_stacks: int,
         num_blocks: int,
         num_classes: int,
         input_channels: int,
         expansion: int = 2,
     ):
@@ -190,29 +187,29 @@
         self.res = nn.ModuleList(res)
         self.fc = nn.ModuleList(fc)
         self.score = nn.ModuleList(score)
         self.fc_ = nn.ModuleList(fc_)
         self.score_ = nn.ModuleList(score_)
 
     def _make_residual(
-        self, block: torch.nn.Module, planes: int, blocks: int, stride: Union[int, Tuple[int, int]] = 1
-    ) -> torch.nn.Module:
+        self, block: Type[Bottleneck2D], planes: int, blocks: int, stride: Union[int, Tuple[int, int]] = 1
+    ) -> Module:
         downsample = None
         if stride != 1 or self.inplanes != planes * self.expansion:
             downsample = nn.Sequential(nn.Conv2d(self.inplanes, planes * self.expansion, kernel_size=1, stride=stride))
 
         layers = []
         layers.append(block(self.inplanes, planes, stride, downsample))
         self.inplanes = planes * self.expansion
         for _ in range(1, blocks):
             layers.append(block(self.inplanes, planes))
 
         return nn.Sequential(*layers)
 
-    def _make_fc(self, inplanes: int, outplanes: int) -> torch.nn.Module:
+    def _make_fc(self, inplanes: int, outplanes: int) -> Module:
         bn = nn.BatchNorm2d(inplanes)
         conv = nn.Conv2d(inplanes, outplanes, kernel_size=1)
         return nn.Sequential(conv, bn, self.relu)
 
     def forward(self, x: torch.Tensor) -> torch.Tensor:
         out = []
         x = self.conv1(x)
@@ -237,53 +234,53 @@
 
         return y
 
 
 def hg(**kwargs):
     model = HourglassNet(
         Bottleneck2D,
-        head=kwargs.get("head", lambda c_in, c_out: nn.Conv2D(c_in, c_out, 1)),
+        head=kwargs.get("head", lambda c_in, c_out: nn.Conv2d(c_in, c_out, 1)),
         depth=kwargs["depth"],
         num_stacks=kwargs["num_stacks"],
         num_blocks=kwargs["num_blocks"],
         num_classes=kwargs["num_classes"],
         input_channels=kwargs["input_channels"],
     )
     return model
 
 
 # [Backbone decoders]
-class SuperpointDecoder(nn.Module):
+class SuperpointDecoder(Module):
     """Junction decoder based on the SuperPoint architecture.
 
     Args:
         input_feat_dim: channel size of the input features.
     Returns:
         the junction heatmap, with shape (B, H, W).
     """
 
     def __init__(self, input_feat_dim: int = 128, grid_size: int = 8):
         super().__init__()
-        self.relu = torch.nn.ReLU(inplace=True)
+        self.relu = nn.ReLU(inplace=True)
         # Perform strided convolution when using lcnn backbone.
-        self.convPa = torch.nn.Conv2d(input_feat_dim, 256, kernel_size=3, stride=2, padding=1)
-        self.convPb = torch.nn.Conv2d(256, 65, kernel_size=1, stride=1, padding=0)
+        self.convPa = nn.Conv2d(input_feat_dim, 256, kernel_size=3, stride=2, padding=1)
+        self.convPb = nn.Conv2d(256, 65, kernel_size=1, stride=1, padding=0)
         self.grid_size = grid_size
 
     def forward(self, input_features: torch.Tensor) -> torch.Tensor:
         feat = self.relu(self.convPa(input_features))
         semi = self.convPb(feat)
 
         # Convert from semi-dense to dense heatmap
         junc_prob = softmax(semi, dim=1)
         junc_pred = pixel_shuffle(junc_prob[:, :-1, :, :], self.grid_size)[:, 0]
         return junc_pred
 
 
-class PixelShuffleDecoder(nn.Module):
+class PixelShuffleDecoder(Module):
     """Pixel shuffle decoder used to predict the line heatmap.
 
     Args:
         input_feat_dim: channel size of the input features.
         num_upsample: how many upsamples are performed.
         output_channel: number of output channels.
     Returns:
@@ -341,52 +338,52 @@
         # Output layer
         out = self.conv_block_lst[-1](out)
         heatmap = softmax(out, dim=1)[:, 1, :, :]
 
         return heatmap
 
 
-class SuperpointDescriptor(nn.Module):
+class SuperpointDescriptor(Module):
     """Descriptor decoder based on the SuperPoint arcihtecture.
 
     Args:
         input_feat_dim: channel size of the input features.
     Returns:
         the semi-dense descriptors with shape (B, 128, H/4, W/4).
     """
 
     def __init__(self, input_feat_dim: int = 128):
         super().__init__()
-        self.relu = torch.nn.ReLU(inplace=True)
-        self.convPa = torch.nn.Conv2d(input_feat_dim, 256, kernel_size=3, stride=1, padding=1)
-        self.convPb = torch.nn.Conv2d(256, 128, kernel_size=1, stride=1, padding=0)
+        self.relu = nn.ReLU(inplace=True)
+        self.convPa = nn.Conv2d(input_feat_dim, 256, kernel_size=3, stride=1, padding=1)
+        self.convPb = nn.Conv2d(256, 128, kernel_size=1, stride=1, padding=0)
 
     def forward(self, input_features: torch.Tensor) -> torch.Tensor:
         feat = self.relu(self.convPa(input_features))
         semi = self.convPb(feat)
 
         return semi
 
 
 # [Combination of all previous models in one]
 
 
-class SOLD2Net(nn.Module):
+class SOLD2Net(Module):
     """Full network for SOLD.
 
     Args:
         model_cfg: the configuration as a Dict.
     Returns:
         a Dict with the following values:
             junctions: heatmap of junctions.
             heatmap: line heatmap.
             descriptors: semi-dense descriptors.
     """
 
-    def __init__(self, model_cfg: Dict):
+    def __init__(self, model_cfg: Dict[str, Any]):
         super().__init__()
         self.cfg = model_cfg
 
         # Backbone
         self.backbone_net = HourglassBackbone(**self.cfg["backbone_cfg"])
         feat_channel = 256
```

### Comparing `kornia-0.6.8/kornia/feature/sold2/sold2.py` & `kornia-0.6.9/kornia/feature/sold2/sold2.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,25 +1,25 @@
-from typing import Dict, Optional, Tuple
+from typing import Any, Dict, Optional, Tuple
 
 import torch
-import torch.nn as nn
 import torch.nn.functional as F
-from torch import Tensor
 
+from kornia.core import Module, Tensor, concatenate, pad, stack
 from kornia.geometry.conversions import normalize_pixel_coordinates
 from kornia.testing import KORNIA_CHECK_SHAPE
+from kornia.utils import map_location_to_cpu
 
 from .backbones import SOLD2Net
 from .sold2_detector import LineSegmentDetectionModule, line_map_to_segments, prob_to_junctions
 
 urls: Dict[str, str] = {}
 urls["wireframe"] = "http://cmp.felk.cvut.cz/~mishkdmy/models/sold2_wireframe.pth"
 
 
-default_cfg: Dict = {
+default_cfg: Dict[str, Any] = {
     'backbone_cfg': {'input_channel': 1, 'depth': 4, 'num_stacks': 2, 'num_blocks': 1, 'num_classes': 5},
     'use_descriptor': True,
     'grid_size': 8,
     'keep_border_valid': True,
     'detection_thresh': 0.0153846,  # = 1/65: threshold of junction detection
     'max_num_junctions': 500,  # maximum number of junctions per image
     'line_detector_cfg': {
@@ -45,15 +45,15 @@
         'min_dist_pts': 8,
         'top_k_candidates': 10,
         'grid_size': 4,
     },
 }
 
 
-class SOLD2(nn.Module):
+class SOLD2(Module):
     r"""Module, which detects and describe line segments in an image.
 
     This is based on the original code from the paper "SOLD: Self-supervised
     Occlusion-aware Line Detector and Descriptor". See :cite:`SOLD22021` for more details.
 
     Args:
         config: Dict specifying parameters. None will load the default parameters,
@@ -71,40 +71,38 @@
         >>> line_seg1 = outputs["line_segments"][0]
         >>> line_seg2 = outputs["line_segments"][1]
         >>> desc1 = outputs["dense_desc"][0]
         >>> desc2 = outputs["dense_desc"][1]
         >>> matches = sold2.match(line_seg1, line_seg2, desc1[None], desc2[None])
     """
 
-    def __init__(self, pretrained: bool = True, config: Optional[Dict] = None):
+    def __init__(self, pretrained: bool = True, config: Optional[Dict[str, Any]] = None):
         super().__init__()
         # Initialize some parameters
         self.config = default_cfg if config is None else config
         self.grid_size = self.config["grid_size"]
         self.junc_detect_thresh = self.config.get("detection_thresh", 1 / 65)
         self.max_num_junctions = self.config.get("max_num_junctions", 500)
 
         # Load the pre-trained model
         self.model = SOLD2Net(self.config)
         if pretrained:
-            pretrained_dict = torch.hub.load_state_dict_from_url(
-                urls["wireframe"], map_location=lambda storage, loc: storage
-            )
+            pretrained_dict = torch.hub.load_state_dict_from_url(urls["wireframe"], map_location=map_location_to_cpu)
             state_dict = self.adapt_state_dict(pretrained_dict['model_state_dict'])
             self.model.load_state_dict(state_dict)
         self.eval()
 
         # Initialize the line detector
         self.line_detector_cfg = self.config["line_detector_cfg"]
         self.line_detector = LineSegmentDetectionModule(**self.config["line_detector_cfg"])
 
         # Initialize the line matcher
         self.line_matcher = WunschLineMatcher(**self.config["line_matcher_cfg"])
 
-    def forward(self, img: Tensor) -> Dict:
+    def forward(self, img: Tensor) -> Dict[str, Any]:
         """
         Args:
             img: batched images with shape :math:`(B, 1, H, W)`.
 
         Return:
             - ``line_segments``: list of N line segments in each of the B images :math:`List[(N, 2, 2)]`.
             - ``junction_heatmap``: raw junction heatmap of shape :math:`(B, H, W)`.
@@ -152,15 +150,15 @@
         state_dict["heatmap_decoder.conv_block_lst.2.0.weight"] = state_dict["heatmap_decoder.conv_block_lst.2.weight"]
         state_dict["heatmap_decoder.conv_block_lst.2.0.bias"] = state_dict["heatmap_decoder.conv_block_lst.2.bias"]
         del state_dict["heatmap_decoder.conv_block_lst.2.weight"]
         del state_dict["heatmap_decoder.conv_block_lst.2.bias"]
         return state_dict
 
 
-class WunschLineMatcher(nn.Module):
+class WunschLineMatcher(Module):
     """Class matching two sets of line segments with the Needleman-Wunsch algorithm.
 
     TODO: move it later in kornia.feature.matching
     """
 
     def __init__(
         self,
@@ -223,15 +221,15 @@
         if self.cross_check:
             matches2 = self.filter_and_match_lines(scores.permute(1, 0, 3, 2))
             mutual = matches2[matches] == torch.arange(len(line_seg1), device=device)
             matches[~mutual] = -1
 
         return matches
 
-    def sample_line_points(self, line_seg: Tensor) -> Tuple:
+    def sample_line_points(self, line_seg: Tensor) -> Tuple[Tensor, Tensor]:
         """Regularly sample points along each line segments, with a minimal distance between each point.
 
         Pad the remaining points.
         Inputs:
             line_seg: an Nx2x2 Tensor.
         Outputs:
             line_points: an N x num_samples x 2 Tensor.
@@ -250,18 +248,18 @@
         valid_points = torch.empty((num_lines, self.num_samples), dtype=torch.bool)
         for n_samp in range(2, self.num_samples + 1):
             # Consider all lines where we can fit up to n_samp points
             cur_mask = num_samples_lst == n_samp
             cur_line_seg = line_seg[cur_mask]
             line_points_x = batched_linspace(cur_line_seg[:, 0, 0], cur_line_seg[:, 1, 0], n_samp, dim=-1)
             line_points_y = batched_linspace(cur_line_seg[:, 0, 1], cur_line_seg[:, 1, 1], n_samp, dim=-1)
-            cur_line_points = torch.stack([line_points_x, line_points_y], dim=-1)
+            cur_line_points = stack([line_points_x, line_points_y], -1)
 
             # Pad
-            cur_line_points = F.pad(cur_line_points, (0, 0, 0, self.num_samples - n_samp))
+            cur_line_points = pad(cur_line_points, (0, 0, 0, self.num_samples - n_samp))
             cur_valid_points = torch.ones(len(cur_line_seg), self.num_samples, dtype=torch.bool)
             cur_valid_points[:, n_samp:] = False
 
             line_points[cur_mask] = cur_line_points
             valid_points[cur_mask] = cur_valid_points
 
         return line_points, valid_points
@@ -284,18 +282,19 @@
         line_scores1 = (line_scores1 * valid_scores1).sum(2) / valid_scores1.sum(2)
         line_scores2 = scores.max(2)[0]
         valid_scores2 = line_scores2 != -1
         line_scores2 = (line_scores2 * valid_scores2).sum(2) / valid_scores2.sum(2)
         line_scores = (line_scores1 + line_scores2) / 2
         topk_lines = torch.argsort(line_scores, dim=1)[:, -self.top_k_candidates :]
         # topk_lines.shape = (n_lines1, top_k_candidates)
+
         top_scores = torch.take_along_dim(scores, topk_lines[:, :, None, None], dim=1)
 
         # Consider the reversed line segments as well
-        top_scores = torch.cat([top_scores, torch.flip(top_scores, dims=[-1])], dim=1)
+        top_scores = concatenate([top_scores, torch.flip(top_scores, dims=[-1])], 1)
 
         # Compute the line distance matrix with Needleman-Wunsch algo and
         # retrieve the closest line neighbor
         n_lines1, top2k, n, m = top_scores.shape
         top_scores = top_scores.reshape((n_lines1 * top2k, n, m))
         nw_scores = self.needleman_wunsch(top_scores)
         nw_scores = nw_scores.reshape(n_lines1, top2k)
@@ -326,15 +325,15 @@
                 nw_grid[:, i + 1, j + 1] = torch.maximum(
                     torch.maximum(nw_grid[:, i + 1, j], nw_grid[:, i, j + 1]), nw_grid[:, i, j] + nw_scores[:, i, j]
                 )
 
         return nw_grid[:, -1, -1]
 
 
-def keypoints_to_grid(keypoints: Tensor, img_size: tuple) -> Tensor:
+def keypoints_to_grid(keypoints: Tensor, img_size: Tuple[int, int]) -> Tensor:
     """Convert a list of keypoints into a grid in [-1, 1] that can be used in torch.nn.functional.interpolate.
 
     Args:
         keypoints: a tensor [N, 2] of N keypoints (ij coordinates convention).
         img_size: the original image size (H, W)
     """
     KORNIA_CHECK_SHAPE(keypoints, ["N", "2"])
```

### Comparing `kornia-0.6.8/kornia/feature/sold2/sold2_detector.py` & `kornia-0.6.9/kornia/feature/sold2/sold2_detector.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,17 +1,16 @@
 import math
-from typing import Dict, Optional, Tuple
+from typing import Any, Dict, Optional, Tuple
 
 import torch
-import torch.nn as nn
-from torch import Tensor, tensor
 
+from kornia.core import Module, Tensor, concatenate, stack, tensor, where, zeros
 from kornia.geometry.bbox import nms
 from kornia.testing import KORNIA_CHECK_SHAPE
-from packaging import version
+from kornia.utils import map_location_to_cpu, torch_meshgrid
 
 from .backbones import SOLD2Net
 
 urls: Dict[str, str] = {}
 urls["wireframe"] = "https://www.polybox.ethz.ch/index.php/s/blOrW89gqSLoHOk/download"
 
 
@@ -38,15 +37,15 @@
         },
         'use_junction_refinement': True,
         'junction_refine_cfg': {'num_perturbs': 9, 'perturb_interval': 0.25},
     },
 }
 
 
-class SOLD2_detector(nn.Module):
+class SOLD2_detector(Module):
     r"""Module, which detects line segments in an image.
 
     This is based on the original code from the paper "SOLD: Self-supervised
     Occlusion-aware Line Detector and Descriptor". See :cite:`SOLD22021` for more details.
 
     Args:
         config: Dict specifying parameters. None will load the default parameters,
@@ -58,28 +57,26 @@
 
     Example:
         >>> img = torch.rand(1, 1, 512, 512)
         >>> sold2_detector = SOLD2_detector()
         >>> line_segments = sold2_detector(img)["line_segments"]
     """
 
-    def __init__(self, pretrained: bool = True, config: Optional[Dict] = None):
+    def __init__(self, pretrained: bool = True, config: Optional[Dict[str, Any]] = None):
         super().__init__()
         # Initialize some parameters
         self.config = default_detector_cfg if config is None else config
         self.grid_size = self.config["grid_size"]
         self.junc_detect_thresh = self.config.get("detection_thresh", 1 / 65)
         self.max_num_junctions = self.config.get("max_num_junctions", 500)
 
         # Load the pre-trained model
         self.model = SOLD2Net(self.config)
         if pretrained:
-            pretrained_dict = torch.hub.load_state_dict_from_url(
-                urls["wireframe"], map_location=lambda storage, loc: storage
-            )
+            pretrained_dict = torch.hub.load_state_dict_from_url(urls["wireframe"], map_location=map_location_to_cpu)
             state_dict = self.adapt_state_dict(pretrained_dict['model_state_dict'])
             self.model.load_state_dict(state_dict)
         self.eval()
 
         # Initialize the line detector
         self.line_detector_cfg = self.config["line_detector_cfg"]
         self.line_detector = LineSegmentDetectionModule(**self.config["line_detector_cfg"])
@@ -90,15 +87,15 @@
         del state_dict["w_desc"]
         state_dict["heatmap_decoder.conv_block_lst.2.0.weight"] = state_dict["heatmap_decoder.conv_block_lst.2.weight"]
         state_dict["heatmap_decoder.conv_block_lst.2.0.bias"] = state_dict["heatmap_decoder.conv_block_lst.2.bias"]
         del state_dict["heatmap_decoder.conv_block_lst.2.weight"]
         del state_dict["heatmap_decoder.conv_block_lst.2.bias"]
         return state_dict
 
-    def forward(self, img: Tensor) -> Dict:
+    def forward(self, img: Tensor) -> Dict[str, Any]:
         """
         Args:
             img: batched images with shape :math:`(B, 1, H, W)`.
 
         Return:
             - ``line_segments``: list of N line segments in each of the B images :math:`List[(N, 2, 2)]`.
             - ``junction_heatmap``: raw junction heatmap of shape :math:`(B, H, W)`.
@@ -189,15 +186,15 @@
 
         # Junction refinement configuration
         self.use_junction_refinement = use_junction_refinement
         self.junction_refine_cfg = junction_refine_cfg
         if self.use_junction_refinement and self.junction_refine_cfg is None:
             raise ValueError("[Error] Missing junction refinement config.")
 
-    def detect(self, junctions: Tensor, heatmap: Tensor) -> Tuple:
+    def detect(self, junctions: Tensor, heatmap: Tensor) -> Tuple[Tensor, Tensor, Tensor]:
         """Main function performing line segment detection."""
         KORNIA_CHECK_SHAPE(heatmap, ["H", "W"])
         H, W = heatmap.shape
         device = junctions.device
 
         # Perform the heatmap refinement
         if self.use_heatmap_refinement:
@@ -212,15 +209,15 @@
                     self.heatmap_refine_cfg["overlap_ratio"],
                     self.heatmap_refine_cfg["ratio"],
                     self.heatmap_refine_cfg["valid_thresh"],
                 )
 
         # Initialize empty line map
         num_junctions = len(junctions)
-        line_map_pred = torch.zeros([num_junctions, num_junctions], device=device, dtype=torch.int32)
+        line_map_pred = zeros([num_junctions, num_junctions], device=device, dtype=torch.int32)
 
         # Stop if there are not enough junctions
         if num_junctions < 2:
             return line_map_pred, junctions, heatmap
 
         # Generate the candidate map
         candidate_map = torch.triu(
@@ -228,16 +225,16 @@
         )
 
         # Optionally perform candidate filtering
         if self.use_candidate_suppression:
             candidate_map = self.candidate_suppression(junctions, candidate_map)
 
         # Fetch the candidates
-        candidate_indices = torch.where(candidate_map)
-        candidate_index_map = torch.cat([candidate_indices[0][..., None], candidate_indices[1][..., None]], dim=-1)
+        candidate_indices = where(candidate_map)
+        candidate_index_map = concatenate([candidate_indices[0][..., None], candidate_indices[1][..., None]], -1)
 
         # Get the corresponding start and end junctions
         candidate_junc_start = junctions[candidate_index_map[:, 0]]
         candidate_junc_end = junctions[candidate_index_map[:, 1]]
 
         # Get the sampling locations (N x 64)
         sampler = self.torch_sampler.to(device)[None]
@@ -268,15 +265,15 @@
                     normalized_seg_length_ = normalized_seg_length[iter_idx * group_size : (iter_idx + 1) * group_size]
                 else:
                     cand_h_ = cand_h[iter_idx * group_size :, :]
                     cand_w_ = cand_w[iter_idx * group_size :, :]
                     normalized_seg_length_ = normalized_seg_length[iter_idx * group_size :]
                 sampled_feat_ = self.detect_local_max(heatmap, cand_h_, cand_w_, H, W, normalized_seg_length_, device)
                 sampled_feat_lst.append(sampled_feat_)
-            sampled_feat = torch.cat(sampled_feat_lst, dim=0)
+            sampled_feat = concatenate(sampled_feat_lst, 0)
         else:
             sampled_feat = self.detect_local_max(heatmap, cand_h, cand_w, H, W, normalized_seg_length, device)
 
         # [Simple threshold detection]
         # detection_results is a mask over all candidates
         detection_results = torch.mean(sampled_feat, dim=-1) > self.detect_thresh
 
@@ -319,16 +316,16 @@
         # Get the shape of the heatmap
         H, W = heatmap.shape
         increase_ratio = 1 - overlap_ratio
         h_block = round(H / (1 + (num_blocks - 1) * increase_ratio))
         w_block = round(W / (1 + (num_blocks - 1) * increase_ratio))
 
         # Iterate through each block
-        count_map = torch.zeros(heatmap.shape, dtype=torch.int, device=heatmap.device)
-        heatmap_output = torch.zeros(heatmap.shape, dtype=torch.float, device=heatmap.device)
+        count_map = zeros(heatmap.shape, dtype=torch.int, device=heatmap.device)
+        heatmap_output = zeros(heatmap.shape, dtype=torch.float, device=heatmap.device)
         for h_idx in range(num_blocks):
             for w_idx in range(num_blocks):
                 # Fetch the heatmap
                 h_start = round(h_idx * h_block * increase_ratio)
                 w_start = round(w_idx * w_block * increase_ratio)
                 h_end = h_start + h_block if h_idx < num_blocks - 1 else H
                 w_end = w_start + w_block if w_idx < num_blocks - 1 else W
@@ -350,15 +347,15 @@
         dist_tolerance = self.nms_dist_tolerance
 
         # Compute distance between junction pairs
         # (num_junc x 1 x 2) - (1 x num_junc x 2) => num_junc x num_junc map
         line_dist_map = torch.sum((torch.unsqueeze(junctions, dim=1) - junctions[None, ...]) ** 2, dim=-1) ** 0.5
 
         # Fetch all the "detected lines"
-        seg_indexes = torch.where(torch.triu(candidate_map, diagonal=1))
+        seg_indexes = where(torch.triu(candidate_map, diagonal=1))
         start_point_idxs = seg_indexes[0]
         end_point_idxs = seg_indexes[1]
         start_points = junctions[start_point_idxs, :]
         end_points = junctions[end_point_idxs, :]
 
         # Fetch corresponding entries
         line_dists = line_dist_map[start_point_idxs, end_point_idxs]
@@ -389,44 +386,43 @@
         final_mask = junc_counts > 0
         candidate_map[start_point_idxs[final_mask], end_point_idxs[final_mask]] = 0
 
         return candidate_map
 
     def refine_junction_perturb(
         self, junctions: Tensor, line_map: Tensor, heatmap: Tensor, H: int, W: int, device: torch.device
-    ) -> Tuple:
+    ) -> Tuple[Tensor, Tensor]:
         """Refine the line endpoints in a similar way as in LSD."""
         # Fetch refinement parameters
         num_perturbs = self.junction_refine_cfg["num_perturbs"]
         perturb_interval = self.junction_refine_cfg["perturb_interval"]
         side_perturbs = (num_perturbs - 1) // 2
 
         # Fetch the 2D perturb mat
         perturb_vec = torch.arange(
             start=-perturb_interval * side_perturbs,
             end=perturb_interval * (side_perturbs + 1),
             step=perturb_interval,
             device=device,
         )
-        if version.parse(torch.__version__) < version.parse('1.10'):
-            h1_grid, w1_grid, h2_grid, w2_grid = torch.meshgrid(perturb_vec, perturb_vec, perturb_vec, perturb_vec)
-        else:
-            h1_grid, w1_grid, h2_grid, w2_grid = torch.meshgrid(
-                perturb_vec, perturb_vec, perturb_vec, perturb_vec, indexing='ij'
-            )
-        perturb_tensor = torch.cat(
-            [h1_grid[..., None], w1_grid[..., None], h2_grid[..., None], w2_grid[..., None]], dim=-1
+
+        h1_grid, w1_grid, h2_grid, w2_grid = torch_meshgrid(
+            [perturb_vec, perturb_vec, perturb_vec, perturb_vec], indexing='ij'
+        )
+
+        perturb_tensor = concatenate(
+            [h1_grid[..., None], w1_grid[..., None], h2_grid[..., None], w2_grid[..., None]], -1
         )
         perturb_tensor_flat = perturb_tensor.view(-1, 2, 2)
 
         # Fetch all the detected lines
-        detected_seg_indexes = torch.where(torch.triu(line_map, diagonal=1))
+        detected_seg_indexes = where(torch.triu(line_map, diagonal=1))
         start_points = junctions[detected_seg_indexes[0]]
         end_points = junctions[detected_seg_indexes[1]]
-        line_segments = torch.stack([start_points, end_points], dim=1)
+        line_segments = stack([start_points, end_points], 1)
 
         line_segment_candidates = line_segments.unsqueeze(dim=1) + perturb_tensor_flat[None]
         # Clip the boundaries
         line_segment_candidates[..., 0] = torch.clamp(line_segment_candidates[..., 0], min=0, max=H - 1)
         line_segment_candidates[..., 1] = torch.clamp(line_segment_candidates[..., 1], min=0, max=W - 1)
 
         # Iterate through all the segments
@@ -450,32 +446,32 @@
             # Perform bilinear sampling
             segment_feat = self.detect_bilinear(heatmap, cand_h, cand_w)
             segment_results = torch.mean(segment_feat, dim=-1)
             max_idx = torch.argmax(segment_results)
             refined_segment_lst.append(segment[max_idx][None])
 
         # Concatenate back to segments
-        refined_segments = torch.cat(refined_segment_lst, dim=0)
+        refined_segments = concatenate(refined_segment_lst, 0)
 
         # Convert back to junctions and line_map
-        junctions_new = torch.cat([refined_segments[:, 0, :], refined_segments[:, 1, :]], dim=0)
+        junctions_new = concatenate([refined_segments[:, 0, :], refined_segments[:, 1, :]], 0)
         junctions_new = torch.unique(junctions_new, dim=0)
         line_map_new = self.segments_to_line_map(junctions_new, refined_segments)
 
         return junctions_new, line_map_new
 
     def segments_to_line_map(self, junctions: Tensor, segments: Tensor) -> Tensor:
         """Convert the list of segments to line map."""
         # Create empty line map
         num_junctions = len(junctions)
-        line_map = torch.zeros([num_junctions, num_junctions], device=junctions.device)
+        line_map = zeros([num_junctions, num_junctions], device=junctions.device)
 
         # Get the indices of paired junctions
-        _, idx_junc1 = torch.where(torch.all(junctions[None] == segments[:, None, 0], dim=2))
-        _, idx_junc2 = torch.where(torch.all(junctions[None] == segments[:, None, 1], dim=2))
+        _, idx_junc1 = where(torch.all(junctions[None] == segments[:, None, 0], dim=2))
+        _, idx_junc2 = where(torch.all(junctions[None] == segments[:, None, 1], dim=2))
 
         # Assign the labels
         line_map[idx_junc1, idx_junc2] = 1
         line_map[idx_junc2, idx_junc1] = 1
 
         return line_map
 
@@ -510,39 +506,37 @@
         """Detection by local maximum search."""
         # Compute the distance threshold
         dist_thresh = 0.5 * (2**0.5) + self.lambda_radius * normalized_seg_length
         # Make it N x 64
         dist_thresh = torch.repeat_interleave(dist_thresh[..., None], self.num_samples, dim=-1)
 
         # Compute the candidate points
-        cand_points = torch.cat([cand_h[..., None], cand_w[..., None]], dim=-1)
+        cand_points = concatenate([cand_h[..., None], cand_w[..., None]], -1)
         cand_points_round = torch.round(cand_points)  # N x 64 x 2
 
         # Construct local patches 9x9 = 81
-        patch_mask = torch.zeros(
-            [int(2 * self.local_patch_radius + 1), int(2 * self.local_patch_radius + 1)], device=device
-        )
+        patch_mask = zeros([int(2 * self.local_patch_radius + 1), int(2 * self.local_patch_radius + 1)], device=device)
         patch_center = tensor([[self.local_patch_radius, self.local_patch_radius]], device=device, dtype=torch.float32)
-        H_patch_points, W_patch_points = torch.where(patch_mask >= 0)
-        patch_points = torch.cat([H_patch_points[..., None], W_patch_points[..., None]], dim=-1)
+        H_patch_points, W_patch_points = where(patch_mask >= 0)
+        patch_points = concatenate([H_patch_points[..., None], W_patch_points[..., None]], -1)
         # Fetch the circle region
         patch_center_dist = torch.sqrt(torch.sum((patch_points - patch_center) ** 2, dim=-1))
         patch_points = patch_points[patch_center_dist <= self.local_patch_radius, :]
         # Shift [0, 0] to the center
         patch_points = patch_points - self.local_patch_radius
 
         # Construct local patch mask
         patch_points_shifted = torch.unsqueeze(cand_points_round, dim=2) + patch_points[None, None]
         patch_dist = torch.sqrt(torch.sum((torch.unsqueeze(cand_points, dim=2) - patch_points_shifted) ** 2, dim=-1))
         patch_dist_mask = patch_dist < dist_thresh[..., None]
 
         # Get all points => num_points_center x num_patch_points x 2
         points_H = torch.clamp(patch_points_shifted[:, :, :, 0], min=0, max=H - 1).to(torch.long)
         points_W = torch.clamp(patch_points_shifted[:, :, :, 1], min=0, max=W - 1).to(torch.long)
-        points = torch.cat([points_H[..., None], points_W[..., None]], dim=-1)
+        points = concatenate([points_H[..., None], points_W[..., None]], -1)
 
         # Sample the feature (N x 64 x 81)
         sampled_feat = heatmap[points[:, :, :, 0], points[:, :, :, 1]]
         # Filtering using the valid mask
         sampled_feat = sampled_feat * patch_dist_mask.to(torch.float32)
         if len(sampled_feat) == 0:
             sampled_feat_lmax = torch.empty(0, self.num_samples)
@@ -550,28 +544,28 @@
             sampled_feat_lmax = torch.max(sampled_feat, dim=-1)[0]
 
         return sampled_feat_lmax
 
 
 def line_map_to_segments(junctions: Tensor, line_map: Tensor) -> Tensor:
     """Convert a junction connectivity map to a Nx2x2 tensor of segments."""
-    junc_loc1, junc_loc2 = torch.where(torch.triu(line_map))
-    segments = torch.stack([junctions[junc_loc1], junctions[junc_loc2]], dim=1)
+    junc_loc1, junc_loc2 = where(torch.triu(line_map))
+    segments = stack([junctions[junc_loc1], junctions[junc_loc2]], 1)
     return segments
 
 
 def prob_to_junctions(prob: Tensor, dist: float, prob_thresh: float = 0.01, top_k: int = 0) -> Tensor:
     """Extract junctions from a probability map, apply NMS, and extract the top k candidates."""
     # Extract the junctions
-    junctions = torch.stack(torch.where(prob >= prob_thresh), dim=-1).float()
+    junctions = stack(where(prob >= prob_thresh), -1).float()
     if len(junctions) == 0:
         return junctions
 
     # Perform NMS
-    boxes = torch.cat([junctions - dist / 2, junctions + dist / 2], dim=1)
+    boxes = concatenate([junctions - dist / 2, junctions + dist / 2], 1)
     scores = prob[prob >= prob_thresh]
     remainings = nms(boxes, scores, 0.001)
     junctions = junctions[remainings]
 
     # Keep only the topk values
     if top_k > 0:
         k = min(len(junctions), top_k)
```

### Comparing `kornia-0.6.8/kornia/feature/sosnet.py` & `kornia-0.6.9/kornia/feature/sosnet.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,13 +1,14 @@
-from typing import Callable, Dict
+from typing import Dict
 
 import torch
 import torch.nn as nn
 
 from kornia.testing import KORNIA_CHECK_SHAPE
+from kornia.utils.helpers import map_location_to_cpu
 
 urls: Dict[str, str] = {}
 urls["lib"] = "https://github.com/yuruntian/SOSNet/raw/master/sosnet-weights/sosnet_32x32_liberty.pth"
 urls["hp_a"] = "https://github.com/yuruntian/SOSNet/raw/master/sosnet-weights/sosnet_32x32_hpatches_a.pth"
 
 
 class SOSNet(nn.Module):
@@ -55,16 +56,15 @@
             nn.Dropout(0.1),
             nn.Conv2d(128, 128, kernel_size=8, bias=False),
             nn.BatchNorm2d(128, affine=False),
         )
         self.desc_norm = nn.Sequential(nn.LocalResponseNorm(256, alpha=256.0, beta=0.5, k=0.0))
         # load pretrained model
         if pretrained:
-            storage_fcn: Callable = lambda storage, loc: storage
-            pretrained_dict = torch.hub.load_state_dict_from_url(urls['lib'], map_location=storage_fcn)
+            pretrained_dict = torch.hub.load_state_dict_from_url(urls['lib'], map_location=map_location_to_cpu)
             self.load_state_dict(pretrained_dict, strict=True)
         self.eval()
         return
 
     def forward(self, input: torch.Tensor, eps: float = 1e-10) -> torch.Tensor:
         KORNIA_CHECK_SHAPE(input, ["B", "1", "32", "32"])
         descr = self.desc_norm(self.layers(input) + eps)
```

### Comparing `kornia-0.6.8/kornia/feature/tfeat.py` & `kornia-0.6.9/kornia/feature/tfeat.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,13 +1,14 @@
-from typing import Callable, Dict
+from typing import Dict
 
 import torch
 import torch.nn as nn
 
 from kornia.testing import KORNIA_CHECK_SHAPE
+from kornia.utils.helpers import map_location_to_cpu
 
 urls: Dict[str, str] = {}
 urls["liberty"] = "https://github.com/vbalnt/tfeat/raw/master/pretrained-models/tfeat-liberty.params"  # pylint: disable
 urls[
     "notredame"
 ] = "https://github.com/vbalnt/tfeat/raw/master/pretrained-models/tfeat-notredame.params"  # pylint: disable
 urls[
@@ -48,16 +49,15 @@
             nn.MaxPool2d(kernel_size=2, stride=2),
             nn.Conv2d(32, 64, kernel_size=6),
             nn.Tanh(),
         )
         self.descr = nn.Sequential(nn.Linear(64 * 8 * 8, 128), nn.Tanh())
         # use torch.hub to load pretrained model
         if pretrained:
-            storage_fcn: Callable = lambda storage, loc: storage
-            pretrained_dict = torch.hub.load_state_dict_from_url(urls['liberty'], map_location=storage_fcn)
+            pretrained_dict = torch.hub.load_state_dict_from_url(urls['liberty'], map_location=map_location_to_cpu)
             self.load_state_dict(pretrained_dict, strict=True)
         self.eval()
 
     def forward(self, input: torch.Tensor) -> torch.Tensor:
         KORNIA_CHECK_SHAPE(input, ["B", "1", "32", "32"])
         x = self.features(input)
         x = x.view(x.size(0), -1)
```

### Comparing `kornia-0.6.8/kornia/filters/__init__.py` & `kornia-0.6.9/kornia/filters/__init__.py`

 * *Files identical despite different names*

### Comparing `kornia-0.6.8/kornia/filters/blur.py` & `kornia-0.6.9/kornia/filters/blur.py`

 * *Files identical despite different names*

### Comparing `kornia-0.6.8/kornia/filters/blur_pool.py` & `kornia-0.6.9/kornia/filters/blur_pool.py`

 * *Files 2% similar despite different names*

```diff
@@ -48,15 +48,15 @@
         self.kernel_size = kernel_size
         self.stride = stride
         self.register_buffer('kernel', get_pascal_kernel_2d(kernel_size, norm=True))
 
     def forward(self, input: Tensor) -> Tensor:
         # To align the logic with the whole lib
         kernel = torch.as_tensor(self.kernel, device=input.device, dtype=input.dtype)
-        return _blur_pool_by_kernel2d(input, kernel.repeat((input.size(1), 1, 1, 1)), self.stride)
+        return _blur_pool_by_kernel2d(input, kernel.repeat((input.shape[1], 1, 1, 1)), self.stride)
 
 
 class MaxBlurPool2D(Module):
     r"""Compute pools and blurs and downsample a given feature map.
 
     Equivalent to ```nn.Sequential(nn.MaxPool2d(...), BlurPool2D(...))```
 
@@ -177,29 +177,29 @@
         >>> input = torch.eye(5)[None, None]
         >>> max_blur_pool2d(input, 3)
         tensor([[[[0.5625, 0.3125],
                   [0.3125, 0.8750]]]])
     """
     if not len(input.shape) == 4:
         raise ValueError(f"Invalid input shape, we expect BxCxHxW. Got: {input.shape}")
-    kernel = get_pascal_kernel_2d(kernel_size, norm=True).repeat((input.size(1), 1, 1, 1)).to(input)
+    kernel = get_pascal_kernel_2d(kernel_size, norm=True).repeat((input.shape[1], 1, 1, 1)).to(input)
     return _max_blur_pool_by_kernel2d(input, kernel, stride, max_pool_size, ceil_mode)
 
 
 def _blur_pool_by_kernel2d(input: Tensor, kernel: Tensor, stride: int):
     """Compute blur_pool by a given :math:`CxC_{out}xNxN` kernel."""
-    if not (len(kernel.shape) == 4 and kernel.size(-1) == kernel.size(-2)):
+    if not (len(kernel.shape) == 4 and kernel.shape[-1] == kernel.shape[-2]):
         raise AssertionError(f"Invalid kernel shape. Expect CxC_outxNxN, Got {kernel.shape}")
     padding: Tuple[int, int] = _compute_zero_padding((kernel.shape[-2], kernel.shape[-1]))
-    return F.conv2d(input, kernel, padding=padding, stride=stride, groups=input.size(1))
+    return F.conv2d(input, kernel, padding=padding, stride=stride, groups=input.shape[1])
 
 
 def _max_blur_pool_by_kernel2d(input: Tensor, kernel: Tensor, stride: int, max_pool_size: int, ceil_mode: bool):
     """Compute max_blur_pool by a given :math:`CxC_{out}xNxN` kernel."""
-    if not (len(kernel.shape) == 4 and kernel.size(-1) == kernel.size(-2)):
+    if not (len(kernel.shape) == 4 and kernel.shape[-1] == kernel.shape[-2]):
         raise AssertionError(f"Invalid kernel shape. Expect CxC_outxNxN, Got {kernel.shape}")
     # compute local maxima
     input = F.max_pool2d(input, kernel_size=max_pool_size, padding=0, stride=1, ceil_mode=ceil_mode)
     # blur and downsample
     padding: Tuple[int, int] = _compute_zero_padding((kernel.shape[-2], kernel.shape[-1]))
     return F.conv2d(input, kernel, padding=padding, stride=stride, groups=input.size(1))
```

### Comparing `kornia-0.6.8/kornia/filters/canny.py` & `kornia-0.6.9/kornia/filters/canny.py`

 * *Files identical despite different names*

### Comparing `kornia-0.6.8/kornia/filters/dexined.py` & `kornia-0.6.9/kornia/filters/dexined.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,14 +1,18 @@
 # adapted from: https://github.com/xavysp/DexiNed/blob/d944b70eb6eaf40e22f8467c1e12919aa600d8e4/model.py
+from collections import OrderedDict
 from typing import List
 
 import torch
 import torch.nn as nn
 import torch.nn.functional as F
 
+from kornia.core import Module, Tensor, concatenate
+from kornia.utils import map_location_to_cpu
+
 url: str = "http://cmp.felk.cvut.cz/~mishkdmy/models/DexiNed_BIPED_10.pth"
 
 
 def weight_init(m):
     if isinstance(m, (nn.Conv2d,)):
         # torch.nn.init.xavier_uniform_(m.weight, gain=1.0)
         torch.nn.init.xavier_normal_(m.weight, gain=1.0)
@@ -27,111 +31,114 @@
 
         if m.weight.data.shape[1] == torch.Size([1]):
             torch.nn.init.normal_(m.weight, std=0.1)
         if m.bias is not None:
             torch.nn.init.zeros_(m.bias)
 
 
-class CoFusion(nn.Module):
+class CoFusion(Module):
     def __init__(self, in_ch, out_ch):
         super().__init__()
         self.conv1 = nn.Conv2d(in_ch, 64, kernel_size=3, stride=1, padding=1)
         self.conv2 = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1)
         self.conv3 = nn.Conv2d(64, out_ch, kernel_size=3, stride=1, padding=1)
         self.relu = nn.ReLU()
         self.norm_layer1 = nn.GroupNorm(4, 64)
         self.norm_layer2 = nn.GroupNorm(4, 64)
 
-    def forward(self, x: torch.Tensor) -> torch.Tensor:
+    def forward(self, x: Tensor) -> Tensor:
         # fusecat = torch.cat(x, dim=1)
         attn = self.relu(self.norm_layer1(self.conv1(x)))
         attn = self.relu(self.norm_layer2(self.conv2(attn)))
         attn = F.softmax(self.conv3(attn), dim=1)
 
         # return ((fusecat * attn).sum(1)).unsqueeze(1)
         return ((x * attn).sum(1)).unsqueeze(1)
 
 
 class _DenseLayer(nn.Sequential):
     def __init__(self, input_features, out_features):
-        super().__init__()
-        self.add_module('relu1', nn.ReLU(inplace=True)),
-        self.add_module(
-            'conv1', nn.Conv2d(input_features, out_features, kernel_size=3, stride=1, padding=2, bias=True)
-        ),
-        self.add_module('norm1', nn.BatchNorm2d(out_features)),
-        self.add_module('relu2', nn.ReLU(inplace=True)),
-        self.add_module('conv2', nn.Conv2d(out_features, out_features, kernel_size=3, stride=1, bias=True)),
-        self.add_module('norm2', nn.BatchNorm2d(out_features))
+        super().__init__(
+            OrderedDict(
+                [
+                    ('relu1', nn.ReLU(inplace=True)),
+                    ('conv1', nn.Conv2d(input_features, out_features, kernel_size=3, stride=1, padding=2, bias=True)),
+                    ('norm1', nn.BatchNorm2d(out_features)),
+                    ('relu2', nn.ReLU(inplace=True)),
+                    ('conv2', nn.Conv2d(out_features, out_features, kernel_size=3, stride=1, bias=True)),
+                    ('norm2', nn.BatchNorm2d(out_features)),
+                ]
+            )
+        )
 
-    def forward(self, x: List[torch.Tensor]) -> List[torch.Tensor]:
+    def forward(self, x: List[Tensor]) -> List[Tensor]:
         x1, x2 = x[0], x[1]
-        x3: torch.Tensor = x1
+        x3: Tensor = x1
         for mod in self:
             x3 = mod(x3)
         return [0.5 * (x3 + x2), x2]
 
 
 class _DenseBlock(nn.Sequential):
     def __init__(self, num_layers, input_features, out_features):
         super().__init__()
         for i in range(num_layers):
             layer = _DenseLayer(input_features, out_features)
             self.add_module('denselayer%d' % (i + 1), layer)
             input_features = out_features
 
-    def forward(self, x: List[torch.Tensor]) -> List[torch.Tensor]:
+    def forward(self, x: List[Tensor]) -> List[Tensor]:
         x_out = x
         for mod in self:
             x_out = mod(x_out)
         return x_out
 
 
-class UpConvBlock(nn.Module):
+class UpConvBlock(Module):
     def __init__(self, in_features, up_scale):
         super().__init__()
         self.up_factor = 2
         self.constant_features = 16
 
         layers = self.make_deconv_layers(in_features, up_scale)
         if layers is None:
             raise Exception("layers cannot be none")
         self.features = nn.Sequential(*layers)
 
-    def make_deconv_layers(self, in_features, up_scale):
-        layers = []
+    def make_deconv_layers(self, in_features: int, up_scale: int) -> List[Module]:
+        layers: List[Module] = []
         all_pads = [0, 0, 1, 3, 7]
         for i in range(up_scale):
             kernel_size = 2**up_scale
             pad = all_pads[up_scale]  # kernel_size-1
             out_features = self.compute_out_features(i, up_scale)
             layers.append(nn.Conv2d(in_features, out_features, 1))
             layers.append(nn.ReLU(inplace=True))
             layers.append(nn.ConvTranspose2d(out_features, out_features, kernel_size, stride=2, padding=pad))
             in_features = out_features
         return layers
 
-    def compute_out_features(self, idx, up_scale):
+    def compute_out_features(self, idx: int, up_scale: int):
         return 1 if idx == up_scale - 1 else self.constant_features
 
-    def forward(self, x: torch.Tensor, out_shape: List[int]) -> torch.Tensor:
+    def forward(self, x: Tensor, out_shape: List[int]) -> Tensor:
         out = self.features(x)
         if out.shape[-2:] != out_shape:
             out = F.interpolate(out, out_shape, mode='bilinear')
         return out
 
 
-class SingleConvBlock(nn.Module):
+class SingleConvBlock(Module):
     def __init__(self, in_features, out_features, stride, use_bs=True):
         super().__init__()
         self.use_bn = use_bs
         self.conv = nn.Conv2d(in_features, out_features, 1, stride=stride, bias=True)
         self.bn = nn.BatchNorm2d(out_features)
 
-    def forward(self, x: torch.Tensor) -> torch.Tensor:
+    def forward(self, x: Tensor) -> Tensor:
         x = self.conv(x)
         if self.use_bn:
             x = self.bn(x)
         return x
 
 
 class DoubleConvBlock(nn.Sequential):
@@ -144,15 +151,15 @@
         self.add_module("relu1", nn.ReLU(inplace=True))
         self.add_module("conv2", nn.Conv2d(mid_features, out_features, 3, padding=1))
         self.add_module("bn2", nn.BatchNorm2d(out_features))
         if use_act:
             self.add_module("relu2", nn.ReLU(inplace=True))
 
 
-class DexiNed(nn.Module):
+class DexiNed(Module):
     r"""Definition of the DXtrem network from :cite:`xsoria2020dexined`.
 
     Return:
         A list of tensor with the intermediate features which the last element
         is the edges map with shape :math:`(B,1,H,W)`.
 
     Example:
@@ -200,19 +207,19 @@
         if pretrained:
             self.load_from_file(url)
         else:
             self.apply(weight_init)
 
     def load_from_file(self, path_file: str):
         # use torch.hub to load pretrained model
-        pretrained_dict = torch.hub.load_state_dict_from_url(path_file, map_location=torch.device("cpu"))
+        pretrained_dict = torch.hub.load_state_dict_from_url(path_file, map_location=map_location_to_cpu)
         self.load_state_dict(pretrained_dict, strict=True)
         self.eval()
 
-    def forward(self, x: torch.Tensor) -> List[torch.Tensor]:
+    def forward(self, x: Tensor) -> List[Tensor]:
         # Block 1
         block_1 = self.block_1(x)
         block_1_side = self.side_1(block_1)
 
         # Block 2
         block_2 = self.block_2(block_1)
         block_2_down = self.maxpool(block_2)
@@ -250,13 +257,13 @@
         out_3 = self.up_block_3(block_3, out_shape)
         out_4 = self.up_block_4(block_4, out_shape)
         out_5 = self.up_block_5(block_5, out_shape)
         out_6 = self.up_block_6(block_6, out_shape)
         results = [out_1, out_2, out_3, out_4, out_5, out_6]
 
         # concatenate multiscale outputs
-        block_cat = torch.cat(results, dim=1)  # Bx6xHxW
+        block_cat = concatenate(results, 1)  # Bx6xHxW
         block_cat = self.block_cat(block_cat)  # Bx1xHxW
 
         # return results
         results.append(block_cat)
         return results
```

### Comparing `kornia-0.6.8/kornia/filters/filter.py` & `kornia-0.6.9/kornia/filters/filter.py`

 * *Files identical despite different names*

### Comparing `kornia-0.6.8/kornia/filters/gaussian.py` & `kornia-0.6.9/kornia/filters/gaussian.py`

 * *Files identical despite different names*

### Comparing `kornia-0.6.8/kornia/filters/kernels.py` & `kornia-0.6.9/kornia/filters/kernels.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,49 +1,50 @@
 import math
 from math import sqrt
-from typing import List, Optional, Tuple
+from typing import List, Tuple
 
 import torch
 
+from kornia.core import Tensor, as_tensor, stack, tensor, zeros
 
-def normalize_kernel2d(input: torch.Tensor) -> torch.Tensor:
+
+def normalize_kernel2d(input: Tensor) -> Tensor:
     r"""Normalize both derivative and smoothing kernel."""
     if len(input.size()) < 2:
         raise TypeError(f"input should be at least 2D tensor. Got {input.size()}")
-    norm: torch.Tensor = input.abs().sum(dim=-1).sum(dim=-1)
+    norm: Tensor = input.abs().sum(dim=-1).sum(dim=-1)
     return input / (norm.unsqueeze(-1).unsqueeze(-1))
 
 
-def gaussian(window_size: int, sigma: float) -> torch.Tensor:
+def gaussian(window_size: int, sigma: float) -> Tensor:
     device, dtype = None, None
-    if isinstance(sigma, torch.Tensor):
+    if isinstance(sigma, Tensor):
         device, dtype = sigma.device, sigma.dtype
     x = torch.arange(window_size, device=device, dtype=dtype) - window_size // 2
     if window_size % 2 == 0:
         x = x + 0.5
     gauss = torch.exp(-(x**2.0) / (2 * sigma**2))
     return gauss / gauss.sum()
 
 
-def gaussian_discrete_erf(window_size: int, sigma) -> torch.Tensor:
+def gaussian_discrete_erf(window_size: int, sigma) -> Tensor:
     r"""Discrete Gaussian by interpolating the error function.
 
-    Adapted from:
-    https://github.com/Project-MONAI/MONAI/blob/master/monai/networks/layers/convutils.py
+    Adapted from: https://github.com/Project-MONAI/MONAI/blob/master/monai/networks/layers/convutils.py
     """
-    device = sigma.device if isinstance(sigma, torch.Tensor) else None
-    sigma = torch.as_tensor(sigma, dtype=torch.float, device=device)
+    device = sigma.device if isinstance(sigma, Tensor) else None
+    sigma = as_tensor(sigma, dtype=torch.float, device=device)
     x = torch.arange(window_size).float() - window_size // 2
     t = 0.70710678 / torch.abs(sigma)
     gauss = 0.5 * ((t * (x + 0.5)).erf() - (t * (x - 0.5)).erf())
     gauss = gauss.clamp(min=0)
     return gauss / gauss.sum()
 
 
-def _modified_bessel_0(x: torch.Tensor) -> torch.Tensor:
+def _modified_bessel_0(x: Tensor) -> Tensor:
     r"""Adapted from:
 
     https://github.com/Project-MONAI/MONAI/blob/master/monai/networks/layers/convutils.py
     """
     if torch.abs(x) < 3.75:
         y = (x / 3.75) * (x / 3.75)
         return 1.0 + y * (
@@ -52,15 +53,15 @@
     ax = torch.abs(x)
     y = 3.75 / ax
     ans = 0.916281e-2 + y * (-0.2057706e-1 + y * (0.2635537e-1 + y * (-0.1647633e-1 + y * 0.392377e-2)))
     coef = 0.39894228 + y * (0.1328592e-1 + y * (0.225319e-2 + y * (-0.157565e-2 + y * ans)))
     return (torch.exp(ax) / torch.sqrt(ax)) * coef
 
 
-def _modified_bessel_1(x: torch.Tensor) -> torch.Tensor:
+def _modified_bessel_1(x: Tensor) -> Tensor:
     r"""adapted from:
 
     https://github.com/Project-MONAI/MONAI/blob/master/monai/networks/layers/convutils.py
     """
     if torch.abs(x) < 3.75:
         y = (x / 3.75) * (x / 3.75)
         ans = 0.51498869 + y * (0.15084934 + y * (0.2658733e-1 + y * (0.301532e-2 + y * 0.32411e-3)))
@@ -69,28 +70,28 @@
     y = 3.75 / ax
     ans = 0.2282967e-1 + y * (-0.2895312e-1 + y * (0.1787654e-1 - y * 0.420059e-2))
     ans = 0.39894228 + y * (-0.3988024e-1 + y * (-0.362018e-2 + y * (0.163801e-2 + y * (-0.1031555e-1 + y * ans))))
     ans = ans * torch.exp(ax) / torch.sqrt(ax)
     return -ans if x < 0.0 else ans
 
 
-def _modified_bessel_i(n: int, x: torch.Tensor) -> torch.Tensor:
+def _modified_bessel_i(n: int, x: Tensor) -> Tensor:
     r"""adapted from:
 
     https://github.com/Project-MONAI/MONAI/blob/master/monai/networks/layers/convutils.py
     """
     if n < 2:
         raise ValueError("n must be greater than 1.")
     if x == 0.0:
         return x
     device = x.device
     tox = 2.0 / torch.abs(x)
-    ans = torch.tensor(0.0, device=device)
-    bip = torch.tensor(0.0, device=device)
-    bi = torch.tensor(1.0, device=device)
+    ans = tensor(0.0, device=device)
+    bip = tensor(0.0, device=device)
+    bi = tensor(1.0, device=device)
     m = int(2 * (n + int(sqrt(40.0 * n))))
     for j in range(m, 0, -1):
         bim = bip + float(j) * tox * bi
         bip = bi
         bi = bim
         if abs(bi) > 1.0e10:
             ans = ans * 1.0e-10
@@ -98,104 +99,103 @@
             bip = bip * 1.0e-10
         if j == n:
             ans = bip
     ans = ans * _modified_bessel_0(x) / bi
     return -ans if x < 0.0 and (n % 2) == 1 else ans
 
 
-def gaussian_discrete(window_size, sigma) -> torch.Tensor:
+def gaussian_discrete(window_size, sigma) -> Tensor:
     r"""Discrete Gaussian kernel based on the modified Bessel functions.
 
-    Adapted from:
-    https://github.com/Project-MONAI/MONAI/blob/master/monai/networks/layers/convutils.py
+    Adapted from: https://github.com/Project-MONAI/MONAI/blob/master/monai/networks/layers/convutils.py
     """
-    device = sigma.device if isinstance(sigma, torch.Tensor) else None
-    sigma = torch.as_tensor(sigma, dtype=torch.float, device=device)
+    device = sigma.device if isinstance(sigma, Tensor) else None
+    sigma = as_tensor(sigma, dtype=torch.float, device=device)
     sigma2 = sigma * sigma
-    tail = int(window_size // 2)
-    out_pos: List[Optional[torch.Tensor]] = [None] * (tail + 1)
-    out_pos[0] = _modified_bessel_0(sigma2)
-    out_pos[1] = _modified_bessel_1(sigma2)
-    for k in range(2, len(out_pos)):
-        out_pos[k] = _modified_bessel_i(k, sigma2)
-    out = out_pos[:0:-1]
-    out.extend(out_pos)
-    out = torch.stack(out) * torch.exp(sigma2)  # type: ignore
-    return out / out.sum()  # type: ignore
+    tail = int(window_size // 2) + 1
+    bessels = [
+        _modified_bessel_0(sigma2),
+        _modified_bessel_1(sigma2),
+        *(_modified_bessel_i(k, sigma2) for k in range(2, tail)),
+    ]
+
+    out = stack(bessels[:0:-1] + bessels, 0) * torch.exp(sigma2)
+
+    return out / out.sum()
 
 
-def laplacian_1d(window_size) -> torch.Tensor:
+def laplacian_1d(window_size) -> Tensor:
     r"""One could also use the Laplacian of Gaussian formula to design the filter."""
 
     filter_1d = torch.ones(window_size)
     filter_1d[window_size // 2] = 1 - window_size
-    laplacian_1d: torch.Tensor = filter_1d
+    laplacian_1d: Tensor = filter_1d
     return laplacian_1d
 
 
-def get_box_kernel2d(kernel_size: Tuple[int, int]) -> torch.Tensor:
+def get_box_kernel2d(kernel_size: Tuple[int, int]) -> Tensor:
     r"""Utility function that returns a box filter."""
     kx: float = float(kernel_size[0])
     ky: float = float(kernel_size[1])
-    scale: torch.Tensor = torch.tensor(1.0) / torch.tensor([kx * ky])
-    tmp_kernel: torch.Tensor = torch.ones(1, kernel_size[0], kernel_size[1])
+    scale: Tensor = tensor(1.0) / tensor([kx * ky])
+    tmp_kernel: Tensor = torch.ones(1, kernel_size[0], kernel_size[1])
     return scale.to(tmp_kernel.dtype) * tmp_kernel
 
 
-def get_binary_kernel2d(window_size: Tuple[int, int]) -> torch.Tensor:
+def get_binary_kernel2d(window_size: Tuple[int, int]) -> Tensor:
     r"""Create a binary kernel to extract the patches.
 
     If the window size is HxW will create a (H*W)xHxW kernel.
     """
     window_range: int = window_size[0] * window_size[1]
-    kernel: torch.Tensor = torch.zeros(window_range, window_range)
+    kernel: Tensor = zeros(window_range, window_range)
     for i in range(window_range):
         kernel[i, i] += 1.0
     return kernel.view(window_range, 1, window_size[0], window_size[1])
 
 
-def get_sobel_kernel_3x3() -> torch.Tensor:
+def get_sobel_kernel_3x3() -> Tensor:
     """Utility function that returns a sobel kernel of 3x3."""
-    return torch.tensor([[-1.0, 0.0, 1.0], [-2.0, 0.0, 2.0], [-1.0, 0.0, 1.0]])
+    return tensor([[-1.0, 0.0, 1.0], [-2.0, 0.0, 2.0], [-1.0, 0.0, 1.0]])
 
 
-def get_sobel_kernel_5x5_2nd_order() -> torch.Tensor:
+def get_sobel_kernel_5x5_2nd_order() -> Tensor:
     """Utility function that returns a 2nd order sobel kernel of 5x5."""
-    return torch.tensor(
+    return tensor(
         [
             [-1.0, 0.0, 2.0, 0.0, -1.0],
             [-4.0, 0.0, 8.0, 0.0, -4.0],
             [-6.0, 0.0, 12.0, 0.0, -6.0],
             [-4.0, 0.0, 8.0, 0.0, -4.0],
             [-1.0, 0.0, 2.0, 0.0, -1.0],
         ]
     )
 
 
-def _get_sobel_kernel_5x5_2nd_order_xy() -> torch.Tensor:
+def _get_sobel_kernel_5x5_2nd_order_xy() -> Tensor:
     """Utility function that returns a 2nd order sobel kernel of 5x5."""
-    return torch.tensor(
+    return tensor(
         [
             [-1.0, -2.0, 0.0, 2.0, 1.0],
             [-2.0, -4.0, 0.0, 4.0, 2.0],
             [0.0, 0.0, 0.0, 0.0, 0.0],
             [2.0, 4.0, 0.0, -4.0, -2.0],
             [1.0, 2.0, 0.0, -2.0, -1.0],
         ]
     )
 
 
-def get_diff_kernel_3x3() -> torch.Tensor:
+def get_diff_kernel_3x3() -> Tensor:
     """Utility function that returns a first order derivative kernel of 3x3."""
-    return torch.tensor([[-0.0, 0.0, 0.0], [-1.0, 0.0, 1.0], [-0.0, 0.0, 0.0]])
+    return tensor([[-0.0, 0.0, 0.0], [-1.0, 0.0, 1.0], [-0.0, 0.0, 0.0]])
 
 
-def get_diff_kernel3d(device=torch.device('cpu'), dtype=torch.float) -> torch.Tensor:
+def get_diff_kernel3d(device=torch.device('cpu'), dtype=torch.float) -> Tensor:
     """Utility function that returns a first order derivative kernel of 3x3x3."""
-    kernel: torch.Tensor = torch.tensor(
+    kernel: Tensor = tensor(
         [
             [
                 [[0.0, 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0, 0.0]],
                 [[0.0, 0.0, 0.0], [-0.5, 0.0, 0.5], [0.0, 0.0, 0.0]],
                 [[0.0, 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0, 0.0]],
             ],
             [
@@ -211,17 +211,17 @@
         ],
         device=device,
         dtype=dtype,
     )
     return kernel.unsqueeze(1)
 
 
-def get_diff_kernel3d_2nd_order(device=torch.device('cpu'), dtype=torch.float) -> torch.Tensor:
+def get_diff_kernel3d_2nd_order(device=torch.device('cpu'), dtype=torch.float) -> Tensor:
     """Utility function that returns a first order derivative kernel of 3x3x3."""
-    kernel: torch.Tensor = torch.tensor(
+    kernel: Tensor = tensor(
         [
             [
                 [[0.0, 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0, 0.0]],
                 [[0.0, 0.0, 0.0], [1.0, -2.0, 1.0], [0.0, 0.0, 0.0]],
                 [[0.0, 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0, 0.0]],
             ],
             [
@@ -252,41 +252,41 @@
         ],
         device=device,
         dtype=dtype,
     )
     return kernel.unsqueeze(1)
 
 
-def get_sobel_kernel2d() -> torch.Tensor:
-    kernel_x: torch.Tensor = get_sobel_kernel_3x3()
-    kernel_y: torch.Tensor = kernel_x.transpose(0, 1)
-    return torch.stack([kernel_x, kernel_y])
+def get_sobel_kernel2d() -> Tensor:
+    kernel_x: Tensor = get_sobel_kernel_3x3()
+    kernel_y: Tensor = kernel_x.transpose(0, 1)
+    return stack([kernel_x, kernel_y])
 
 
-def get_diff_kernel2d() -> torch.Tensor:
-    kernel_x: torch.Tensor = get_diff_kernel_3x3()
-    kernel_y: torch.Tensor = kernel_x.transpose(0, 1)
-    return torch.stack([kernel_x, kernel_y])
+def get_diff_kernel2d() -> Tensor:
+    kernel_x: Tensor = get_diff_kernel_3x3()
+    kernel_y: Tensor = kernel_x.transpose(0, 1)
+    return stack([kernel_x, kernel_y])
 
 
-def get_sobel_kernel2d_2nd_order() -> torch.Tensor:
-    gxx: torch.Tensor = get_sobel_kernel_5x5_2nd_order()
-    gyy: torch.Tensor = gxx.transpose(0, 1)
-    gxy: torch.Tensor = _get_sobel_kernel_5x5_2nd_order_xy()
-    return torch.stack([gxx, gxy, gyy])
+def get_sobel_kernel2d_2nd_order() -> Tensor:
+    gxx: Tensor = get_sobel_kernel_5x5_2nd_order()
+    gyy: Tensor = gxx.transpose(0, 1)
+    gxy: Tensor = _get_sobel_kernel_5x5_2nd_order_xy()
+    return stack([gxx, gxy, gyy])
 
 
-def get_diff_kernel2d_2nd_order() -> torch.Tensor:
-    gxx: torch.Tensor = torch.tensor([[0.0, 0.0, 0.0], [1.0, -2.0, 1.0], [0.0, 0.0, 0.0]])
-    gyy: torch.Tensor = gxx.transpose(0, 1)
-    gxy: torch.Tensor = torch.tensor([[-1.0, 0.0, 1.0], [0.0, 0.0, 0.0], [1.0, 0.0, -1.0]])
-    return torch.stack([gxx, gxy, gyy])
+def get_diff_kernel2d_2nd_order() -> Tensor:
+    gxx: Tensor = tensor([[0.0, 0.0, 0.0], [1.0, -2.0, 1.0], [0.0, 0.0, 0.0]])
+    gyy: Tensor = gxx.transpose(0, 1)
+    gxy: Tensor = tensor([[-1.0, 0.0, 1.0], [0.0, 0.0, 0.0], [1.0, 0.0, -1.0]])
+    return stack([gxx, gxy, gyy])
 
 
-def get_spatial_gradient_kernel2d(mode: str, order: int) -> torch.Tensor:
+def get_spatial_gradient_kernel2d(mode: str, order: int) -> Tensor:
     r"""Function that returns kernel for 1st or 2nd order image gradients, using one of the following operators:
 
     sobel, diff.
     """
     if mode not in ['sobel', 'diff']:
         raise TypeError(
             "mode should be either sobel\
@@ -298,27 +298,27 @@
         raise TypeError(
             "order should be either 1 or 2\
                          Got {}".format(
                 order
             )
         )
     if mode == 'sobel' and order == 1:
-        kernel: torch.Tensor = get_sobel_kernel2d()
+        kernel: Tensor = get_sobel_kernel2d()
     elif mode == 'sobel' and order == 2:
         kernel = get_sobel_kernel2d_2nd_order()
     elif mode == 'diff' and order == 1:
         kernel = get_diff_kernel2d()
     elif mode == 'diff' and order == 2:
         kernel = get_diff_kernel2d_2nd_order()
     else:
         raise NotImplementedError("")
     return kernel
 
 
-def get_spatial_gradient_kernel3d(mode: str, order: int, device=torch.device('cpu'), dtype=torch.float) -> torch.Tensor:
+def get_spatial_gradient_kernel3d(mode: str, order: int, device=torch.device('cpu'), dtype=torch.float) -> Tensor:
     r"""Function that returns kernel for 1st or 2nd order scale pyramid gradients, using one of the following
     operators: sobel, diff."""
     if mode not in ['sobel', 'diff']:
         raise TypeError(
             "mode should be either sobel\
                          or diff. Got {}".format(
                 mode
@@ -338,15 +338,15 @@
     elif mode == 'diff' and order == 2:
         kernel = get_diff_kernel3d_2nd_order(device, dtype)
     else:
         raise NotImplementedError("")
     return kernel
 
 
-def get_gaussian_kernel1d(kernel_size: int, sigma: float, force_even: bool = False) -> torch.Tensor:
+def get_gaussian_kernel1d(kernel_size: int, sigma: float, force_even: bool = False) -> Tensor:
     r"""Function that returns Gaussian filter coefficients.
 
     Args:
         kernel_size: filter size. It should be odd and positive.
         sigma: gaussian standard deviation.
         force_even: overrides requirement for odd kernel size.
 
@@ -362,19 +362,19 @@
         tensor([0.3243, 0.3513, 0.3243])
 
         >>> get_gaussian_kernel1d(5, 1.5)
         tensor([0.1201, 0.2339, 0.2921, 0.2339, 0.1201])
     """
     if not isinstance(kernel_size, int) or ((kernel_size % 2 == 0) and not force_even) or (kernel_size <= 0):
         raise TypeError("kernel_size must be an odd positive integer. " "Got {}".format(kernel_size))
-    window_1d: torch.Tensor = gaussian(kernel_size, sigma)
+    window_1d: Tensor = gaussian(kernel_size, sigma)
     return window_1d
 
 
-def get_gaussian_discrete_kernel1d(kernel_size: int, sigma: float, force_even: bool = False) -> torch.Tensor:
+def get_gaussian_discrete_kernel1d(kernel_size: int, sigma: float, force_even: bool = False) -> Tensor:
     r"""Function that returns Gaussian filter coefficients based on the modified Bessel functions. Adapted from:
     https://github.com/Project-MONAI/MONAI/blob/master/monai/networks/layers/convutils.py.
 
     Args:
         kernel_size: filter size. It should be odd and positive.
         sigma: gaussian standard deviation.
         force_even: overrides requirement for odd kernel size.
@@ -395,15 +395,15 @@
     """
     if not isinstance(kernel_size, int) or ((kernel_size % 2 == 0) and not force_even) or (kernel_size <= 0):
         raise TypeError("kernel_size must be an odd positive integer. " "Got {}".format(kernel_size))
     window_1d = gaussian_discrete(kernel_size, sigma)
     return window_1d
 
 
-def get_gaussian_erf_kernel1d(kernel_size: int, sigma: float, force_even: bool = False) -> torch.Tensor:
+def get_gaussian_erf_kernel1d(kernel_size: int, sigma: float, force_even: bool = False) -> Tensor:
     r"""Function that returns Gaussian filter coefficients by interpolating the error function, adapted from:
     https://github.com/Project-MONAI/MONAI/blob/master/monai/networks/layers/convutils.py.
 
     Args:
         kernel_size: filter size. It should be odd and positive.
         sigma: gaussian standard deviation.
         force_even: overrides requirement for odd kernel size.
@@ -424,17 +424,15 @@
     """
     if not isinstance(kernel_size, int) or ((kernel_size % 2 == 0) and not force_even) or (kernel_size <= 0):
         raise TypeError("kernel_size must be an odd positive integer. " "Got {}".format(kernel_size))
     window_1d = gaussian_discrete_erf(kernel_size, sigma)
     return window_1d
 
 
-def get_gaussian_kernel2d(
-    kernel_size: Tuple[int, int], sigma: Tuple[float, float], force_even: bool = False
-) -> torch.Tensor:
+def get_gaussian_kernel2d(kernel_size: Tuple[int, int], sigma: Tuple[float, float], force_even: bool = False) -> Tensor:
     r"""Function that returns Gaussian filter matrix coefficients.
 
     Args:
         kernel_size: filter sizes in the x and y direction.
          Sizes should be odd and positive.
         sigma: gaussian standard deviation in the x and y
          direction.
@@ -458,21 +456,21 @@
     """
     if not isinstance(kernel_size, tuple) or len(kernel_size) != 2:
         raise TypeError(f"kernel_size must be a tuple of length two. Got {kernel_size}")
     if not isinstance(sigma, tuple) or len(sigma) != 2:
         raise TypeError(f"sigma must be a tuple of length two. Got {sigma}")
     ksize_x, ksize_y = kernel_size
     sigma_x, sigma_y = sigma
-    kernel_x: torch.Tensor = get_gaussian_kernel1d(ksize_x, sigma_x, force_even)
-    kernel_y: torch.Tensor = get_gaussian_kernel1d(ksize_y, sigma_y, force_even)
-    kernel_2d: torch.Tensor = torch.matmul(kernel_x.unsqueeze(-1), kernel_y.unsqueeze(-1).t())
+    kernel_x: Tensor = get_gaussian_kernel1d(ksize_x, sigma_x, force_even)
+    kernel_y: Tensor = get_gaussian_kernel1d(ksize_y, sigma_y, force_even)
+    kernel_2d: Tensor = torch.matmul(kernel_x.unsqueeze(-1), kernel_y.unsqueeze(-1).t())
     return kernel_2d
 
 
-def get_laplacian_kernel1d(kernel_size: int) -> torch.Tensor:
+def get_laplacian_kernel1d(kernel_size: int) -> Tensor:
     r"""Function that returns the coefficients of a 1D Laplacian filter.
 
     Args:
         kernel_size: filter size. It should be odd and positive.
 
     Returns:
         1D tensor with laplacian filter coefficients.
@@ -484,19 +482,19 @@
         >>> get_laplacian_kernel1d(3)
         tensor([ 1., -2.,  1.])
         >>> get_laplacian_kernel1d(5)
         tensor([ 1.,  1., -4.,  1.,  1.])
     """
     if not isinstance(kernel_size, int) or kernel_size % 2 == 0 or kernel_size <= 0:
         raise TypeError(f"ksize must be an odd positive integer. Got {kernel_size}")
-    window_1d: torch.Tensor = laplacian_1d(kernel_size)
+    window_1d: Tensor = laplacian_1d(kernel_size)
     return window_1d
 
 
-def get_laplacian_kernel2d(kernel_size: int) -> torch.Tensor:
+def get_laplacian_kernel2d(kernel_size: int) -> Tensor:
     r"""Function that returns Gaussian filter matrix coefficients.
 
     Args:
         kernel_size: filter size should be odd.
 
     Returns:
         2D tensor with laplacian filter matrix coefficients.
@@ -518,19 +516,19 @@
     """
     if not isinstance(kernel_size, int) or kernel_size % 2 == 0 or kernel_size <= 0:
         raise TypeError(f"ksize must be an odd positive integer. Got {kernel_size}")
 
     kernel = torch.ones((kernel_size, kernel_size))
     mid = kernel_size // 2
     kernel[mid, mid] = 1 - kernel_size**2
-    kernel_2d: torch.Tensor = kernel
+    kernel_2d: Tensor = kernel
     return kernel_2d
 
 
-def get_pascal_kernel_2d(kernel_size: int, norm: bool = True) -> torch.Tensor:
+def get_pascal_kernel_2d(kernel_size: int, norm: bool = True) -> Tensor:
     """Generate pascal filter kernel by kernel size.
 
     Args:
         kernel_size: height and width of the kernel.
         norm: if to normalize the kernel or not. Default: True.
 
     Returns:
@@ -554,15 +552,15 @@
 
     filt = a[:, None] * a[None, :]
     if norm:
         filt = filt / torch.sum(filt)
     return filt
 
 
-def get_pascal_kernel_1d(kernel_size: int, norm: bool = False) -> torch.Tensor:
+def get_pascal_kernel_1d(kernel_size: int, norm: bool = False) -> Tensor:
     """Generate Yang Hui triangle (Pascal's triangle) by a given number.
 
     Args:
         kernel_size: height and width of the kernel.
         norm: if to normalize the kernel or not. Default: False.
 
     Returns:
@@ -590,23 +588,23 @@
         for j in range(1, i // 2 + 1):
             value = pre[j - 1] + pre[j]
             cur[j] = value
             if i != 2 * j:
                 cur[-j - 1] = value
         pre = cur
 
-    out = torch.as_tensor(cur)
+    out = as_tensor(cur)
     if norm:
         out = out / torch.sum(out)
     return out
 
 
-def get_canny_nms_kernel(device=torch.device('cpu'), dtype=torch.float) -> torch.Tensor:
+def get_canny_nms_kernel(device=torch.device('cpu'), dtype=torch.float) -> Tensor:
     """Utility function that returns 3x3 kernels for the Canny Non-maximal suppression."""
-    kernel: torch.Tensor = torch.tensor(
+    kernel: Tensor = tensor(
         [
             [[0.0, 0.0, 0.0], [0.0, 1.0, -1.0], [0.0, 0.0, 0.0]],
             [[0.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, -1.0]],
             [[0.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, -1.0, 0.0]],
             [[0.0, 0.0, 0.0], [0.0, 1.0, 0.0], [-1.0, 0.0, 0.0]],
             [[0.0, 0.0, 0.0], [-1.0, 1.0, 0.0], [0.0, 0.0, 0.0]],
             [[-1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 0.0]],
@@ -615,17 +613,17 @@
         ],
         device=device,
         dtype=dtype,
     )
     return kernel.unsqueeze(1)
 
 
-def get_hysteresis_kernel(device=torch.device('cpu'), dtype=torch.float) -> torch.Tensor:
+def get_hysteresis_kernel(device=torch.device('cpu'), dtype=torch.float) -> Tensor:
     """Utility function that returns the 3x3 kernels for the Canny hysteresis."""
-    kernel: torch.Tensor = torch.tensor(
+    kernel: Tensor = tensor(
         [
             [[0.0, 0.0, 0.0], [0.0, 0.0, 1.0], [0.0, 0.0, 0.0]],
             [[0.0, 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0, 1.0]],
             [[0.0, 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 1.0, 0.0]],
             [[0.0, 0.0, 0.0], [0.0, 0.0, 0.0], [1.0, 0.0, 0.0]],
             [[0.0, 0.0, 0.0], [1.0, 0.0, 0.0], [0.0, 0.0, 0.0]],
             [[1.0, 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0, 0.0]],
@@ -634,15 +632,15 @@
         ],
         device=device,
         dtype=dtype,
     )
     return kernel.unsqueeze(1)
 
 
-def get_hanning_kernel1d(kernel_size: int, device=torch.device('cpu'), dtype=torch.float) -> torch.Tensor:
+def get_hanning_kernel1d(kernel_size: int, device=torch.device('cpu'), dtype=torch.float) -> Tensor:
     r"""Returns Hanning (also known as Hann) kernel, used in signal processing and KCF tracker.
 
     .. math::  w(n) = 0.5 - 0.5cos\\left(\\frac{2\\pi{n}}{M-1}\\right)
                \\qquad 0 \\leq n \\leq M-1
 
     See further in numpy docs https://numpy.org/doc/stable/reference/generated/numpy.hanning.html
 
@@ -659,31 +657,31 @@
     Examples:
         >>> get_hanning_kernel1d(4)
         tensor([0.0000, 0.7500, 0.7500, 0.0000])
     """
     if not isinstance(kernel_size, int) or kernel_size <= 2:
         raise TypeError(f"ksize must be an positive integer > 2. Got {kernel_size}")
 
-    x: torch.Tensor = torch.arange(kernel_size, device=device, dtype=dtype)
+    x: Tensor = torch.arange(kernel_size, device=device, dtype=dtype)
     x = 0.5 - 0.5 * torch.cos(2.0 * math.pi * x / float(kernel_size - 1))
     return x
 
 
-def get_hanning_kernel2d(kernel_size: Tuple[int, int], device=torch.device('cpu'), dtype=torch.float) -> torch.Tensor:
+def get_hanning_kernel2d(kernel_size: Tuple[int, int], device=torch.device('cpu'), dtype=torch.float) -> Tensor:
     r"""Returns 2d Hanning kernel, used in signal processing and KCF tracker.
 
     Args:
         kernel_size: The size of the kernel for the filter. It should be positive.
 
     Returns:
         2D tensor with Hanning filter coefficients.
             .. math::  w(n) = 0.5 - 0.5cos\\left(\\frac{2\\pi{n}}{M-1}\\right)
 
     Shape:
         - Output: math:`(\text{kernel_size[0], kernel_size[1]})`
     """
     if kernel_size[0] <= 2 or kernel_size[1] <= 2:
         raise TypeError(f"ksize must be an tuple of positive integers > 2. Got {kernel_size}")
-    ky: torch.Tensor = get_hanning_kernel1d(kernel_size[0], device, dtype)[None].T
-    kx: torch.Tensor = get_hanning_kernel1d(kernel_size[1], device, dtype)[None]
+    ky: Tensor = get_hanning_kernel1d(kernel_size[0], device, dtype)[None].T
+    kx: Tensor = get_hanning_kernel1d(kernel_size[1], device, dtype)[None]
     kernel2d = ky @ kx
     return kernel2d
```

### Comparing `kornia-0.6.8/kornia/filters/kernels_geometry.py` & `kornia-0.6.9/kornia/filters/kernels_geometry.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,21 +1,19 @@
-from typing import Tuple, Union, cast
+from typing import Tuple, Union
 
 import torch
 
+from kornia.core import Tensor, pad, stack, tensor, zeros
 from kornia.geometry.transform import rotate, rotate3d
 from kornia.utils import _extract_device_dtype
 
 
 def get_motion_kernel2d(
-    kernel_size: int,
-    angle: Union[torch.Tensor, float],
-    direction: Union[torch.Tensor, float] = 0.0,
-    mode: str = 'nearest',
-) -> torch.Tensor:
+    kernel_size: int, angle: Union[Tensor, float], direction: Union[Tensor, float] = 0.0, mode: str = 'nearest'
+) -> Tensor:
     r"""Return 2D motion blur filter.
 
     Args:
         kernel_size: motion kernel width and height. It should be odd and positive.
         angle: angle of the motion blur in degrees (anti-clockwise rotation).
         direction: forward/backward direction of the motion blur.
             Lower values towards -1.0 will point the motion blur towards the back (with angle provided via angle),
@@ -36,35 +34,31 @@
 
         >>> get_motion_kernel2d(3, 215., -0.5)
         tensor([[[0.0000, 0.0000, 0.1667],
                  [0.0000, 0.3333, 0.0000],
                  [0.5000, 0.0000, 0.0000]]])
     """
     device, dtype = _extract_device_dtype(
-        [angle if isinstance(angle, torch.Tensor) else None, direction if isinstance(direction, torch.Tensor) else None]
+        [angle if isinstance(angle, Tensor) else None, direction if isinstance(direction, Tensor) else None]
     )
 
     if not isinstance(kernel_size, int) or kernel_size % 2 == 0 or kernel_size < 3:
         raise TypeError("ksize must be an odd integer >= than 3")
 
-    if not isinstance(angle, torch.Tensor):
-        angle = torch.tensor([angle], device=device, dtype=dtype)
-
-    angle = cast(torch.Tensor, angle)
+    if not isinstance(angle, Tensor):
+        angle = tensor([angle], device=device, dtype=dtype)
 
     if angle.dim() == 0:
         angle = angle.unsqueeze(0)
 
     if angle.dim() != 1:
         raise AssertionError(f"angle must be a 1-dim tensor. Got {angle}.")
 
-    if not isinstance(direction, torch.Tensor):
-        direction = torch.tensor([direction], device=device, dtype=dtype)
-
-    direction = cast(torch.Tensor, direction)
+    if not isinstance(direction, Tensor):
+        direction = tensor([direction], device=device, dtype=dtype)
 
     if direction.dim() == 0:
         direction = direction.unsqueeze(0)
 
     if direction.dim() != 1:
         raise AssertionError(f"direction must be a 1-dim tensor. Got {direction}.")
 
@@ -79,34 +73,34 @@
 
     # Element-wise linspace
     # kernel[:, kernel_size // 2, :] = torch.stack(
     #     [(direction + ((1 - 2 * direction) / (kernel_size - 1)) * i) for i in range(kernel_size)], dim=-1)
     # Alternatively
     # m = ((1 - 2 * direction)[:, None].repeat(1, kernel_size) / (kernel_size - 1))
     # kernel[:, kernel_size // 2, :] = direction[:, None].repeat(1, kernel_size) + m * torch.arange(0, kernel_size)
-    k = torch.stack([(direction + ((1 - 2 * direction) / (kernel_size - 1)) * i) for i in range(kernel_size)], dim=-1)
-    kernel = torch.nn.functional.pad(k[:, None], [0, 0, kernel_size // 2, kernel_size // 2, 0, 0])
+    k = stack([(direction + ((1 - 2 * direction) / (kernel_size - 1)) * i) for i in range(kernel_size)], -1)
+    kernel = pad(k[:, None], [0, 0, kernel_size // 2, kernel_size // 2, 0, 0])
 
     if kernel.shape != torch.Size([direction.size(0), *kernel_tuple]):
         raise AssertionError
     kernel = kernel.unsqueeze(1)
 
     # rotate (counterclockwise) kernel by given angle
     kernel = rotate(kernel, angle, mode=mode, align_corners=True)
     kernel = kernel[:, 0]
     kernel = kernel / kernel.sum(dim=(1, 2), keepdim=True)
     return kernel
 
 
 def get_motion_kernel3d(
     kernel_size: int,
-    angle: Union[torch.Tensor, Tuple[float, float, float]],
-    direction: Union[torch.Tensor, float] = 0.0,
+    angle: Union[Tensor, Tuple[float, float, float]],
+    direction: Union[Tensor, float] = 0.0,
     mode: str = 'nearest',
-) -> torch.Tensor:
+) -> Tensor:
     r"""Return 3D motion blur filter.
 
     Args:
         kernel_size: motion kernel width, height and depth. It should be odd and positive.
         angle: Range of yaw (x-axis), pitch (y-axis), roll (z-axis) to select from.
             If tensor, it must be :math:`(B, 3)`.
             If tuple, it must be (yaw, pitch, raw).
@@ -146,55 +140,49 @@
                   [0.0000, 0.0000, 0.0000],
                   [0.0000, 0.0000, 0.0000]]]])
     """
     if not isinstance(kernel_size, int) or kernel_size % 2 == 0 or kernel_size < 3:
         raise TypeError(f"ksize must be an odd integer >= than 3. Got {kernel_size}.")
 
     device, dtype = _extract_device_dtype(
-        [angle if isinstance(angle, torch.Tensor) else None, direction if isinstance(direction, torch.Tensor) else None]
+        [angle if isinstance(angle, Tensor) else None, direction if isinstance(direction, Tensor) else None]
     )
 
-    if not isinstance(angle, torch.Tensor):
-        angle = torch.tensor([angle], device=device, dtype=dtype)
-
-    angle = cast(torch.Tensor, angle)
+    if not isinstance(angle, Tensor):
+        angle = tensor([angle], device=device, dtype=dtype)
 
     if angle.dim() == 1:
         angle = angle.unsqueeze(0)
 
     if not (len(angle.shape) == 2 and angle.size(1) == 3):
         raise AssertionError(f"angle must be (B, 3). Got {angle}.")
 
-    if not isinstance(direction, torch.Tensor):
-        direction = torch.tensor([direction], device=device, dtype=dtype)
-
-    direction = cast(torch.Tensor, direction)
+    if not isinstance(direction, Tensor):
+        direction = tensor([direction], device=device, dtype=dtype)
 
     if direction.dim() == 0:
         direction = direction.unsqueeze(0)
 
     if direction.dim() != 1:
         raise AssertionError(f"direction must be a 1-dim tensor. Got {direction}.")
 
     if direction.size(0) != angle.size(0):
         raise AssertionError(f"direction and angle must have the same length. Got {direction} and {angle}.")
 
     kernel_tuple: Tuple[int, int, int] = (kernel_size, kernel_size, kernel_size)
 
     # direction from [-1, 1] to [0, 1] range
     direction = (torch.clamp(direction, -1.0, 1.0) + 1.0) / 2.0
-    kernel = torch.zeros((direction.size(0), *kernel_tuple), device=device, dtype=dtype)
+    kernel = zeros((direction.size(0), *kernel_tuple), device=device, dtype=dtype)
 
     # Element-wise linspace
     # kernel[:, kernel_size // 2, kernel_size // 2, :] = torch.stack(
     #     [(direction + ((1 - 2 * direction) / (kernel_size - 1)) * i) for i in range(kernel_size)], dim=-1)
-    k = torch.stack([(direction + ((1 - 2 * direction) / (kernel_size - 1)) * i) for i in range(kernel_size)], dim=-1)
-    kernel = torch.nn.functional.pad(
-        k[:, None, None], [0, 0, kernel_size // 2, kernel_size // 2, kernel_size // 2, kernel_size // 2, 0, 0]
-    )
+    k = stack([(direction + ((1 - 2 * direction) / (kernel_size - 1)) * i) for i in range(kernel_size)], -1)
+    kernel = pad(k[:, None, None], [0, 0, kernel_size // 2, kernel_size // 2, kernel_size // 2, kernel_size // 2, 0, 0])
 
     if kernel.shape != torch.Size([direction.size(0), *kernel_tuple]):
         raise AssertionError
     kernel = kernel.unsqueeze(1)
 
     # rotate (counterclockwise) kernel by given angle
     kernel = rotate3d(kernel, angle[:, 0], angle[:, 1], angle[:, 2], mode=mode, align_corners=True)
```

### Comparing `kornia-0.6.8/kornia/filters/laplacian.py` & `kornia-0.6.9/kornia/filters/laplacian.py`

 * *Files identical despite different names*

### Comparing `kornia-0.6.8/kornia/filters/median.py` & `kornia-0.6.9/kornia/filters/median.py`

 * *Files identical despite different names*

### Comparing `kornia-0.6.8/kornia/filters/sobel.py` & `kornia-0.6.9/kornia/filters/sobel.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,15 +1,16 @@
 import torch
-import torch.nn as nn
 import torch.nn.functional as F
 
+from kornia.core import Module, Tensor, pad
+
 from .kernels import get_spatial_gradient_kernel2d, get_spatial_gradient_kernel3d, normalize_kernel2d
 
 
-def spatial_gradient(input: torch.Tensor, mode: str = 'sobel', order: int = 1, normalized: bool = True) -> torch.Tensor:
+def spatial_gradient(input: Tensor, mode: str = 'sobel', order: int = 1, normalized: bool = True) -> Tensor:
     r"""Compute the first order image derivative in both x and y using a Sobel operator.
 
     .. image:: _static/img/spatial_gradient.png
 
     Args:
         input: input image tensor with shape :math:`(B, C, H, W)`.
         mode: derivatives modality, can be: `sobel` or `diff`.
@@ -25,38 +26,38 @@
 
     Examples:
         >>> input = torch.rand(1, 3, 4, 4)
         >>> output = spatial_gradient(input)  # 1x3x2x4x4
         >>> output.shape
         torch.Size([1, 3, 2, 4, 4])
     """
-    if not isinstance(input, torch.Tensor):
-        raise TypeError(f"Input type is not a torch.Tensor. Got {type(input)}")
+    if not isinstance(input, Tensor):
+        raise TypeError(f"Input type is not a Tensor. Got {type(input)}")
 
     if not len(input.shape) == 4:
         raise ValueError(f"Invalid input shape, we expect BxCxHxW. Got: {input.shape}")
     # allocate kernel
-    kernel: torch.Tensor = get_spatial_gradient_kernel2d(mode, order)
+    kernel: Tensor = get_spatial_gradient_kernel2d(mode, order)
     if normalized:
         kernel = normalize_kernel2d(kernel)
 
     # prepare kernel
     b, c, h, w = input.shape
-    tmp_kernel: torch.Tensor = kernel.to(input).detach()
+    tmp_kernel: Tensor = kernel.to(input).detach()
     tmp_kernel = tmp_kernel.unsqueeze(1)
 
     # Pad with "replicate for spatial dims, but with zeros for channel
     spatial_pad = [kernel.size(1) // 2, kernel.size(1) // 2, kernel.size(2) // 2, kernel.size(2) // 2]
     out_channels: int = 3 if order == 2 else 2
-    padded_inp: torch.Tensor = F.pad(input.reshape(b * c, 1, h, w), spatial_pad, 'replicate')
+    padded_inp: Tensor = pad(input.reshape(b * c, 1, h, w), spatial_pad, 'replicate')
     out = F.conv2d(padded_inp, tmp_kernel, groups=1, padding=0, stride=1)
     return out.reshape(b, c, out_channels, h, w)
 
 
-def spatial_gradient3d(input: torch.Tensor, mode: str = 'diff', order: int = 1) -> torch.Tensor:
+def spatial_gradient3d(input: Tensor, mode: str = 'diff', order: int = 1) -> Tensor:
     r"""Compute the first and second order volume derivative in x, y and d using a diff operator.
 
     Args:
         input: input features tensor with shape :math:`(B, C, D, H, W)`.
         mode: derivatives modality, can be: `sobel` or `diff`.
         order: the order of the derivatives.
 
@@ -66,61 +67,61 @@
 
     Examples:
         >>> input = torch.rand(1, 4, 2, 4, 4)
         >>> output = spatial_gradient3d(input)
         >>> output.shape
         torch.Size([1, 4, 3, 2, 4, 4])
     """
-    if not isinstance(input, torch.Tensor):
-        raise TypeError(f"Input type is not a torch.Tensor. Got {type(input)}")
+    if not isinstance(input, Tensor):
+        raise TypeError(f"Input type is not a Tensor. Got {type(input)}")
 
     if not len(input.shape) == 5:
         raise ValueError(f"Invalid input shape, we expect BxCxDxHxW. Got: {input.shape}")
     b, c, d, h, w = input.shape
     dev = input.device
     dtype = input.dtype
     if (mode == 'diff') and (order == 1):
         # we go for the special case implementation due to conv3d bad speed
-        x: torch.Tensor = F.pad(input, 6 * [1], 'replicate')
+        x: Tensor = pad(input, 6 * [1], 'replicate')
         center = slice(1, -1)
         left = slice(0, -2)
         right = slice(2, None)
         out = torch.empty(b, c, 3, d, h, w, device=dev, dtype=dtype)
         out[..., 0, :, :, :] = x[..., center, center, right] - x[..., center, center, left]
         out[..., 1, :, :, :] = x[..., center, right, center] - x[..., center, left, center]
         out[..., 2, :, :, :] = x[..., right, center, center] - x[..., left, center, center]
         out = 0.5 * out
     else:
         # prepare kernel
         # allocate kernel
-        kernel: torch.Tensor = get_spatial_gradient_kernel3d(mode, order)
+        kernel: Tensor = get_spatial_gradient_kernel3d(mode, order)
 
-        tmp_kernel: torch.Tensor = kernel.to(input).detach()
+        tmp_kernel: Tensor = kernel.to(input).detach()
         tmp_kernel = tmp_kernel.repeat(c, 1, 1, 1, 1)
 
         # convolve input tensor with grad kernel
-        kernel_flip: torch.Tensor = tmp_kernel.flip(-3)
+        kernel_flip: Tensor = tmp_kernel.flip(-3)
 
         # Pad with "replicate for spatial dims, but with zeros for channel
         spatial_pad = [
             kernel.size(2) // 2,
             kernel.size(2) // 2,
             kernel.size(3) // 2,
             kernel.size(3) // 2,
             kernel.size(4) // 2,
             kernel.size(4) // 2,
         ]
         out_ch: int = 6 if order == 2 else 3
-        out = F.conv3d(F.pad(input, spatial_pad, 'replicate'), kernel_flip, padding=0, groups=c).view(
+        out = F.conv3d(pad(input, spatial_pad, 'replicate'), kernel_flip, padding=0, groups=c).view(
             b, c, out_ch, d, h, w
         )
     return out
 
 
-def sobel(input: torch.Tensor, normalized: bool = True, eps: float = 1e-6) -> torch.Tensor:
+def sobel(input: Tensor, normalized: bool = True, eps: float = 1e-6) -> Tensor:
     r"""Compute the Sobel operator and returns the magnitude per channel.
 
     .. image:: _static/img/sobel.png
 
     Args:
         input: the input image with shape :math:`(B,C,H,W)`.
         normalized: if True, L1 norm of the kernel is set to 1.
@@ -135,34 +136,34 @@
 
     Example:
         >>> input = torch.rand(1, 3, 4, 4)
         >>> output = sobel(input)  # 1x3x4x4
         >>> output.shape
         torch.Size([1, 3, 4, 4])
     """
-    if not isinstance(input, torch.Tensor):
-        raise TypeError(f"Input type is not a torch.Tensor. Got {type(input)}")
+    if not isinstance(input, Tensor):
+        raise TypeError(f"Input type is not a Tensor. Got {type(input)}")
 
     if not len(input.shape) == 4:
         raise ValueError(f"Invalid input shape, we expect BxCxHxW. Got: {input.shape}")
 
     # comput the x/y gradients
-    edges: torch.Tensor = spatial_gradient(input, normalized=normalized)
+    edges: Tensor = spatial_gradient(input, normalized=normalized)
 
     # unpack the edges
-    gx: torch.Tensor = edges[:, :, 0]
-    gy: torch.Tensor = edges[:, :, 1]
+    gx: Tensor = edges[:, :, 0]
+    gy: Tensor = edges[:, :, 1]
 
     # compute gradient maginitude
-    magnitude: torch.Tensor = torch.sqrt(gx * gx + gy * gy + eps)
+    magnitude: Tensor = torch.sqrt(gx * gx + gy * gy + eps)
 
     return magnitude
 
 
-class SpatialGradient(nn.Module):
+class SpatialGradient(Module):
     r"""Compute the first order image derivative in both x and y using a Sobel operator.
 
     Args:
         mode: derivatives modality, can be: `sobel` or `diff`.
         order: the order of the derivatives.
         normalized: whether the output is normalized.
 
@@ -186,19 +187,19 @@
 
     def __repr__(self) -> str:
         return (
             self.__class__.__name__ + '('
             'order=' + str(self.order) + ', ' + 'normalized=' + str(self.normalized) + ', ' + 'mode=' + self.mode + ')'
         )
 
-    def forward(self, input: torch.Tensor) -> torch.Tensor:
+    def forward(self, input: Tensor) -> Tensor:
         return spatial_gradient(input, self.mode, self.order, self.normalized)
 
 
-class SpatialGradient3d(nn.Module):
+class SpatialGradient3d(Module):
     r"""Compute the first and second order volume derivative in x, y and d using a diff operator.
 
     Args:
         mode: derivatives modality, can be: `sobel` or `diff`.
         order: the order of the derivatives.
 
     Return:
@@ -221,19 +222,19 @@
         self.mode: str = mode
         self.kernel = get_spatial_gradient_kernel3d(mode, order)
         return
 
     def __repr__(self) -> str:
         return self.__class__.__name__ + '(' 'order=' + str(self.order) + ', ' + 'mode=' + self.mode + ')'
 
-    def forward(self, input: torch.Tensor) -> torch.Tensor:  # type: ignore
+    def forward(self, input: Tensor) -> Tensor:
         return spatial_gradient3d(input, self.mode, self.order)
 
 
-class Sobel(nn.Module):
+class Sobel(Module):
     r"""Compute the Sobel operator and returns the magnitude per channel.
 
     Args:
         normalized: if True, L1 norm of the kernel is set to 1.
         eps: regularization number to avoid NaN during backprop.
 
     Return:
@@ -252,9 +253,9 @@
         super().__init__()
         self.normalized: bool = normalized
         self.eps: float = eps
 
     def __repr__(self) -> str:
         return self.__class__.__name__ + '(' 'normalized=' + str(self.normalized) + ')'
 
-    def forward(self, input: torch.Tensor) -> torch.Tensor:
+    def forward(self, input: Tensor) -> Tensor:
         return sobel(input, self.normalized, self.eps)
```

### Comparing `kornia-0.6.8/kornia/filters/unsharp.py` & `kornia-0.6.9/kornia/filters/unsharp.py`

 * *Files identical despite different names*

### Comparing `kornia-0.6.8/kornia/geometry/bbox.py` & `kornia-0.6.9/kornia/geometry/bbox.py`

 * *Files identical despite different names*

### Comparing `kornia-0.6.8/kornia/geometry/boxes.py` & `kornia-0.6.9/kornia/geometry/boxes.py`

 * *Files identical despite different names*

### Comparing `kornia-0.6.8/kornia/geometry/calibration/distort.py` & `kornia-0.6.9/kornia/geometry/calibration/distort.py`

 * *Files identical despite different names*

### Comparing `kornia-0.6.8/kornia/geometry/calibration/pnp.py` & `kornia-0.6.9/kornia/geometry/calibration/pnp.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,15 +1,15 @@
 from typing import Optional, Tuple
 
 import torch
+from torch.linalg import qr as linalg_qr
 
 from kornia.geometry.conversions import convert_points_to_homogeneous
 from kornia.geometry.linalg import transform_points
 from kornia.utils import eye_like
-from kornia.utils._compat import linalg_qr
 from kornia.utils.helpers import _torch_linalg_svdvals
 
 
 def _mean_isotropic_scale_normalize(points: torch.Tensor, eps: float = 1e-8) -> Tuple[torch.Tensor, torch.Tensor]:
     r"""Normalizes points.
 
     Args:
```

### Comparing `kornia-0.6.8/kornia/geometry/calibration/undistort.py` & `kornia-0.6.9/kornia/geometry/calibration/undistort.py`

 * *Files identical despite different names*

### Comparing `kornia-0.6.8/kornia/geometry/camera/pinhole.py` & `kornia-0.6.9/kornia/geometry/camera/pinhole.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,11 +1,12 @@
-from typing import Iterable, List, Optional
+from typing import Iterable, List
 
 import torch
 
+from kornia.core import Device, Tensor
 from kornia.geometry.conversions import convert_points_from_homogeneous, convert_points_to_homogeneous
 from kornia.geometry.linalg import inverse_transformation, transform_points
 from kornia.testing import KORNIA_CHECK_SAME_DEVICE
 from kornia.utils.helpers import _torch_inverse_cast
 
 
 class PinholeCamera:
@@ -20,80 +21,78 @@
         width: tensor with shape :math:`(B)` containing the image width.
 
     .. note::
         We assume that the class attributes are in batch form in order to take
         advantage of PyTorch parallelism to boost computing performance.
     """
 
-    def __init__(
-        self, intrinsics: torch.Tensor, extrinsics: torch.Tensor, height: torch.Tensor, width: torch.Tensor
-    ) -> None:
+    def __init__(self, intrinsics: Tensor, extrinsics: Tensor, height: Tensor, width: Tensor) -> None:
         # verify batch size and shapes
         self._check_valid([intrinsics, extrinsics, height, width])
         self._check_valid_params(intrinsics, "intrinsics")
         self._check_valid_params(extrinsics, "extrinsics")
         self._check_valid_shape(height, "height")
         self._check_valid_shape(width, "width")
         self._check_consistent_device([intrinsics, extrinsics, height, width])
         # set class attributes
-        self.height: torch.Tensor = height
-        self.width: torch.Tensor = width
-        self._intrinsics: torch.Tensor = intrinsics
-        self._extrinsics: torch.Tensor = extrinsics
+        self.height: Tensor = height
+        self.width: Tensor = width
+        self._intrinsics: Tensor = intrinsics
+        self._extrinsics: Tensor = extrinsics
 
     @staticmethod
-    def _check_valid(data_iter: Iterable[torch.Tensor]) -> bool:
+    def _check_valid(data_iter: Iterable[Tensor]) -> bool:
         if not all(data.shape[0] for data in data_iter):
             raise ValueError("Arguments shapes must match")
         return True
 
     @staticmethod
-    def _check_valid_params(data: torch.Tensor, data_name: str) -> bool:
+    def _check_valid_params(data: Tensor, data_name: str) -> bool:
         if len(data.shape) not in (3, 4) and data.shape[-2:] != (4, 4):  # Shouldn't this be an OR logic than AND?
             raise ValueError(
                 "Argument {} shape must be in the following shape"
                 " Bx4x4 or BxNx4x4. Got {}".format(data_name, data.shape)
             )
         return True
 
     @staticmethod
-    def _check_valid_shape(data: torch.Tensor, data_name: str) -> bool:
+    def _check_valid_shape(data: Tensor, data_name: str) -> bool:
         if not len(data.shape) == 1:
             raise ValueError(
                 "Argument {} shape must be in the following shape" " B. Got {}".format(data_name, data.shape)
             )
         return True
 
     @staticmethod
-    def _check_consistent_device(data_iter: List[torch.Tensor]) -> None:
+    def _check_consistent_device(data_iter: List[Tensor]) -> None:
         first = data_iter[0]
         for data in data_iter:
             KORNIA_CHECK_SAME_DEVICE(data, first)
 
     def device(self) -> torch.device:
         r"""Returns the device for camera buffers.
 
         Returns:
             Device type
         """
         return self._intrinsics.device
 
     @property
-    def intrinsics(self) -> torch.Tensor:
+    def intrinsics(self) -> Tensor:
         r"""The full 4x4 intrinsics matrix.
 
         Returns:
             tensor of shape :math:`(B, 4, 4)`.
         """
         if not self._check_valid_params(self._intrinsics, "intrinsics"):
             raise AssertionError
         return self._intrinsics
 
     @property
-    def extrinsics(self) -> torch.Tensor:
+    def extrinsics(self) -> Tensor:
         r"""The full 4x4 extrinsics matrix.
 
         Returns:
             tensor of shape :math:`(B, 4, 4)`.
         """
         if not self._check_valid_params(self._extrinsics, "extrinsics"):
             raise AssertionError
@@ -105,139 +104,139 @@
 
         Returns:
             scalar with the batch size.
         """
         return self.intrinsics.shape[0]
 
     @property
-    def fx(self) -> torch.Tensor:
+    def fx(self) -> Tensor:
         r"""Return the focal length in the x-direction.
 
         Returns:
             tensor of shape :math:`(B)`.
         """
         return self.intrinsics[..., 0, 0]
 
     @property
-    def fy(self) -> torch.Tensor:
+    def fy(self) -> Tensor:
         r"""Return the focal length in the y-direction.
 
         Returns:
             tensor of shape :math:`(B)`.
         """
         return self.intrinsics[..., 1, 1]
 
     @property
-    def cx(self) -> torch.Tensor:
+    def cx(self) -> Tensor:
         r"""Return the x-coordinate of the principal point.
 
         Returns:
             tensor of shape :math:`(B)`.
         """
         return self.intrinsics[..., 0, 2]
 
     @property
-    def cy(self) -> torch.Tensor:
+    def cy(self) -> Tensor:
         r"""Return the y-coordinate of the principal point.
 
         Returns:
             tensor of shape :math:`(B)`.
         """
         return self.intrinsics[..., 1, 2]
 
     @property
-    def tx(self) -> torch.Tensor:
+    def tx(self) -> Tensor:
         r"""Return the x-coordinate of the translation vector.
 
         Returns:
             tensor of shape :math:`(B)`.
         """
         return self.extrinsics[..., 0, -1]
 
     @tx.setter
     def tx(self, value) -> 'PinholeCamera':
         r"""Set the x-coordinate of the translation vector with the given value."""
         self.extrinsics[..., 0, -1] = value
         return self
 
     @property
-    def ty(self) -> torch.Tensor:
+    def ty(self) -> Tensor:
         r"""Return the y-coordinate of the translation vector.
 
         Returns:
             tensor of shape :math:`(B)`.
         """
         return self.extrinsics[..., 1, -1]
 
     @ty.setter
     def ty(self, value) -> 'PinholeCamera':
         r"""Set the y-coordinate of the translation vector with the given value."""
         self.extrinsics[..., 1, -1] = value
         return self
 
     @property
-    def tz(self) -> torch.Tensor:
+    def tz(self) -> Tensor:
         r"""Returns the z-coordinate of the translation vector.
 
         Returns:
             tensor of shape :math:`(B)`.
         """
         return self.extrinsics[..., 2, -1]
 
     @tz.setter
     def tz(self, value) -> 'PinholeCamera':
         r"""Set the y-coordinate of the translation vector with the given value."""
         self.extrinsics[..., 2, -1] = value
         return self
 
     @property
-    def rt_matrix(self) -> torch.Tensor:
+    def rt_matrix(self) -> Tensor:
         r"""Return the 3x4 rotation-translation matrix.
 
         Returns:
             tensor of shape :math:`(B, 3, 4)`.
         """
         return self.extrinsics[..., :3, :4]
 
     @property
-    def camera_matrix(self) -> torch.Tensor:
+    def camera_matrix(self) -> Tensor:
         r"""Return the 3x3 camera matrix containing the intrinsics.
 
         Returns:
             tensor of shape :math:`(B, 3, 3)`.
         """
         return self.intrinsics[..., :3, :3]
 
     @property
-    def rotation_matrix(self) -> torch.Tensor:
+    def rotation_matrix(self) -> Tensor:
         r"""Return the 3x3 rotation matrix from the extrinsics.
 
         Returns:
             tensor of shape :math:`(B, 3, 3)`.
         """
         return self.extrinsics[..., :3, :3]
 
     @property
-    def translation_vector(self) -> torch.Tensor:
+    def translation_vector(self) -> Tensor:
         r"""Return the translation vector from the extrinsics.
 
         Returns:
             tensor of shape :math:`(B, 3, 1)`.
         """
         return self.extrinsics[..., :3, -1:]
 
     def clone(self) -> 'PinholeCamera':
         r"""Return a deep copy of the current object instance."""
-        height: torch.Tensor = self.height.clone()
-        width: torch.Tensor = self.width.clone()
-        intrinsics: torch.Tensor = self.intrinsics.clone()
-        extrinsics: torch.Tensor = self.extrinsics.clone()
+        height: Tensor = self.height.clone()
+        width: Tensor = self.width.clone()
+        intrinsics: Tensor = self.intrinsics.clone()
+        extrinsics: Tensor = self.extrinsics.clone()
         return PinholeCamera(intrinsics, extrinsics, height, width)
 
-    def intrinsics_inverse(self) -> torch.Tensor:
+    def intrinsics_inverse(self) -> Tensor:
         r"""Return the inverse of the 4x4 instrisics matrix.
 
         Returns:
             tensor of shape :math:`(B, 4, 4)`.
         """
         return self.intrinsics.inverse()
 
@@ -249,22 +248,22 @@
               to be broadcastable with class members. The expected shape is
               :math:`(B)` or :math:`(1)`.
 
         Returns:
             the camera model with scaled parameters.
         """
         # scale the intrinsic parameters
-        intrinsics: torch.Tensor = self.intrinsics.clone()
+        intrinsics: Tensor = self.intrinsics.clone()
         intrinsics[..., 0, 0] *= scale_factor
         intrinsics[..., 1, 1] *= scale_factor
         intrinsics[..., 0, 2] *= scale_factor
         intrinsics[..., 1, 2] *= scale_factor
         # scale the image height/width
-        height: torch.Tensor = scale_factor * self.height.clone()
-        width: torch.Tensor = scale_factor * self.width.clone()
+        height: Tensor = scale_factor * self.height.clone()
+        width: Tensor = scale_factor * self.width.clone()
         return PinholeCamera(intrinsics, self.extrinsics, height, width)
 
     def scale_(self, scale_factor) -> 'PinholeCamera':
         r"""Scale the pinhole model in-place.
 
         Args:
             scale_factor: a tensor with the scale factor. It has
@@ -280,15 +279,15 @@
         self.intrinsics[..., 0, 2] *= scale_factor
         self.intrinsics[..., 1, 2] *= scale_factor
         # scale the image height/width
         self.height *= scale_factor
         self.width *= scale_factor
         return self
 
-    def project(self, point_3d: torch.Tensor) -> torch.Tensor:
+    def project(self, point_3d: Tensor) -> Tensor:
         r"""Project a 3d point in world coordinates onto the 2d camera plane.
 
         Args:
             point3d: tensor containing the 3d points to be projected
                 to the camera plane. The shape of the tensor can be :math:`(*, 3)`.
 
         Returns:
@@ -304,15 +303,15 @@
             >>> pinhole = kornia.geometry.camera.PinholeCamera(K, E, h, w)
             >>> pinhole.project(X)
             tensor([[5.6088, 8.6827]])
         """
         P = self.intrinsics @ self.extrinsics
         return convert_points_from_homogeneous(transform_points(P, point_3d))
 
-    def unproject(self, point_2d: torch.Tensor, depth: torch.Tensor):
+    def unproject(self, point_2d: Tensor, depth: Tensor):
         r"""Unproject a 2d point in 3d.
 
         Transform coordinates in the pixel frame to the world frame.
 
         Args:
             point2d: tensor containing the 2d to be projected to
                 world coordinates. The shape of the tensor can be :math:`(*, 2)`.
@@ -340,27 +339,15 @@
         P = self.intrinsics @ self.extrinsics
         P_inv = _torch_inverse_cast(P)
         return transform_points(P_inv, convert_points_to_homogeneous(point_2d) * depth)
 
     # NOTE: just for test. Decide if we keep it.
     @classmethod
     def from_parameters(
-        self,
-        fx,
-        fy,
-        cx,
-        cy,
-        height,
-        width,
-        tx,
-        ty,
-        tz,
-        batch_size=1,
-        device: Optional[torch.device] = None,
-        dtype: Optional[torch.dtype] = None,
+        self, fx, fy, cx, cy, height, width, tx, ty, tz, batch_size, device: Device, dtype: torch.dtype
     ):
         # create the camera matrix
         intrinsics = torch.zeros(batch_size, 4, 4, device=device, dtype=dtype)
         intrinsics[..., 0, 0] += fx
         intrinsics[..., 1, 1] += fy
         intrinsics[..., 0, 2] += cx
         intrinsics[..., 1, 2] += cy
@@ -408,38 +395,38 @@
             if not isinstance(pinhole, PinholeCamera):
                 raise TypeError("Argument pinhole must be from type " "PinholeCamera. Got {}".format(type(pinhole)))
             height.append(pinhole.height)
             width.append(pinhole.width)
             intrinsics.append(pinhole.intrinsics)
             extrinsics.append(pinhole.extrinsics)
         # contatenate and set members. We will assume BxNx4x4
-        self.height: torch.Tensor = torch.stack(height, dim=1)
-        self.width: torch.Tensor = torch.stack(width, dim=1)
-        self._intrinsics: torch.Tensor = torch.stack(intrinsics, dim=1)
-        self._extrinsics: torch.Tensor = torch.stack(extrinsics, dim=1)
+        self.height: Tensor = torch.stack(height, dim=1)
+        self.width: Tensor = torch.stack(width, dim=1)
+        self._intrinsics: Tensor = torch.stack(intrinsics, dim=1)
+        self._extrinsics: Tensor = torch.stack(extrinsics, dim=1)
         return self
 
     @property
     def num_cameras(self) -> int:
         r"""Return the number of pinholes cameras per batch."""
         num_cameras: int = -1
         if self.intrinsics is not None:
             num_cameras = int(self.intrinsics.shape[1])
         return num_cameras
 
     def get_pinhole(self, idx: int) -> PinholeCamera:
         r"""Return a PinholeCamera object with parameters such as Bx4x4."""
-        height: torch.Tensor = self.height[..., idx]
-        width: torch.Tensor = self.width[..., idx]
-        intrinsics: torch.Tensor = self.intrinsics[:, idx]
-        extrinsics: torch.Tensor = self.extrinsics[:, idx]
+        height: Tensor = self.height[..., idx]
+        width: Tensor = self.width[..., idx]
+        intrinsics: Tensor = self.intrinsics[:, idx]
+        extrinsics: Tensor = self.extrinsics[:, idx]
         return PinholeCamera(intrinsics, extrinsics, height, width)
 
 
-def pinhole_matrix(pinholes: torch.Tensor, eps: float = 1e-6) -> torch.Tensor:
+def pinhole_matrix(pinholes: Tensor, eps: float = 1e-6) -> Tensor:
     r"""Function that returns the pinhole matrix from a pinhole model.
 
     .. note::
         This method is going to be deprecated in version 0.2 in favour of
         :attr:`kornia.PinholeCamera.camera_matrix`.
 
     Args:
@@ -475,15 +462,15 @@
     k[..., 0, 0:1] = fx
     k[..., 0, 2:3] = cx
     k[..., 1, 1:2] = fy
     k[..., 1, 2:3] = cy
     return k
 
 
-def inverse_pinhole_matrix(pinhole: torch.Tensor, eps: float = 1e-6) -> torch.Tensor:
+def inverse_pinhole_matrix(pinhole: Tensor, eps: float = 1e-6) -> Tensor:
     r"""Return the inverted pinhole matrix from a pinhole model.
 
     .. note::
         This method is going to be deprecated in version 0.2 in favour of
         :attr:`kornia.PinholeCamera.intrinsics_inverse()`.
 
     Args:
@@ -519,15 +506,15 @@
     k[..., 0, 0:1] = 1.0 / (fx + eps)
     k[..., 1, 1:2] = 1.0 / (fy + eps)
     k[..., 0, 2:3] = -1.0 * cx / (fx + eps)
     k[..., 1, 2:3] = -1.0 * cy / (fy + eps)
     return k
 
 
-def scale_pinhole(pinholes: torch.Tensor, scale: torch.Tensor) -> torch.Tensor:
+def scale_pinhole(pinholes: Tensor, scale: Tensor) -> Tensor:
     r"""Scale the pinhole matrix for each pinhole model.
 
     .. note::
         This method is going to be deprecated in version 0.2 in favour of
         :attr:`kornia.PinholeCamera.scale()`.
 
     Args:
@@ -557,15 +544,15 @@
     if len(scale.shape) != 1:
         raise AssertionError(scale.shape)
     pinholes_scaled = pinholes.clone()
     pinholes_scaled[..., :6] = pinholes[..., :6] * scale.unsqueeze(-1)
     return pinholes_scaled
 
 
-def get_optical_pose_base(pinholes: torch.Tensor) -> torch.Tensor:
+def get_optical_pose_base(pinholes: Tensor) -> Tensor:
     """Compute extrinsic transformation matrices for pinholes.
 
     Args:
         pinholes: tensor of form [fx fy cx cy h w rx ry rz tx ty tz]
                            of size (N, 12).
 
     Returns:
@@ -578,15 +565,15 @@
     # TODO: We have rtvec_to_pose in torchgeometry
     # https://github.com/whh14/torchgeometry/blob/master/torchgeometry/conversions.py#L240
     # But it relies on angle_axis_to_rotation_matrix
     # And since then, it was changed from returning Nx4x4 matrix to Nx3x3
     # return rtvec_to_pose(optical_pose_parent)   type: ignore
 
 
-def homography_i_H_ref(pinhole_i: torch.Tensor, pinhole_ref: torch.Tensor) -> torch.Tensor:
+def homography_i_H_ref(pinhole_i: Tensor, pinhole_ref: Tensor) -> Tensor:
     r"""Homography from reference to ith pinhole.
 
     .. note::
         The pinhole model is represented in a single vector as follows:
 
         .. math::
             pinhole = (f_x, f_y, c_x, c_y, height, width,
@@ -629,15 +616,15 @@
     return torch.matmul(pinhole_matrix(pinhole_i), torch.matmul(i_pose_ref, inverse_pinhole_matrix(pinhole_ref)))
 
 
 # based on:
 # https://github.com/ClementPinard/SfmLearner-Pytorch/blob/master/inverse_warp.py#L26
 
 
-def pixel2cam(depth: torch.Tensor, intrinsics_inv: torch.Tensor, pixel_coords: torch.Tensor) -> torch.Tensor:
+def pixel2cam(depth: Tensor, intrinsics_inv: Tensor, pixel_coords: Tensor) -> Tensor:
     r"""Transform coordinates in the pixel frame to the camera frame.
 
     Args:
         depth: the source depth maps. Shape must be Bx1xHxW.
         intrinsics_inv: the inverse intrinsics camera matrix. Shape must be Bx4x4.
         pixel_coords: the grid with (u, v, 1) pixel coordinates. Shape must be BxHxWx3.
 
@@ -646,23 +633,23 @@
     """
     if not len(depth.shape) == 4 and depth.shape[1] == 1:
         raise ValueError("Input depth has to be in the shape of " "Bx1xHxW. Got {}".format(depth.shape))
     if not len(intrinsics_inv.shape) == 3:
         raise ValueError("Input intrinsics_inv has to be in the shape of " "Bx4x4. Got {}".format(intrinsics_inv.shape))
     if not len(pixel_coords.shape) == 4 and pixel_coords.shape[3] == 3:
         raise ValueError("Input pixel_coords has to be in the shape of " "BxHxWx3. Got {}".format(intrinsics_inv.shape))
-    cam_coords: torch.Tensor = transform_points(intrinsics_inv[:, None], pixel_coords)
+    cam_coords: Tensor = transform_points(intrinsics_inv[:, None], pixel_coords)
     return cam_coords * depth.permute(0, 2, 3, 1)
 
 
 # based on
 # https://github.com/ClementPinard/SfmLearner-Pytorch/blob/master/inverse_warp.py#L43
 
 
-def cam2pixel(cam_coords_src: torch.Tensor, dst_proj_src: torch.Tensor, eps: float = 1e-12) -> torch.Tensor:
+def cam2pixel(cam_coords_src: Tensor, dst_proj_src: Tensor, eps: float = 1e-12) -> Tensor:
     r"""Transform coordinates in the camera frame to the pixel frame.
 
     Args:
         cam_coords: (x, y, z) coordinates defined in the first camera coordinates system. Shape must be BxHxWx3.
         dst_proj_src: the projection matrix between the
           reference and the non reference camera frame. Shape must be Bx4x4.
         eps: small value to avoid division by zero error.
@@ -673,25 +660,25 @@
     if not len(cam_coords_src.shape) == 4 and cam_coords_src.shape[3] == 3:
         raise ValueError(
             "Input cam_coords_src has to be in the shape of " "BxHxWx3. Got {}".format(cam_coords_src.shape)
         )
     if not len(dst_proj_src.shape) == 3 and dst_proj_src.shape[-2:] == (4, 4):
         raise ValueError("Input dst_proj_src has to be in the shape of " "Bx4x4. Got {}".format(dst_proj_src.shape))
     # apply projection matrix to points
-    point_coords: torch.Tensor = transform_points(dst_proj_src[:, None], cam_coords_src)
-    x_coord: torch.Tensor = point_coords[..., 0]
-    y_coord: torch.Tensor = point_coords[..., 1]
-    z_coord: torch.Tensor = point_coords[..., 2]
+    point_coords: Tensor = transform_points(dst_proj_src[:, None], cam_coords_src)
+    x_coord: Tensor = point_coords[..., 0]
+    y_coord: Tensor = point_coords[..., 1]
+    z_coord: Tensor = point_coords[..., 2]
 
     # compute pixel coordinates
-    u_coord: torch.Tensor = x_coord / (z_coord + eps)
-    v_coord: torch.Tensor = y_coord / (z_coord + eps)
+    u_coord: Tensor = x_coord / (z_coord + eps)
+    v_coord: Tensor = y_coord / (z_coord + eps)
 
     # stack and return the coordinates, that's the actual flow
-    pixel_coords_dst: torch.Tensor = torch.stack([u_coord, v_coord], dim=-1)
+    pixel_coords_dst: Tensor = torch.stack([u_coord, v_coord], dim=-1)
     return pixel_coords_dst  # BxHxWx2
 
 
 # layer api
 '''class PinholeMatrix(nn.Module):
     r"""Create an object that returns the pinhole matrix from a pinhole model
```

### Comparing `kornia-0.6.8/kornia/geometry/camera/stereo.py` & `kornia-0.6.9/kornia/geometry/camera/stereo.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,9 +1,10 @@
 import torch
 
+from kornia.core import Tensor, stack, zeros
 from kornia.geometry.linalg import transform_points
 from kornia.utils.grid import create_meshgrid
 
 
 class StereoException(Exception):
     def __init__(self, msg: str, *args, **kwargs):
         r"""Custom exception for the :module:`~kornia.geometry.camera.stereo` module.
@@ -18,38 +19,38 @@
         doc_help = (
             "\n Please check documents here: "
             "https://kornia.readthedocs.io/en/latest/geometry.camera.stereo.html for further information and examples."
         )
         final_msg = msg + doc_help
         # type ignore because of mypy error:
         # Too many arguments for "__init__" of "BaseException"
-        super().__init__(final_msg, *args, **kwargs)  # type: ignore
+        super().__init__(final_msg, *args, **kwargs)
 
 
 class StereoCamera:
-    def __init__(self, rectified_left_camera: torch.Tensor, rectified_right_camera: torch.Tensor):
+    def __init__(self, rectified_left_camera: Tensor, rectified_right_camera: Tensor):
         r"""Class representing a horizontal stereo camera setup.
 
         Args:
             rectified_left_camera: The rectified left camera projection matrix
               of shape :math:`(B, 3, 4)`
             rectified_right_camera: The rectified right camera projection matrix
               of shape :math:`(B, 3, 4)`
         """
         self._check_stereo_camera(rectified_left_camera, rectified_right_camera)
-        self.rectified_left_camera: torch.Tensor = rectified_left_camera
-        self.rectified_right_camera: torch.Tensor = rectified_right_camera
+        self.rectified_left_camera: Tensor = rectified_left_camera
+        self.rectified_right_camera: Tensor = rectified_right_camera
 
-        self.device: torch.device = self.rectified_left_camera.device
-        self.dtype: torch.dtype = self.rectified_left_camera.dtype
+        self.device = self.rectified_left_camera.device
+        self.dtype = self.rectified_left_camera.dtype
 
         self._Q_matrix = self._init_Q_matrix()
 
     @staticmethod
-    def _check_stereo_camera(rectified_left_camera: torch.Tensor, rectified_right_camera: torch.Tensor):
+    def _check_stereo_camera(rectified_left_camera: Tensor, rectified_right_camera: Tensor):
         r"""Utility function to ensure user specified correct camera matrices.
 
         Args:
             rectified_left_camera: The rectified left camera projection matrix
               of shape :math:`(B, 3, 4)`
             rectified_right_camera: The rectified right camera projection matrix
               of shape :math:`(B, 3, 4)`
@@ -112,127 +113,127 @@
 
         Returns:
            scalar with the batch size
         """
         return self.rectified_left_camera.shape[0]
 
     @property
-    def fx(self) -> torch.Tensor:
+    def fx(self) -> Tensor:
         r"""Return the focal length in the x-direction.
 
         Note that the focal lengths of the rectified left and right
         camera are assumed to be equal.
 
         Returns:
             tensor of shape :math:`(B)`
         """
         return self.rectified_left_camera[..., 0, 0]
 
     @property
-    def fy(self) -> torch.Tensor:
+    def fy(self) -> Tensor:
         r"""Returns the focal length in the y-direction.
 
         Note that the focal lengths of the rectified left and right
         camera are assumed to be equal.
 
         Returns:
             tensor of shape :math:`(B)`
         """
         return self.rectified_left_camera[..., 1, 1]
 
     @property
-    def cx_left(self) -> torch.Tensor:
+    def cx_left(self) -> Tensor:
         r"""Return the x-coordinate of the principal point for the left camera.
 
         Returns:
             tensor of shape :math:`(B)`
         """
         return self.rectified_left_camera[..., 0, 2]
 
     @property
-    def cx_right(self) -> torch.Tensor:
+    def cx_right(self) -> Tensor:
         r"""Return the x-coordinate of the principal point for the right camera.
 
         Returns:
             tensor of shape :math:`(B)`
         """
         return self.rectified_right_camera[..., 0, 2]
 
     @property
-    def cy(self) -> torch.Tensor:
+    def cy(self) -> Tensor:
         r"""Return the y-coordinate of the principal point.
 
         Note that the y-coordinate of the principal points
         is assumed to be equal for the left and right camera.
 
         Returns:
             tensor of shape :math:`(B)`
         """
         return self.rectified_left_camera[..., 1, 2]
 
     @property
-    def tx(self) -> torch.Tensor:
+    def tx(self) -> Tensor:
         r"""The horizontal baseline between the two cameras.
 
         Returns:
             Tensor of shape :math:`(B)`
         """
         return -self.rectified_right_camera[..., 0, 3] / self.fx
 
     @property
-    def Q(self) -> torch.Tensor:
+    def Q(self) -> Tensor:
         r"""The Q matrix of the horizontal stereo setup.
 
         This matrix is used for reprojecting a disparity tensor to
         the corresponding point cloud. Note that this is in a general form that allows different focal
         lengths in the x and y direction.
 
         Return:
             The Q matrix of shape :math:`(B, 4, 4)`.
         """
         return self._Q_matrix
 
-    def _init_Q_matrix(self) -> torch.Tensor:
+    def _init_Q_matrix(self) -> Tensor:
         r"""Initialized the Q matrix of the horizontal stereo setup. See the Q property.
 
         Returns:
             The Q matrix of shape :math:`(B, 4, 4)`.
         """
-        Q = torch.zeros((self.batch_size, 4, 4), device=self.device, dtype=self.dtype)
-        baseline: torch.Tensor = -self.tx
+        Q = zeros((self.batch_size, 4, 4), device=self.device, dtype=self.dtype)
+        baseline: Tensor = -self.tx
         Q[:, 0, 0] = self.fy * baseline
         Q[:, 0, 3] = -self.fy * self.cx_left * baseline
         Q[:, 1, 1] = self.fx * baseline
         Q[:, 1, 3] = -self.fx * self.cy * baseline
         Q[:, 2, 3] = self.fx * self.fy * baseline
         Q[:, 3, 2] = -self.fy
         Q[:, 3, 3] = self.fy * (self.cx_left - self.cx_right)  # NOTE: This is usually zero.
         return Q
 
-    def reproject_disparity_to_3D(self, disparity_tensor: torch.Tensor) -> torch.Tensor:
+    def reproject_disparity_to_3D(self, disparity_tensor: Tensor) -> Tensor:
         r"""Reproject the disparity tensor to a 3D point cloud.
 
         Args:
             disparity_tensor: Disparity tensor of shape :math:`(B, 1, H, W)`.
 
         Returns:
             The 3D point cloud of shape :math:`(B, H, W, 3)`
         """
         return reproject_disparity_to_3D(disparity_tensor, self.Q)
 
 
-def _check_disparity_tensor(disparity_tensor: torch.Tensor):
+def _check_disparity_tensor(disparity_tensor: Tensor):
     r"""Utility function to ensure correct user provided correct disparity tensor.
 
     Args:
         disparity_tensor: The disparity tensor of shape :math:`(B, 1, H, W)`.
     """
-    if not isinstance(disparity_tensor, torch.Tensor):
+    if not isinstance(disparity_tensor, Tensor):
         raise StereoException(
-            f"Expected 'disparity_tensor' to be an instance of torch.Tensor but got {type(disparity_tensor)}."
+            f"Expected 'disparity_tensor' to be an instance of Tensor but got {type(disparity_tensor)}."
         )
 
     if len(disparity_tensor.shape) != 4:
         raise StereoException(f"Expected 'disparity_tensor' to have 4 dimensions." f"Got {disparity_tensor.shape}.")
 
     if disparity_tensor.shape[-1] != 1:
         raise StereoException(
@@ -243,23 +244,23 @@
     if disparity_tensor.dtype not in (torch.float16, torch.float32, torch.float64):
         raise StereoException(
             f"Expected 'disparity_tensor' to have dtype torch.float16, torch.float32 or torch.float64."
             f"Got {disparity_tensor.dtype}"
         )
 
 
-def _check_Q_matrix(Q_matrix: torch.Tensor):
+def _check_Q_matrix(Q_matrix: Tensor):
     r"""Utility function to ensure Q matrix is of correct form.
 
     Args:
         Q_matrix: The Q matrix for reprojecting disparity to a point cloud of shape :math:`(B, 4, 4)`
     """
 
-    if not isinstance(Q_matrix, torch.Tensor):
-        raise StereoException(f"Expected 'Q_matrix' to be an instance of torch.Tensor but got {type(Q_matrix)}.")
+    if not isinstance(Q_matrix, Tensor):
+        raise StereoException(f"Expected 'Q_matrix' to be an instance of Tensor but got {type(Q_matrix)}.")
 
     if not len(Q_matrix.shape) == 3:
         raise StereoException(f"Expected 'Q_matrix' to have 3 dimensions." f"Got {Q_matrix.shape}")
 
     if not Q_matrix.shape[1:] == (4, 4):
         raise StereoException(
             f"Expected last two dimensions of 'Q_matrix' to be of shape (4, 4)." f"Got {Q_matrix.shape}"
@@ -267,15 +268,15 @@
 
     if Q_matrix.dtype not in (torch.float16, torch.float32, torch.float64):
         raise StereoException(
             f"Expected 'Q_matrix' to be of type torch.float16, torch.float32 or torch.float64." f"Got {Q_matrix.dtype}"
         )
 
 
-def reproject_disparity_to_3D(disparity_tensor: torch.Tensor, Q_matrix: torch.Tensor) -> torch.Tensor:
+def reproject_disparity_to_3D(disparity_tensor: Tensor, Q_matrix: Tensor) -> Tensor:
     r"""Reproject the disparity tensor to a 3D point cloud.
 
     Args:
         disparity_tensor: Disparity tensor of shape :math:`(B, H, W, 1)`.
         Q_matrix: Tensor of Q matrices of shapes :math:`(B, 4, 4)`.
 
     Returns:
@@ -288,15 +289,15 @@
     dtype = disparity_tensor.dtype
     device = disparity_tensor.device
 
     uv = create_meshgrid(rows, cols, normalized_coordinates=False, device=device, dtype=dtype)
     uv = uv.expand(batch_size, -1, -1, -1)
     v, u = torch.unbind(uv, dim=-1)
     v, u = torch.unsqueeze(v, -1), torch.unsqueeze(u, -1)
-    uvd = torch.stack((u, v, disparity_tensor), 1).reshape(batch_size, 3, -1).permute(0, 2, 1)
+    uvd = stack((u, v, disparity_tensor), 1).reshape(batch_size, 3, -1).permute(0, 2, 1)
     points = transform_points(Q_matrix, uvd).reshape(batch_size, rows, cols, 3)
 
     # Final check that everything went well.
     if not points.shape == (batch_size, rows, cols, 3):
         raise StereoException(
             f"Something went wrong in `reproject_disparity_to_3D`. Expected the final output"
             f"to be of shape {(batch_size, rows, cols, 3)}."
```

### Comparing `kornia-0.6.8/kornia/geometry/conversions.py` & `kornia-0.6.9/kornia/geometry/conversions.py`

 * *Files 17% similar despite different names*

```diff
@@ -1,18 +1,19 @@
 import enum
 import warnings
 from typing import Optional, Tuple
 
 import torch
 import torch.nn.functional as F
-from torch import Tensor, tensor
 
 from kornia.constants import pi
-from kornia.testing import KORNIA_CHECK_SHAPE
+from kornia.core import Tensor, concatenate, pad, stack, tensor, where
+from kornia.testing import KORNIA_CHECK, KORNIA_CHECK_SHAPE
 from kornia.utils.helpers import _torch_inverse_cast
+from kornia.utils.misc import eye_like
 
 __all__ = [
     "rad2deg",
     "deg2rad",
     "pol2cart",
     "cart2pol",
     "convert_points_from_homogeneous",
@@ -23,25 +24,29 @@
     "angle_axis_to_quaternion",
     "rotation_matrix_to_angle_axis",
     "rotation_matrix_to_quaternion",
     "quaternion_to_angle_axis",
     "quaternion_to_rotation_matrix",
     "quaternion_log_to_exp",
     "quaternion_exp_to_log",
+    "quaternion_from_euler",
+    "euler_from_quaternion",
     "denormalize_pixel_coordinates",
     "normalize_pixel_coordinates",
     "normalize_quaternion",
     "denormalize_pixel_coordinates3d",
     "normalize_pixel_coordinates3d",
     "angle_to_rotation_matrix",
     "normalize_homography",
     "denormalize_homography",
     "normalize_homography3d",
     "normal_transform_pixel",
     "normal_transform_pixel3d",
+    "normalize_points_with_intrinsics",
+    "denormalize_points_with_intrinsics",
     "worldtocam_to_camtoworld_Rt",
     "camtoworld_to_worldtocam_Rt",
     "Rt_to_matrix4x4",
     "matrix4x4_to_Rt",
     "camtoworld_graphics_to_vision_4x4",
     "camtoworld_vision_to_graphics_4x4",
     "camtoworld_graphics_to_vision_Rt",
@@ -51,55 +56,55 @@
 
 
 class QuaternionCoeffOrder(enum.Enum):
     XYZW = 'xyzw'
     WXYZ = 'wxyz'
 
 
-def rad2deg(tensor: torch.Tensor) -> torch.Tensor:
+def rad2deg(tensor: Tensor) -> Tensor:
     r"""Function that converts angles from radians to degrees.
 
     Args:
         tensor: Tensor of arbitrary shape.
 
     Returns:
         Tensor with same shape as input.
 
     Example:
-        >>> input = torch.tensor(3.1415926535)
+        >>> input = tensor(3.1415926535)
         >>> rad2deg(input)
         tensor(180.)
     """
-    if not isinstance(tensor, torch.Tensor):
-        raise TypeError(f"Input type is not a torch.Tensor. Got {type(tensor)}")
+    if not isinstance(tensor, Tensor):
+        raise TypeError(f"Input type is not a Tensor. Got {type(tensor)}")
 
     return 180.0 * tensor / pi.to(tensor.device).type(tensor.dtype)
 
 
-def deg2rad(tensor: torch.Tensor) -> torch.Tensor:
+def deg2rad(tensor: Tensor) -> Tensor:
     r"""Function that converts angles from degrees to radians.
 
     Args:
         tensor: Tensor of arbitrary shape.
 
     Returns:
         tensor with same shape as input.
 
     Examples:
-        >>> input = torch.tensor(180.)
+        >>> input = tensor(180.)
         >>> deg2rad(input)
         tensor(3.1416)
     """
-    if not isinstance(tensor, torch.Tensor):
-        raise TypeError(f"Input type is not a torch.Tensor. Got {type(tensor)}")
+    if not isinstance(tensor, Tensor):
+        raise TypeError(f"Input type is not a Tensor. Got {type(tensor)}")
 
     return tensor * pi.to(tensor.device).type(tensor.dtype) / 180.0
 
 
-def pol2cart(rho: torch.Tensor, phi: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
+def pol2cart(rho: Tensor, phi: Tensor) -> Tuple[Tensor, Tensor]:
     r"""Function that converts polar coordinates to cartesian coordinates.
 
     Args:
         rho: Tensor of arbitrary shape.
         phi: Tensor of same arbitrary shape.
 
     Returns:
@@ -107,23 +112,23 @@
         - y: Tensor with same shape as input.
 
     Example:
         >>> rho = torch.rand(1, 3, 3)
         >>> phi = torch.rand(1, 3, 3)
         >>> x, y = pol2cart(rho, phi)
     """
-    if not (isinstance(rho, torch.Tensor) & isinstance(phi, torch.Tensor)):
-        raise TypeError(f"Input type is not a torch.Tensor. Got {type(rho)}, {type(phi)}")
+    if not (isinstance(rho, Tensor) & isinstance(phi, Tensor)):
+        raise TypeError(f"Input type is not a Tensor. Got {type(rho)}, {type(phi)}")
 
     x = rho * torch.cos(phi)
     y = rho * torch.sin(phi)
     return x, y
 
 
-def cart2pol(x: torch.Tensor, y: torch.Tensor, eps: float = 1.0e-8) -> Tuple[torch.Tensor, torch.Tensor]:
+def cart2pol(x: Tensor, y: Tensor, eps: float = 1.0e-8) -> Tuple[Tensor, Tensor]:
     """Function that converts cartesian coordinates to polar coordinates.
 
     Args:
         x: Tensor of arbitrary shape.
         y: Tensor of same arbitrary shape.
         eps: To avoid division by zero.
 
@@ -132,161 +137,161 @@
         - phi: Tensor with same shape as input.
 
     Example:
         >>> x = torch.rand(1, 3, 3)
         >>> y = torch.rand(1, 3, 3)
         >>> rho, phi = cart2pol(x, y)
     """
-    if not (isinstance(x, torch.Tensor) & isinstance(y, torch.Tensor)):
-        raise TypeError(f"Input type is not a torch.Tensor. Got {type(x)}, {type(y)}")
+    if not (isinstance(x, Tensor) & isinstance(y, Tensor)):
+        raise TypeError(f"Input type is not a Tensor. Got {type(x)}, {type(y)}")
 
     rho = torch.sqrt(x**2 + y**2 + eps)
     phi = torch.atan2(y, x)
     return rho, phi
 
 
-def convert_points_from_homogeneous(points: torch.Tensor, eps: float = 1e-8) -> torch.Tensor:
+def convert_points_from_homogeneous(points: Tensor, eps: float = 1e-8) -> Tensor:
     r"""Function that converts points from homogeneous to Euclidean space.
 
     Args:
         points: the points to be transformed of shape :math:`(B, N, D)`.
         eps: to avoid division by zero.
 
     Returns:
         the points in Euclidean space :math:`(B, N, D-1)`.
 
     Examples:
-        >>> input = torch.tensor([[0., 0., 1.]])
+        >>> input = tensor([[0., 0., 1.]])
         >>> convert_points_from_homogeneous(input)
         tensor([[0., 0.]])
     """
-    if not isinstance(points, torch.Tensor):
-        raise TypeError(f"Input type is not a torch.Tensor. Got {type(points)}")
+    if not isinstance(points, Tensor):
+        raise TypeError(f"Input type is not a Tensor. Got {type(points)}")
 
     if len(points.shape) < 2:
         raise ValueError(f"Input must be at least a 2D tensor. Got {points.shape}")
 
     # we check for points at max_val
-    z_vec: torch.Tensor = points[..., -1:]
+    z_vec: Tensor = points[..., -1:]
 
     # set the results of division by zeror/near-zero to 1.0
     # follow the convention of opencv:
     # https://github.com/opencv/opencv/pull/14411/files
-    mask: torch.Tensor = torch.abs(z_vec) > eps
-    scale = torch.where(mask, 1.0 / (z_vec + eps), torch.ones_like(z_vec))
+    mask: Tensor = torch.abs(z_vec) > eps
+    scale = where(mask, 1.0 / (z_vec + eps), torch.ones_like(z_vec))
 
     return scale * points[..., :-1]
 
 
-def convert_points_to_homogeneous(points: torch.Tensor) -> torch.Tensor:
+def convert_points_to_homogeneous(points: Tensor) -> Tensor:
     r"""Function that converts points from Euclidean to homogeneous space.
 
     Args:
         points: the points to be transformed with shape :math:`(*, N, D)`.
 
     Returns:
         the points in homogeneous coordinates :math:`(*, N, D+1)`.
 
     Examples:
-        >>> input = torch.tensor([[0., 0.]])
+        >>> input = tensor([[0., 0.]])
         >>> convert_points_to_homogeneous(input)
         tensor([[0., 0., 1.]])
     """
-    if not isinstance(points, torch.Tensor):
-        raise TypeError(f"Input type is not a torch.Tensor. Got {type(points)}")
+    if not isinstance(points, Tensor):
+        raise TypeError(f"Input type is not a Tensor. Got {type(points)}")
     if len(points.shape) < 2:
         raise ValueError(f"Input must be at least a 2D tensor. Got {points.shape}")
 
-    return torch.nn.functional.pad(points, [0, 1], "constant", 1.0)
+    return pad(points, [0, 1], "constant", 1.0)
 
 
-def _convert_affinematrix_to_homography_impl(A: torch.Tensor) -> torch.Tensor:
-    H: torch.Tensor = torch.nn.functional.pad(A, [0, 0, 0, 1], "constant", value=0.0)
+def _convert_affinematrix_to_homography_impl(A: Tensor) -> Tensor:
+    H: Tensor = pad(A, [0, 0, 0, 1], "constant", value=0.0)
     H[..., -1, -1] += 1.0
     return H
 
 
-def convert_affinematrix_to_homography(A: torch.Tensor) -> torch.Tensor:
+def convert_affinematrix_to_homography(A: Tensor) -> Tensor:
     r"""Function that converts batch of affine matrices.
 
     Args:
         A: the affine matrix with shape :math:`(B,2,3)`.
 
     Returns:
          the homography matrix with shape of :math:`(B,3,3)`.
 
     Examples:
-        >>> A = torch.tensor([[[1., 0., 0.],
+        >>> A = tensor([[[1., 0., 0.],
         ...                    [0., 1., 0.]]])
         >>> convert_affinematrix_to_homography(A)
         tensor([[[1., 0., 0.],
                  [0., 1., 0.],
                  [0., 0., 1.]]])
     """
-    if not isinstance(A, torch.Tensor):
-        raise TypeError(f"Input type is not a torch.Tensor. Got {type(A)}")
+    if not isinstance(A, Tensor):
+        raise TypeError(f"Input type is not a Tensor. Got {type(A)}")
 
     if not (len(A.shape) == 3 and A.shape[-2:] == (2, 3)):
         raise ValueError(f"Input matrix must be a Bx2x3 tensor. Got {A.shape}")
 
     return _convert_affinematrix_to_homography_impl(A)
 
 
-def convert_affinematrix_to_homography3d(A: torch.Tensor) -> torch.Tensor:
+def convert_affinematrix_to_homography3d(A: Tensor) -> Tensor:
     r"""Function that converts batch of 3d affine matrices.
 
     Args:
         A: the affine matrix with shape :math:`(B,3,4)`.
 
     Returns:
          the homography matrix with shape of :math:`(B,4,4)`.
 
     Examples:
-        >>> A = torch.tensor([[[1., 0., 0., 0.],
+        >>> A = tensor([[[1., 0., 0., 0.],
         ...                    [0., 1., 0., 0.],
         ...                    [0., 0., 1., 0.]]])
         >>> convert_affinematrix_to_homography3d(A)
         tensor([[[1., 0., 0., 0.],
                  [0., 1., 0., 0.],
                  [0., 0., 1., 0.],
                  [0., 0., 0., 1.]]])
     """
-    if not isinstance(A, torch.Tensor):
-        raise TypeError(f"Input type is not a torch.Tensor. Got {type(A)}")
+    if not isinstance(A, Tensor):
+        raise TypeError(f"Input type is not a Tensor. Got {type(A)}")
 
     if not (len(A.shape) == 3 and A.shape[-2:] == (3, 4)):
         raise ValueError(f"Input matrix must be a Bx3x4 tensor. Got {A.shape}")
 
     return _convert_affinematrix_to_homography_impl(A)
 
 
-def angle_axis_to_rotation_matrix(angle_axis: torch.Tensor) -> torch.Tensor:
+def angle_axis_to_rotation_matrix(angle_axis: Tensor) -> Tensor:
     r"""Convert 3d vector of axis-angle rotation to 3x3 rotation matrix.
 
     Args:
         angle_axis: tensor of 3d vector of axis-angle rotations in radians with shape :math:`(N, 3)`.
 
     Returns:
         tensor of rotation matrices of shape :math:`(N, 3, 3)`.
 
     Example:
-        >>> input = torch.tensor([[0., 0., 0.]])
+        >>> input = tensor([[0., 0., 0.]])
         >>> angle_axis_to_rotation_matrix(input)
         tensor([[[1., 0., 0.],
                  [0., 1., 0.],
                  [0., 0., 1.]]])
 
-        >>> input = torch.tensor([[1.5708, 0., 0.]])
+        >>> input = tensor([[1.5708, 0., 0.]])
         >>> angle_axis_to_rotation_matrix(input)
         tensor([[[ 1.0000e+00,  0.0000e+00,  0.0000e+00],
                  [ 0.0000e+00, -3.6200e-06, -1.0000e+00],
                  [ 0.0000e+00,  1.0000e+00, -3.6200e-06]]])
     """
-    if not isinstance(angle_axis, torch.Tensor):
-        raise TypeError(f"Input type is not a torch.Tensor. Got {type(angle_axis)}")
+    if not isinstance(angle_axis, Tensor):
+        raise TypeError(f"Input type is not a Tensor. Got {type(angle_axis)}")
 
     if not angle_axis.shape[-1] == 3:
         raise ValueError(f"Input size must be a (*, 3) tensor. Got {angle_axis.shape}")
 
     def _compute_rotation_matrix(angle_axis, theta2, eps=1e-6):
         # We want to be careful to only evaluate the square root if the
         # norm of the angle_axis vector is greater than zero. Otherwise
@@ -303,21 +308,21 @@
         r20 = -wy * sin_theta + wx * wz * (k_one - cos_theta)
         r01 = wx * wy * (k_one - cos_theta) - wz * sin_theta
         r11 = cos_theta + wy * wy * (k_one - cos_theta)
         r21 = wx * sin_theta + wy * wz * (k_one - cos_theta)
         r02 = wy * sin_theta + wx * wz * (k_one - cos_theta)
         r12 = -wx * sin_theta + wy * wz * (k_one - cos_theta)
         r22 = cos_theta + wz * wz * (k_one - cos_theta)
-        rotation_matrix = torch.cat([r00, r01, r02, r10, r11, r12, r20, r21, r22], dim=1)
+        rotation_matrix = concatenate([r00, r01, r02, r10, r11, r12, r20, r21, r22], dim=1)
         return rotation_matrix.view(-1, 3, 3)
 
     def _compute_rotation_matrix_taylor(angle_axis):
         rx, ry, rz = torch.chunk(angle_axis, 3, dim=1)
         k_one = torch.ones_like(rx)
-        rotation_matrix = torch.cat([k_one, -rz, ry, rz, k_one, -rx, -ry, rx, k_one], dim=1)
+        rotation_matrix = concatenate([k_one, -rz, ry, rz, k_one, -rx, -ry, rx, k_one], dim=1)
         return rotation_matrix.view(-1, 3, 3)
 
     # stolen from ceres/rotation.h
 
     _angle_axis = torch.unsqueeze(angle_axis, dim=1)
     theta2 = torch.matmul(_angle_axis, _angle_axis.transpose(1, 2))
     theta2 = torch.squeeze(theta2, dim=1)
@@ -329,56 +334,54 @@
     # create mask to handle both cases
     eps = 1e-6
     mask = (theta2 > eps).view(-1, 1, 1).to(theta2.device)
     mask_pos = (mask).type_as(theta2)
     mask_neg = (~mask).type_as(theta2)
 
     # create output pose matrix
-    batch_size = angle_axis.shape[0]
-    rotation_matrix = torch.eye(3).to(angle_axis.device).type_as(angle_axis)
-    rotation_matrix = rotation_matrix.view(1, 3, 3).repeat(batch_size, 1, 1)
+    rotation_matrix = eye_like(3, angle_axis, shared_memory=False)
     # fill output matrix with masked values
     rotation_matrix[..., :3, :3] = mask_pos * rotation_matrix_normal + mask_neg * rotation_matrix_taylor
     return rotation_matrix  # Nx3x3
 
 
-def rotation_matrix_to_angle_axis(rotation_matrix: torch.Tensor) -> torch.Tensor:
+def rotation_matrix_to_angle_axis(rotation_matrix: Tensor) -> Tensor:
     r"""Convert 3x3 rotation matrix to Rodrigues vector in radians.
 
     Args:
         rotation_matrix: rotation matrix of shape :math:`(N, 3, 3)`.
 
     Returns:
         Rodrigues vector transformation of shape :math:`(N, 3)`.
 
     Example:
-        >>> input = torch.tensor([[1., 0., 0.],
+        >>> input = tensor([[1., 0., 0.],
         ...                       [0., 1., 0.],
         ...                       [0., 0., 1.]])
         >>> rotation_matrix_to_angle_axis(input)
         tensor([0., 0., 0.])
 
-        >>> input = torch.tensor([[1., 0., 0.],
+        >>> input = tensor([[1., 0., 0.],
         ...                       [0., 0., -1.],
         ...                       [0., 1., 0.]])
         >>> rotation_matrix_to_angle_axis(input)
         tensor([1.5708, 0.0000, 0.0000])
     """
-    if not isinstance(rotation_matrix, torch.Tensor):
-        raise TypeError(f"Input type is not a torch.Tensor. Got {type(rotation_matrix)}")
+    if not isinstance(rotation_matrix, Tensor):
+        raise TypeError(f"Input type is not a Tensor. Got {type(rotation_matrix)}")
 
     if not rotation_matrix.shape[-2:] == (3, 3):
         raise ValueError(f"Input size must be a (*, 3, 3) tensor. Got {rotation_matrix.shape}")
-    quaternion: torch.Tensor = rotation_matrix_to_quaternion(rotation_matrix, order=QuaternionCoeffOrder.WXYZ)
+    quaternion: Tensor = rotation_matrix_to_quaternion(rotation_matrix, order=QuaternionCoeffOrder.WXYZ)
     return quaternion_to_angle_axis(quaternion, order=QuaternionCoeffOrder.WXYZ)
 
 
 def rotation_matrix_to_quaternion(
-    rotation_matrix: torch.Tensor, eps: float = 1.0e-8, order: QuaternionCoeffOrder = QuaternionCoeffOrder.XYZW
-) -> torch.Tensor:
+    rotation_matrix: Tensor, eps: float = 1.0e-8, order: QuaternionCoeffOrder = QuaternionCoeffOrder.XYZW
+) -> Tensor:
     r"""Convert 3x3 rotation matrix to 4d quaternion vector.
 
     The quaternion vector has components in (w, x, y, z) or (x, y, z, w) format.
 
     .. note::
         The (x, y, z, w) order is going to be deprecated in favor of efficiency.
 
@@ -387,23 +390,23 @@
         eps: small value to avoid zero division.
         order: quaternion coefficient order. Note: 'xyzw' will be deprecated in favor of 'wxyz'.
 
     Return:
         the rotation in quaternion with shape :math:`(*, 4)`.
 
     Example:
-        >>> input = torch.tensor([[1., 0., 0.],
+        >>> input = tensor([[1., 0., 0.],
         ...                       [0., 1., 0.],
         ...                       [0., 0., 1.]])
         >>> rotation_matrix_to_quaternion(input, eps=torch.finfo(input.dtype).eps,
         ...                               order=QuaternionCoeffOrder.WXYZ)
         tensor([1., 0., 0., 0.])
     """
-    if not isinstance(rotation_matrix, torch.Tensor):
-        raise TypeError(f"Input type is not a torch.Tensor. Got {type(rotation_matrix)}")
+    if not isinstance(rotation_matrix, Tensor):
+        raise TypeError(f"Input type is not a Tensor. Got {type(rotation_matrix)}")
 
     if not rotation_matrix.shape[-2:] == (3, 3):
         raise ValueError(f"Input size must be a (*, 3, 3) tensor. Got {rotation_matrix.shape}")
 
     if not torch.jit.is_scripting():
         if order.name not in QuaternionCoeffOrder.__members__.keys():
             raise ValueError(f"order must be one of {QuaternionCoeffOrder.__members__.keys()}")
@@ -411,126 +414,126 @@
     if order == QuaternionCoeffOrder.XYZW:
         warnings.warn(
             "`XYZW` quaternion coefficient order is deprecated and"
             " will be removed after > 0.6. "
             "Please use `QuaternionCoeffOrder.WXYZ` instead."
         )
 
-    def safe_zero_division(numerator: torch.Tensor, denominator: torch.Tensor) -> torch.Tensor:
-        eps: float = torch.finfo(numerator.dtype).tiny  # type: ignore
+    def safe_zero_division(numerator: Tensor, denominator: Tensor) -> Tensor:
+        eps: float = torch.finfo(numerator.dtype).tiny
         return numerator / torch.clamp(denominator, min=eps)
 
-    rotation_matrix_vec: torch.Tensor = rotation_matrix.view(*rotation_matrix.shape[:-2], 9)
+    rotation_matrix_vec: Tensor = rotation_matrix.view(*rotation_matrix.shape[:-2], 9)
 
     m00, m01, m02, m10, m11, m12, m20, m21, m22 = torch.chunk(rotation_matrix_vec, chunks=9, dim=-1)
 
-    trace: torch.Tensor = m00 + m11 + m22
+    trace: Tensor = m00 + m11 + m22
 
     def trace_positive_cond():
         sq = torch.sqrt(trace + 1.0 + eps) * 2.0  # sq = 4 * qw.
         qw = 0.25 * sq
         qx = safe_zero_division(m21 - m12, sq)
         qy = safe_zero_division(m02 - m20, sq)
         qz = safe_zero_division(m10 - m01, sq)
         if order == QuaternionCoeffOrder.XYZW:
-            return torch.cat((qx, qy, qz, qw), dim=-1)
-        return torch.cat((qw, qx, qy, qz), dim=-1)
+            return concatenate((qx, qy, qz, qw), dim=-1)
+        return concatenate((qw, qx, qy, qz), dim=-1)
 
     def cond_1():
         sq = torch.sqrt(1.0 + m00 - m11 - m22 + eps) * 2.0  # sq = 4 * qx.
         qw = safe_zero_division(m21 - m12, sq)
         qx = 0.25 * sq
         qy = safe_zero_division(m01 + m10, sq)
         qz = safe_zero_division(m02 + m20, sq)
         if order == QuaternionCoeffOrder.XYZW:
-            return torch.cat((qx, qy, qz, qw), dim=-1)
-        return torch.cat((qw, qx, qy, qz), dim=-1)
+            return concatenate((qx, qy, qz, qw), dim=-1)
+        return concatenate((qw, qx, qy, qz), dim=-1)
 
     def cond_2():
         sq = torch.sqrt(1.0 + m11 - m00 - m22 + eps) * 2.0  # sq = 4 * qy.
         qw = safe_zero_division(m02 - m20, sq)
         qx = safe_zero_division(m01 + m10, sq)
         qy = 0.25 * sq
         qz = safe_zero_division(m12 + m21, sq)
         if order == QuaternionCoeffOrder.XYZW:
-            return torch.cat((qx, qy, qz, qw), dim=-1)
-        return torch.cat((qw, qx, qy, qz), dim=-1)
+            return concatenate((qx, qy, qz, qw), dim=-1)
+        return concatenate((qw, qx, qy, qz), dim=-1)
 
     def cond_3():
         sq = torch.sqrt(1.0 + m22 - m00 - m11 + eps) * 2.0  # sq = 4 * qz.
         qw = safe_zero_division(m10 - m01, sq)
         qx = safe_zero_division(m02 + m20, sq)
         qy = safe_zero_division(m12 + m21, sq)
         qz = 0.25 * sq
         if order == QuaternionCoeffOrder.XYZW:
-            return torch.cat((qx, qy, qz, qw), dim=-1)
-        return torch.cat((qw, qx, qy, qz), dim=-1)
+            return concatenate((qx, qy, qz, qw), dim=-1)
+        return concatenate((qw, qx, qy, qz), dim=-1)
 
-    where_2 = torch.where(m11 > m22, cond_2(), cond_3())
-    where_1 = torch.where((m00 > m11) & (m00 > m22), cond_1(), where_2)
+    where_2 = where(m11 > m22, cond_2(), cond_3())
+    where_1 = where((m00 > m11) & (m00 > m22), cond_1(), where_2)
 
-    quaternion: torch.Tensor = torch.where(trace > 0.0, trace_positive_cond(), where_1)
+    quaternion: Tensor = where(trace > 0.0, trace_positive_cond(), where_1)
     return quaternion
 
 
-def normalize_quaternion(quaternion: torch.Tensor, eps: float = 1.0e-12) -> torch.Tensor:
+def normalize_quaternion(quaternion: Tensor, eps: float = 1.0e-12) -> Tensor:
     r"""Normalize a quaternion.
 
     The quaternion should be in (x, y, z, w) or (w, x, y, z) format.
 
     Args:
         quaternion: a tensor containing a quaternion to be normalized.
           The tensor can be of shape :math:`(*, 4)`.
         eps: small value to avoid division by zero.
 
     Return:
         the normalized quaternion of shape :math:`(*, 4)`.
 
     Example:
-        >>> quaternion = torch.tensor((1., 0., 1., 0.))
+        >>> quaternion = tensor((1., 0., 1., 0.))
         >>> normalize_quaternion(quaternion)
         tensor([0.7071, 0.0000, 0.7071, 0.0000])
     """
-    if not isinstance(quaternion, torch.Tensor):
-        raise TypeError(f"Input type is not a torch.Tensor. Got {type(quaternion)}")
+    if not isinstance(quaternion, Tensor):
+        raise TypeError(f"Input type is not a Tensor. Got {type(quaternion)}")
 
     if not quaternion.shape[-1] == 4:
         raise ValueError(f"Input must be a tensor of shape (*, 4). Got {quaternion.shape}")
     return F.normalize(quaternion, p=2.0, dim=-1, eps=eps)
 
 
 # based on:
 # https://github.com/matthew-brett/transforms3d/blob/8965c48401d9e8e66b6a8c37c65f2fc200a076fa/transforms3d/quaternions.py#L101
 # https://github.com/tensorflow/graphics/blob/master/tensorflow_graphics/geometry/transformation/rotation_matrix_3d.py#L247
 
 
 def quaternion_to_rotation_matrix(
-    quaternion: torch.Tensor, order: QuaternionCoeffOrder = QuaternionCoeffOrder.XYZW
-) -> torch.Tensor:
+    quaternion: Tensor, order: QuaternionCoeffOrder = QuaternionCoeffOrder.XYZW
+) -> Tensor:
     r"""Convert a quaternion to a rotation matrix.
 
     The quaternion should be in (x, y, z, w) or (w, x, y, z) format.
 
     Args:
         quaternion: a tensor containing a quaternion to be converted.
           The tensor can be of shape :math:`(*, 4)`.
         order: quaternion coefficient order. Note: 'xyzw' will be deprecated in favor of 'wxyz'.
 
     Return:
         the rotation matrix of shape :math:`(*, 3, 3)`.
 
     Example:
-        >>> quaternion = torch.tensor((0., 0., 0., 1.))
+        >>> quaternion = tensor((0., 0., 0., 1.))
         >>> quaternion_to_rotation_matrix(quaternion, order=QuaternionCoeffOrder.WXYZ)
         tensor([[-1.,  0.,  0.],
                 [ 0., -1.,  0.],
                 [ 0.,  0.,  1.]])
     """
-    if not isinstance(quaternion, torch.Tensor):
-        raise TypeError(f"Input type is not a torch.Tensor. Got {type(quaternion)}")
+    if not isinstance(quaternion, Tensor):
+        raise TypeError(f"Input type is not a Tensor. Got {type(quaternion)}")
 
     if not quaternion.shape[-1] == 4:
         raise ValueError(f"Input must be a tensor of shape (*, 4). Got {quaternion.shape}")
 
     if not torch.jit.is_scripting():
         if order.name not in QuaternionCoeffOrder.__members__.keys():
             raise ValueError(f"order must be one of {QuaternionCoeffOrder.__members__.keys()}")
@@ -539,38 +542,38 @@
         warnings.warn(
             "`XYZW` quaternion coefficient order is deprecated and"
             " will be removed after > 0.6. "
             "Please use `QuaternionCoeffOrder.WXYZ` instead."
         )
 
     # normalize the input quaternion
-    quaternion_norm: torch.Tensor = normalize_quaternion(quaternion)
+    quaternion_norm: Tensor = normalize_quaternion(quaternion)
 
     # unpack the normalized quaternion components
     if order == QuaternionCoeffOrder.XYZW:
         x, y, z, w = torch.chunk(quaternion_norm, chunks=4, dim=-1)
     else:
         w, x, y, z = torch.chunk(quaternion_norm, chunks=4, dim=-1)
 
     # compute the actual conversion
-    tx: torch.Tensor = 2.0 * x
-    ty: torch.Tensor = 2.0 * y
-    tz: torch.Tensor = 2.0 * z
-    twx: torch.Tensor = tx * w
-    twy: torch.Tensor = ty * w
-    twz: torch.Tensor = tz * w
-    txx: torch.Tensor = tx * x
-    txy: torch.Tensor = ty * x
-    txz: torch.Tensor = tz * x
-    tyy: torch.Tensor = ty * y
-    tyz: torch.Tensor = tz * y
-    tzz: torch.Tensor = tz * z
-    one: torch.Tensor = torch.tensor(1.0)
+    tx: Tensor = 2.0 * x
+    ty: Tensor = 2.0 * y
+    tz: Tensor = 2.0 * z
+    twx: Tensor = tx * w
+    twy: Tensor = ty * w
+    twz: Tensor = tz * w
+    txx: Tensor = tx * x
+    txy: Tensor = ty * x
+    txz: Tensor = tz * x
+    tyy: Tensor = ty * y
+    tyz: Tensor = tz * y
+    tzz: Tensor = tz * z
+    one: Tensor = tensor(1.0)
 
-    matrix: torch.Tensor = torch.stack(
+    matrix: Tensor = stack(
         (
             one - (tyy + tzz),
             txy - twz,
             txz + twy,
             txy + twz,
             one - (txx + tzz),
             tyz - twx,
@@ -582,17 +585,15 @@
     ).view(-1, 3, 3)
 
     if len(quaternion.shape) == 1:
         matrix = torch.squeeze(matrix, dim=0)
     return matrix
 
 
-def quaternion_to_angle_axis(
-    quaternion: torch.Tensor, order: QuaternionCoeffOrder = QuaternionCoeffOrder.XYZW
-) -> torch.Tensor:
+def quaternion_to_angle_axis(quaternion: Tensor, order: QuaternionCoeffOrder = QuaternionCoeffOrder.XYZW) -> Tensor:
     """Convert quaternion vector to angle axis of rotation in radians.
 
     The quaternion should be in (x, y, z, w) or (w, x, y, z) format.
 
     Adapted from ceres C++ library: ceres-solver/include/ceres/rotation.h
 
     Args:
@@ -603,20 +604,20 @@
         tensor with angle axis of rotation.
 
     Shape:
         - Input: :math:`(*, 4)` where `*` means, any number of dimensions
         - Output: :math:`(*, 3)`
 
     Example:
-        >>> quaternion = torch.tensor((1., 0., 0., 0.))
+        >>> quaternion = tensor((1., 0., 0., 0.))
         >>> quaternion_to_angle_axis(quaternion)
         tensor([3.1416, 0.0000, 0.0000])
     """
     if not torch.is_tensor(quaternion):
-        raise TypeError(f"Input type is not a torch.Tensor. Got {type(quaternion)}")
+        raise TypeError(f"Input type is not a Tensor. Got {type(quaternion)}")
 
     if not quaternion.shape[-1] == 4:
         raise ValueError(f"Input must be a tensor of shape Nx4 or 4. Got {quaternion.shape}")
 
     if not torch.jit.is_scripting():
         if order.name not in QuaternionCoeffOrder.__members__.keys():
             raise ValueError(f"order must be one of {QuaternionCoeffOrder.__members__.keys()}")
@@ -624,72 +625,72 @@
     if order == QuaternionCoeffOrder.XYZW:
         warnings.warn(
             "`XYZW` quaternion coefficient order is deprecated and"
             " will be removed after > 0.6. "
             "Please use `QuaternionCoeffOrder.WXYZ` instead."
         )
     # unpack input and compute conversion
-    q1: torch.Tensor = torch.tensor([])
-    q2: torch.Tensor = torch.tensor([])
-    q3: torch.Tensor = torch.tensor([])
-    cos_theta: torch.Tensor = torch.tensor([])
+    q1: Tensor = tensor([])
+    q2: Tensor = tensor([])
+    q3: Tensor = tensor([])
+    cos_theta: Tensor = tensor([])
 
     if order == QuaternionCoeffOrder.XYZW:
         q1 = quaternion[..., 0]
         q2 = quaternion[..., 1]
         q3 = quaternion[..., 2]
         cos_theta = quaternion[..., 3]
     else:
         cos_theta = quaternion[..., 0]
         q1 = quaternion[..., 1]
         q2 = quaternion[..., 2]
         q3 = quaternion[..., 3]
 
-    sin_squared_theta: torch.Tensor = q1 * q1 + q2 * q2 + q3 * q3
+    sin_squared_theta: Tensor = q1 * q1 + q2 * q2 + q3 * q3
 
-    sin_theta: torch.Tensor = torch.sqrt(sin_squared_theta)
-    two_theta: torch.Tensor = 2.0 * torch.where(
+    sin_theta: Tensor = torch.sqrt(sin_squared_theta)
+    two_theta: Tensor = 2.0 * where(
         cos_theta < 0.0, torch.atan2(-sin_theta, -cos_theta), torch.atan2(sin_theta, cos_theta)
     )
 
-    k_pos: torch.Tensor = two_theta / sin_theta
-    k_neg: torch.Tensor = 2.0 * torch.ones_like(sin_theta)
-    k: torch.Tensor = torch.where(sin_squared_theta > 0.0, k_pos, k_neg)
+    k_pos: Tensor = two_theta / sin_theta
+    k_neg: Tensor = 2.0 * torch.ones_like(sin_theta)
+    k: Tensor = where(sin_squared_theta > 0.0, k_pos, k_neg)
 
-    angle_axis: torch.Tensor = torch.zeros_like(quaternion)[..., :3]
+    angle_axis: Tensor = torch.zeros_like(quaternion)[..., :3]
     angle_axis[..., 0] += q1 * k
     angle_axis[..., 1] += q2 * k
     angle_axis[..., 2] += q3 * k
     return angle_axis
 
 
 def quaternion_log_to_exp(
-    quaternion: torch.Tensor, eps: float = 1.0e-8, order: QuaternionCoeffOrder = QuaternionCoeffOrder.XYZW
-) -> torch.Tensor:
+    quaternion: Tensor, eps: float = 1.0e-8, order: QuaternionCoeffOrder = QuaternionCoeffOrder.XYZW
+) -> Tensor:
     r"""Apply exponential map to log quaternion.
 
     The quaternion should be in (x, y, z, w) or (w, x, y, z) format.
 
     Args:
         quaternion: a tensor containing a quaternion to be converted.
           The tensor can be of shape :math:`(*, 3)`.
         eps: a small number for clamping.
         order: quaternion coefficient order. Note: 'xyzw' will be deprecated in favor of 'wxyz'.
 
     Return:
         the quaternion exponential map of shape :math:`(*, 4)`.
 
     Example:
-        >>> quaternion = torch.tensor((0., 0., 0.))
+        >>> quaternion = tensor((0., 0., 0.))
         >>> quaternion_log_to_exp(quaternion, eps=torch.finfo(quaternion.dtype).eps,
         ...                       order=QuaternionCoeffOrder.WXYZ)
         tensor([1., 0., 0., 0.])
     """
-    if not isinstance(quaternion, torch.Tensor):
-        raise TypeError(f"Input type is not a torch.Tensor. Got {type(quaternion)}")
+    if not isinstance(quaternion, Tensor):
+        raise TypeError(f"Input type is not a Tensor. Got {type(quaternion)}")
 
     if not quaternion.shape[-1] == 3:
         raise ValueError(f"Input must be a tensor of shape (*, 3). Got {quaternion.shape}")
 
     if not torch.jit.is_scripting():
         if order.name not in QuaternionCoeffOrder.__members__.keys():
             raise ValueError(f"order must be one of {QuaternionCoeffOrder.__members__.keys()}")
@@ -698,54 +699,54 @@
         warnings.warn(
             "`XYZW` quaternion coefficient order is deprecated and"
             " will be removed after > 0.6. "
             "Please use `QuaternionCoeffOrder.WXYZ` instead."
         )
 
     # compute quaternion norm
-    norm_q: torch.Tensor = torch.norm(quaternion, p=2, dim=-1, keepdim=True).clamp(min=eps)
+    norm_q: Tensor = torch.norm(quaternion, p=2, dim=-1, keepdim=True).clamp(min=eps)
 
     # compute scalar and vector
-    quaternion_vector: torch.Tensor = quaternion * torch.sin(norm_q) / norm_q
-    quaternion_scalar: torch.Tensor = torch.cos(norm_q)
+    quaternion_vector: Tensor = quaternion * torch.sin(norm_q) / norm_q
+    quaternion_scalar: Tensor = torch.cos(norm_q)
 
     # compose quaternion and return
-    quaternion_exp: torch.Tensor = torch.tensor([])
+    quaternion_exp: Tensor = tensor([])
     if order == QuaternionCoeffOrder.XYZW:
-        quaternion_exp = torch.cat((quaternion_vector, quaternion_scalar), dim=-1)
+        quaternion_exp = concatenate((quaternion_vector, quaternion_scalar), dim=-1)
     else:
-        quaternion_exp = torch.cat((quaternion_scalar, quaternion_vector), dim=-1)
+        quaternion_exp = concatenate((quaternion_scalar, quaternion_vector), dim=-1)
 
     return quaternion_exp
 
 
 def quaternion_exp_to_log(
-    quaternion: torch.Tensor, eps: float = 1.0e-8, order: QuaternionCoeffOrder = QuaternionCoeffOrder.XYZW
-) -> torch.Tensor:
+    quaternion: Tensor, eps: float = 1.0e-8, order: QuaternionCoeffOrder = QuaternionCoeffOrder.XYZW
+) -> Tensor:
     r"""Apply the log map to a quaternion.
 
     The quaternion should be in (x, y, z, w) format.
 
     Args:
         quaternion: a tensor containing a quaternion to be converted.
           The tensor can be of shape :math:`(*, 4)`.
         eps: a small number for clamping.
         order: quaternion coefficient order. Note: 'xyzw' will be deprecated in favor of 'wxyz'.
 
     Return:
         the quaternion log map of shape :math:`(*, 3)`.
 
     Example:
-        >>> quaternion = torch.tensor((1., 0., 0., 0.))
+        >>> quaternion = tensor((1., 0., 0., 0.))
         >>> quaternion_exp_to_log(quaternion, eps=torch.finfo(quaternion.dtype).eps,
         ...                       order=QuaternionCoeffOrder.WXYZ)
         tensor([0., 0., 0.])
     """
-    if not isinstance(quaternion, torch.Tensor):
-        raise TypeError(f"Input type is not a torch.Tensor. Got {type(quaternion)}")
+    if not isinstance(quaternion, Tensor):
+        raise TypeError(f"Input type is not a Tensor. Got {type(quaternion)}")
 
     if not quaternion.shape[-1] == 4:
         raise ValueError(f"Input must be a tensor of shape (*, 4). Got {quaternion.shape}")
 
     if not torch.jit.is_scripting():
         if order.name not in QuaternionCoeffOrder.__members__.keys():
             raise ValueError(f"order must be one of {QuaternionCoeffOrder.__members__.keys()}")
@@ -754,42 +755,38 @@
         warnings.warn(
             "`XYZW` quaternion coefficient order is deprecated and"
             " will be removed after > 0.6. "
             "Please use `QuaternionCoeffOrder.WXYZ` instead."
         )
 
     # unpack quaternion vector and scalar
-    quaternion_vector: torch.Tensor = torch.tensor([])
-    quaternion_scalar: torch.Tensor = torch.tensor([])
+    quaternion_vector: Tensor = tensor([])
+    quaternion_scalar: Tensor = tensor([])
 
     if order == QuaternionCoeffOrder.XYZW:
         quaternion_vector = quaternion[..., 0:3]
         quaternion_scalar = quaternion[..., 3:4]
     else:
         quaternion_scalar = quaternion[..., 0:1]
         quaternion_vector = quaternion[..., 1:4]
 
     # compute quaternion norm
-    norm_q: torch.Tensor = torch.norm(quaternion_vector, p=2, dim=-1, keepdim=True).clamp(min=eps)
+    norm_q: Tensor = torch.norm(quaternion_vector, p=2, dim=-1, keepdim=True).clamp(min=eps)
 
     # apply log map
-    quaternion_log: torch.Tensor = (
-        quaternion_vector * torch.acos(torch.clamp(quaternion_scalar, min=-1.0, max=1.0)) / norm_q
-    )
+    quaternion_log: Tensor = quaternion_vector * torch.acos(torch.clamp(quaternion_scalar, min=-1.0, max=1.0)) / norm_q
 
     return quaternion_log
 
 
 # based on:
 # https://github.com/facebookresearch/QuaterNet/blob/master/common/quaternion.py#L138
 
 
-def angle_axis_to_quaternion(
-    angle_axis: torch.Tensor, order: QuaternionCoeffOrder = QuaternionCoeffOrder.XYZW
-) -> torch.Tensor:
+def angle_axis_to_quaternion(angle_axis: Tensor, order: QuaternionCoeffOrder = QuaternionCoeffOrder.XYZW) -> Tensor:
     r"""Convert an angle axis to a quaternion.
 
     The quaternion vector has components in (x, y, z, w) or (w, x, y, z) format.
 
     Adapted from ceres C++ library: ceres-solver/include/ceres/rotation.h
 
     Args:
@@ -800,20 +797,20 @@
         tensor with quaternion.
 
     Shape:
         - Input: :math:`(*, 3)` where `*` means, any number of dimensions
         - Output: :math:`(*, 4)`
 
     Example:
-        >>> angle_axis = torch.tensor((0., 1., 0.))
+        >>> angle_axis = tensor((0., 1., 0.))
         >>> angle_axis_to_quaternion(angle_axis, order=QuaternionCoeffOrder.WXYZ)
         tensor([0.8776, 0.0000, 0.4794, 0.0000])
     """
     if not torch.is_tensor(angle_axis):
-        raise TypeError(f"Input type is not a torch.Tensor. Got {type(angle_axis)}")
+        raise TypeError(f"Input type is not a Tensor. Got {type(angle_axis)}")
 
     if not angle_axis.shape[-1] == 3:
         raise ValueError(f"Input must be a tensor of shape Nx3 or 3. Got {angle_axis.shape}")
 
     if not torch.jit.is_scripting():
         if order.name not in QuaternionCoeffOrder.__members__.keys():
             raise ValueError(f"order must be one of {QuaternionCoeffOrder.__members__.keys()}")
@@ -822,127 +819,188 @@
         warnings.warn(
             "`XYZW` quaternion coefficient order is deprecated and"
             " will be removed after > 0.6. "
             "Please use `QuaternionCoeffOrder.WXYZ` instead."
         )
 
     # unpack input and compute conversion
-    a0: torch.Tensor = angle_axis[..., 0:1]
-    a1: torch.Tensor = angle_axis[..., 1:2]
-    a2: torch.Tensor = angle_axis[..., 2:3]
-    theta_squared: torch.Tensor = a0 * a0 + a1 * a1 + a2 * a2
-
-    theta: torch.Tensor = torch.sqrt(theta_squared)
-    half_theta: torch.Tensor = theta * 0.5
-
-    mask: torch.Tensor = theta_squared > 0.0
-    ones: torch.Tensor = torch.ones_like(half_theta)
-
-    k_neg: torch.Tensor = 0.5 * ones
-    k_pos: torch.Tensor = torch.sin(half_theta) / theta
-    k: torch.Tensor = torch.where(mask, k_pos, k_neg)
-    w: torch.Tensor = torch.where(mask, torch.cos(half_theta), ones)
+    a0: Tensor = angle_axis[..., 0:1]
+    a1: Tensor = angle_axis[..., 1:2]
+    a2: Tensor = angle_axis[..., 2:3]
+    theta_squared: Tensor = a0 * a0 + a1 * a1 + a2 * a2
+
+    theta: Tensor = torch.sqrt(theta_squared)
+    half_theta: Tensor = theta * 0.5
+
+    mask: Tensor = theta_squared > 0.0
+    ones: Tensor = torch.ones_like(half_theta)
+
+    k_neg: Tensor = 0.5 * ones
+    k_pos: Tensor = torch.sin(half_theta) / theta
+    k: Tensor = where(mask, k_pos, k_neg)
+    w: Tensor = where(mask, torch.cos(half_theta), ones)
 
-    quaternion: torch.Tensor = torch.zeros(
-        size=(*angle_axis.shape[:-1], 4), dtype=angle_axis.dtype, device=angle_axis.device
-    )
+    quaternion: Tensor = torch.zeros(size=(*angle_axis.shape[:-1], 4), dtype=angle_axis.dtype, device=angle_axis.device)
     if order == QuaternionCoeffOrder.XYZW:
         quaternion[..., 0:1] = a0 * k
         quaternion[..., 1:2] = a1 * k
         quaternion[..., 2:3] = a2 * k
         quaternion[..., 3:4] = w
     else:
         quaternion[..., 1:2] = a0 * k
         quaternion[..., 2:3] = a1 * k
         quaternion[..., 3:4] = a2 * k
         quaternion[..., 0:1] = w
     return quaternion
 
 
+# inspired by: https://stackoverflow.com/questions/56207448/efficient-quaternions-to-euler-transformation
+
+
+def euler_from_quaternion(w: Tensor, x: Tensor, y: Tensor, z: Tensor) -> Tuple[Tensor, Tensor, Tensor]:
+    """Convert a quaternion coefficients to Euler angles.
+
+    Returned angles are in radians in XYZ convention.
+
+    Args:
+        w: quaternion :math:`q_w` coefficient.
+        x: quaternion :math:`q_x` coefficient.
+        y: quaternion :math:`q_y` coefficient.
+        z: quaternion :math:`q_z` coefficient.
+
+    Return:
+        A tuple with euler angles`roll`, `pitch`, `yaw`.
+    """
+    KORNIA_CHECK(w.shape == x.shape)
+    KORNIA_CHECK(x.shape == y.shape)
+    KORNIA_CHECK(y.shape == z.shape)
+
+    yy = y * y
+
+    sinr_cosp = 2.0 * (w * x + y * z)
+    cosr_cosp = 1.0 - 2.0 * (x * x + yy)
+    roll = sinr_cosp.atan2(cosr_cosp)
+
+    sinp = 2.0 * (w * y - z * x)
+    sinp = sinp.clamp(min=-1.0, max=1.0)
+    pitch = sinp.asin()
+
+    siny_cosp = 2.0 * (w * z + x * y)
+    cosy_cosp = 1.0 - 2.0 * (yy + z * z)
+    yaw = siny_cosp.atan2(cosy_cosp)
+
+    return roll, pitch, yaw
+
+
+def quaternion_from_euler(roll: Tensor, pitch: Tensor, yaw: Tensor) -> Tuple[Tensor, Tensor, Tensor, Tensor]:
+    """Convert Euler angles to quaternion coefficients.
+
+    Euler angles are assumed to be in radians in XYZ convention.
+
+    Args:
+        roll: the roll euler angle.
+        pitch: the pitch euler angle.
+        yaw: the yaw euler angle.
+
+    Return:
+        A tuple with quaternion coefficients in order of `wxyz`.
+    """
+    KORNIA_CHECK(roll.shape == pitch.shape)
+    KORNIA_CHECK(pitch.shape == yaw.shape)
+
+    roll_half = roll * 0.5
+    pitch_half = pitch * 0.5
+    yaw_half = yaw * 0.5
+
+    cy = yaw_half.cos()
+    sy = yaw_half.sin()
+    cp = pitch_half.cos()
+    sp = pitch_half.sin()
+    cr = roll_half.cos()
+    sr = roll_half.sin()
+
+    qw = cy * cp * cr + sy * sp * sr
+    qx = cy * cp * sr - sy * sp * cr
+    qy = sy * cp * sr + cy * sp * cr
+    qz = sy * cp * cr - cy * sp * sr
+
+    return qw, qx, qy, qz
+
+
 # based on:
 # https://github.com/ClementPinard/SfmLearner-Pytorch/blob/master/inverse_warp.py#L65-L71
 
 
-def normalize_pixel_coordinates(
-    pixel_coordinates: torch.Tensor, height: int, width: int, eps: float = 1e-8
-) -> torch.Tensor:
+def normalize_pixel_coordinates(pixel_coordinates: Tensor, height: int, width: int, eps: float = 1e-8) -> Tensor:
     r"""Normalize pixel coordinates between -1 and 1.
 
     Normalized, -1 if on extreme left, 1 if on extreme right (x = w-1).
 
     Args:
         pixel_coordinates: the grid with pixel coordinates. Shape can be :math:`(*, 2)`.
         width: the maximum width in the x-axis.
         height: the maximum height in the y-axis.
         eps: safe division by zero.
 
     Return:
         the normalized pixel coordinates with shape :math:`(*, 2)`.
 
     Examples:
-        >>> coords = torch.tensor([[50., 100.]])
+        >>> coords = tensor([[50., 100.]])
         >>> normalize_pixel_coordinates(coords, 100, 50)
         tensor([[1.0408, 1.0202]])
     """
     if pixel_coordinates.shape[-1] != 2:
         raise ValueError("Input pixel_coordinates must be of shape (*, 2). " "Got {}".format(pixel_coordinates.shape))
 
     # compute normalization factor
-    hw: torch.Tensor = torch.stack(
+    hw: Tensor = stack(
         [
-            torch.tensor(width, device=pixel_coordinates.device, dtype=pixel_coordinates.dtype),
-            torch.tensor(height, device=pixel_coordinates.device, dtype=pixel_coordinates.dtype),
+            tensor(width, device=pixel_coordinates.device, dtype=pixel_coordinates.dtype),
+            tensor(height, device=pixel_coordinates.device, dtype=pixel_coordinates.dtype),
         ]
     )
 
-    factor: torch.Tensor = torch.tensor(2.0, device=pixel_coordinates.device, dtype=pixel_coordinates.dtype) / (
-        hw - 1
-    ).clamp(eps)
+    factor: Tensor = tensor(2.0, device=pixel_coordinates.device, dtype=pixel_coordinates.dtype) / (hw - 1).clamp(eps)
 
     return factor * pixel_coordinates - 1
 
 
-def denormalize_pixel_coordinates(
-    pixel_coordinates: torch.Tensor, height: int, width: int, eps: float = 1e-8
-) -> torch.Tensor:
+def denormalize_pixel_coordinates(pixel_coordinates: Tensor, height: int, width: int, eps: float = 1e-8) -> Tensor:
     r"""Denormalize pixel coordinates.
 
     The input is assumed to be -1 if on extreme left, 1 if on extreme right (x = w-1).
 
     Args:
         pixel_coordinates: the normalized grid coordinates. Shape can be :math:`(*, 2)`.
         width: the maximum width in the x-axis.
         height: the maximum height in the y-axis.
         eps: safe division by zero.
 
     Return:
         the denormalized pixel coordinates with shape :math:`(*, 2)`.
 
     Examples:
-        >>> coords = torch.tensor([[-1., -1.]])
+        >>> coords = tensor([[-1., -1.]])
         >>> denormalize_pixel_coordinates(coords, 100, 50)
         tensor([[0., 0.]])
     """
     if pixel_coordinates.shape[-1] != 2:
         raise ValueError("Input pixel_coordinates must be of shape (*, 2). " "Got {}".format(pixel_coordinates.shape))
     # compute normalization factor
-    hw: torch.Tensor = (
-        torch.stack([torch.tensor(width), torch.tensor(height)])
-        .to(pixel_coordinates.device)
-        .to(pixel_coordinates.dtype)
-    )
+    hw: Tensor = stack([tensor(width), tensor(height)]).to(pixel_coordinates.device).to(pixel_coordinates.dtype)
 
-    factor: torch.Tensor = torch.tensor(2.0) / (hw - 1).clamp(eps)
+    factor: Tensor = tensor(2.0) / (hw - 1).clamp(eps)
 
-    return torch.tensor(1.0) / factor * (pixel_coordinates + 1)
+    return tensor(1.0) / factor * (pixel_coordinates + 1)
 
 
 def normalize_pixel_coordinates3d(
-    pixel_coordinates: torch.Tensor, depth: int, height: int, width: int, eps: float = 1e-8
-) -> torch.Tensor:
+    pixel_coordinates: Tensor, depth: int, height: int, width: int, eps: float = 1e-8
+) -> Tensor:
     r"""Normalize pixel coordinates between -1 and 1.
 
     Normalized, -1 if on extreme left, 1 if on extreme right (x = w-1).
 
     Args:
         pixel_coordinates: the grid with pixel coordinates. Shape can be :math:`(*, 3)`.
         depth: the maximum depth in the z-axis.
@@ -952,28 +1010,26 @@
 
     Return:
         the normalized pixel coordinates.
     """
     if pixel_coordinates.shape[-1] != 3:
         raise ValueError("Input pixel_coordinates must be of shape (*, 3). " "Got {}".format(pixel_coordinates.shape))
     # compute normalization factor
-    dhw: torch.Tensor = (
-        torch.stack([torch.tensor(depth), torch.tensor(width), torch.tensor(height)])
-        .to(pixel_coordinates.device)
-        .to(pixel_coordinates.dtype)
+    dhw: Tensor = (
+        stack([tensor(depth), tensor(width), tensor(height)]).to(pixel_coordinates.device).to(pixel_coordinates.dtype)
     )
 
-    factor: torch.Tensor = torch.tensor(2.0) / (dhw - 1).clamp(eps)
+    factor: Tensor = tensor(2.0) / (dhw - 1).clamp(eps)
 
     return factor * pixel_coordinates - 1
 
 
 def denormalize_pixel_coordinates3d(
-    pixel_coordinates: torch.Tensor, depth: int, height: int, width: int, eps: float = 1e-8
-) -> torch.Tensor:
+    pixel_coordinates: Tensor, depth: int, height: int, width: int, eps: float = 1e-8
+) -> Tensor:
     r"""Denormalize pixel coordinates.
 
     The input is assumed to be -1 if on extreme left, 1 if on extreme right (x = w-1).
 
     Args:
         pixel_coordinates: the normalized grid coordinates. Shape can be :math:`(*, 3)`.
         depth: the maximum depth in the x-axis.
@@ -983,97 +1039,95 @@
 
     Return:
         the denormalized pixel coordinates.
     """
     if pixel_coordinates.shape[-1] != 3:
         raise ValueError("Input pixel_coordinates must be of shape (*, 3). " "Got {}".format(pixel_coordinates.shape))
     # compute normalization factor
-    dhw: torch.Tensor = (
-        torch.stack([torch.tensor(depth), torch.tensor(width), torch.tensor(height)])
-        .to(pixel_coordinates.device)
-        .to(pixel_coordinates.dtype)
+    dhw: Tensor = (
+        stack([tensor(depth), tensor(width), tensor(height)]).to(pixel_coordinates.device).to(pixel_coordinates.dtype)
     )
 
-    factor: torch.Tensor = torch.tensor(2.0) / (dhw - 1).clamp(eps)
+    factor: Tensor = tensor(2.0) / (dhw - 1).clamp(eps)
 
-    return torch.tensor(1.0) / factor * (pixel_coordinates + 1)
+    return tensor(1.0) / factor * (pixel_coordinates + 1)
 
 
-def angle_to_rotation_matrix(angle: torch.Tensor) -> torch.Tensor:
+def angle_to_rotation_matrix(angle: Tensor) -> Tensor:
     r"""Create a rotation matrix out of angles in degrees.
 
     Args:
         angle: tensor of angles in degrees, any shape :math:`(*)`.
 
     Returns:
         tensor of rotation matrices with shape :math:`(*, 2, 2)`.
 
     Example:
         >>> input = torch.rand(1, 3)  # Nx3
         >>> output = angle_to_rotation_matrix(input)  # Nx3x2x2
     """
     ang_rad = deg2rad(angle)
-    cos_a: torch.Tensor = torch.cos(ang_rad)
-    sin_a: torch.Tensor = torch.sin(ang_rad)
-    return torch.stack([cos_a, sin_a, -sin_a, cos_a], dim=-1).view(*angle.shape, 2, 2)
+    cos_a: Tensor = torch.cos(ang_rad)
+    sin_a: Tensor = torch.sin(ang_rad)
+    return stack([cos_a, sin_a, -sin_a, cos_a], dim=-1).view(*angle.shape, 2, 2)
 
 
 def normalize_homography(
-    dst_pix_trans_src_pix: torch.Tensor, dsize_src: Tuple[int, int], dsize_dst: Tuple[int, int]
-) -> torch.Tensor:
+    dst_pix_trans_src_pix: Tensor, dsize_src: Tuple[int, int], dsize_dst: Tuple[int, int]
+) -> Tensor:
     r"""Normalize a given homography in pixels to [-1, 1].
 
     Args:
         dst_pix_trans_src_pix: homography/ies from source to destination to be
           normalized. :math:`(B, 3, 3)`
         dsize_src: size of the source image (height, width).
         dsize_dst: size of the destination image (height, width).
 
     Returns:
         the normalized homography of shape :math:`(B, 3, 3)`.
     """
-    if not isinstance(dst_pix_trans_src_pix, torch.Tensor):
-        raise TypeError(f"Input type is not a torch.Tensor. Got {type(dst_pix_trans_src_pix)}")
+    if not isinstance(dst_pix_trans_src_pix, Tensor):
+        raise TypeError(f"Input type is not a Tensor. Got {type(dst_pix_trans_src_pix)}")
 
     if not (len(dst_pix_trans_src_pix.shape) == 3 or dst_pix_trans_src_pix.shape[-2:] == (3, 3)):
         raise ValueError(f"Input dst_pix_trans_src_pix must be a Bx3x3 tensor. Got {dst_pix_trans_src_pix.shape}")
 
     # source and destination sizes
     src_h, src_w = dsize_src
     dst_h, dst_w = dsize_dst
 
     # compute the transformation pixel/norm for src/dst
-    src_norm_trans_src_pix: torch.Tensor = normal_transform_pixel(src_h, src_w).to(dst_pix_trans_src_pix)
+    src_norm_trans_src_pix: Tensor = normal_transform_pixel(src_h, src_w).to(dst_pix_trans_src_pix)
 
     src_pix_trans_src_norm = _torch_inverse_cast(src_norm_trans_src_pix)
-    dst_norm_trans_dst_pix: torch.Tensor = normal_transform_pixel(dst_h, dst_w).to(dst_pix_trans_src_pix)
+    dst_norm_trans_dst_pix: Tensor = normal_transform_pixel(dst_h, dst_w).to(dst_pix_trans_src_pix)
 
     # compute chain transformations
-    dst_norm_trans_src_norm: torch.Tensor = dst_norm_trans_dst_pix @ (dst_pix_trans_src_pix @ src_pix_trans_src_norm)
+    dst_norm_trans_src_norm: Tensor = dst_norm_trans_dst_pix @ (dst_pix_trans_src_pix @ src_pix_trans_src_norm)
     return dst_norm_trans_src_norm
 
 
 def normal_transform_pixel(
     height: int,
     width: int,
     eps: float = 1e-14,
     device: Optional[torch.device] = None,
     dtype: Optional[torch.dtype] = None,
-) -> torch.Tensor:
+) -> Tensor:
     r"""Compute the normalization matrix from image size in pixels to [-1, 1].
 
     Args:
         height image height.
         width: image width.
         eps: epsilon to prevent divide-by-zero errors
 
     Returns:
         normalized transform with shape :math:`(1, 3, 3)`.
     """
-    tr_mat = torch.tensor([[1.0, 0.0, -1.0], [0.0, 1.0, -1.0], [0.0, 0.0, 1.0]], device=device, dtype=dtype)  # 3x3
+    tr_mat = tensor([[1.0, 0.0, -1.0], [0.0, 1.0, -1.0], [0.0, 0.0, 1.0]], device=device, dtype=dtype)  # 3x3
 
     # prevent divide by zero bugs
     width_denom: float = eps if width == 1 else width - 1.0
     height_denom: float = eps if height == 1 else height - 1.0
 
     tr_mat[0, 0] = tr_mat[0, 0] * 2.0 / width_denom
     tr_mat[1, 1] = tr_mat[1, 1] * 2.0 / height_denom
@@ -1084,27 +1138,27 @@
 def normal_transform_pixel3d(
     depth: int,
     height: int,
     width: int,
     eps: float = 1e-14,
     device: Optional[torch.device] = None,
     dtype: Optional[torch.dtype] = None,
-) -> torch.Tensor:
+) -> Tensor:
     r"""Compute the normalization matrix from image size in pixels to [-1, 1].
 
     Args:
         depth: image depth.
         height: image height.
         width: image width.
         eps: epsilon to prevent divide-by-zero errors
 
     Returns:
         normalized transform with shape :math:`(1, 4, 4)`.
     """
-    tr_mat = torch.tensor(
+    tr_mat = tensor(
         [[1.0, 0.0, 0.0, -1.0], [0.0, 1.0, 0.0, -1.0], [0.0, 0.0, 1.0, -1.0], [0.0, 0.0, 0.0, 1.0]],
         device=device,
         dtype=dtype,
     )  # 4x4
 
     # prevent divide by zero bugs
     width_denom: float = eps if width == 1 else width - 1.0
@@ -1115,83 +1169,168 @@
     tr_mat[1, 1] = tr_mat[1, 1] * 2.0 / height_denom
     tr_mat[2, 2] = tr_mat[2, 2] * 2.0 / depth_denom
 
     return tr_mat.unsqueeze(0)  # 1x4x4
 
 
 def denormalize_homography(
-    dst_pix_trans_src_pix: torch.Tensor, dsize_src: Tuple[int, int], dsize_dst: Tuple[int, int]
-) -> torch.Tensor:
+    dst_pix_trans_src_pix: Tensor, dsize_src: Tuple[int, int], dsize_dst: Tuple[int, int]
+) -> Tensor:
     r"""De-normalize a given homography in pixels from [-1, 1] to actual height and width.
 
     Args:
         dst_pix_trans_src_pix: homography/ies from source to destination to be
           denormalized. :math:`(B, 3, 3)`
         dsize_src: size of the source image (height, width).
         dsize_dst: size of the destination image (height, width).
 
     Returns:
         the denormalized homography of shape :math:`(B, 3, 3)`.
     """
-    if not isinstance(dst_pix_trans_src_pix, torch.Tensor):
-        raise TypeError(f"Input type is not a torch.Tensor. Got {type(dst_pix_trans_src_pix)}")
+    if not isinstance(dst_pix_trans_src_pix, Tensor):
+        raise TypeError(f"Input type is not a Tensor. Got {type(dst_pix_trans_src_pix)}")
 
     if not (len(dst_pix_trans_src_pix.shape) == 3 or dst_pix_trans_src_pix.shape[-2:] == (3, 3)):
         raise ValueError(f"Input dst_pix_trans_src_pix must be a Bx3x3 tensor. Got {dst_pix_trans_src_pix.shape}")
 
     # source and destination sizes
     src_h, src_w = dsize_src
     dst_h, dst_w = dsize_dst
 
     # compute the transformation pixel/norm for src/dst
-    src_norm_trans_src_pix: torch.Tensor = normal_transform_pixel(src_h, src_w).to(dst_pix_trans_src_pix)
+    src_norm_trans_src_pix: Tensor = normal_transform_pixel(src_h, src_w).to(dst_pix_trans_src_pix)
 
-    dst_norm_trans_dst_pix: torch.Tensor = normal_transform_pixel(dst_h, dst_w).to(dst_pix_trans_src_pix)
+    dst_norm_trans_dst_pix: Tensor = normal_transform_pixel(dst_h, dst_w).to(dst_pix_trans_src_pix)
     dst_denorm_trans_dst_pix = _torch_inverse_cast(dst_norm_trans_dst_pix)
     # compute chain transformations
-    dst_norm_trans_src_norm: torch.Tensor = dst_denorm_trans_dst_pix @ (dst_pix_trans_src_pix @ src_norm_trans_src_pix)
+    dst_norm_trans_src_norm: Tensor = dst_denorm_trans_dst_pix @ (dst_pix_trans_src_pix @ src_norm_trans_src_pix)
     return dst_norm_trans_src_norm
 
 
 def normalize_homography3d(
-    dst_pix_trans_src_pix: torch.Tensor, dsize_src: Tuple[int, int, int], dsize_dst: Tuple[int, int, int]
-) -> torch.Tensor:
+    dst_pix_trans_src_pix: Tensor, dsize_src: Tuple[int, int, int], dsize_dst: Tuple[int, int, int]
+) -> Tensor:
     r"""Normalize a given homography in pixels to [-1, 1].
 
     Args:
         dst_pix_trans_src_pix: homography/ies from source to destination to be
           normalized. :math:`(B, 4, 4)`
         dsize_src: size of the source image (depth, height, width).
         dsize_src: size of the destination image (depth, height, width).
 
     Returns:
         the normalized homography.
 
     Shape:
         Output: :math:`(B, 4, 4)`
     """
-    if not isinstance(dst_pix_trans_src_pix, torch.Tensor):
-        raise TypeError(f"Input type is not a torch.Tensor. Got {type(dst_pix_trans_src_pix)}")
+    if not isinstance(dst_pix_trans_src_pix, Tensor):
+        raise TypeError(f"Input type is not a Tensor. Got {type(dst_pix_trans_src_pix)}")
 
     if not (len(dst_pix_trans_src_pix.shape) == 3 or dst_pix_trans_src_pix.shape[-2:] == (4, 4)):
         raise ValueError(f"Input dst_pix_trans_src_pix must be a Bx3x3 tensor. Got {dst_pix_trans_src_pix.shape}")
 
     # source and destination sizes
     src_d, src_h, src_w = dsize_src
     dst_d, dst_h, dst_w = dsize_dst
     # compute the transformation pixel/norm for src/dst
-    src_norm_trans_src_pix: torch.Tensor = normal_transform_pixel3d(src_d, src_h, src_w).to(dst_pix_trans_src_pix)
+    src_norm_trans_src_pix: Tensor = normal_transform_pixel3d(src_d, src_h, src_w).to(dst_pix_trans_src_pix)
 
     src_pix_trans_src_norm = _torch_inverse_cast(src_norm_trans_src_pix)
-    dst_norm_trans_dst_pix: torch.Tensor = normal_transform_pixel3d(dst_d, dst_h, dst_w).to(dst_pix_trans_src_pix)
+    dst_norm_trans_dst_pix: Tensor = normal_transform_pixel3d(dst_d, dst_h, dst_w).to(dst_pix_trans_src_pix)
     # compute chain transformations
-    dst_norm_trans_src_norm: torch.Tensor = dst_norm_trans_dst_pix @ (dst_pix_trans_src_pix @ src_pix_trans_src_norm)
+    dst_norm_trans_src_norm: Tensor = dst_norm_trans_dst_pix @ (dst_pix_trans_src_pix @ src_pix_trans_src_norm)
     return dst_norm_trans_src_norm
 
 
+def normalize_points_with_intrinsics(point_2d: Tensor, camera_matrix: Tensor):
+    r"""Normalizes points with intrinsics. Useful for conversion of keypoints to be used with essential matrix.
+
+    Args:
+        point_2d: tensor containing the 2d points in the image pixel coordinates.
+        The shape of the tensor can be :math:`(*, 2)`.
+        camera_matrix: tensor containing the intrinsics camera
+            matrix. The tensor shape must be :math:`(*, 3, 3)`.
+
+    Returns:
+        tensor of (u, v) cam coordinates with shape :math:`(*, 2)`.
+
+    Example:
+        >>> _ = torch.manual_seed(0)
+        >>> X = torch.rand(1, 2)
+        >>> K = torch.eye(3)[None]
+        >>> normalize_points_with_intrinsics(X, K)
+        tensor([[0.4963, 0.7682]])
+    """
+    KORNIA_CHECK_SHAPE(point_2d, ["*", "2"])
+    KORNIA_CHECK_SHAPE(camera_matrix, ["*", "3", "3"])
+    # projection eq. K_inv * [u v 1]'
+    # x = (u - cx) * Z / fx
+    # y = (v - cy) * Z / fy
+
+    # unpack coordinates
+    u_coord: Tensor = point_2d[..., 0]
+    v_coord: Tensor = point_2d[..., 1]
+
+    # unpack intrinsics
+    fx: Tensor = camera_matrix[..., 0, 0]
+    fy: Tensor = camera_matrix[..., 1, 1]
+    cx: Tensor = camera_matrix[..., 0, 2]
+    cy: Tensor = camera_matrix[..., 1, 2]
+
+    # projective
+    x_coord: Tensor = (u_coord - cx) / fx
+    y_coord: Tensor = (v_coord - cy) / fy
+
+    xy: Tensor = stack([x_coord, y_coord], dim=-1)
+    return xy
+
+
+def denormalize_points_with_intrinsics(point_2d_norm: Tensor, camera_matrix: Tensor):
+    r"""Normalizes points with intrinsics. Useful for conversion of keypoints to be used with essential matrix.
+
+    Args:
+        point_2d_norm: tensor containing the 2d points in the image pixel coordinates.
+        The shape of the tensor can be :math:`(*, 2)`.
+        camera_matrix: tensor containing the intrinsics camera
+            matrix. The tensor shape must be :math:`(*, 3, 3)`.
+
+    Returns:
+        tensor of (u, v) cam coordinates with shape :math:`(*, 2)`.
+
+    Example:
+        >>> _ = torch.manual_seed(0)
+        >>> X = torch.rand(1, 2)
+        >>> K = torch.eye(3)[None]
+        >>> denormalize_points_with_intrinsics(X, K)
+        tensor([[0.4963, 0.7682]])
+    """
+    KORNIA_CHECK_SHAPE(point_2d_norm, ["*", "2"])
+    KORNIA_CHECK_SHAPE(camera_matrix, ["*", "3", "3"])
+    # projection eq. [u, v, w]' = K * [x y z 1]'
+    # u = fx * X + cx
+    # v = fy * Y + cy
+
+    # unpack coordinates
+    x_coord: Tensor = point_2d_norm[..., 0]
+    y_coord: Tensor = point_2d_norm[..., 1]
+
+    # unpack intrinsics
+    fx: Tensor = camera_matrix[..., 0, 0]
+    fy: Tensor = camera_matrix[..., 1, 1]
+    cx: Tensor = camera_matrix[..., 0, 2]
+    cy: Tensor = camera_matrix[..., 1, 2]
+
+    # apply intrinsics ans return
+    u_coord: Tensor = x_coord * fx + cx
+    v_coord: Tensor = y_coord * fy + cy
+
+    return stack([u_coord, v_coord], dim=-1)
+
+
 def Rt_to_matrix4x4(R: Tensor, t: Tensor) -> Tensor:
     r"""Combines 3x3 rotation matrix R and 1x3 translation vector t into 4x4 extrinsics.
 
     Args:
         R: Rotation matrix, :math:`(B, 3, 3).`
         t: Translation matrix :math:`(B, 3, 1)`.
 
@@ -1204,15 +1343,15 @@
         tensor([[[1., 0., 0., 1.],
                  [0., 1., 0., 1.],
                  [0., 0., 1., 1.],
                  [0., 0., 0., 1.]]])
     """
     KORNIA_CHECK_SHAPE(R, ["B", "3", "3"])
     KORNIA_CHECK_SHAPE(t, ["B", "3", "1"])
-    Rt = torch.cat([R, t], dim=2)
+    Rt = concatenate([R, t], dim=2)
     return convert_affinematrix_to_homography3d(Rt)
 
 
 def matrix4x4_to_Rt(extrinsics: Tensor) -> Tuple[Tensor, Tensor]:
     r"""Converts 4x4 extrinsics into 3x3 rotation matrix R and 1x3 translation vector ts.
 
     Args:
@@ -1310,15 +1449,15 @@
         >>> camtoworld_vision_to_graphics_4x4(ext)
         tensor([[[ 1.,  0.,  0.,  0.],
                  [ 0., -1.,  0.,  0.],
                  [ 0.,  0., -1.,  0.],
                  [ 0.,  0.,  0.,  1.]]])
     """
     KORNIA_CHECK_SHAPE(extrinsics_vision, ["B", "4", "4"])
-    invert_yz = torch.tensor(
+    invert_yz = tensor(
         [[[1, 0, 0, 0], [0, -1, 0, 0], [0, 0, -1, 0], [0, 0, 0, 1.0]]],
         dtype=extrinsics_vision.dtype,
         device=extrinsics_vision.device,
     )
     return extrinsics_vision @ invert_yz
 
 
@@ -1422,15 +1561,15 @@
         tvec: translation vector :math:`(B, 3, 1)`, [x, y, z]
 
     Returns:
         qvec: Colmap rotation quaternion :math:`(B, 4)`, [w, x, y, z] format.
         tvec: translation vector :math:`(B, 3, 1)`, [x, y, z]
 
     Example:
-        >>> q, t = torch.tensor([0, 1, 0, 1.])[None], torch.ones(3).reshape(1, 3, 1)
+        >>> q, t = tensor([0, 1, 0, 1.])[None], torch.ones(3).reshape(1, 3, 1)
         >>> ARKitQTVecs_to_ColmapQTVecs(q, t)
         (tensor([[0.7071, 0.0000, 0.7071, 0.0000]]), tensor([[[-1.0000],
                  [-1.0000],
                  [ 1.0000]]]))
     """
     # ToDo:  integrate QuaterniaonAPI
```

### Comparing `kornia-0.6.8/kornia/geometry/depth.py` & `kornia-0.6.9/kornia/geometry/depth.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,25 +1,25 @@
 """Module containing operators to work on RGB-Depth images."""
 from typing import Union
 
 import torch
-import torch.nn as nn
 import torch.nn.functional as F
 
+from kornia.core import Module, Tensor, concatenate, tensor
 from kornia.filters.sobel import spatial_gradient
 from kornia.utils import create_meshgrid
 
 from .camera import PinholeCamera, cam2pixel, pixel2cam, project_points, unproject_points
 from .conversions import normalize_pixel_coordinates
 from .linalg import compose_transformations, convert_points_to_homogeneous, inverse_transformation, transform_points
 
 __all__ = ["depth_to_3d", "depth_to_normals", "warp_frame_depth", "depth_warp", "DepthWarper"]
 
 
-def depth_to_3d(depth: torch.Tensor, camera_matrix: torch.Tensor, normalize_points: bool = False) -> torch.Tensor:
+def depth_to_3d(depth: Tensor, camera_matrix: Tensor, normalize_points: bool = False) -> Tensor:
     """Compute a 3d point per pixel given its depth value and the camera intrinsics.
 
     Args:
         depth: image tensor containing a depth value per pixel with shape :math:`(B, 1, H, W)`.
         camera_matrix: tensor containing the camera intrinsics with shape :math:`(B, 3, 3)`.
         normalize_points: whether to normalise the pointcloud. This must be set to `True` when the depth is
           represented as the Euclidean ray length from the camera position.
@@ -29,44 +29,44 @@
 
     Example:
         >>> depth = torch.rand(1, 1, 4, 4)
         >>> K = torch.eye(3)[None]
         >>> depth_to_3d(depth, K).shape
         torch.Size([1, 3, 4, 4])
     """
-    if not isinstance(depth, torch.Tensor):
-        raise TypeError(f"Input depht type is not a torch.Tensor. Got {type(depth)}.")
+    if not isinstance(depth, Tensor):
+        raise TypeError(f"Input depht type is not a Tensor. Got {type(depth)}.")
 
     if not (len(depth.shape) == 4 and depth.shape[-3] == 1):
         raise ValueError(f"Input depth musth have a shape (B, 1, H, W). Got: {depth.shape}")
 
-    if not isinstance(camera_matrix, torch.Tensor):
-        raise TypeError(f"Input camera_matrix type is not a torch.Tensor. " f"Got {type(camera_matrix)}.")
+    if not isinstance(camera_matrix, Tensor):
+        raise TypeError(f"Input camera_matrix type is not a Tensor. " f"Got {type(camera_matrix)}.")
 
     if not (len(camera_matrix.shape) == 3 and camera_matrix.shape[-2:] == (3, 3)):
         raise ValueError(f"Input camera_matrix must have a shape (B, 3, 3). " f"Got: {camera_matrix.shape}.")
 
     # create base coordinates grid
     _, _, height, width = depth.shape
-    points_2d: torch.Tensor = create_meshgrid(height, width, normalized_coordinates=False)  # 1xHxWx2
+    points_2d: Tensor = create_meshgrid(height, width, normalized_coordinates=False)  # 1xHxWx2
     points_2d = points_2d.to(depth.device).to(depth.dtype)
 
     # depth should come in Bx1xHxW
-    points_depth: torch.Tensor = depth.permute(0, 2, 3, 1)  # 1xHxWx1
+    points_depth: Tensor = depth.permute(0, 2, 3, 1)  # 1xHxWx1
 
     # project pixels to camera frame
-    camera_matrix_tmp: torch.Tensor = camera_matrix[:, None, None]  # Bx1x1x3x3
-    points_3d: torch.Tensor = unproject_points(
+    camera_matrix_tmp: Tensor = camera_matrix[:, None, None]  # Bx1x1x3x3
+    points_3d: Tensor = unproject_points(
         points_2d, points_depth, camera_matrix_tmp, normalize=normalize_points
     )  # BxHxWx3
 
     return points_3d.permute(0, 3, 1, 2)  # Bx3xHxW
 
 
-def depth_to_normals(depth: torch.Tensor, camera_matrix: torch.Tensor, normalize_points: bool = False) -> torch.Tensor:
+def depth_to_normals(depth: Tensor, camera_matrix: Tensor, normalize_points: bool = False) -> Tensor:
     """Compute the normal surface per pixel.
 
     Args:
         depth: image tensor containing a depth value per pixel with shape :math:`(B, 1, H, W)`.
         camera_matrix: tensor containing the camera intrinsics with shape :math:`(B, 3, 3)`.
         normalize_points: whether to normalise the pointcloud. This must be set to `True` when the depth is
         represented as the Euclidean ray length from the camera position.
@@ -76,46 +76,42 @@
 
     Example:
         >>> depth = torch.rand(1, 1, 4, 4)
         >>> K = torch.eye(3)[None]
         >>> depth_to_normals(depth, K).shape
         torch.Size([1, 3, 4, 4])
     """
-    if not isinstance(depth, torch.Tensor):
-        raise TypeError(f"Input depht type is not a torch.Tensor. Got {type(depth)}.")
+    if not isinstance(depth, Tensor):
+        raise TypeError(f"Input depht type is not a Tensor. Got {type(depth)}.")
 
     if not (len(depth.shape) == 4 and depth.shape[-3] == 1):
         raise ValueError(f"Input depth musth have a shape (B, 1, H, W). Got: {depth.shape}")
 
-    if not isinstance(camera_matrix, torch.Tensor):
-        raise TypeError(f"Input camera_matrix type is not a torch.Tensor. " f"Got {type(camera_matrix)}.")
+    if not isinstance(camera_matrix, Tensor):
+        raise TypeError(f"Input camera_matrix type is not a Tensor. " f"Got {type(camera_matrix)}.")
 
     if not (len(camera_matrix.shape) == 3 and camera_matrix.shape[-2:] == (3, 3)):
         raise ValueError(f"Input camera_matrix must have a shape (B, 3, 3). " f"Got: {camera_matrix.shape}.")
 
     # compute the 3d points from depth
-    xyz: torch.Tensor = depth_to_3d(depth, camera_matrix, normalize_points)  # Bx3xHxW
+    xyz: Tensor = depth_to_3d(depth, camera_matrix, normalize_points)  # Bx3xHxW
 
     # compute the pointcloud spatial gradients
-    gradients: torch.Tensor = spatial_gradient(xyz)  # Bx3x2xHxW
+    gradients: Tensor = spatial_gradient(xyz)  # Bx3x2xHxW
 
     # compute normals
     a, b = gradients[:, :, 0], gradients[:, :, 1]  # Bx3xHxW
 
-    normals: torch.Tensor = torch.cross(a, b, dim=1)  # Bx3xHxW
+    normals: Tensor = torch.cross(a, b, dim=1)  # Bx3xHxW
     return F.normalize(normals, dim=1, p=2)
 
 
 def warp_frame_depth(
-    image_src: torch.Tensor,
-    depth_dst: torch.Tensor,
-    src_trans_dst: torch.Tensor,
-    camera_matrix: torch.Tensor,
-    normalize_points: bool = False,
-) -> torch.Tensor:
+    image_src: Tensor, depth_dst: Tensor, src_trans_dst: Tensor, camera_matrix: Tensor, normalize_points: bool = False
+) -> Tensor:
     """Warp a tensor from a source to destination frame by the depth in the destination.
 
     Compute 3d points from the depth, transform them using given transformation, then project the point cloud to an
     image plane.
 
     Args:
         image_src: image tensor in the source frame with shape :math:`(B,D,H,W)`.
@@ -124,58 +120,58 @@
         camera_matrix: tensor containing the camera intrinsics with shape :math:`(B,3,3)`.
         normalize_points: whether to normalise the pointcloud. This must be set to ``True`` when the depth
            is represented as the Euclidean ray length from the camera position.
 
     Return:
         the warped tensor in the source frame with shape :math:`(B,3,H,W)`.
     """
-    if not isinstance(image_src, torch.Tensor):
-        raise TypeError(f"Input image_src type is not a torch.Tensor. Got {type(image_src)}.")
+    if not isinstance(image_src, Tensor):
+        raise TypeError(f"Input image_src type is not a Tensor. Got {type(image_src)}.")
 
     if not len(image_src.shape) == 4:
         raise ValueError(f"Input image_src musth have a shape (B, D, H, W). Got: {image_src.shape}")
 
-    if not isinstance(depth_dst, torch.Tensor):
-        raise TypeError(f"Input depht_dst type is not a torch.Tensor. Got {type(depth_dst)}.")
+    if not isinstance(depth_dst, Tensor):
+        raise TypeError(f"Input depht_dst type is not a Tensor. Got {type(depth_dst)}.")
 
     if not (len(depth_dst.shape) == 4 and depth_dst.shape[-3] == 1):
         raise ValueError(f"Input depth_dst musth have a shape (B, 1, H, W). Got: {depth_dst.shape}")
 
-    if not isinstance(src_trans_dst, torch.Tensor):
-        raise TypeError(f"Input src_trans_dst type is not a torch.Tensor. " f"Got {type(src_trans_dst)}.")
+    if not isinstance(src_trans_dst, Tensor):
+        raise TypeError(f"Input src_trans_dst type is not a Tensor. " f"Got {type(src_trans_dst)}.")
 
     if not (len(src_trans_dst.shape) == 3 and src_trans_dst.shape[-2:] == (4, 4)):
         raise ValueError(f"Input src_trans_dst must have a shape (B, 4, 4). " f"Got: {src_trans_dst.shape}.")
 
-    if not isinstance(camera_matrix, torch.Tensor):
-        raise TypeError(f"Input camera_matrix type is not a torch.Tensor. " f"Got {type(camera_matrix)}.")
+    if not isinstance(camera_matrix, Tensor):
+        raise TypeError(f"Input camera_matrix type is not a Tensor. " f"Got {type(camera_matrix)}.")
 
     if not (len(camera_matrix.shape) == 3 and camera_matrix.shape[-2:] == (3, 3)):
         raise ValueError(f"Input camera_matrix must have a shape (B, 3, 3). " f"Got: {camera_matrix.shape}.")
     # unproject source points to camera frame
-    points_3d_dst: torch.Tensor = depth_to_3d(depth_dst, camera_matrix, normalize_points)  # Bx3xHxW
+    points_3d_dst: Tensor = depth_to_3d(depth_dst, camera_matrix, normalize_points)  # Bx3xHxW
 
     # transform points from source to destination
     points_3d_dst = points_3d_dst.permute(0, 2, 3, 1)  # BxHxWx3
 
     # apply transformation to the 3d points
     points_3d_src = transform_points(src_trans_dst[:, None], points_3d_dst)  # BxHxWx3
 
     # project back to pixels
-    camera_matrix_tmp: torch.Tensor = camera_matrix[:, None, None]  # Bx1x1xHxW
-    points_2d_src: torch.Tensor = project_points(points_3d_src, camera_matrix_tmp)  # BxHxWx2
+    camera_matrix_tmp: Tensor = camera_matrix[:, None, None]  # Bx1x1xHxW
+    points_2d_src: Tensor = project_points(points_3d_src, camera_matrix_tmp)  # BxHxWx2
 
     # normalize points between [-1 / 1]
     height, width = depth_dst.shape[-2:]
-    points_2d_src_norm: torch.Tensor = normalize_pixel_coordinates(points_2d_src, height, width)  # BxHxWx2
+    points_2d_src_norm: Tensor = normalize_pixel_coordinates(points_2d_src, height, width)  # BxHxWx2
 
-    return F.grid_sample(image_src, points_2d_src_norm, align_corners=True)  # type: ignore
+    return F.grid_sample(image_src, points_2d_src_norm, align_corners=True)
 
 
-class DepthWarper(nn.Module):
+class DepthWarper(Module):
     r"""Warp a patch by depth.
 
     .. math::
         P_{src}^{\{dst\}} = K_{dst} * T_{src}^{\{dst\}}
 
         I_{src} = \\omega(I_{dst}, P_{src}^{\{dst\}}, D_{src})
 
@@ -205,21 +201,21 @@
         self.padding_mode: str = padding_mode
         self.eps = 1e-6
         self.align_corners: bool = align_corners
 
         # state members
         self._pinhole_dst: PinholeCamera = pinhole_dst
         self._pinhole_src: Union[None, PinholeCamera] = None
-        self._dst_proj_src: Union[None, torch.Tensor] = None
+        self._dst_proj_src: Union[None, Tensor] = None
 
-        self.grid: torch.Tensor = self._create_meshgrid(height, width)
+        self.grid: Tensor = self._create_meshgrid(height, width)
 
     @staticmethod
-    def _create_meshgrid(height: int, width: int) -> torch.Tensor:
-        grid: torch.Tensor = create_meshgrid(height, width, normalized_coordinates=False)  # 1xHxWx2
+    def _create_meshgrid(height: int, width: int) -> Tensor:
+        grid: Tensor = create_meshgrid(height, width, normalized_coordinates=False)  # 1xHxWx2
         return convert_points_to_homogeneous(grid)  # append ones to last dim
 
     def compute_projection_matrix(self, pinhole_src: PinholeCamera) -> 'DepthWarper':
         r"""Compute the projection matrix from the source to destination frame."""
         if not isinstance(self._pinhole_dst, PinholeCamera):
             raise TypeError(
                 "Member self._pinhole_dst expected to be of class "
@@ -227,56 +223,54 @@
             )
         if not isinstance(pinhole_src, PinholeCamera):
             raise TypeError(
                 "Argument pinhole_src expected to be of class " "PinholeCamera. Got {}".format(type(pinhole_src))
             )
         # compute the relative pose between the non reference and the reference
         # camera frames.
-        dst_trans_src: torch.Tensor = compose_transformations(
+        dst_trans_src: Tensor = compose_transformations(
             self._pinhole_dst.extrinsics, inverse_transformation(pinhole_src.extrinsics)
         )
 
         # compute the projection matrix between the non reference cameras and
         # the reference.
-        dst_proj_src: torch.Tensor = torch.matmul(self._pinhole_dst.intrinsics, dst_trans_src)
+        dst_proj_src: Tensor = torch.matmul(self._pinhole_dst.intrinsics, dst_trans_src)
 
         # update class members
         self._pinhole_src = pinhole_src
         self._dst_proj_src = dst_proj_src
         return self
 
     def _compute_projection(self, x, y, invd):
         if self._dst_proj_src is None or self._pinhole_src is None:
             raise ValueError("Please, call compute_projection_matrix.")
 
-        point = torch.tensor(
-            [[[x], [y], [invd], [1.0]]], device=self._dst_proj_src.device, dtype=self._dst_proj_src.dtype
-        )
+        point = tensor([[[x], [y], [invd], [1.0]]], device=self._dst_proj_src.device, dtype=self._dst_proj_src.dtype)
         flow = torch.matmul(self._dst_proj_src, point)
         z = 1.0 / flow[:, 2]
         x = flow[:, 0] * z
         y = flow[:, 1] * z
-        return torch.cat([x, y], 1)
+        return concatenate([x, y], 1)
 
-    def compute_subpixel_step(self) -> torch.Tensor:
+    def compute_subpixel_step(self) -> Tensor:
         """Compute the required inverse depth step to achieve sub pixel accurate sampling of the depth cost volume,
         per camera.
 
         Szeliski, Richard, and Daniel Scharstein. "Symmetric sub-pixel stereo matching." European Conference on Computer
         Vision. Springer Berlin Heidelberg, 2002.
         """
         delta_d = 0.01
         xy_m1 = self._compute_projection(self.width / 2, self.height / 2, 1.0 - delta_d)
         xy_p1 = self._compute_projection(self.width / 2, self.height / 2, 1.0 + delta_d)
         dx = torch.norm((xy_p1 - xy_m1), 2, dim=-1) / 2.0
         dxdd = dx / (delta_d)  # pixel*(1/meter)
         # half pixel sampling, we're interested in the min for all cameras
         return torch.min(0.5 / dxdd)
 
-    def warp_grid(self, depth_src: torch.Tensor) -> torch.Tensor:
+    def warp_grid(self, depth_src: Tensor) -> Tensor:
         """Compute a grid for warping a given the depth from the reference pinhole camera.
 
         The function `compute_projection_matrix` has to be called beforehand in order to have precomputed the relative
         projection matrices encoding the relative pose and the intrinsics between the reference and a non reference
         camera.
         """
         # TODO: add type and value checkings
@@ -288,31 +282,31 @@
 
         # unpack depth attributes
         batch_size, _, _, _ = depth_src.shape
         device: torch.device = depth_src.device
         dtype: torch.dtype = depth_src.dtype
 
         # expand the base coordinate grid according to the input batch size
-        pixel_coords: torch.Tensor = self.grid.to(device=device, dtype=dtype).expand(batch_size, -1, -1, -1)  # BxHxWx3
+        pixel_coords: Tensor = self.grid.to(device=device, dtype=dtype).expand(batch_size, -1, -1, -1)  # BxHxWx3
 
         # reproject the pixel coordinates to the camera frame
-        cam_coords_src: torch.Tensor = pixel2cam(
+        cam_coords_src: Tensor = pixel2cam(
             depth_src, self._pinhole_src.intrinsics_inverse().to(device=device, dtype=dtype), pixel_coords
         )  # BxHxWx3
 
         # reproject the camera coordinates to the pixel
-        pixel_coords_src: torch.Tensor = cam2pixel(
+        pixel_coords_src: Tensor = cam2pixel(
             cam_coords_src, self._dst_proj_src.to(device=device, dtype=dtype)
         )  # (B*N)xHxWx2
 
         # normalize between -1 and 1 the coordinates
-        pixel_coords_src_norm: torch.Tensor = normalize_pixel_coordinates(pixel_coords_src, self.height, self.width)
+        pixel_coords_src_norm: Tensor = normalize_pixel_coordinates(pixel_coords_src, self.height, self.width)
         return pixel_coords_src_norm
 
-    def forward(self, depth_src: torch.Tensor, patch_dst: torch.Tensor) -> torch.Tensor:
+    def forward(self, depth_src: Tensor, patch_dst: Tensor) -> Tensor:
         """Warp a tensor from destination frame to reference given the depth in the reference frame.
 
         Args:
             depth_src: the depth in the reference frame. The tensor must have a shape :math:`(B, 1, H, W)`.
             patch_dst: the patch in the destination frame. The tensor must have a shape :math:`(B, C, H, W)`.
 
         Return:
@@ -333,26 +327,26 @@
             >>> # warp the destination frame to reference by depth
             >>> depth_src = torch.ones(1, 1, 32, 32)  # Nx1xHxW
             >>> image_dst = torch.rand(1, 3, 32, 32)  # NxCxHxW
             >>> image_src = warper(depth_src, image_dst)  # NxCxHxW
         """
         return F.grid_sample(
             patch_dst,
-            self.warp_grid(depth_src),  # type: ignore
+            self.warp_grid(depth_src),
             mode=self.mode,
             padding_mode=self.padding_mode,
             align_corners=self.align_corners,
         )
 
 
 def depth_warp(
     pinhole_dst: PinholeCamera,
     pinhole_src: PinholeCamera,
-    depth_src: torch.Tensor,
-    patch_dst: torch.Tensor,
+    depth_src: Tensor,
+    patch_dst: Tensor,
     height: int,
     width: int,
     align_corners: bool = True,
 ):
     r"""Function that warps a tensor from destination frame to reference given the depth in the reference frame.
 
     See :class:`~kornia.geometry.warp.DepthWarper` for details.
```

### Comparing `kornia-0.6.8/kornia/geometry/epipolar/__init__.py` & `kornia-0.6.9/kornia/geometry/epipolar/__init__.py`

 * *Files identical despite different names*

### Comparing `kornia-0.6.8/kornia/geometry/epipolar/_metrics.py` & `kornia-0.6.9/kornia/geometry/epipolar/_metrics.py`

 * *Files identical despite different names*

### Comparing `kornia-0.6.8/kornia/geometry/epipolar/essential.py` & `kornia-0.6.9/kornia/geometry/epipolar/essential.py`

 * *Files identical despite different names*

### Comparing `kornia-0.6.8/kornia/geometry/epipolar/fundamental.py` & `kornia-0.6.9/kornia/geometry/epipolar/fundamental.py`

 * *Files identical despite different names*

### Comparing `kornia-0.6.8/kornia/geometry/epipolar/numeric.py` & `kornia-0.6.9/kornia/geometry/epipolar/numeric.py`

 * *Files identical despite different names*

### Comparing `kornia-0.6.8/kornia/geometry/epipolar/projection.py` & `kornia-0.6.9/kornia/geometry/epipolar/projection.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,14 +1,14 @@
 """Module for image projections."""
 from typing import Tuple, Union
 
 import torch
+from torch.linalg import qr as linalg_qr
 
 from kornia.utils import eye_like, vec_like
-from kornia.utils._compat import linalg_qr
 from kornia.utils.helpers import _torch_svd_cast
 
 from .numeric import cross_product_matrix
 
 
 def intrinsics_like(focal: float, input: torch.Tensor) -> torch.Tensor:
     r"""Return a 3x3 instrinsics matrix, with same size as the input.
@@ -45,15 +45,15 @@
         low: lower range (inclusive).
         high: upper range (exclusive).
 
     Returns:
         the random camera matrix with the shape of :math:`(1, 3, 3)`.
     """
     sampler = torch.distributions.Uniform(low, high)
-    fx, fy, cx, cy = (sampler.sample((1,)) for _ in range(4))
+    fx, fy, cx, cy = (sampler.sample(torch.Size((1,))) for _ in range(4))
     zeros, ones = torch.zeros_like(fx), torch.ones_like(fx)
     camera_matrix: torch.Tensor = torch.cat([fx, zeros, cx, zeros, fy, cy, zeros, zeros, ones])
     return camera_matrix.view(1, 3, 3)
 
 
 def scale_intrinsics(camera_matrix: torch.Tensor, scale_factor: Union[float, torch.Tensor]) -> torch.Tensor:
     r"""Scale a camera matrix containing the intrinsics.
```

### Comparing `kornia-0.6.8/kornia/geometry/epipolar/scene.py` & `kornia-0.6.9/kornia/geometry/epipolar/scene.py`

 * *Files identical despite different names*

### Comparing `kornia-0.6.8/kornia/geometry/epipolar/triangulation.py` & `kornia-0.6.9/kornia/geometry/epipolar/triangulation.py`

 * *Files identical despite different names*

### Comparing `kornia-0.6.8/kornia/geometry/homography.py` & `kornia-0.6.9/kornia/geometry/homography.py`

 * *Files identical despite different names*

### Comparing `kornia-0.6.8/kornia/geometry/linalg.py` & `kornia-0.6.9/kornia/geometry/linalg.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,21 +1,22 @@
 import torch
 from torch import Tensor
 
-from kornia.testing import KORNIA_CHECK_IS_TENSOR, check_is_tensor
-
-from .conversions import convert_points_from_homogeneous, convert_points_to_homogeneous
+from kornia.geometry.conversions import convert_points_from_homogeneous, convert_points_to_homogeneous
+from kornia.testing import KORNIA_CHECK, KORNIA_CHECK_IS_TENSOR, KORNIA_CHECK_SHAPE, check_is_tensor
 
 __all__ = [
     "compose_transformations",
     "relative_transformation",
     "inverse_transformation",
     "transform_points",
     "point_line_distance",
     "squared_norm",
+    "batched_dot_product",
+    "euclidean_distance",
 ]
 
 
 def compose_transformations(trans_01: torch.Tensor, trans_12: torch.Tensor) -> torch.Tensor:
     r"""Function that composes two homogeneous transformations.
 
     .. math::
@@ -221,14 +222,41 @@
 
     numerator = (line[..., 0] * point[..., 0] + line[..., 1] * point[..., 1] + line[..., 2]).abs()
     denominator = line[..., :2].norm(dim=-1)
 
     return numerator / (denominator + eps)
 
 
-def squared_norm(x: Tensor) -> Tensor:
+def batched_dot_product(x: Tensor, y: Tensor, keepdim: bool = False) -> Tensor:
+    """Return a batched version of .dot()"""
+    KORNIA_CHECK_SHAPE(x, ["*", "N"])
+    KORNIA_CHECK_SHAPE(y, ["*", "N"])
+    return (x * y).sum(-1, keepdim)
+
+
+def batched_squared_norm(x: Tensor, keepdim: bool = False) -> Tensor:
     """Return the squared norm of a vector."""
-    return x.pow(2).sum(-1)
+    return batched_dot_product(x, x, keepdim)
+
+
+def euclidean_distance(x: Tensor, y: Tensor, keepdim: bool = False, eps: float = 1e-6) -> Tensor:
+    """Compute the Euclidean distance between two set of n-dimensional points.
+
+    More: https://en.wikipedia.org/wiki/Euclidean_distance
+    Args:
+        x: first set of points of shape :math:`(*, N)`.
+        y: second set of points of shape :math:`(*, N)`.
+        keepdim: whether to keep the dimension after reduction.
+        eps: small value to have numerical stability.
+    """
+    KORNIA_CHECK_SHAPE(x, ["*", "N"])
+    KORNIA_CHECK_SHAPE(y, ["*", "N"])
+    KORNIA_CHECK(x.shape == y.shape)
+
+    return (x - y + eps).pow(2).sum(-1, keepdim).sqrt()
+
 
+# aliases
+squared_norm = batched_squared_norm
 
 # TODO:
 # - project_points: from opencv
```

### Comparing `kornia-0.6.8/kornia/geometry/line.py` & `kornia-0.6.9/kornia/geometry/line.py`

 * *Files 11% similar despite different names*

```diff
@@ -1,28 +1,22 @@
 # kornia.geometry.line module inspired by Eigen::geometry::ParametrizedLine
 # https://gitlab.com/libeigen/eigen/-/blob/master/Eigen/src/Geometry/ParametrizedLine.h
-from typing import Optional, Union
+from typing import Optional, Tuple, Union
 
 import torch
 
-from kornia.core import Module, Parameter, Tensor, normalize
-from kornia.geometry.linalg import squared_norm
+from kornia.core import Module, Parameter, Tensor, normalize, where
+from kornia.geometry.linalg import batched_dot_product, squared_norm
+from kornia.geometry.plane import Hyperplane
 from kornia.testing import KORNIA_CHECK, KORNIA_CHECK_IS_TENSOR, KORNIA_CHECK_SHAPE
 from kornia.utils.helpers import _torch_svd_cast
 
 __all__ = ["ParametrizedLine", "fit_line"]
 
 
-# TODO: implement me: https://gitlab.com/libeigen/eigen/-/blob/master/Eigen/src/Geometry/Hyperplane.h
-class Hyperplane:
-    """Not implemented yet: https://gitlab.com/libeigen/eigen/-/blob/master/Eigen/src/Geometry/Hyperplane.h."""
-
-    pass
-
-
 class ParametrizedLine(Module):
     """Class that describes a parametrize line.
 
     A parametrized line is defined by an origin point :math:`o` and a unit
     direction vector :math:`d` such that the line corresponds to the set
 
     .. math::
@@ -83,18 +77,14 @@
         Example:
             >>> p0 = torch.tensor([0.0, 0.0])
             >>> p1 = torch.tensor([1.0, 1.0])
             >>> l = ParametrizedLine.through(p0, p1)
         """
         return ParametrizedLine(p0, normalize((p1 - p0), p=2, dim=-1))
 
-    @classmethod
-    def from_hyperplane(cls, plane: Hyperplane) -> "ParametrizedLine":
-        raise NotImplementedError(f"Plane not implemented yet {plane}.")
-
     def point_at(self, t: Union[float, Tensor]) -> Tensor:
         """The point at :math:`t` along this line.
 
         Args:
             t: step along the line.
 
         Return:
@@ -136,14 +126,38 @@
         return self.squared_distance(point).sqrt()
 
     # TODO(edgar) implement the following:
     # - intersection
     # - intersection_parameter
     # - intersection_point
 
+    # TODO: add tests, and possibly return a mask
+    def intersect(self, plane: Hyperplane, eps: float = 1e-6) -> Tuple[Tensor, Tensor]:
+        """Return the intersection point between the line and a given plane.
+
+        Args:
+            plane: the plane to compute the intersection point.
+
+        Return:
+            - the lambda value used to compute the look at point.
+            - the intersected point.
+        """
+        dot_prod = batched_dot_product(plane.normal.data, self.direction.data)
+        dot_prod_mask = dot_prod.abs() >= eps
+
+        # TODO: add check for dot product
+        res_lambda = where(
+            dot_prod_mask,
+            -(plane.offset + batched_dot_product(plane.normal.data, self.origin.data)) / dot_prod,
+            torch.empty_like(dot_prod),
+        )
+
+        res_point = self.point_at(res_lambda)
+        return res_lambda, res_point
+
 
 def fit_line(points: Tensor, weights: Optional[Tensor] = None) -> ParametrizedLine:
     """Fit a line from a set of points.
 
     Args:
         points: tensor containing a batch of sets of n-dimensional points. The  expected
             shape of the tensor is :math:`(B, N, D)`.
```

### Comparing `kornia-0.6.8/kornia/geometry/quaternion.py` & `kornia-0.6.9/kornia/geometry/quaternion.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,23 +1,24 @@
 # kornia.geometry.quaternion module inspired by Eigen, Sophus-sympy, and PyQuaternion.
 # https://github.com/strasdat/Sophus/blob/master/sympy/sophus/quaternion.py
 # https://github.com/KieranWynn/pyquaternion/blob/master/pyquaternion/quaternion.py
 # https://gitlab.com/libeigen/eigen/-/blob/master/Eigen/src/Geometry/Quaternion.h
 from math import pi
-from typing import Tuple, Union
+from typing import Optional, Tuple, Union
 
-from kornia.core import Module, Parameter, Tensor, as_tensor, concatenate, rand, stack
+from kornia.core import Module, Parameter, Tensor, concatenate, rand, stack, tensor, where
 from kornia.geometry.conversions import (
     QuaternionCoeffOrder,
     angle_axis_to_quaternion,
     normalize_quaternion,
     quaternion_to_rotation_matrix,
     rotation_matrix_to_quaternion,
 )
-from kornia.testing import KORNIA_CHECK, KORNIA_CHECK_SHAPE, KORNIA_CHECK_TYPE
+from kornia.geometry.linalg import batched_dot_product
+from kornia.testing import KORNIA_CHECK_TYPE
 
 
 class Quaternion(Module):
     r"""Base class to represent a Quaternion.
 
     A quaternion is a four dimensional vector representation of a rotation transformation in 3d.
     See more: https://en.wikipedia.org/wiki/Quaternion
@@ -40,18 +41,15 @@
         >>> q.data
         Parameter containing:
         tensor([[1., 0., 0., 0.],
                 [1., 0., 0., 0.],
                 [1., 0., 0., 0.],
                 [1., 0., 0., 0.]], requires_grad=True)
         >>> q.real
-        tensor([[1.],
-                [1.],
-                [1.],
-                [1.]], grad_fn=<SliceBackward0>)
+        tensor([1., 1., 1., 1.], grad_fn=<SelectBackward0>)
         >>> q.vec
         tensor([[0., 0., 0.],
                 [0., 0., 0.],
                 [0., 0., 0.],
                 [0., 0., 0.]], grad_fn=<SliceBackward0>)
     """
 
@@ -64,197 +62,195 @@
         Example:
             >>> data = torch.rand(2, 4)
             >>> q = Quaternion(data)
             >>> q.shape
             (2, 4)
         """
         super().__init__()
-        KORNIA_CHECK_SHAPE(data, ["B", "4"])
+        # KORNIA_CHECK_SHAPE(data, ["B", "4"])  # FIXME: resolve shape bugs. @edgarriba
         self._data = Parameter(data)
 
     def __repr__(self) -> str:
-        return f"real: {self.real} \nvec: {self.vec}"
+        return f"{self.data}"
 
     def __getitem__(self, idx) -> 'Quaternion':
-        return Quaternion(self.data[idx].reshape(1, -1))
+        return Quaternion(self.data[idx])
 
     def __neg__(self) -> 'Quaternion':
         """Inverts the sign of the quaternion data.
 
         Example:
-            >>> q = Quaternion.identity(batch_size=1)
+            >>> q = Quaternion.identity()
             >>> -q.data
-            tensor([[-1., -0., -0., -0.]], grad_fn=<NegBackward0>)
+            tensor([-1., -0., -0., -0.], grad_fn=<NegBackward0>)
         """
         return Quaternion(-self.data)
 
     def __add__(self, right: 'Quaternion') -> 'Quaternion':
         """Add a given quaternion.
 
         Args:
             right: the quaternion to add.
 
         Example:
-            >>> q1 = Quaternion.identity(batch_size=1)
-            >>> q2 = Quaternion(Tensor([[2., 0., 1., 1.]]))
+            >>> q1 = Quaternion.identity()
+            >>> q2 = Quaternion(tensor([2., 0., 1., 1.]))
             >>> q3 = q1 + q2
             >>> q3.data
             Parameter containing:
-            tensor([[3., 0., 1., 1.]], requires_grad=True)
+            tensor([3., 0., 1., 1.], requires_grad=True)
         """
         KORNIA_CHECK_TYPE(right, Quaternion)
         return Quaternion(self.data + right.data)
 
     def __sub__(self, right: 'Quaternion') -> 'Quaternion':
         """Subtract a given quaternion.
 
         Args:
             right: the quaternion to subtract.
 
         Example:
-            >>> q1 = Quaternion(Tensor([[2., 0., 1., 1.]]))
-            >>> q2 = Quaternion.identity(batch_size=1)
+            >>> q1 = Quaternion(tensor([2., 0., 1., 1.]))
+            >>> q2 = Quaternion.identity()
             >>> q3 = q1 - q2
             >>> q3.data
             Parameter containing:
-            tensor([[1., 0., 1., 1.]], requires_grad=True)
+            tensor([1., 0., 1., 1.], requires_grad=True)
         """
         KORNIA_CHECK_TYPE(right, Quaternion)
         return Quaternion(self.data - right.data)
 
     def __mul__(self, right: 'Quaternion') -> 'Quaternion':
         KORNIA_CHECK_TYPE(right, Quaternion)
         # NOTE: borrowed from sophus sympy. Produce less multiplications compared to others.
         # https://github.com/strasdat/Sophus/blob/785fef35b7d9e0fc67b4964a69124277b7434a44/sympy/sophus/quaternion.py#L19
-        new_real = self.real * right.real - self._batched_squared_norm(self.vec, right.vec)
-        new_vec = self.real * right.vec + right.real * self.vec + self.vec.cross(right.vec)
-        return Quaternion(concatenate((new_real, new_vec), -1))
+        new_real = self.real * right.real - batched_dot_product(self.vec, right.vec)
+        new_vec = self.real[..., None] * right.vec + right.real[..., None] * self.vec + self.vec.cross(right.vec)
+        return Quaternion(concatenate((new_real[..., None], new_vec), -1))
 
     def __div__(self, right: Union[Tensor, 'Quaternion']) -> 'Quaternion':
         if isinstance(right, Tensor):
-            return Quaternion(self.data / right)
+            return Quaternion(self.data / right[..., None])
         KORNIA_CHECK_TYPE(right, Quaternion)
         return self * right.inv()
 
     def __truediv__(self, right: 'Quaternion') -> 'Quaternion':
         return self.__div__(right)
 
-    def __pow__(self, t: 'int') -> 'Quaternion':
+    def __pow__(self, t: Union[int, float]) -> 'Quaternion':
         """Return the power of a quaternion raised to exponent t.
 
         Args:
             t: raised exponent.
 
         Example:
-            >>> q = Quaternion(torch.tensor([1., .5, 0., 0.]))
+            >>> q = Quaternion(tensor([1., .5, 0., 0.]))
             >>> q_pow = q**2
         """
-        theta = self.polar_angle
-        n = self.vec / self.vec.norm(dim=-1, keepdim=True)
+        theta = self.polar_angle[..., None]
+        vec_norm = self.vec.norm(dim=-1, keepdim=True)
+        n = where(vec_norm != 0, self.vec / vec_norm, self.vec * 0)
         w = (t * theta).cos()
         xyz = (t * theta).sin() * n
         return Quaternion(concatenate((w, xyz), -1))
 
     @property
     def data(self) -> Tensor:
-        """Return the underlying data with shape :math:`(B,4).`"""
+        """Return the underlying data with shape :math:`(B, 4).`"""
         return self._data
 
     @property
-    def coeffs(self) -> Tensor:
-        """Return the underlying data with shape :math:`(B,4)`.
-
-        Alias for :func:`~kornia.geometry.quaternion.Quaternion.data`
-        """
-        return self._data
+    def coeffs(self) -> Tuple[Tensor, Tensor, Tensor, Tensor]:
+        """Return a tuple with the underlying coefficients in WXYZ order."""
+        return self.w, self.x, self.y, self.z
 
     @property
     def real(self) -> Tensor:
-        """Return the real part with shape :math:`(B,1)`.
+        """Return the real part with shape :math:`(B,)`.
 
         Alias for :func:`~kornia.geometry.quaternion.Quaternion.w`
         """
         return self.w
 
     @property
     def vec(self) -> Tensor:
-        """Return the vector with the imaginary part with shape :math:`(B,3)`."""
+        """Return the vector with the imaginary part with shape :math:`(B, 3)`."""
         return self.data[..., 1:]
 
     @property
     def q(self) -> Tensor:
-        """Return the underlying data with shape :math:`(B,4)`.
+        """Return the underlying data with shape :math:`(B, 4)`.
 
         Alias for :func:`~kornia.geometry.quaternion.Quaternion.data`
         """
         return self.data
 
     @property
     def scalar(self) -> Tensor:
-        """Return a scalar with the real with shape :math:`(B,1)`.
+        """Return a scalar with the real with shape :math:`(B,)`.
 
         Alias for :func:`~kornia.geometry.quaternion.Quaternion.w`
         """
         return self.real
 
     @property
     def w(self) -> Tensor:
-        """Return the :math:`q_w` with shape :math:`(B,1)`."""
-        return self.data[..., 0:1]
+        """Return the :math:`q_w` with shape :math:`(B,)`."""
+        return self.data[..., 0]
 
     @property
     def x(self) -> Tensor:
-        """Return the :math:`q_x` with shape :math:`(B,1)`."""
-        return self.data[..., 1:2]
+        """Return the :math:`q_x` with shape :math:`(B,)`."""
+        return self.data[..., 1]
 
     @property
     def y(self) -> Tensor:
-        """Return the :math:`q_y` with shape :math:`(B,1)`."""
-        return self.data[..., 2:3]
+        """Return the :math:`q_y` with shape :math:`(B,)`."""
+        return self.data[..., 2]
 
     @property
     def z(self) -> Tensor:
-        """Return the :math:`q_z` with shape :math:`(B,1)`."""
-        return self.data[..., 3:4]
+        """Return the :math:`q_z` with shape :math:`(B,)`."""
+        return self.data[..., 3]
 
     @property
     def shape(self) -> Tuple[int, ...]:
-        """Return the shape of the underlying data with shape :math:`(B,4)`."""
+        """Return the shape of the underlying data with shape :math:`(B, 4)`."""
         return tuple(self.data.shape)
 
     @property
     def polar_angle(self) -> Tensor:
         """Return the polar angle with shape :math:`(B,1)`.
 
         Example:
-            >>> q = Quaternion.identity(batch_size=1)
+            >>> q = Quaternion.identity()
             >>> q.polar_angle
-            tensor([[0.]], grad_fn=<AcosBackward0>)
+            tensor(0., grad_fn=<AcosBackward0>)
         """
         return (self.scalar / self.norm()).acos()
 
     def matrix(self) -> Tensor:
-        """Convert the quaternion to a rotation matrix of shape :math:`(B,3,3)`.
+        """Convert the quaternion to a rotation matrix of shape :math:`(B, 3, 3)`.
 
         Example:
-            >>> q = Quaternion.identity(batch_size=1)
+            >>> q = Quaternion.identity()
             >>> m = q.matrix()
             >>> m
-            tensor([[[1., 0., 0.],
-                     [0., 1., 0.],
-                     [0., 0., 1.]]], grad_fn=<ViewBackward0>)
+            tensor([[1., 0., 0.],
+                    [0., 1., 0.],
+                    [0., 0., 1.]], grad_fn=<SqueezeBackward1>)
         """
         return quaternion_to_rotation_matrix(self.data, order=QuaternionCoeffOrder.WXYZ)
 
     @classmethod
     def from_matrix(cls, matrix: Tensor) -> 'Quaternion':
         """Create a quaternion from a rotation matrix.
 
         Args:
-            matrix: the rotation matrix to convert of shape :math:`(B,3,3)`.
+            matrix: the rotation matrix to convert of shape :math:`(B, 3, 3)`.
 
         Example:
             >>> m = torch.eye(3)[None]
             >>> q = Quaternion.from_matrix(m)
             >>> q.data
             Parameter containing:
             tensor([[1., 0., 0., 0.]], requires_grad=True)
@@ -262,41 +258,41 @@
         return cls(rotation_matrix_to_quaternion(matrix, order=QuaternionCoeffOrder.WXYZ))
 
     @classmethod
     def from_axis_angle(cls, axis_angle: Tensor) -> 'Quaternion':
         """Create a quaternion from axis-angle representation.
 
         Args:
-            axis_angle: rotation vector of shape :math:`(B,3)`.
+            axis_angle: rotation vector of shape :math:`(B, 3)`.
 
         Example:
             >>> axis_angle = torch.tensor([[1., 0., 0.]])
             >>> q = Quaternion.from_axis_angle(axis_angle)
             >>> q.data
             Parameter containing:
             tensor([[0.8776, 0.4794, 0.0000, 0.0000]], requires_grad=True)
         """
         return cls(angle_axis_to_quaternion(axis_angle, order=QuaternionCoeffOrder.WXYZ))
 
     @classmethod
-    def identity(cls, batch_size: int) -> 'Quaternion':
+    def identity(cls, batch_size: Optional[int] = None, device=None, dtype=None) -> 'Quaternion':
         """Create a quaternion representing an identity rotation.
 
         Args:
             batch_size: the batch size of the underlying data.
 
         Example:
-            >>> q = Quaternion.identity(batch_size=2)
+            >>> q = Quaternion.identity()
             >>> q.data
             Parameter containing:
-            tensor([[1., 0., 0., 0.],
-                    [1., 0., 0., 0.]], requires_grad=True)
+            tensor([1., 0., 0., 0.], requires_grad=True)
         """
-        data: Tensor = as_tensor([1.0, 0.0, 0.0, 0.0])
-        data = data.repeat(batch_size, 1)
+        data: Tensor = tensor([1.0, 0.0, 0.0, 0.0], device=device, dtype=dtype)
+        if batch_size is not None:
+            data = data.repeat(batch_size, 1)
         return cls(data)
 
     @classmethod
     def from_coeffs(cls, w: float, x: float, y: float, z: float) -> 'Quaternion':
         """Create a quaternion from the data coefficients.
 
         Args:
@@ -305,34 +301,36 @@
             y: a float representing the :math:`q_y` component.
             z: a float representing the :math:`q_z` component.
 
         Example:
             >>> q = Quaternion.from_coeffs(1., 0., 0., 0.)
             >>> q.data
             Parameter containing:
-            tensor([[1., 0., 0., 0.]], requires_grad=True)
+            tensor([1., 0., 0., 0.], requires_grad=True)
         """
-        return cls(as_tensor([[w, x, y, z]]))
+        return cls(tensor([w, x, y, z]))
 
+    # TODO: update signature
+    # def random(cls, shape: Optional[List] = None, device = None, dtype = None) -> 'Quaternion':
     @classmethod
-    def random(cls, batch_size: int) -> 'Quaternion':
-        """Create a random unit quaternion of shape :math:`(B,4)`.
+    def random(cls, batch_size: Optional[int] = None, device=None, dtype=None) -> 'Quaternion':
+        """Create a random unit quaternion of shape :math:`(B, 4)`.
 
         Uniformly distributed across the rotation space as per: http://planning.cs.uiuc.edu/node198.html
 
         Args:
             batch_size: the batch size of the underlying data.
 
         Example:
+            >>> q = Quaternion.random()
             >>> q = Quaternion.random(batch_size=2)
-            >>> q.norm()
-            tensor([[1.],
-                    [1.]], grad_fn=<NormBackward1>)
         """
-        r1, r2, r3 = rand(3, batch_size)
+        rand_shape = (batch_size,) if batch_size is not None else ()
+
+        r1, r2, r3 = rand((3,) + rand_shape, device=device, dtype=dtype)
         q1 = (1.0 - r1).sqrt() * ((2 * pi * r2).sin())
         q2 = (1.0 - r1).sqrt() * ((2 * pi * r2).cos())
         q3 = r1.sqrt() * (2 * pi * r3).sin()
         q4 = r1.sqrt() * (2 * pi * r3).cos()
         return cls(stack((q1, q2, q3, q4), -1))
 
     def slerp(self, q1: 'Quaternion', t: float) -> 'Quaternion':
@@ -341,36 +339,36 @@
         See more: https://en.wikipedia.org/wiki/Slerp
 
         Args:
             q1: second quaternion to be interpolated between.
             t: interpolation ratio, range [0-1]
 
         Example:
-            >>> q0 = Quaternion.identity(batch_size=1)
-            >>> q1 = Quaternion(torch.tensor([[1., .5, 0., 0.]]))
+            >>> q0 = Quaternion.identity()
+            >>> q1 = Quaternion(torch.tensor([1., .5, 0., 0.]))
             >>> q2 = q0.slerp(q1, .3)
         """
         KORNIA_CHECK_TYPE(q1, Quaternion)
         q0 = self.normalize()
         q1 = q1.normalize()
         return q0 * (q0.inv() * q1) ** t
 
-    def norm(self) -> Tensor:
-        return self.data.norm(p=2, dim=-1, keepdim=True)
+    # TODO: add docs
+    def norm(self, keepdim: bool = False) -> Tensor:
+        # p==2, dim|axis==-1, keepdim
+        return self.data.norm(2, -1, keepdim)
 
+    # TODO: add docs
     def normalize(self) -> 'Quaternion':
         return Quaternion(normalize_quaternion(self.data))
 
+    # TODO: add docs
     def conj(self) -> 'Quaternion':
-        return Quaternion(concatenate((self.real, -self.vec), -1))
+        return Quaternion(concatenate((self.real[..., None], -self.vec), -1))
 
+    # TODO: add docs
     def inv(self) -> 'Quaternion':
         return self.conj() / self.squared_norm()
 
+    # TODO: add docs
     def squared_norm(self) -> Tensor:
-        return self._batched_squared_norm(self.vec) + self.real**2
-
-    def _batched_squared_norm(self, x, y=None):
-        if y is None:
-            y = x
-        KORNIA_CHECK(x.shape == y.shape)
-        return (x[..., None, :] @ y[..., :, None])[..., 0]
+        return batched_dot_product(self.vec, self.vec) + self.real**2
```

### Comparing `kornia-0.6.8/kornia/geometry/ransac.py` & `kornia-0.6.9/kornia/geometry/ransac.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,15 +1,14 @@
 """Module containing RANSAC modules."""
 import math
-from typing import Optional, Tuple
+from typing import Callable, Optional, Tuple
 
 import torch
-import torch.nn as nn
 
-from kornia.core import Tensor
+from kornia.core import Device, Module, Tensor, zeros
 from kornia.geometry import (
     find_fundamental,
     find_homography_dlt,
     find_homography_dlt_iterated,
     find_homography_lines_dlt,
     find_homography_lines_dlt_iterated,
     symmetrical_epipolar_distance,
@@ -20,15 +19,15 @@
     sample_is_valid_for_homography,
 )
 from kornia.testing import KORNIA_CHECK_SHAPE
 
 __all__ = ["RANSAC"]
 
 
-class RANSAC(nn.Module):
+class RANSAC(Module):
     """Module for robust geometry estimation with RANSAC. https://en.wikipedia.org/wiki/Random_sample_consensus.
 
     Args:
         model_type: type of model to estimate, e.g. "homography" or "fundamental".
         inliers_threshold: threshold for the correspondence to be an inlier.
         batch_size: number of generated samples at once.
         max_iterations: maximum batches to generate. Actual number of models to try is ``batch_size * max_iterations``.
@@ -51,37 +50,40 @@
         self.inl_th = inl_th
         self.max_iter = max_iter
         self.batch_size = batch_size
         self.model_type = model_type
         self.confidence = confidence
         self.max_lo_iters = max_lo_iters
         self.model_type = model_type
+
+        self.error_fn: Callable[..., Tensor]
+        self.minimal_solver: Callable[..., Tensor]
+        self.polisher_solver: Callable[..., Tensor]
+
         if model_type == 'homography':
-            self.error_fn = oneway_transfer_error  # type: ignore
-            self.minimal_solver = find_homography_dlt  # type: ignore
-            self.polisher_solver = find_homography_dlt_iterated  # type: ignore
+            self.error_fn = oneway_transfer_error
+            self.minimal_solver = find_homography_dlt
+            self.polisher_solver = find_homography_dlt_iterated
             self.minimal_sample_size = 4
         elif model_type == 'homography_from_linesegments':
-            self.error_fn = line_segment_transfer_error_one_way  # type: ignore
-            self.minimal_solver = find_homography_lines_dlt  # type: ignore
-            self.polisher_solver = find_homography_lines_dlt_iterated  # type: ignore
+            self.error_fn = line_segment_transfer_error_one_way
+            self.minimal_solver = find_homography_lines_dlt
+            self.polisher_solver = find_homography_lines_dlt_iterated
             self.minimal_sample_size = 4
         elif model_type == 'fundamental':
-            self.error_fn = symmetrical_epipolar_distance  # type: ignore
-            self.minimal_solver = find_fundamental  # type: ignore
+            self.error_fn = symmetrical_epipolar_distance
+            self.minimal_solver = find_fundamental
             self.minimal_sample_size = 8
             # ToDo: implement 7pt solver instead of 8pt minimal_solver
             # https://github.com/opencv/opencv/blob/master/modules/calib3d/src/fundam.cpp#L498
-            self.polisher_solver = find_fundamental  # type: ignore
+            self.polisher_solver = find_fundamental
         else:
             raise NotImplementedError(f"{model_type} is unknown. Try one of {self.supported_models}")
 
-    def sample(
-        self, sample_size: int, pop_size: int, batch_size: int, device: torch.device = torch.device('cpu')
-    ) -> Tensor:
+    def sample(self, sample_size: int, pop_size: int, batch_size: int, device: Device = torch.device('cpu')) -> Tensor:
         """Minimal sampler, but unlike traditional RANSAC we sample in batches to get benefit of the parallel
         processing, esp.
 
         on GPU.
         """
         rand = torch.rand(batch_size, pop_size, device=device)
         _, out = rand.topk(k=sample_size, dim=1)
@@ -175,16 +177,16 @@
         Returns:
             - Estimated model, shape of :math:`(1, 3, 3)`.
             - The inlier/outlier mask, shape of :math:`(1, N)`, where N is number of input correspondences.
         """
         self.validate_inputs(kp1, kp2, weights)
         best_score_total: float = float(self.minimal_sample_size)
         num_tc: int = len(kp1)
-        best_model_total = torch.zeros(3, 3, dtype=kp1.dtype, device=kp1.device)
-        inliers_best_total: Tensor = torch.zeros(num_tc, 1, device=kp1.device, dtype=torch.bool)
+        best_model_total = zeros(3, 3, dtype=kp1.dtype, device=kp1.device)
+        inliers_best_total: Tensor = zeros(num_tc, 1, device=kp1.device, dtype=torch.bool)
         for i in range(self.max_iter):
             # Sample minimal samples in batch to estimate models
             idxs = self.sample(self.minimal_sample_size, num_tc, self.batch_size, kp1.device)
             kp1_sampled = kp1[idxs]
             kp2_sampled = kp2[idxs]
 
             kp1_sampled, kp2_sampled = self.remove_bad_samples(kp1_sampled, kp2_sampled)
```

### Comparing `kornia-0.6.8/kornia/geometry/subpix/__init__.py` & `kornia-0.6.9/kornia/geometry/subpix/__init__.py`

 * *Files identical despite different names*

### Comparing `kornia-0.6.8/kornia/geometry/subpix/dsnt.py` & `kornia-0.6.9/kornia/geometry/subpix/dsnt.py`

 * *Files identical despite different names*

### Comparing `kornia-0.6.8/kornia/geometry/subpix/nms.py` & `kornia-0.6.9/kornia/geometry/subpix/nms.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,34 +1,36 @@
 from typing import Tuple
 
 import torch
-import torch.nn as nn
 import torch.nn.functional as F
 
+from kornia.core import Module, Tensor, eye, pad, zeros
 
-def _get_nms_kernel2d(kx: int, ky: int) -> torch.Tensor:
+
+def _get_nms_kernel2d(kx: int, ky: int) -> Tensor:
     """Utility function, which returns neigh2channels conv kernel."""
     numel: int = ky * kx
     center: int = numel // 2
-    weight = torch.eye(numel)
+    weight = eye(numel)
     weight[center, center] = 0
     return weight.view(numel, 1, ky, kx)
 
 
-def _get_nms_kernel3d(kd: int, ky: int, kx: int) -> torch.Tensor:
+def _get_nms_kernel3d(kd: int, ky: int, kx: int) -> Tensor:
     """Utility function, which returns neigh2channels conv kernel."""
     numel: int = kd * ky * kx
     center: int = numel // 2
-    weight = torch.eye(numel)
+    weight = eye(numel)
     weight[center, center] = 0
     return weight.view(numel, 1, kd, ky, kx)
 
 
-class NonMaximaSuppression2d(nn.Module):
+class NonMaximaSuppression2d(Module):
     r"""Apply non maxima suppression to filter."""
+    kernel: Tensor
 
     def __init__(self, kernel_size: Tuple[int, int]):
         super().__init__()
         self.kernel_size: Tuple[int, int] = kernel_size
         self.padding: Tuple[int, int, int, int] = self._compute_zero_padding2d(kernel_size)
         self.register_buffer('kernel', _get_nms_kernel2d(*kernel_size))
 
@@ -41,34 +43,34 @@
 
         def pad(x):
             return (x - 1) // 2  # zero padding function
 
         ky, kx = kernel_size  # we assume a cubic kernel
         return (pad(ky), pad(ky), pad(kx), pad(kx))
 
-    def forward(self, x: torch.Tensor, mask_only: bool = False) -> torch.Tensor:  # type: ignore
+    def forward(self, x: Tensor, mask_only: bool = False) -> Tensor:
         if len(x.shape) != 4:
             raise AssertionError(x.shape)
         B, CH, H, W = x.size()
         # find local maximum values
-        x_padded = F.pad(x, list(self.padding)[::-1], mode='replicate')
+        x_padded = pad(x, list(self.padding)[::-1], mode='replicate')
         B, CH, HP, WP = x_padded.size()
 
         max_non_center = (
-            F.conv2d(x_padded.view(B * CH, 1, HP, WP), self.kernel.to(x.device, x.dtype), stride=1)  # type: ignore
+            F.conv2d(x_padded.view(B * CH, 1, HP, WP), self.kernel.to(x.device, x.dtype), stride=1)
             .view(B, CH, -1, H, W)
             .max(dim=2)[0]
         )
         mask = x > max_non_center
         if mask_only:
             return mask
         return x * (mask.to(x.dtype))
 
 
-class NonMaximaSuppression3d(nn.Module):
+class NonMaximaSuppression3d(Module):
     r"""Apply non maxima suppression to filter."""
 
     def __init__(self, kernel_size: Tuple[int, int, int]):
         super().__init__()
         self.kernel_size: Tuple[int, int, int] = kernel_size
         self.padding: Tuple[int, int, int, int, int, int] = self._compute_zero_padding3d(kernel_size)
         self.kernel = _get_nms_kernel3d(*kernel_size)
@@ -82,21 +84,21 @@
 
         def pad(x):
             return (x - 1) // 2  # zero padding function
 
         kd, ky, kx = kernel_size  # we assume a cubic kernel
         return pad(kd), pad(kd), pad(ky), pad(ky), pad(kx), pad(kx)
 
-    def forward(self, x: torch.Tensor, mask_only: bool = False) -> torch.Tensor:  # type: ignore
+    def forward(self, x: Tensor, mask_only: bool = False) -> Tensor:
         if len(x.shape) != 5:
             raise AssertionError(x.shape)
         # find local maximum values
         B, CH, D, H, W = x.size()
         if self.kernel_size == (3, 3, 3):
-            mask = torch.zeros(B, CH, D, H, W, device=x.device, dtype=torch.bool)
+            mask = zeros(B, CH, D, H, W, device=x.device, dtype=torch.bool)
             center = slice(1, -1)
             left = slice(0, -2)
             right = slice(2, None)
             center_tensor = x[..., center, center, center]
             mask[..., 1:-1, 1:-1, 1:-1] = (
                 (center_tensor > x[..., center, center, left])
                 & (center_tensor > x[..., center, center, right])
@@ -124,15 +126,15 @@
                 & (center_tensor > x[..., right, right, center])
                 & (center_tensor > x[..., right, right, left])
                 & (center_tensor > x[..., right, right, right])
             )
         else:
             max_non_center = (
                 F.conv3d(
-                    F.pad(x, list(self.padding)[::-1], mode='replicate'),
+                    pad(x, list(self.padding)[::-1], mode='replicate'),
                     self.kernel.repeat(CH, 1, 1, 1, 1).to(x.device, x.dtype),
                     stride=1,
                     groups=CH,
                 )
                 .view(B, CH, -1, D, H, W)
                 .max(dim=2, keepdim=False)[0]
             )
@@ -141,21 +143,21 @@
             return mask
         return x * (mask.to(x.dtype))
 
 
 # functional api
 
 
-def nms2d(input: torch.Tensor, kernel_size: Tuple[int, int], mask_only: bool = False) -> torch.Tensor:
+def nms2d(input: Tensor, kernel_size: Tuple[int, int], mask_only: bool = False) -> Tensor:
     r"""Apply non maxima suppression to filter.
 
     See :class:`~kornia.geometry.subpix.NonMaximaSuppression2d` for details.
     """
     return NonMaximaSuppression2d(kernel_size)(input, mask_only)
 
 
-def nms3d(input: torch.Tensor, kernel_size: Tuple[int, int, int], mask_only: bool = False) -> torch.Tensor:
+def nms3d(input: Tensor, kernel_size: Tuple[int, int, int], mask_only: bool = False) -> Tensor:
     r"""Apply non maxima suppression to filter.
 
     See :class:`~kornia.feature.NonMaximaSuppression3d` for details.
     """
     return NonMaximaSuppression3d(kernel_size)(input, mask_only)
```

### Comparing `kornia-0.6.8/kornia/geometry/subpix/spatial_soft_argmax.py` & `kornia-0.6.9/kornia/geometry/subpix/spatial_soft_argmax.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,24 +1,24 @@
 from typing import Tuple, Union
 
 import torch
-import torch.nn as nn
 import torch.nn.functional as F
 
+from kornia.core import Module, Tensor, concatenate, stack, tensor, zeros
 from kornia.filters.sobel import spatial_gradient3d
 from kornia.geometry.conversions import normalize_pixel_coordinates, normalize_pixel_coordinates3d
 from kornia.utils import create_meshgrid, create_meshgrid3d
 from kornia.utils._compat import torch_version_geq
 from kornia.utils.helpers import safe_solve_with_mask
 
 from .dsnt import spatial_expectation2d, spatial_softmax2d
 from .nms import nms3d
 
 
-def _get_window_grid_kernel2d(h: int, w: int, device: torch.device = torch.device('cpu')) -> torch.Tensor:
+def _get_window_grid_kernel2d(h: int, w: int, device: torch.device = torch.device('cpu')) -> Tensor:
     r"""Helper function, which generates a kernel to with window coordinates, residual to window center.
 
     Args:
          h: kernel height.
          : kernel width.
          device: device, on which generate.
 
@@ -27,27 +27,27 @@
     """
     window_grid2d = create_meshgrid(h, w, False, device=device)
     window_grid2d = normalize_pixel_coordinates(window_grid2d, h, w)
     conv_kernel = window_grid2d.permute(3, 0, 1, 2)
     return conv_kernel
 
 
-def _get_center_kernel2d(h: int, w: int, device: torch.device = torch.device('cpu')) -> torch.Tensor:
+def _get_center_kernel2d(h: int, w: int, device: torch.device = torch.device('cpu')) -> Tensor:
     r"""Helper function, which generates a kernel to return center coordinates, when applied with F.conv2d to 2d
     coordinates grid.
 
     Args:
         h: kernel height.
         w: kernel width.
         device: device, on which generate.
 
     Returns:
         conv_kernel [2x2xhxw].
     """
-    center_kernel = torch.zeros(2, 2, h, w, device=device)
+    center_kernel = zeros(2, 2, h, w, device=device)
 
     #  If the size is odd, we have one pixel for center, if even - 2
     if h % 2 != 0:
         h_i1 = h // 2
         h_i2 = (h // 2) + 1
     else:
         h_i1 = (h // 2) - 1
@@ -58,28 +58,28 @@
     else:
         w_i1 = (w // 2) - 1
         w_i2 = (w // 2) + 1
     center_kernel[(0, 1), (0, 1), h_i1:h_i2, w_i1:w_i2] = 1.0 / float((h_i2 - h_i1) * (w_i2 - w_i1))
     return center_kernel
 
 
-def _get_center_kernel3d(d: int, h: int, w: int, device: torch.device = torch.device('cpu')) -> torch.Tensor:
+def _get_center_kernel3d(d: int, h: int, w: int, device: torch.device = torch.device('cpu')) -> Tensor:
     r"""Helper function, which generates a kernel to return center coordinates, when applied with F.conv2d to 3d
     coordinates grid.
 
     Args:
         d: kernel depth.
         h: kernel height.
         w: kernel width.
         device: device, on which generate.
 
     Returns:
         conv_kernel [3x3xdxhxw].
     """
-    center_kernel = torch.zeros(3, 3, d, h, w, device=device)
+    center_kernel = zeros(3, 3, d, h, w, device=device)
     #  If the size is odd, we have one pixel for center, if even - 2
     if h % 2 != 0:
         h_i1 = h // 2
         h_i2 = (h // 2) + 1
     else:
         h_i1 = (h // 2) - 1
         h_i2 = (h // 2) + 1
@@ -96,15 +96,15 @@
         d_i1 = (d // 2) - 1
         d_i2 = (d // 2) + 1
     center_num = float((h_i2 - h_i1) * (w_i2 - w_i1) * (d_i2 - d_i1))
     center_kernel[(0, 1, 2), (0, 1, 2), d_i1:d_i2, h_i1:h_i2, w_i1:w_i2] = 1.0 / center_num
     return center_kernel
 
 
-def _get_window_grid_kernel3d(d: int, h: int, w: int, device: torch.device = torch.device('cpu')) -> torch.Tensor:
+def _get_window_grid_kernel3d(d: int, h: int, w: int, device: torch.device = torch.device('cpu')) -> Tensor:
     r"""Helper function, which generates a kernel to return coordinates, residual to window center.
 
     Args:
         d: kernel depth.
         h: kernel height.
         w: kernel width.
         device: device, on which generate.
@@ -112,32 +112,32 @@
     Returns:
         conv_kernel [3x1xdxhxw]
     """
     grid2d = create_meshgrid(h, w, True, device=device)
     if d > 1:
         z = torch.linspace(-1, 1, d, device=device).view(d, 1, 1, 1)
     else:  # only onr channel with index == 0
-        z = torch.zeros(1, 1, 1, 1, device=device)
-    grid3d = torch.cat([z.repeat(1, h, w, 1).contiguous(), grid2d.repeat(d, 1, 1, 1)], dim=3)
+        z = zeros(1, 1, 1, 1, device=device)
+    grid3d = concatenate([z.repeat(1, h, w, 1).contiguous(), grid2d.repeat(d, 1, 1, 1)], 3)
     conv_kernel = grid3d.permute(3, 0, 1, 2).unsqueeze(1)
     return conv_kernel
 
 
-class ConvSoftArgmax2d(nn.Module):
+class ConvSoftArgmax2d(Module):
     r"""Module that calculates soft argmax 2d per window.
 
     See :func:`~kornia.geometry.subpix.conv_soft_argmax2d` for details.
     """
 
     def __init__(
         self,
         kernel_size: Tuple[int, int] = (3, 3),
         stride: Tuple[int, int] = (1, 1),
         padding: Tuple[int, int] = (1, 1),
-        temperature: Union[torch.Tensor, float] = torch.tensor(1.0),
+        temperature: Union[Tensor, float] = tensor(1.0),
         normalized_coordinates: bool = True,
         eps: float = 1e-8,
         output_value: bool = False,
     ) -> None:
         super().__init__()
         self.kernel_size = kernel_size
         self.stride = stride
@@ -170,39 +170,39 @@
             + str(self.eps)
             + ', '
             + 'output_value='
             + str(self.output_value)
             + ')'
         )
 
-    def forward(self, x: torch.Tensor):  # type: ignore
+    def forward(self, x: Tensor):
         return conv_soft_argmax2d(
             x,
             self.kernel_size,
             self.stride,
             self.padding,
             self.temperature,
             self.normalized_coordinates,
             self.eps,
             self.output_value,
         )
 
 
-class ConvSoftArgmax3d(nn.Module):
+class ConvSoftArgmax3d(Module):
     r"""Module that calculates soft argmax 3d per window.
 
     See :func:`~kornia.geometry.subpix.conv_soft_argmax3d` for details.
     """
 
     def __init__(
         self,
         kernel_size: Tuple[int, int, int] = (3, 3, 3),
         stride: Tuple[int, int, int] = (1, 1, 1),
         padding: Tuple[int, int, int] = (1, 1, 1),
-        temperature: Union[torch.Tensor, float] = torch.tensor(1.0),
+        temperature: Union[Tensor, float] = tensor(1.0),
         normalized_coordinates: bool = False,
         eps: float = 1e-8,
         output_value: bool = True,
         strict_maxima_bonus: float = 0.0,
     ) -> None:
         super().__init__()
         self.kernel_size = kernel_size
@@ -241,38 +241,38 @@
             + str(self.strict_maxima_bonus)
             + ', '
             + 'output_value='
             + str(self.output_value)
             + ')'
         )
 
-    def forward(self, x: torch.Tensor):  # type: ignore
+    def forward(self, x: Tensor):
         return conv_soft_argmax3d(
             x,
             self.kernel_size,
             self.stride,
             self.padding,
             self.temperature,
             self.normalized_coordinates,
             self.eps,
             self.output_value,
             self.strict_maxima_bonus,
         )
 
 
 def conv_soft_argmax2d(
-    input: torch.Tensor,
+    input: Tensor,
     kernel_size: Tuple[int, int] = (3, 3),
     stride: Tuple[int, int] = (1, 1),
     padding: Tuple[int, int] = (1, 1),
-    temperature: Union[torch.Tensor, float] = torch.tensor(1.0),
+    temperature: Union[Tensor, float] = tensor(1.0),
     normalized_coordinates: bool = True,
     eps: float = 1e-8,
     output_value: bool = False,
-) -> Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:
+) -> Union[Tensor, Tuple[Tensor, Tensor]]:
     r"""Compute the convolutional spatial Soft-Argmax 2D over the windows of a given heatmap.
 
     .. math::
         ij(X) = \frac{\sum{(i,j)} * exp(x / T)  \in X} {\sum{exp(x / T)  \in X}}
 
     .. math::
         val(X) = \frac{\sum{x * exp(x / T)  \in X}} {\sum{exp(x / T)  \in X}}
@@ -306,30 +306,30 @@
                (\text{kernel\_size}[1] - 1) - 1}{\text{stride}[1]} + 1\right\rfloor
 
     Examples:
         >>> input = torch.randn(20, 16, 50, 32)
         >>> nms_coords, nms_val = conv_soft_argmax2d(input, (3,3), (2,2), (1,1), output_value=True)
     """
     if not torch.is_tensor(input):
-        raise TypeError(f"Input type is not a torch.Tensor. Got {type(input)}")
+        raise TypeError(f"Input type is not a Tensor. Got {type(input)}")
 
     if not len(input.shape) == 4:
         raise ValueError(f"Invalid input shape, we expect BxCxHxW. Got: {input.shape}")
 
     if temperature <= 0:
         raise ValueError(f"Temperature should be positive float or tensor. Got: {temperature}")
 
     b, c, h, w = input.shape
     kx, ky = kernel_size
     device: torch.device = input.device
     dtype: torch.dtype = input.dtype
     input = input.view(b * c, 1, h, w)
 
-    center_kernel: torch.Tensor = _get_center_kernel2d(kx, ky, device).to(dtype)
-    window_kernel: torch.Tensor = _get_window_grid_kernel2d(kx, ky, device).to(dtype)
+    center_kernel: Tensor = _get_center_kernel2d(kx, ky, device).to(dtype)
+    window_kernel: Tensor = _get_window_grid_kernel2d(kx, ky, device).to(dtype)
 
     # applies exponential normalization trick
     # https://timvieira.github.io/blog/post/2014/02/11/exp-normalize-trick/
     # https://github.com/pytorch/pytorch/blob/bcb0bb7e0e03b386ad837015faba6b4b16e3bfb9/aten/src/ATen/native/SoftMax.cpp#L44
     x_max = F.adaptive_max_pool2d(input, (1, 1))
 
     # max is detached to prevent undesired backprop loops in the graph
@@ -343,21 +343,21 @@
     den = pool_coef * F.avg_pool2d(x_exp, kernel_size, stride=stride, padding=padding) + eps
 
     x_softmaxpool = pool_coef * F.avg_pool2d(x_exp * input, kernel_size, stride=stride, padding=padding) / den
     x_softmaxpool = x_softmaxpool.view(b, c, x_softmaxpool.size(2), x_softmaxpool.size(3))
 
     # We need to output also coordinates
     # Pooled window center coordinates
-    grid_global: torch.Tensor = create_meshgrid(h, w, False, device).to(dtype).permute(0, 3, 1, 2)
+    grid_global: Tensor = create_meshgrid(h, w, False, device).to(dtype).permute(0, 3, 1, 2)
 
     grid_global_pooled = F.conv2d(grid_global, center_kernel, stride=stride, padding=padding)
 
     # Coordinates of maxima residual to window center
     # prepare kernel
-    coords_max: torch.Tensor = F.conv2d(x_exp, window_kernel, stride=stride, padding=padding)
+    coords_max: Tensor = F.conv2d(x_exp, window_kernel, stride=stride, padding=padding)
 
     coords_max = coords_max / den.expand_as(coords_max)
     coords_max = coords_max + grid_global_pooled.expand_as(coords_max)
     # [:,:, 0, ...] is x
     # [:,:, 1, ...] is y
 
     if normalized_coordinates:
@@ -369,24 +369,24 @@
 
     if output_value:
         return coords_max, x_softmaxpool
     return coords_max
 
 
 def conv_soft_argmax3d(
-    input: torch.Tensor,
+    input: Tensor,
     kernel_size: Tuple[int, int, int] = (3, 3, 3),
     stride: Tuple[int, int, int] = (1, 1, 1),
     padding: Tuple[int, int, int] = (1, 1, 1),
-    temperature: Union[torch.Tensor, float] = torch.tensor(1.0),
+    temperature: Union[Tensor, float] = tensor(1.0),
     normalized_coordinates: bool = False,
     eps: float = 1e-8,
     output_value: bool = True,
     strict_maxima_bonus: float = 0.0,
-) -> Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:
+) -> Union[Tensor, Tuple[Tensor, Tensor]]:
     r"""Compute the convolutional spatial Soft-Argmax 3D over the windows of a given heatmap.
 
     .. math::
              ijk(X) = \frac{\sum{(i,j,k)} * exp(x / T)  \in X} {\sum{exp(x / T)  \in X}}
 
     .. math::
              val(X) = \frac{\sum{x * exp(x / T)  \in X}} {\sum{exp(x / T)  \in X}}
@@ -426,30 +426,30 @@
              (\text{kernel\_size}[2] - 1) - 1}{\text{stride}[2]} + 1\right\rfloor
 
     Examples:
         >>> input = torch.randn(20, 16, 3, 50, 32)
         >>> nms_coords, nms_val = conv_soft_argmax3d(input, (3, 3, 3), (1, 2, 2), (0, 1, 1))
     """
     if not torch.is_tensor(input):
-        raise TypeError(f"Input type is not a torch.Tensor. Got {type(input)}")
+        raise TypeError(f"Input type is not a Tensor. Got {type(input)}")
 
     if not len(input.shape) == 5:
         raise ValueError(f"Invalid input shape, we expect BxCxDxHxW. Got: {input.shape}")
 
     if temperature <= 0:
         raise ValueError(f"Temperature should be positive float or tensor. Got: {temperature}")
 
     b, c, d, h, w = input.shape
     kx, ky, kz = kernel_size
     device: torch.device = input.device
     dtype: torch.dtype = input.dtype
     input = input.view(b * c, 1, d, h, w)
 
-    center_kernel: torch.Tensor = _get_center_kernel3d(kx, ky, kz, device).to(dtype)
-    window_kernel: torch.Tensor = _get_window_grid_kernel3d(kx, ky, kz, device).to(dtype)
+    center_kernel: Tensor = _get_center_kernel3d(kx, ky, kz, device).to(dtype)
+    window_kernel: Tensor = _get_window_grid_kernel3d(kx, ky, kz, device).to(dtype)
 
     # applies exponential normalization trick
     # https://timvieira.github.io/blog/post/2014/02/11/exp-normalize-trick/
     # https://github.com/pytorch/pytorch/blob/bcb0bb7e0e03b386ad837015faba6b4b16e3bfb9/aten/src/ATen/native/SoftMax.cpp#L44
     x_max = F.adaptive_max_pool3d(input, (1, 1, 1))
 
     # max is detached to prevent undesired backprop loops in the graph
@@ -458,21 +458,21 @@
     pool_coef: float = float(kx * ky * kz)
 
     # softmax denominator
     den = pool_coef * F.avg_pool3d(x_exp.view_as(input), kernel_size, stride=stride, padding=padding) + eps
 
     # We need to output also coordinates
     # Pooled window center coordinates
-    grid_global: torch.Tensor = create_meshgrid3d(d, h, w, False, device=device).to(dtype).permute(0, 4, 1, 2, 3)
+    grid_global: Tensor = create_meshgrid3d(d, h, w, False, device=device).to(dtype).permute(0, 4, 1, 2, 3)
 
     grid_global_pooled = F.conv3d(grid_global, center_kernel, stride=stride, padding=padding)
 
     # Coordinates of maxima residual to window center
     # prepare kernel
-    coords_max: torch.Tensor = F.conv3d(x_exp, window_kernel, stride=stride, padding=padding)
+    coords_max: Tensor = F.conv3d(x_exp, window_kernel, stride=stride, padding=padding)
 
     coords_max = coords_max / den.expand_as(coords_max)
     coords_max = coords_max + grid_global_pooled.expand_as(coords_max)
     # [:,:, 0, ...] is depth (scale)
     # [:,:, 1, ...] is x
     # [:,:, 2, ...] is y
 
@@ -489,24 +489,24 @@
     x_softmaxpool = (
         pool_coef * F.avg_pool3d(x_exp.view(input.size()) * input, kernel_size, stride=stride, padding=padding) / den
     )
     if strict_maxima_bonus > 0:
         in_levels: int = input.size(2)
         out_levels: int = x_softmaxpool.size(2)
         skip_levels: int = (in_levels - out_levels) // 2
-        strict_maxima: torch.Tensor = F.avg_pool3d(nms3d(input, kernel_size), 1, stride, 0)
+        strict_maxima: Tensor = F.avg_pool3d(nms3d(input, kernel_size), 1, stride, 0)
         strict_maxima = strict_maxima[:, :, skip_levels : out_levels - skip_levels]
         x_softmaxpool *= 1.0 + strict_maxima_bonus * strict_maxima
     x_softmaxpool = x_softmaxpool.view(b, c, x_softmaxpool.size(2), x_softmaxpool.size(3), x_softmaxpool.size(4))
     return coords_max, x_softmaxpool
 
 
 def spatial_soft_argmax2d(
-    input: torch.Tensor, temperature: torch.Tensor = torch.tensor(1.0), normalized_coordinates: bool = True
-) -> torch.Tensor:
+    input: Tensor, temperature: Tensor = tensor(1.0), normalized_coordinates: bool = True
+) -> Tensor:
     r"""Compute the Spatial Soft-Argmax 2D of a given input heatmap.
 
     Args:
         input: the given heatmap with shape :math:`(B, N, H, W)`.
         temperature: factor to apply to input.
         normalized_coordinates: whether to return the coordinates normalized in the range of :math:`[-1, 1]`.
             Otherwise, it will return the coordinates in the range of the input shape.
@@ -519,48 +519,46 @@
         >>> input = torch.tensor([[[
         ... [0., 0., 0.],
         ... [0., 10., 0.],
         ... [0., 0., 0.]]]])
         >>> spatial_soft_argmax2d(input, normalized_coordinates=False)
         tensor([[[1.0000, 1.0000]]])
     """
-    input_soft: torch.Tensor = spatial_softmax2d(input, temperature)
-    output: torch.Tensor = spatial_expectation2d(input_soft, normalized_coordinates)
+    input_soft: Tensor = spatial_softmax2d(input, temperature)
+    output: Tensor = spatial_expectation2d(input_soft, normalized_coordinates)
     return output
 
 
-class SpatialSoftArgmax2d(nn.Module):
+class SpatialSoftArgmax2d(Module):
     r"""Compute the Spatial Soft-Argmax 2D of a given heatmap.
 
     See :func:`~kornia.geometry.subpix.spatial_soft_argmax2d` for details.
     """
 
-    def __init__(self, temperature: torch.Tensor = torch.tensor(1.0), normalized_coordinates: bool = True) -> None:
+    def __init__(self, temperature: Tensor = tensor(1.0), normalized_coordinates: bool = True) -> None:
         super().__init__()
-        self.temperature: torch.Tensor = temperature
+        self.temperature: Tensor = temperature
         self.normalized_coordinates: bool = normalized_coordinates
 
     def __repr__(self) -> str:
         return (
             self.__class__.__name__
             + '(temperature='
             + str(self.temperature)
             + ', '
             + 'normalized_coordinates='
             + str(self.normalized_coordinates)
             + ')'
         )
 
-    def forward(self, input: torch.Tensor) -> torch.Tensor:
+    def forward(self, input: Tensor) -> Tensor:
         return spatial_soft_argmax2d(input, self.temperature, self.normalized_coordinates)
 
 
-def conv_quad_interp3d(
-    input: torch.Tensor, strict_maxima_bonus: float = 10.0, eps: float = 1e-7
-) -> Tuple[torch.Tensor, torch.Tensor]:
+def conv_quad_interp3d(input: Tensor, strict_maxima_bonus: float = 10.0, eps: float = 1e-7) -> Tuple[Tensor, Tensor]:
     r"""Compute the single iteration of quadratic interpolation of the extremum (max or min).
 
     Args:
         input: the given heatmap with shape :math:`(N, C, D_{in}, H_{in}, W_{in})`.
         strict_maxima_bonus: pixels, which are strict maxima will score (1 + strict_maxima_bonus) * value.
           This is needed for mimic behavior of strict NMS in classic local features
         eps: parameter to control the hessian matrix ill-condition number.
@@ -584,76 +582,76 @@
              (\text{kernel\_size}[2] - 1) - 1}{\text{stride}[2]} + 1\right\rfloor
 
     Examples:
         >>> input = torch.randn(20, 16, 3, 50, 32)
         >>> nms_coords, nms_val = conv_quad_interp3d(input, 1.0)
     """
     if not torch.is_tensor(input):
-        raise TypeError(f"Input type is not a torch.Tensor. Got {type(input)}")
+        raise TypeError(f"Input type is not a Tensor. Got {type(input)}")
 
     if not len(input.shape) == 5:
         raise ValueError(f"Invalid input shape, we expect BxCxDxHxW. Got: {input.shape}")
 
     B, CH, D, H, W = input.shape
-    grid_global: torch.Tensor = create_meshgrid3d(D, H, W, False, device=input.device).permute(0, 4, 1, 2, 3)
+    grid_global: Tensor = create_meshgrid3d(D, H, W, False, device=input.device).permute(0, 4, 1, 2, 3)
     grid_global = grid_global.to(input.dtype)
 
     # to determine the location we are solving system of linear equations Ax = b, where b is 1st order gradient
     # and A is Hessian matrix
-    b: torch.Tensor = spatial_gradient3d(input, order=1, mode='diff')  #
+    b: Tensor = spatial_gradient3d(input, order=1, mode='diff')  #
     b = b.permute(0, 1, 3, 4, 5, 2).reshape(-1, 3, 1)
-    A: torch.Tensor = spatial_gradient3d(input, order=2, mode='diff')
+    A: Tensor = spatial_gradient3d(input, order=2, mode='diff')
     A = A.permute(0, 1, 3, 4, 5, 2).reshape(-1, 6)
     dxx = A[..., 0]
     dyy = A[..., 1]
     dss = A[..., 2]
     dxy = 0.25 * A[..., 3]  # normalization to match OpenCV implementation
     dys = 0.25 * A[..., 4]  # normalization to match OpenCV implementation
     dxs = 0.25 * A[..., 5]  # normalization to match OpenCV implementation
 
-    Hes = torch.stack([dxx, dxy, dxs, dxy, dyy, dys, dxs, dys, dss], dim=-1).view(-1, 3, 3)
+    Hes = stack([dxx, dxy, dxs, dxy, dyy, dys, dxs, dys, dss], -1).view(-1, 3, 3)
     if not torch_version_geq(1, 10):
         # The following is needed to avoid singular cases
         Hes += torch.rand(Hes[0].size(), device=Hes.device).abs()[None] * eps
 
-    nms_mask: torch.Tensor = nms3d(input, (3, 3, 3), True)
-    x_solved: torch.Tensor = torch.zeros_like(b)
+    nms_mask: Tensor = nms3d(input, (3, 3, 3), True)
+    x_solved: Tensor = torch.zeros_like(b)
     x_solved_masked, _, solved_correctly = safe_solve_with_mask(b[nms_mask.view(-1)], Hes[nms_mask.view(-1)])
 
     #  Kill those points, where we cannot solve
     new_nms_mask = nms_mask.masked_scatter(nms_mask, solved_correctly)
 
     x_solved.masked_scatter_(new_nms_mask.view(-1, 1, 1), x_solved_masked[solved_correctly])
-    dx: torch.Tensor = -x_solved
+    dx: Tensor = -x_solved
 
     # Ignore ones, which are far from window center
     mask1 = dx.abs().max(dim=1, keepdim=True)[0] > 0.7
     dx.masked_fill_(mask1.expand_as(dx), 0)
-    dy: torch.Tensor = 0.5 * torch.bmm(b.permute(0, 2, 1), dx)
+    dy: Tensor = 0.5 * torch.bmm(b.permute(0, 2, 1), dx)
     y_max = input + dy.view(B, CH, D, H, W)
     if strict_maxima_bonus > 0:
         y_max += strict_maxima_bonus * new_nms_mask.to(input.dtype)
 
-    dx_res: torch.Tensor = dx.flip(1).reshape(B, CH, D, H, W, 3).permute(0, 1, 5, 2, 3, 4)
-    coords_max: torch.Tensor = grid_global.repeat(B, 1, 1, 1, 1).unsqueeze(1)
+    dx_res: Tensor = dx.flip(1).reshape(B, CH, D, H, W, 3).permute(0, 1, 5, 2, 3, 4)
+    coords_max: Tensor = grid_global.repeat(B, 1, 1, 1, 1).unsqueeze(1)
     coords_max = coords_max + dx_res
 
     return coords_max, y_max
 
 
-class ConvQuadInterp3d(nn.Module):
+class ConvQuadInterp3d(Module):
     r"""Calculate soft argmax 3d per window.
 
     See :func:`~kornia.geometry.subpix.conv_quad_interp3d` for details.
     """
 
     def __init__(self, strict_maxima_bonus: float = 10.0, eps: float = 1e-7) -> None:
         super().__init__()
         self.strict_maxima_bonus = strict_maxima_bonus
         self.eps = eps
         return
 
     def __repr__(self) -> str:
         return self.__class__.__name__ + '(' + 'strict_maxima_bonus=' + str(self.strict_maxima_bonus) + ')'
 
-    def forward(self, x: torch.Tensor):  # type: ignore
+    def forward(self, x: Tensor):
         return conv_quad_interp3d(x, self.strict_maxima_bonus, self.eps)
```

### Comparing `kornia-0.6.8/kornia/geometry/transform/affwarp.py` & `kornia-0.6.9/kornia/geometry/transform/affwarp.py`

 * *Files 1% similar despite different names*

```diff
@@ -2,14 +2,15 @@
 
 import torch
 import torch.nn as nn
 
 from kornia.filters import gaussian_blur2d
 from kornia.utils import _extract_device_dtype
 from kornia.utils.image import perform_keep_shape_image
+from kornia.utils.misc import eye_like
 
 from .imgwarp import get_affine_matrix2d, get_projective_transform, get_rotation_matrix2d, warp_affine, warp_affine3d
 
 __all__ = [
     "affine",
     "affine3d",
     "scale",
@@ -82,16 +83,15 @@
     scales: torch.Tensor = torch.ones_like(yaw)
     matrix: torch.Tensor = get_projective_transform(center, angles, scales)
     return matrix
 
 
 def _compute_translation_matrix(translation: torch.Tensor) -> torch.Tensor:
     """Compute affine matrix for translation."""
-    matrix: torch.Tensor = torch.eye(3, device=translation.device, dtype=translation.dtype)
-    matrix = matrix.repeat(translation.shape[0], 1, 1)
+    matrix: torch.Tensor = eye_like(3, translation, shared_memory=False)
 
     dx, dy = torch.chunk(translation, chunks=2, dim=-1)
     matrix[..., 0, 2:3] += dx
     matrix[..., 1, 2:3] += dy
     return matrix
 
 
@@ -100,16 +100,15 @@
     angle: torch.Tensor = torch.zeros(scale.shape[:1], device=scale.device, dtype=scale.dtype)
     matrix: torch.Tensor = get_rotation_matrix2d(center, angle, scale)
     return matrix
 
 
 def _compute_shear_matrix(shear: torch.Tensor) -> torch.Tensor:
     """Compute affine matrix for shearing."""
-    matrix: torch.Tensor = torch.eye(3, device=shear.device, dtype=shear.dtype)
-    matrix = matrix.repeat(shear.shape[0], 1, 1)
+    matrix: torch.Tensor = eye_like(3, shear, shared_memory=False)
 
     shx, shy = torch.chunk(shear, chunks=2, dim=-1)
     matrix[..., 0, 1:2] += shx
     matrix[..., 1, 0:1] += shy
     return matrix
```

### Comparing `kornia-0.6.8/kornia/geometry/transform/crop2d.py` & `kornia-0.6.9/kornia/geometry/transform/crop2d.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,46 +1,46 @@
 from typing import Optional, Tuple
 
 import torch
-import torch.nn.functional as F
 
+from kornia.core import Tensor, as_tensor, pad, tensor
 from kornia.geometry.bbox import infer_bbox_shape, validate_bbox
 
 from .affwarp import resize
 from .imgwarp import get_perspective_transform, warp_affine
 
 __all__ = ["crop_and_resize", "crop_by_boxes", "crop_by_transform_mat", "crop_by_indices", "center_crop"]
 
 
 def crop_and_resize(
-    tensor: torch.Tensor,
-    boxes: torch.Tensor,
+    input_tensor: Tensor,
+    boxes: Tensor,
     size: Tuple[int, int],
     mode: str = 'bilinear',
     padding_mode: str = 'zeros',
     align_corners: bool = True,
-) -> torch.Tensor:
+) -> Tensor:
     r"""Extract crops from 2D images (4D tensor) and resize given a bounding box.
 
     Args:
-        tensor: the 2D image tensor with shape (B, C, H, W).
+        input_tensor: the 2D image tensor with shape (B, C, H, W).
         boxes : a tensor containing the coordinates of the bounding boxes to be extracted.
             The tensor must have the shape of Bx4x2, where each box is defined in the following (clockwise)
             order: top-left, top-right, bottom-right and bottom-left. The coordinates must be in the x, y order.
             The coordinates would compose a rectangle with a shape of (N1, N2).
         size: a tuple with the height and width that will be
             used to resize the extracted patches.
         mode: interpolation mode to calculate output values
           ``'bilinear'`` | ``'nearest'``.
         padding_mode: padding mode for outside grid values
           ``'zeros'`` | ``'border'`` | 'reflection'.
         align_corners: mode for grid_generation.
 
     Returns:
-        torch.Tensor: tensor containing the patches with shape BxCxN1xN2.
+        Tensor: tensor containing the patches with shape BxCxN1xN2.
 
     Example:
         >>> input = torch.tensor([[[
         ...     [1., 2., 3., 4.],
         ...     [5., 6., 7., 8.],
         ...     [9., 10., 11., 12.],
         ...     [13., 14., 15., 16.],
@@ -51,53 +51,55 @@
         ...     [2., 2.],
         ...     [1., 2.],
         ... ]])  # 1x4x2
         >>> crop_and_resize(input, boxes, (2, 2), mode='nearest', align_corners=True)
         tensor([[[[ 6.,  7.],
                   [10., 11.]]]])
     """
-    if not isinstance(tensor, torch.Tensor):
-        raise TypeError(f"Input tensor type is not a torch.Tensor. Got {type(tensor)}")
+    if not isinstance(input_tensor, Tensor):
+        raise TypeError(f"Input tensor type is not a Tensor. Got {type(input_tensor)}")
 
-    if not isinstance(boxes, torch.Tensor):
-        raise TypeError(f"Input boxes type is not a torch.Tensor. Got {type(boxes)}")
+    if not isinstance(boxes, Tensor):
+        raise TypeError(f"Input boxes type is not a Tensor. Got {type(boxes)}")
 
     if not isinstance(size, (tuple, list)) and len(size) == 2:
         raise ValueError(f"Input size must be a tuple/list of length 2. Got {size}")
 
-    if len(tensor.shape) != 4:
-        raise AssertionError(f"Only tensor with shape (B, C, H, W) supported. Got {tensor.shape}.")
+    if len(input_tensor.shape) != 4:
+        raise AssertionError(f"Only tensor with shape (B, C, H, W) supported. Got {input_tensor.shape}.")
 
     # unpack input data
     dst_h, dst_w = size
 
     # [x, y] origin
     # top-left, top-right, bottom-right, bottom-left
-    points_src: torch.Tensor = boxes.to(tensor)
+    points_src = boxes.to(input_tensor)
 
     # [x, y] destination
     # top-left, top-right, bottom-right, bottom-left
-    points_dst: torch.Tensor = torch.tensor(
-        [[[0, 0], [dst_w - 1, 0], [dst_w - 1, dst_h - 1], [0, dst_h - 1]]], device=tensor.device, dtype=tensor.dtype
+    points_dst = tensor(
+        [[[0, 0], [dst_w - 1, 0], [dst_w - 1, dst_h - 1], [0, dst_h - 1]]],
+        device=input_tensor.device,
+        dtype=input_tensor.dtype,
     ).expand(points_src.shape[0], -1, -1)
 
-    return crop_by_boxes(tensor, points_src, points_dst, mode, padding_mode, align_corners)
+    return crop_by_boxes(input_tensor, points_src, points_dst, mode, padding_mode, align_corners)
 
 
 def center_crop(
-    tensor: torch.Tensor,
+    input_tensor: Tensor,
     size: Tuple[int, int],
     mode: str = 'bilinear',
     padding_mode: str = 'zeros',
     align_corners: bool = True,
-) -> torch.Tensor:
+) -> Tensor:
     r"""Crop the 2D images (4D tensor) from the center.
 
     Args:
-        tensor: the 2D image tensor with shape (B, C, H, W).
+        input_tensor: the 2D image tensor with shape (B, C, H, W).
         size: a tuple with the expected height and width
           of the output patch.
         mode: interpolation mode to calculate output values
           ``'bilinear'`` | ``'nearest'``.
         padding_mode: padding mode for outside grid values
           ``'zeros'`` | ``'border'`` | ``'reflection'``.
         align_corners: mode for grid_generation.
@@ -112,26 +114,26 @@
         ...     [9., 10., 11., 12.],
         ...     [13., 14., 15., 16.],
         ...  ]]])
         >>> center_crop(input, (2, 4), mode='nearest', align_corners=True)
         tensor([[[[ 5.,  6.,  7.,  8.],
                   [ 9., 10., 11., 12.]]]])
     """
-    if not isinstance(tensor, torch.Tensor):
-        raise TypeError(f"Input tensor type is not a torch.Tensor. Got {type(tensor)}")
+    if not isinstance(input_tensor, Tensor):
+        raise TypeError(f"Input tensor type is not a Tensor. Got {type(input_tensor)}")
 
     if not isinstance(size, (tuple, list)) and len(size) == 2:
         raise ValueError(f"Input size must be a tuple/list of length 2. Got {size}")
 
-    if len(tensor.shape) != 4:
-        raise AssertionError(f"Only tensor with shape (B, C, H, W) supported. Got {tensor.shape}.")
+    if len(input_tensor.shape) != 4:
+        raise AssertionError(f"Only tensor with shape (B, C, H, W) supported. Got {input_tensor.shape}.")
 
     # unpack input sizes
     dst_h, dst_w = size
-    src_h, src_w = tensor.shape[-2:]
+    src_h, src_w = input_tensor.shape[-2:]
 
     # compute start/end offsets
     dst_h_half: float = dst_h / 2
     dst_w_half: float = dst_w / 2
     src_h_half: float = src_h / 2
     src_w_half: float = src_w / 2
 
@@ -139,62 +141,64 @@
     start_y: float = src_h_half - dst_h_half
 
     end_x: float = start_x + dst_w - 1
     end_y: float = start_y + dst_h - 1
 
     # [y, x] origin
     # top-left, top-right, bottom-right, bottom-left
-    points_src: torch.Tensor = torch.tensor(
+    points_src: Tensor = tensor(
         [[[start_x, start_y], [end_x, start_y], [end_x, end_y], [start_x, end_y]]],
-        device=tensor.device,
-        dtype=tensor.dtype,
+        device=input_tensor.device,
+        dtype=input_tensor.dtype,
     )
 
     # [y, x] destination
     # top-left, top-right, bottom-right, bottom-left
-    points_dst: torch.Tensor = torch.tensor(
-        [[[0, 0], [dst_w - 1, 0], [dst_w - 1, dst_h - 1], [0, dst_h - 1]]], device=tensor.device, dtype=tensor.dtype
+    points_dst: Tensor = tensor(
+        [[[0, 0], [dst_w - 1, 0], [dst_w - 1, dst_h - 1], [0, dst_h - 1]]],
+        device=input_tensor.device,
+        dtype=input_tensor.dtype,
     ).expand(points_src.shape[0], -1, -1)
 
-    return crop_by_boxes(tensor, points_src, points_dst, mode, padding_mode, align_corners)
+    return crop_by_boxes(input_tensor, points_src, points_dst, mode, padding_mode, align_corners)
 
 
 def crop_by_boxes(
-    tensor: torch.Tensor,
-    src_box: torch.Tensor,
-    dst_box: torch.Tensor,
+    input_tensor: Tensor,
+    src_box: Tensor,
+    dst_box: Tensor,
     mode: str = 'bilinear',
     padding_mode: str = 'zeros',
     align_corners: bool = True,
     validate_boxes: bool = True,
-) -> torch.Tensor:
+) -> Tensor:
     """Perform crop transform on 2D images (4D tensor) given two bounding boxes.
 
     Given an input tensor, this function selected the interested areas by the provided bounding boxes (src_box).
     Then the selected areas would be fitted into the targeted bounding boxes (dst_box) by a perspective transformation.
     So far, the ragged tensor is not supported by PyTorch right now. This function hereby requires the bounding boxes
     in a batch must be rectangles with same width and height.
 
     Args:
-        tensor: the 2D image tensor with shape (B, C, H, W).
+        input_tensor: the 2D image tensor with shape (B, C, H, W).
         src_box: a tensor with shape (B, 4, 2) containing the coordinates of the bounding boxes
             to be extracted. The tensor must have the shape of Bx4x2, where each box is defined in the clockwise
             order: top-left, top-right, bottom-right and bottom-left. The coordinates must be in x, y order.
         dst_box: a tensor with shape (B, 4, 2) containing the coordinates of the bounding boxes
             to be placed. The tensor must have the shape of Bx4x2, where each box is defined in the clockwise
             order: top-left, top-right, bottom-right and bottom-left. The coordinates must be in x, y order.
         mode: interpolation mode to calculate output values
           ``'bilinear'`` | ``'nearest'``.
         padding_mode: padding mode for outside grid values
           ``'zeros'`` | ``'border'`` | ``'reflection'``.
         align_corners: mode for grid_generation.
         validate_boxes: flag to perform validation on boxes.
 
     Returns:
-        torch.Tensor: the output tensor with patches.
+        Tensor: the output tensor with patches.
 
     Examples:
         >>> input = torch.arange(16, dtype=torch.float32).reshape((1, 1, 4, 4))
         >>> src_box = torch.tensor([[
         ...     [1., 1.],
         ...     [2., 1.],
         ...     [2., 2.],
@@ -214,78 +218,85 @@
         If the src_box is smaller than dst_box, the following error will be thrown.
         RuntimeError: solve_cpu: For batch 0: U(2,2) is zero, singular U.
     """
     if validate_boxes:
         validate_bbox(src_box)
         validate_bbox(dst_box)
 
-    if len(tensor.shape) != 4:
-        raise AssertionError(f"Only tensor with shape (B, C, H, W) supported. Got {tensor.shape}.")
+    if len(input_tensor.shape) != 4:
+        raise AssertionError(f"Only tensor with shape (B, C, H, W) supported. Got {input_tensor.shape}.")
 
     # compute transformation between points and warp
     # Note: Tensor.dtype must be float. "solve_cpu" not implemented for 'Long'
-    dst_trans_src: torch.Tensor = get_perspective_transform(src_box.to(tensor), dst_box.to(tensor))
+    dst_trans_src: Tensor = get_perspective_transform(src_box.to(input_tensor), dst_box.to(input_tensor))
 
-    bbox: Tuple[torch.Tensor, torch.Tensor] = infer_bbox_shape(dst_box)
+    bbox: Tuple[Tensor, Tensor] = infer_bbox_shape(dst_box)
     if not ((bbox[0] == bbox[0][0]).all() and (bbox[1] == bbox[1][0]).all()):
         raise AssertionError(
             f"Cropping height, width and depth must be exact same in a batch. "
             f"Got height {bbox[0]} and width {bbox[1]}."
         )
 
     h_out: int = int(bbox[0][0].item())
     w_out: int = int(bbox[1][0].item())
 
     return crop_by_transform_mat(
-        tensor, dst_trans_src, (h_out, w_out), mode=mode, padding_mode=padding_mode, align_corners=align_corners
+        input_tensor, dst_trans_src, (h_out, w_out), mode=mode, padding_mode=padding_mode, align_corners=align_corners
     )
 
 
 def crop_by_transform_mat(
-    tensor: torch.Tensor,
-    transform: torch.Tensor,
+    input_tensor: Tensor,
+    transform: Tensor,
     out_size: Tuple[int, int],
     mode: str = 'bilinear',
     padding_mode: str = 'zeros',
     align_corners: bool = True,
-) -> torch.Tensor:
+) -> Tensor:
     """Perform crop transform on 2D images (4D tensor) given a perspective transformation matrix.
 
     Args:
-        tensor: the 2D image tensor with shape (B, C, H, W).
+        input_tensor: the 2D image tensor with shape (B, C, H, W).
         transform: a perspective transformation matrix with shape (B, 3, 3).
         out_size: size of the output image (height, width).
         mode: interpolation mode to calculate output values
           ``'bilinear'`` | ``'nearest'``.
         padding_mode (str): padding mode for outside grid values
           ``'zeros'`` | ``'border'`` | ``'reflection'``.
         align_corners: mode for grid_generation.
 
     Returns:
         the output tensor with patches.
     """
     # simulate broadcasting
-    dst_trans_src = torch.as_tensor(transform.expand(tensor.shape[0], -1, -1), device=tensor.device, dtype=tensor.dtype)
+    dst_trans_src = as_tensor(
+        transform.expand(input_tensor.shape[0], -1, -1), device=input_tensor.device, dtype=input_tensor.dtype
+    )
 
-    patches: torch.Tensor = warp_affine(
-        tensor, dst_trans_src[:, :2, :], out_size, mode=mode, padding_mode=padding_mode, align_corners=align_corners
+    patches: Tensor = warp_affine(
+        input_tensor,
+        dst_trans_src[:, :2, :],
+        out_size,
+        mode=mode,
+        padding_mode=padding_mode,
+        align_corners=align_corners,
     )
 
     return patches
 
 
 def crop_by_indices(
-    input: torch.Tensor,
-    src_box: torch.Tensor,
-    size: Optional[Tuple] = None,
+    input_tensor: Tensor,
+    src_box: Tensor,
+    size: Optional[Tuple[int, int]] = None,
     interpolation: str = 'bilinear',
     align_corners: Optional[bool] = None,
     antialias: bool = False,
     shape_compensation: str = "resize",
-) -> torch.Tensor:
+) -> Tensor:
     """Crop tensors with naive indices.
 
     Args:
         input: the 2D image tensor with shape (B, C, H, W).
         src_box: a tensor with shape (B, 4, 2) containing the coordinates of the bounding boxes
             to be extracted. The tensor must have the shape of Bx4x2, where each box is defined in the clockwise
             order: top-left, top-right, bottom-right and bottom-left. The coordinates must be in x, y order.
@@ -296,49 +307,49 @@
             'bicubic' | 'trilinear' | 'area'.
         align_corners: interpolation flag.
         antialias: if True, then image will be filtered with Gaussian before downscaling.
             No effect for upscaling.
         shape_compensation: if the cropped slice sizes are not exactly align `size`, the image can either be padded
             or resized.
     """
-    B, C, _, _ = input.shape
-    src = torch.as_tensor(src_box, device=input.device, dtype=torch.long)
+    B, C, _, _ = input_tensor.shape
+    src = as_tensor(src_box, device=input_tensor.device, dtype=torch.long)
     x1 = src[:, 0, 0]
     x2 = src[:, 1, 0] + 1
     y1 = src[:, 0, 1]
     y2 = src[:, 3, 1] + 1
 
     if (
         len(x1.unique(sorted=False))
         == len(x2.unique(sorted=False))
         == len(y1.unique(sorted=False))
         == len(y2.unique(sorted=False))
         == 1
     ):
-        out = input[..., y1[0] : y2[0], x1[0] : x2[0]]  # type:ignore
+        out = input_tensor[..., int(y1[0]) : int(y2[0]), int(x1[0]) : int(x2[0])]
         if size is not None and out.shape[-2:] != size:
             return resize(
                 out, size, interpolation=interpolation, align_corners=align_corners, side="short", antialias=antialias
             )
 
     if size is None:
         h, w = infer_bbox_shape(src)
         size = h.unique(sorted=False), w.unique(sorted=False)
-    out = torch.empty(B, C, *size, device=input.device, dtype=input.dtype)
+    out = torch.empty(B, C, *size, device=input_tensor.device, dtype=input_tensor.dtype)
     # Find out the cropped shapes that need to be resized.
     for i, _ in enumerate(out):
-        _out = input[i : i + 1, :, y1[i] : y2[i], x1[i] : x2[i]]  # type: ignore[misc]
+        _out = input_tensor[i : i + 1, :, int(y1[i]) : int(y2[i]), int(x1[i]) : int(x2[i])]
         if _out.shape[-2:] != size:
             if shape_compensation == "resize":
                 out[i] = resize(
-                    _out,  # type:ignore
+                    _out,
                     size,
                     interpolation=interpolation,
                     align_corners=align_corners,
                     side="short",
                     antialias=antialias,
                 )
             else:
-                out[i] = F.pad(_out, [0, size[1] - _out.shape[-1], 0, size[0] - _out.shape[-2]])
+                out[i] = pad(_out, [0, size[1] - _out.shape[-1], 0, size[0] - _out.shape[-2]])
         else:
-            out[i] = _out  # type:ignore
+            out[i] = _out
     return out
```

### Comparing `kornia-0.6.8/kornia/geometry/transform/crop3d.py` & `kornia-0.6.9/kornia/geometry/transform/crop3d.py`

 * *Files identical despite different names*

### Comparing `kornia-0.6.8/kornia/geometry/transform/elastic_transform.py` & `kornia-0.6.9/kornia/geometry/transform/elastic_transform.py`

 * *Files identical despite different names*

### Comparing `kornia-0.6.8/kornia/geometry/transform/flips.py` & `kornia-0.6.9/kornia/geometry/transform/flips.py`

 * *Files 13% similar despite different names*

```diff
@@ -1,14 +1,15 @@
 import torch
-import torch.nn as nn
+
+from kornia.core import Module, Tensor
 
 __all__ = ["Vflip", "Hflip", "Rot180", "rot180", "hflip", "vflip"]
 
 
-class Vflip(nn.Module):
+class Vflip(Module):
     r"""Vertically flip a tensor image or a batch of tensor images.
 
     Input must be a tensor of shape (C, H, W) or a batch of tensors :math:`(*, C, H, W)`.
 
     Args:
         input: input tensor.
 
@@ -24,22 +25,22 @@
         ... ]]])
         >>> vflip(input)
         tensor([[[[0., 1., 1.],
                   [0., 0., 0.],
                   [0., 0., 0.]]]])
     """
 
-    def forward(self, input: torch.Tensor) -> torch.Tensor:  # type: ignore
+    def forward(self, input: Tensor) -> Tensor:
         return vflip(input)
 
     def __repr__(self):
         return self.__class__.__name__
 
 
-class Hflip(nn.Module):
+class Hflip(Module):
     r"""Horizontally flip a tensor image or a batch of tensor images.
 
     Input must be a tensor of shape (C, H, W) or a batch of tensors :math:`(*, C, H, W)`.
 
     Args:
         input: input tensor.
 
@@ -55,22 +56,22 @@
         ... ]]])
         >>> hflip(input)
         tensor([[[[0., 0., 0.],
                   [0., 0., 0.],
                   [1., 1., 0.]]]])
     """
 
-    def forward(self, input: torch.Tensor) -> torch.Tensor:  # type: ignore
+    def forward(self, input: Tensor) -> Tensor:
         return hflip(input)
 
     def __repr__(self):
         return self.__class__.__name__
 
 
-class Rot180(nn.Module):
+class Rot180(Module):
     r"""Rotate a tensor image or a batch of tensor images 180 degrees.
 
     Input must be a tensor of shape (C, H, W) or a batch of tensors :math:`(*, C, H, W)`.
 
     Args:
         input: input tensor.
 
@@ -83,22 +84,22 @@
         ... ]]])
         >>> rot180(input)
         tensor([[[[1., 1., 0.],
                   [0., 0., 0.],
                   [0., 0., 0.]]]])
     """
 
-    def forward(self, input: torch.Tensor) -> torch.Tensor:  # type: ignore
+    def forward(self, input: Tensor) -> Tensor:
         return rot180(input)
 
     def __repr__(self):
         return self.__class__.__name__
 
 
-def rot180(input: torch.Tensor) -> torch.Tensor:
+def rot180(input: Tensor) -> Tensor:
     r"""Rotate a tensor image or a batch of tensor images 180 degrees.
 
     .. image:: _static/img/rot180.png
 
     Input must be a tensor of shape (C, H, W) or a batch of tensors :math:`(*, C, H, W)`.
 
     Args:
@@ -107,15 +108,15 @@
     Returns:
         The rotated image tensor.
     """
 
     return torch.flip(input, [-2, -1])
 
 
-def hflip(input: torch.Tensor) -> torch.Tensor:
+def hflip(input: Tensor) -> Tensor:
     r"""Horizontally flip a tensor image or a batch of tensor images.
 
     .. image:: _static/img/hflip.png
 
     Input must be a tensor of shape (C, H, W) or a batch of tensors :math:`(*, C, H, W)`.
 
     Args:
@@ -124,15 +125,15 @@
     Returns:
         The horizontally flipped image tensor.
     """
     w = input.shape[-1]
     return input[..., torch.arange(w - 1, -1, -1, device=input.device)]
 
 
-def vflip(input: torch.Tensor) -> torch.Tensor:
+def vflip(input: Tensor) -> Tensor:
     r"""Vertically flip a tensor image or a batch of tensor images.
 
     .. image:: _static/img/vflip.png
 
     Input must be a tensor of shape (C, H, W) or a batch of tensors :math:`(*, C, H, W)`.
 
     Args:
```

### Comparing `kornia-0.6.8/kornia/geometry/transform/homography_warper.py` & `kornia-0.6.9/kornia/geometry/transform/homography_warper.py`

 * *Files identical despite different names*

### Comparing `kornia-0.6.8/kornia/geometry/transform/image_registrator.py` & `kornia-0.6.9/kornia/geometry/transform/image_registrator.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,14 +1,15 @@
 from typing import Callable, List, Tuple, Union
 
 import torch
 import torch.nn as nn
 import torch.nn.functional as F
 import torch.optim as optim
 
+from kornia.core import Tensor
 from kornia.geometry.conversions import angle_to_rotation_matrix, convert_affinematrix_to_homography
 
 from .homography_warper import HomographyWarper
 from .pyramid import build_pyramid
 
 __all__ = ["ImageRegistrator", "Homography", "Similarity"]
 
@@ -25,23 +26,23 @@
     def __repr__(self) -> str:
         return f'{self.__class__.__name__}({self.model})'
 
     def reset_model(self):
         """Initializes the model with identity transform."""
         torch.nn.init.eye_(self.model)
 
-    def forward(self) -> torch.Tensor:
+    def forward(self) -> Tensor:
         r"""Single-batch homography".
 
         Returns:
             Homography matrix with shape :math:`(1, 3, 3)`.
         """
         return torch.unsqueeze(self.model / self.model[2, 2], dim=0)  # 1x3x3
 
-    def forward_inverse(self) -> torch.Tensor:
+    def forward_inverse(self) -> Tensor:
         r"""Interted Single-batch homography".
 
         Returns:
             Homography martix with shape :math:`(1, 3, 3)`.
         """
         return torch.unsqueeze(torch.inverse(self.model), dim=0)
 
@@ -78,25 +79,25 @@
 
     def reset_model(self) -> None:
         """Initialize the model with identity transform."""
         torch.nn.init.zeros_(self.rot)
         torch.nn.init.zeros_(self.shift)
         torch.nn.init.ones_(self.scale)
 
-    def forward(self) -> torch.Tensor:
+    def forward(self) -> Tensor:
         r"""Single-batch similarity transform".
 
         Returns:
             Similarity with shape :math:`(1, 3, 3)`
         """
         rot = self.scale * angle_to_rotation_matrix(self.rot)
         out = convert_affinematrix_to_homography(torch.cat([rot, self.shift], dim=2))
         return out
 
-    def forward_inverse(self) -> torch.Tensor:
+    def forward_inverse(self) -> Tensor:
         r"""Single-batch inverse similarity transform".
 
         Returns:
             Similarity with shape :math:`(1, 3, 3)`
         """
         return torch.inverse(self.forward())
 
@@ -124,15 +125,15 @@
     known_models = ['homography', 'similarity', 'translation', 'scale', 'rotation']
 
     # TODO: resolve better type, potentially using factory.
     def __init__(
         self,
         model_type='homography',
         optimizer=optim.Adam,
-        loss_fn: Callable = F.l1_loss,
+        loss_fn: Callable[..., Tensor] = F.l1_loss,
         pyramid_levels: int = 5,
         lr: float = 1e-3,
         num_iterations: int = 100,
         tolerance: float = 1e-4,
         warper=None,
     ) -> None:
         super().__init__()
@@ -164,17 +165,15 @@
         self.pyramid_levels = pyramid_levels
         self.optimizer = optimizer
         self.lr = lr
         self.loss_fn = loss_fn
         self.num_iterations = num_iterations
         self.tolerance = tolerance
 
-    def get_single_level_loss(
-        self, img_src: torch.Tensor, img_dst: torch.Tensor, transform_model: torch.Tensor
-    ) -> torch.Tensor:
+    def get_single_level_loss(self, img_src: Tensor, img_dst: Tensor, transform_model: Tensor) -> Tensor:
         """Warp img_src into img_dst with transform_model and returns loss."""
         # ToDo: Make possible registration of images of different shape
         if img_src.shape != img_dst.shape:
             raise ValueError(
                 f"Cannot register images of different shapes\
                               {img_src.shape} {img_dst.shape:} "
             )
@@ -188,20 +187,16 @@
         return loss
 
     def reset_model(self) -> None:
         """Calls model reset function."""
         self.model.reset_model()
 
     def register(
-        self,
-        src_img: torch.Tensor,
-        dst_img: torch.Tensor,
-        verbose: bool = False,
-        output_intermediate_models: bool = False,
-    ) -> Union[torch.Tensor, Tuple[torch.Tensor, List[torch.Tensor]]]:
+        self, src_img: Tensor, dst_img: Tensor, verbose: bool = False, output_intermediate_models: bool = False
+    ) -> Union[Tensor, Tuple[Tensor, List[Tensor]]]:
         r"""Estimate the tranformation' which warps src_img into dst_img by gradient descent. The shape of the
         tensors is not checked, because it may depend on the model, e.g. volume registration.
 
         Args:
             src_img: Input image tensor.
             dst_img: Input image tensor.
             verbose: if True, outputs loss every 10 iterations.
@@ -239,20 +234,20 @@
                 opt.step()
             if output_intermediate_models:
                 aux_models.append(self.model().clone().detach())
         if output_intermediate_models:
             return self.model(), aux_models
         return self.model()
 
-    def warp_src_into_dst(self, src_img: torch.Tensor) -> torch.Tensor:
+    def warp_src_into_dst(self, src_img: Tensor) -> Tensor:
         r"""Warp src_img with estimated model."""
         _height, _width = src_img.shape[-2:]
         warper = self.warper(_height, _width)
         img_src_to_dst = warper(src_img, self.model())
         return img_src_to_dst
 
-    def warp_dst_inro_src(self, dst_img: torch.Tensor) -> torch.Tensor:
+    def warp_dst_inro_src(self, dst_img: Tensor) -> Tensor:
         r"""Warp src_img with inverted estimated model."""
         _height, _width = dst_img.shape[-2:]
         warper = self.warper(_height, _width)
         img_dst_to_src = warper(dst_img, self.model.forward_inverse())
         return img_dst_to_src
```

### Comparing `kornia-0.6.8/kornia/geometry/transform/imgwarp.py` & `kornia-0.6.9/kornia/geometry/transform/imgwarp.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,14 +1,14 @@
 from typing import List, Optional, Tuple
 
 import torch
 import torch.nn.functional as F
 from torch.nn.functional import grid_sample
 
-from kornia.core import Tensor, stack
+from kornia.core import Tensor, concatenate, stack, tensor, zeros
 from kornia.geometry.conversions import (
     angle_axis_to_rotation_matrix,
     angle_to_rotation_matrix,
     convert_affinematrix_to_homography,
     convert_affinematrix_to_homography3d,
     deg2rad,
     normalize_homography,
@@ -40,22 +40,22 @@
     "warp_grid3d",
     "homography_warp",
     "homography_warp3d",
 ]
 
 
 def warp_perspective(
-    src: torch.Tensor,
-    M: torch.Tensor,
+    src: Tensor,
+    M: Tensor,
     dsize: Tuple[int, int],
     mode: str = 'bilinear',
     padding_mode: str = 'zeros',
     align_corners: bool = True,
-    fill_value: torch.Tensor = torch.zeros(3),  # needed for jit
-) -> torch.Tensor:
+    fill_value: Tensor = zeros(3),  # needed for jit
+) -> Tensor:
     r"""Apply a perspective transformation to an image.
 
     .. image:: https://kornia-tutorials.readthedocs.io/en/latest/_images/warp_perspective_10_1.png
 
     The function warp_perspective transforms the source image using
     the specified matrix:
 
@@ -87,19 +87,19 @@
     .. note::
         This function is often used in conjunction with :func:`get_perspective_transform`.
 
     .. note::
         See a working example `here <https://kornia-tutorials.readthedocs.io/en/
         latest/warp_perspective.html>`_.
     """
-    if not isinstance(src, torch.Tensor):
-        raise TypeError(f"Input src type is not a torch.Tensor. Got {type(src)}")
+    if not isinstance(src, Tensor):
+        raise TypeError(f"Input src type is not a Tensor. Got {type(src)}")
 
-    if not isinstance(M, torch.Tensor):
-        raise TypeError(f"Input M type is not a torch.Tensor. Got {type(M)}")
+    if not isinstance(M, Tensor):
+        raise TypeError(f"Input M type is not a Tensor. Got {type(M)}")
 
     if not len(src.shape) == 4:
         raise ValueError(f"Input src must be a BxCxHxW tensor. Got {src.shape}")
 
     if not (len(M.shape) == 3 and M.shape[-2:] == (3, 3)):
         raise ValueError(f"Input M must be a Bx3x3 tensor. Got {M.shape}")
 
@@ -108,38 +108,40 @@
     if padding_mode == "fill" and fill_value.shape != torch.Size([3]):
         raise ValueError(f"Padding_tensor only supported for 3 channels. Got {fill_value.shape}")
 
     B, _, H, W = src.size()
     h_out, w_out = dsize
 
     # we normalize the 3x3 transformation matrix and convert to 3x4
-    dst_norm_trans_src_norm: torch.Tensor = normalize_homography(M, (H, W), (h_out, w_out))  # Bx3x3
+    dst_norm_trans_src_norm: Tensor = normalize_homography(M, (H, W), (h_out, w_out))  # Bx3x3
 
     src_norm_trans_dst_norm = _torch_inverse_cast(dst_norm_trans_src_norm)  # Bx3x3
 
     # this piece of code substitutes F.affine_grid since it does not support 3x3
     grid = (
-        create_meshgrid(h_out, w_out, normalized_coordinates=True, device=src.device).to(src.dtype).repeat(B, 1, 1, 1)
+        create_meshgrid(h_out, w_out, normalized_coordinates=True, device=src.device)
+        .to(src.dtype)
+        .expand(B, h_out, w_out, 2)
     )
     grid = transform_points(src_norm_trans_dst_norm[:, None, None], grid)
 
     if padding_mode == "fill":
         return _fill_and_warp(src, grid, align_corners=align_corners, mode=mode, fill_value=fill_value)
     return F.grid_sample(src, grid, align_corners=align_corners, mode=mode, padding_mode=padding_mode)
 
 
 def warp_affine(
-    src: torch.Tensor,
-    M: torch.Tensor,
+    src: Tensor,
+    M: Tensor,
     dsize: Tuple[int, int],
     mode: str = 'bilinear',
     padding_mode: str = 'zeros',
     align_corners: bool = True,
-    fill_value: torch.Tensor = torch.zeros(3),  # needed for jit
-) -> torch.Tensor:
+    fill_value: Tensor = zeros(3),  # needed for jit
+) -> Tensor:
     r"""Apply an affine transformation to a tensor.
 
     .. image:: _static/img/warp_affine.png
 
     The function warp_affine transforms the source tensor using
     the specified matrix:
 
@@ -170,19 +172,19 @@
     Example:
        >>> img = torch.rand(1, 4, 5, 6)
        >>> A = torch.eye(2, 3)[None]
        >>> out = warp_affine(img, A, (4, 2), align_corners=True)
        >>> print(out.shape)
        torch.Size([1, 4, 4, 2])
     """
-    if not isinstance(src, torch.Tensor):
-        raise TypeError(f"Input src type is not a torch.Tensor. Got {type(src)}")
+    if not isinstance(src, Tensor):
+        raise TypeError(f"Input src type is not a Tensor. Got {type(src)}")
 
-    if not isinstance(M, torch.Tensor):
-        raise TypeError(f"Input M type is not a torch.Tensor. Got {type(M)}")
+    if not isinstance(M, Tensor):
+        raise TypeError(f"Input M type is not a Tensor. Got {type(M)}")
 
     if not len(src.shape) == 4:
         raise ValueError(f"Input src must be a BxCxHxW tensor. Got {src.shape}")
 
     if not (len(M.shape) == 3 or M.shape[-2:] == (2, 3)):
         raise ValueError(f"Input M must be a Bx2x3 tensor. Got {M.shape}")
 
@@ -190,30 +192,28 @@
     # to None as this gives jit issues.
     if padding_mode == "fill" and fill_value.shape != torch.Size([3]):
         raise ValueError(f"Padding_tensor only supported for 3 channels. Got {fill_value.shape}")
 
     B, C, H, W = src.size()
 
     # we generate a 3x3 transformation matrix from 2x3 affine
-    M_3x3: torch.Tensor = convert_affinematrix_to_homography(M)
-    dst_norm_trans_src_norm: torch.Tensor = normalize_homography(M_3x3, (H, W), dsize)
+    M_3x3: Tensor = convert_affinematrix_to_homography(M)
+    dst_norm_trans_src_norm: Tensor = normalize_homography(M_3x3, (H, W), dsize)
 
     # src_norm_trans_dst_norm = torch.inverse(dst_norm_trans_src_norm)
     src_norm_trans_dst_norm = _torch_inverse_cast(dst_norm_trans_src_norm)
 
     grid = F.affine_grid(src_norm_trans_dst_norm[:, :2, :], [B, C, dsize[0], dsize[1]], align_corners=align_corners)
 
     if padding_mode == "fill":
         return _fill_and_warp(src, grid, align_corners=align_corners, mode=mode, fill_value=fill_value)
     return F.grid_sample(src, grid, align_corners=align_corners, mode=mode, padding_mode=padding_mode)
 
 
-def _fill_and_warp(
-    src: torch.Tensor, grid: torch.Tensor, mode: str, align_corners: bool, fill_value: torch.Tensor
-) -> torch.Tensor:
+def _fill_and_warp(src: Tensor, grid: Tensor, mode: str, align_corners: bool, fill_value: Tensor) -> Tensor:
     r"""Warp a mask of ones, then multiple with fill_value and add to default warp.
 
     Args:
         src: input tensor of shape :math:`(B, 3, H, W)`.
         grid: grid tensor from `transform_points`.
         mode: interpolation mode to calculate output values ``'bilinear'`` | ``'nearest'``.
         align_corners: interpolation flag.
@@ -225,15 +225,15 @@
     ones_mask = torch.ones_like(src)
     fill_value = fill_value.to(ones_mask)[None, :, None, None]  # cast and add dimensions for broadcasting
     inv_ones_mask = 1 - F.grid_sample(ones_mask, grid, align_corners=align_corners, mode=mode, padding_mode="zeros")
     inv_color_mask = inv_ones_mask * fill_value
     return F.grid_sample(src, grid, align_corners=align_corners, mode=mode, padding_mode="zeros") + inv_color_mask
 
 
-def warp_grid(grid: torch.Tensor, src_homo_dst: torch.Tensor) -> torch.Tensor:
+def warp_grid(grid: Tensor, src_homo_dst: Tensor) -> Tensor:
     r"""Compute the grid to warp the coordinates grid by the homography/ies.
 
     Args:
         grid: Unwrapped grid of the shape :math:`(1, N, W, 2)`.
         src_homo_dst: Homography or homographies (stacked) to
           transform all points in the grid. Shape of the homography
           has to be :math:`(1, 3, 3)` or :math:`(N, 1, 3, 3)`.
@@ -245,19 +245,19 @@
     _, height, width, _ = grid.size()
     # expand grid to match the input batch size
     grid = grid.expand(batch_size, -1, -1, -1)  # NxHxWx2
     if len(src_homo_dst.shape) == 3:  # local homography case
         src_homo_dst = src_homo_dst.view(batch_size, 1, 3, 3)  # Nx1x3x3
     # perform the actual grid transformation,
     # the grid is copied to input device and casted to the same type
-    flow: torch.Tensor = transform_points(src_homo_dst, grid.to(src_homo_dst))  # NxHxWx2
+    flow: Tensor = transform_points(src_homo_dst, grid.to(src_homo_dst))  # NxHxWx2
     return flow.view(batch_size, height, width, 2)  # NxHxWx2
 
 
-def warp_grid3d(grid: torch.Tensor, src_homo_dst: torch.Tensor) -> torch.Tensor:
+def warp_grid3d(grid: Tensor, src_homo_dst: Tensor) -> Tensor:
     r"""Compute the grid to warp the coordinates grid by the homography/ies.
 
     Args:
         grid: Unwrapped grid of the shape :math:`(1, D, H, W, 3)`.
         src_homo_dst: Homography or homographies (stacked) to
           transform all points in the grid. Shape of the homography
           has to be :math:`(1, 4, 4)` or :math:`(N, 1, 4, 4)`.
@@ -269,15 +269,15 @@
     _, depth, height, width, _ = grid.size()
     # expand grid to match the input batch size
     grid = grid.expand(batch_size, -1, -1, -1, -1)  # NxDxHxWx3
     if len(src_homo_dst.shape) == 3:  # local homography case
         src_homo_dst = src_homo_dst.view(batch_size, 1, 4, 4)  # Nx1x3x3
     # perform the actual grid transformation,
     # the grid is copied to input device and casted to the same type
-    flow: torch.Tensor = transform_points(src_homo_dst, grid.to(src_homo_dst))  # NxDxHxWx3
+    flow: Tensor = transform_points(src_homo_dst, grid.to(src_homo_dst))  # NxDxHxWx3
     return flow.view(batch_size, depth, height, width, 3)  # NxDxHxWx3
 
 
 # TODO: move to kornia.geometry.projective
 # TODO: create the nn.Module -- TBD what inputs/outputs etc
 # class PerspectiveTransform(nn.Module):
 #     def __init__(self) -> None:
@@ -340,23 +340,23 @@
 
     # create the lhs tensor with shape # Bx8x8
     B: int = points_src.shape[0]  # batch_size
 
     A = torch.empty(B, 8, 8, device=points_src.device, dtype=points_src.dtype)
 
     # we need to perform in batch
-    zeros = torch.zeros(B, device=points_src.device, dtype=points_src.dtype)
-    ones = torch.ones(B, device=points_src.device, dtype=points_src.dtype)
+    _zeros = zeros(B, device=points_src.device, dtype=points_src.dtype)
+    _ones = torch.ones(B, device=points_src.device, dtype=points_src.dtype)
 
     for i in range(4):
         x1, y1 = points_src[..., i, 0], points_src[..., i, 1]  # Bx4
         x2, y2 = points_dst[..., i, 0], points_dst[..., i, 1]  # Bx4
 
-        A[:, 2 * i] = stack([x1, y1, ones, zeros, zeros, zeros, -x1 * x2, -y1 * x2], -1)
-        A[:, 2 * i + 1] = stack([zeros, zeros, zeros, x1, y1, ones, -x1 * y2, -y1 * y2], -1)
+        A[:, 2 * i] = stack([x1, y1, _ones, _zeros, _zeros, _zeros, -x1 * x2, -y1 * x2], -1)
+        A[:, 2 * i + 1] = stack([_zeros, _zeros, _zeros, x1, y1, _ones, -x1 * y2, -y1 * y2], -1)
 
     # the rhs tensor
     b = points_dst.view(-1, 8, 1)
 
     # solve the system Ax = b
     X: Tensor = _torch_solve_cast(A, b)
 
@@ -365,15 +365,15 @@
     M[..., :8] = X[..., 0]  # Bx8
     M[..., -1].fill_(1)
 
     return M.view(-1, 3, 3)  # Bx3x3
 
 
 # TODO: move to kornia.geometry.affine
-def get_rotation_matrix2d(center: torch.Tensor, angle: torch.Tensor, scale: torch.Tensor) -> torch.Tensor:
+def get_rotation_matrix2d(center: Tensor, angle: Tensor, scale: Tensor) -> Tensor:
     r"""Calculate an affine matrix of 2D rotation.
 
     The function calculates the following matrix:
 
     .. math::
         \begin{bmatrix}
             \alpha & \beta & (1 - \alpha) \cdot \text{x}
@@ -398,32 +398,32 @@
             be the top-left corner) with shape :math:`(B)`.
         scale: scale factor for x, y scaling with shape :math:`(B, 2)`.
 
     Returns:
         the affine matrix of 2D rotation with shape :math:`(B, 2, 3)`.
 
     Example:
-        >>> center = torch.zeros(1, 2)
+        >>> center = zeros(1, 2)
         >>> scale = torch.ones((1, 2))
         >>> angle = 45. * torch.ones(1)
         >>> get_rotation_matrix2d(center, angle, scale)
         tensor([[[ 0.7071,  0.7071,  0.0000],
                  [-0.7071,  0.7071,  0.0000]]])
 
     .. note::
         This function is often used in conjunction with :func:`warp_affine`.
     """
-    if not isinstance(center, torch.Tensor):
-        raise TypeError(f"Input center type is not a torch.Tensor. Got {type(center)}")
+    if not isinstance(center, Tensor):
+        raise TypeError(f"Input center type is not a Tensor. Got {type(center)}")
 
-    if not isinstance(angle, torch.Tensor):
-        raise TypeError(f"Input angle type is not a torch.Tensor. Got {type(angle)}")
+    if not isinstance(angle, Tensor):
+        raise TypeError(f"Input angle type is not a Tensor. Got {type(angle)}")
 
-    if not isinstance(scale, torch.Tensor):
-        raise TypeError(f"Input scale type is not a torch.Tensor. Got {type(scale)}")
+    if not isinstance(scale, Tensor):
+        raise TypeError(f"Input scale type is not a Tensor. Got {type(scale)}")
 
     if not (len(center.shape) == 2 and center.shape[1] == 2):
         raise ValueError(f"Input center must be a Bx2 tensor. Got {center.shape}")
 
     if not len(angle.shape) == 1:
         raise ValueError(f"Input angle must be a B tensor. Got {angle.shape}")
 
@@ -526,15 +526,15 @@
     # simulate broadcasting since grid_sample does not support it
     map_xy = map_xy.expand(batch_size, -1, -1, -1)
 
     # warp the image tensor and return
     return grid_sample(image, map_xy, mode=mode, padding_mode=padding_mode, align_corners=align_corners)
 
 
-def invert_affine_transform(matrix: torch.Tensor) -> torch.Tensor:
+def invert_affine_transform(matrix: Tensor) -> Tensor:
     r"""Invert an affine transformation.
 
     The function computes an inverse affine transformation represented by
     23 matrix:
 
     .. math::
         \begin{bmatrix}
@@ -550,34 +550,34 @@
 
     Return:
         the reverse affine transform with shape :math:`(B, 2, 3)`.
 
     .. note::
         This function is often used in conjunction with :func:`warp_affine`.
     """
-    if not isinstance(matrix, torch.Tensor):
-        raise TypeError(f"Input matrix type is not a torch.Tensor. Got {type(matrix)}")
+    if not isinstance(matrix, Tensor):
+        raise TypeError(f"Input matrix type is not a Tensor. Got {type(matrix)}")
 
     if not (len(matrix.shape) == 3 and matrix.shape[-2:] == (2, 3)):
         raise ValueError(f"Input matrix must be a Bx2x3 tensor. Got {matrix.shape}")
 
-    matrix_tmp: torch.Tensor = convert_affinematrix_to_homography(matrix)
-    matrix_inv: torch.Tensor = _torch_inverse_cast(matrix_tmp)
+    matrix_tmp: Tensor = convert_affinematrix_to_homography(matrix)
+    matrix_inv: Tensor = _torch_inverse_cast(matrix_tmp)
 
     return matrix_inv[..., :2, :3]
 
 
 def get_affine_matrix2d(
-    translations: torch.Tensor,
-    center: torch.Tensor,
-    scale: torch.Tensor,
-    angle: torch.Tensor,
-    sx: Optional[torch.Tensor] = None,
-    sy: Optional[torch.Tensor] = None,
-) -> torch.Tensor:
+    translations: Tensor,
+    center: Tensor,
+    scale: Tensor,
+    angle: Tensor,
+    sx: Optional[Tensor] = None,
+    sy: Optional[Tensor] = None,
+) -> Tensor:
     r"""Compose affine matrix from the components.
 
     Args:
         translations: tensor containing the translation vector with shape :math:`(B, 2)`.
         center: tensor containing the center vector with shape :math:`(B, 2)`.
         scale: tensor containing the scale factor with shape :math:`(B, 2)`.
         angle: tensor of angles in degrees :math:`(B)`.
@@ -586,28 +586,28 @@
 
     Returns:
         the affine transformation matrix :math:`(B, 3, 3)`.
 
     .. note::
         This function is often used in conjunction with :func:`warp_affine`, :func:`warp_perspective`.
     """
-    transform: torch.Tensor = get_rotation_matrix2d(center, -angle, scale)
+    transform: Tensor = get_rotation_matrix2d(center, -angle, scale)
     transform[..., 2] += translations  # tx/ty
 
     # pad transform to get Bx3x3
     transform_h = convert_affinematrix_to_homography(transform)
 
     if any(s is not None for s in [sx, sy]):
         shear_mat = get_shear_matrix2d(center, sx, sy)
         transform_h = transform_h @ shear_mat
 
     return transform_h
 
 
-def get_shear_matrix2d(center: torch.Tensor, sx: Optional[torch.Tensor] = None, sy: Optional[torch.Tensor] = None):
+def get_shear_matrix2d(center: Tensor, sx: Optional[Tensor] = None, sy: Optional[Tensor] = None):
     r"""Compose shear matrix Bx4x4 from the components.
 
     Note: Ordered shearing, shear x-axis then y-axis.
 
     .. math::
         \begin{bmatrix}
             1 & b \\
@@ -632,43 +632,43 @@
         tensor([[[  1.0000, -33.5468,   0.0000],
                  [ -0.0000,   1.0000,   0.0000],
                  [  0.0000,   0.0000,   1.0000]]])
 
     .. note::
         This function is often used in conjunction with :func:`warp_affine`, :func:`warp_perspective`.
     """
-    sx = torch.tensor([0.0]).repeat(center.size(0)) if sx is None else sx
-    sy = torch.tensor([0.0]).repeat(center.size(0)) if sy is None else sy
+    sx = tensor([0.0]).repeat(center.size(0)) if sx is None else sx
+    sy = tensor([0.0]).repeat(center.size(0)) if sy is None else sy
 
     x, y = torch.split(center, 1, dim=-1)
     x, y = x.view(-1), y.view(-1)
 
-    sx_tan = torch.tan(sx)  # type: ignore
-    sy_tan = torch.tan(sy)  # type: ignore
-    ones = torch.ones_like(sx)  # type: ignore
-    shear_mat = torch.stack(
-        [ones, -sx_tan, sx_tan * y, -sy_tan, ones + sx_tan * sy_tan, sy_tan * (sx_tan * y + x)], dim=-1  # type: ignore
-    ).view(-1, 2, 3)
+    sx_tan = torch.tan(sx)
+    sy_tan = torch.tan(sy)
+    ones = torch.ones_like(sx)
+    shear_mat = stack([ones, -sx_tan, sx_tan * y, -sy_tan, ones + sx_tan * sy_tan, sy_tan * (sx_tan * y + x)], -1).view(
+        -1, 2, 3
+    )
 
     shear_mat = convert_affinematrix_to_homography(shear_mat)
     return shear_mat
 
 
 def get_affine_matrix3d(
-    translations: torch.Tensor,
-    center: torch.Tensor,
-    scale: torch.Tensor,
-    angles: torch.Tensor,
-    sxy: Optional[torch.Tensor] = None,
-    sxz: Optional[torch.Tensor] = None,
-    syx: Optional[torch.Tensor] = None,
-    syz: Optional[torch.Tensor] = None,
-    szx: Optional[torch.Tensor] = None,
-    szy: Optional[torch.Tensor] = None,
-) -> torch.Tensor:
+    translations: Tensor,
+    center: Tensor,
+    scale: Tensor,
+    angles: Tensor,
+    sxy: Optional[Tensor] = None,
+    sxz: Optional[Tensor] = None,
+    syx: Optional[Tensor] = None,
+    syz: Optional[Tensor] = None,
+    szx: Optional[Tensor] = None,
+    szy: Optional[Tensor] = None,
+) -> Tensor:
     r"""Compose 3d affine matrix from the components.
 
     Args:
         translations: tensor containing the translation vector (dx,dy,dz) with shape :math:`(B, 3)`.
         center: tensor containing the center vector (x,y,z) with shape :math:`(B, 3)`.
         scale: tensor containing the scale factor with shape :math:`(B)`.
         angle: angle axis vector containing the rotation angles in degrees in the form
@@ -683,34 +683,34 @@
 
     Returns:
         the 3d affine transformation matrix :math:`(B, 3, 3)`.
 
     .. note::
         This function is often used in conjunction with :func:`warp_perspective`.
     """
-    transform: torch.Tensor = get_projective_transform(center, -angles, scale)
+    transform: Tensor = get_projective_transform(center, -angles, scale)
     transform[..., 3] += translations  # tx/ty/tz
 
     # pad transform to get Bx3x3
     transform_h = convert_affinematrix_to_homography3d(transform)
     if any(s is not None for s in [sxy, sxz, syx, syz, szx, szy]):
         shear_mat = get_shear_matrix3d(center, sxy, sxz, syx, syz, szx, szy)
         transform_h = transform_h @ shear_mat
 
     return transform_h
 
 
 def get_shear_matrix3d(
-    center: torch.Tensor,
-    sxy: Optional[torch.Tensor] = None,
-    sxz: Optional[torch.Tensor] = None,
-    syx: Optional[torch.Tensor] = None,
-    syz: Optional[torch.Tensor] = None,
-    szx: Optional[torch.Tensor] = None,
-    szy: Optional[torch.Tensor] = None,
+    center: Tensor,
+    sxy: Optional[Tensor] = None,
+    sxz: Optional[Tensor] = None,
+    syx: Optional[Tensor] = None,
+    syz: Optional[Tensor] = None,
+    szx: Optional[Tensor] = None,
+    szy: Optional[Tensor] = None,
 ):
     r"""Compose shear matrix Bx4x4 from the components.
     Note: Ordered shearing, shear x-axis then y-axis then z-axis.
 
     .. math::
         \begin{bmatrix}
             1 & o & r & oy + rz \\
@@ -751,30 +751,30 @@
                  [-33.5468,  49.2039,   0.0000,   0.0000],
                  [  0.3022,  -1.0729,   1.0000,   0.0000],
                  [  0.0000,   0.0000,   0.0000,   1.0000]]])
 
     .. note::
         This function is often used in conjunction with :func:`warp_perspective3d`.
     """
-    sxy = torch.tensor([0.0]).repeat(center.size(0)) if sxy is None else sxy
-    sxz = torch.tensor([0.0]).repeat(center.size(0)) if sxz is None else sxz
-    syx = torch.tensor([0.0]).repeat(center.size(0)) if syx is None else syx
-    syz = torch.tensor([0.0]).repeat(center.size(0)) if syz is None else syz
-    szx = torch.tensor([0.0]).repeat(center.size(0)) if szx is None else szx
-    szy = torch.tensor([0.0]).repeat(center.size(0)) if szy is None else szy
+    sxy = tensor([0.0]).repeat(center.size(0)) if sxy is None else sxy
+    sxz = tensor([0.0]).repeat(center.size(0)) if sxz is None else sxz
+    syx = tensor([0.0]).repeat(center.size(0)) if syx is None else syx
+    syz = tensor([0.0]).repeat(center.size(0)) if syz is None else syz
+    szx = tensor([0.0]).repeat(center.size(0)) if szx is None else szx
+    szy = tensor([0.0]).repeat(center.size(0)) if szy is None else szy
 
     x, y, z = torch.split(center, 1, dim=-1)
     x, y, z = x.view(-1), y.view(-1), z.view(-1)
     # Prepare parameters
-    sxy_tan = torch.tan(sxy)  # type: ignore
-    sxz_tan = torch.tan(sxz)  # type: ignore
-    syx_tan = torch.tan(syx)  # type: ignore
-    syz_tan = torch.tan(syz)  # type: ignore
-    szx_tan = torch.tan(szx)  # type: ignore
-    szy_tan = torch.tan(szy)  # type: ignore
+    sxy_tan = torch.tan(sxy)
+    sxz_tan = torch.tan(sxz)
+    syx_tan = torch.tan(syx)
+    syz_tan = torch.tan(syz)
+    szx_tan = torch.tan(szx)
+    szy_tan = torch.tan(szy)
 
     # compute translation matrix
     m00, m10, m20, m01, m11, m21, m02, m12, m22 = _compute_shear_matrix_3d(
         sxy_tan, sxz_tan, syx_tan, syz_tan, szx_tan, szy_tan
     )
 
     m03 = m01 * y + m02 * z
@@ -783,39 +783,39 @@
 
     # shear matrix is implemented with negative values
     sxy_tan, sxz_tan, syx_tan, syz_tan, szx_tan, szy_tan = -sxy_tan, -sxz_tan, -syx_tan, -syz_tan, -szx_tan, -szy_tan
     m00, m10, m20, m01, m11, m21, m02, m12, m22 = _compute_shear_matrix_3d(
         sxy_tan, sxz_tan, syx_tan, syz_tan, szx_tan, szy_tan
     )
 
-    shear_mat = torch.stack([m00, m01, m02, m03, m10, m11, m12, m13, m20, m21, m22, m23], dim=-1).view(-1, 3, 4)
+    shear_mat = stack([m00, m01, m02, m03, m10, m11, m12, m13, m20, m21, m22, m23], -1).view(-1, 3, 4)
     shear_mat = convert_affinematrix_to_homography3d(shear_mat)
 
     return shear_mat
 
 
 def _compute_shear_matrix_3d(sxy_tan, sxz_tan, syx_tan, syz_tan, szx_tan, szy_tan):
-    ones = torch.ones_like(sxy_tan)  # type: ignore
+    ones = torch.ones_like(sxy_tan)
 
     m00, m10, m20 = ones, sxy_tan, sxz_tan
     m01, m11, m21 = syx_tan, sxy_tan * syx_tan + ones, sxz_tan * syx_tan + syz_tan
     m02 = syx_tan * szy_tan + szx_tan
     m12 = sxy_tan * szx_tan + szy_tan * m11
     m22 = sxz_tan * szx_tan + szy_tan * m21 + ones
     return m00, m10, m20, m01, m11, m21, m02, m12, m22
 
 
 def warp_affine3d(
-    src: torch.Tensor,
-    M: torch.Tensor,
+    src: Tensor,
+    M: Tensor,
     dsize: Tuple[int, int, int],
     flags: str = 'bilinear',
     padding_mode: str = 'zeros',
     align_corners: bool = True,
-) -> torch.Tensor:
+) -> Tensor:
     r"""Apply a projective transformation a to 3d tensor.
 
     .. warning::
         This API signature it is experimental and might suffer some changes in the future.
 
     Args:
         src : input tensor of shape :math:`(B, C, D, H, W)`.
@@ -824,15 +824,15 @@
         mode: interpolation mode to calculate output values
           ``'bilinear'`` | ``'nearest'``.
         padding_mode: padding mode for outside grid values
           ``'zeros'`` | ``'border'`` | ``'reflection'``.
         align_corners : mode for grid_generation.
 
     Returns:
-        torch.Tensor: the warped 3d tensor with shape :math:`(B, C, D, H, W)`.
+        Tensor: the warped 3d tensor with shape :math:`(B, C, D, H, W)`.
 
     .. note::
         This function is often used in conjunction with :func:`get_perspective_transform3d`.
     """
     if len(src.shape) != 5:
         raise AssertionError(src.shape)
     if not (len(M.shape) == 3 and M.shape[-2:] == (3, 4)):
@@ -843,28 +843,26 @@
 
     size_src: Tuple[int, int, int] = (D, H, W)
     size_out: Tuple[int, int, int] = dsize
 
     M_4x4 = convert_affinematrix_to_homography3d(M)  # Bx4x4
 
     # we need to normalize the transformation since grid sample needs -1/1 coordinates
-    dst_norm_trans_src_norm: torch.Tensor = normalize_homography3d(M_4x4, size_src, size_out)  # Bx4x4
+    dst_norm_trans_src_norm: Tensor = normalize_homography3d(M_4x4, size_src, size_out)  # Bx4x4
 
     src_norm_trans_dst_norm = _torch_inverse_cast(dst_norm_trans_src_norm)
-    P_norm: torch.Tensor = src_norm_trans_dst_norm[:, :3]  # Bx3x4
+    P_norm: Tensor = src_norm_trans_dst_norm[:, :3]  # Bx3x4
 
     # compute meshgrid and apply to input
     dsize_out: List[int] = [B, C] + list(size_out)
-    grid = torch.nn.functional.affine_grid(P_norm, dsize_out, align_corners=align_corners)
-    return torch.nn.functional.grid_sample(
-        src, grid, align_corners=align_corners, mode=flags, padding_mode=padding_mode
-    )
+    grid = F.affine_grid(P_norm, dsize_out, align_corners=align_corners)
+    return grid_sample(src, grid, align_corners=align_corners, mode=flags, padding_mode=padding_mode)
 
 
-def projection_from_Rt(rmat: torch.Tensor, tvec: torch.Tensor) -> torch.Tensor:
+def projection_from_Rt(rmat: Tensor, tvec: Tensor) -> Tensor:
     r"""Compute the projection matrix from Rotation and translation.
 
     .. warning::
         This API signature it is experimental and might suffer some changes in the future.
 
     Concatenates the batch of rotations and translations such that :math:`P = [R | t]`.
 
@@ -876,18 +874,18 @@
        the projection matrix with shape :math:`(*, 3, 4)`.
     """
     if not (len(rmat.shape) >= 2 and rmat.shape[-2:] == (3, 3)):
         raise AssertionError(rmat.shape)
     if not (len(tvec.shape) >= 2 and tvec.shape[-2:] == (3, 1)):
         raise AssertionError(tvec.shape)
 
-    return torch.cat([rmat, tvec], dim=-1)  # Bx3x4
+    return concatenate([rmat, tvec], -1)  # Bx3x4
 
 
-def get_projective_transform(center: torch.Tensor, angles: torch.Tensor, scales: torch.Tensor) -> torch.Tensor:
+def get_projective_transform(center: Tensor, angles: Tensor, scales: Tensor) -> Tensor:
     r"""Calculate the projection matrix for a 3D rotation.
 
     .. warning::
         This API signature it is experimental and might suffer some changes in the future.
 
     The function computes the projection matrix given the center and angles per axis.
 
@@ -910,22 +908,22 @@
         raise AssertionError(angles.shape)
     if center.device != angles.device:
         raise AssertionError(center.device, angles.device)
     if center.dtype != angles.dtype:
         raise AssertionError(center.dtype, angles.dtype)
 
     # create rotation matrix
-    angle_axis_rad: torch.Tensor = deg2rad(angles)
-    rmat: torch.Tensor = angle_axis_to_rotation_matrix(angle_axis_rad)  # Bx3x3
-    scaling_matrix: torch.Tensor = eye_like(3, rmat)
+    angle_axis_rad: Tensor = deg2rad(angles)
+    rmat: Tensor = angle_axis_to_rotation_matrix(angle_axis_rad)  # Bx3x3
+    scaling_matrix: Tensor = eye_like(3, rmat)
     scaling_matrix = scaling_matrix * scales.unsqueeze(dim=1)
     rmat = rmat @ scaling_matrix.to(rmat)
 
     # define matrix to move forth and back to origin
-    from_origin_mat = torch.eye(4)[None].repeat(rmat.shape[0], 1, 1).type_as(center)  # Bx4x4
+    from_origin_mat = eye_like(4, rmat, shared_memory=False)  # Bx4x4
     from_origin_mat[..., :3, -1] += center
 
     to_origin_mat = from_origin_mat.clone()
     to_origin_mat = _torch_inverse_cast(from_origin_mat)
 
     # append translation with zeros
     proj_mat = projection_from_Rt(rmat, torch.zeros_like(center)[..., None])  # Bx3x4
@@ -933,15 +931,15 @@
     # chain 4x4 transforms
     proj_mat = convert_affinematrix_to_homography3d(proj_mat)  # Bx4x4
     proj_mat = from_origin_mat @ proj_mat @ to_origin_mat
 
     return proj_mat[..., :3, :]  # Bx3x4
 
 
-def get_perspective_transform3d(src: torch.Tensor, dst: torch.Tensor) -> torch.Tensor:
+def get_perspective_transform3d(src: Tensor, dst: Tensor) -> Tensor:
     r"""Calculate a 3d perspective transform from four pairs of the corresponding points.
 
     The function calculates the matrix of a perspective transform so that:
 
     .. math ::
 
         \begin{bmatrix}
@@ -1002,19 +1000,19 @@
 
     Returns:
         the perspective transformation with shape :math:`(B, 4, 4)`.
 
     .. note::
         This function is often used in conjunction with :func:`warp_perspective3d`.
     """
-    if not isinstance(src, (torch.Tensor)):
-        raise TypeError(f"Input type is not a torch.Tensor. Got {type(src)}")
+    if not isinstance(src, (Tensor)):
+        raise TypeError(f"Input type is not a Tensor. Got {type(src)}")
 
-    if not isinstance(dst, (torch.Tensor)):
-        raise TypeError(f"Input type is not a torch.Tensor. Got {type(dst)}")
+    if not isinstance(dst, (Tensor)):
+        raise TypeError(f"Input type is not a Tensor. Got {type(dst)}")
 
     if not src.shape[-2:] == (8, 3):
         raise ValueError(f"Inputs must be a Bx8x3 tensor. Got {src.shape}")
 
     if not src.shape == dst.shape:
         raise ValueError(f"Inputs must have the same shape. Got {dst.shape}")
 
@@ -1035,18 +1033,18 @@
     # 000, 100, 110, 101, 011
     for i in [0, 1, 2, 5, 7]:
         p.append(_build_perspective_param3d(src[:, i], dst[:, i], 'x'))
         p.append(_build_perspective_param3d(src[:, i], dst[:, i], 'y'))
         p.append(_build_perspective_param3d(src[:, i], dst[:, i], 'z'))
 
     # A is Bx15x15
-    A = torch.stack(p, dim=1)
+    A = stack(p, 1)
 
     # b is a Bx15x1
-    b = torch.stack(
+    b = stack(
         [
             dst[:, 0:1, 0],
             dst[:, 0:1, 1],
             dst[:, 0:1, 2],
             dst[:, 1:2, 0],
             dst[:, 1:2, 1],
             dst[:, 1:2, 2],
@@ -1059,35 +1057,35 @@
             dst[:, 5:6, 1],
             dst[:, 5:6, 2],
             # dst[:, 6:7, 0], dst[:, 6:7, 1], dst[:, 6:7, 2],
             dst[:, 7:8, 0],
             dst[:, 7:8, 1],
             dst[:, 7:8, 2],
         ],
-        dim=1,
+        1,
     )
 
     # solve the system Ax = b
     X: Tensor = _torch_solve_cast(A, b)
 
     # create variable to return
     batch_size: int = src.shape[0]
     M = torch.empty(batch_size, 16, device=src.device, dtype=src.dtype)
     M[..., :15] = X[..., 0]
     M[..., -1].fill_(1)
 
     return M.view(-1, 4, 4)  # Bx4x4
 
 
-def _build_perspective_param3d(p: torch.Tensor, q: torch.Tensor, axis: str) -> torch.Tensor:
+def _build_perspective_param3d(p: Tensor, q: Tensor, axis: str) -> Tensor:
     ones = torch.ones_like(p)[..., 0:1]
     zeros = torch.zeros_like(p)[..., 0:1]
 
     if axis == 'x':
-        return torch.cat(
+        return concatenate(
             [
                 p[:, 0:1],
                 p[:, 1:2],
                 p[:, 2:3],
                 ones,
                 zeros,
                 zeros,
@@ -1097,19 +1095,19 @@
                 zeros,
                 zeros,
                 zeros,
                 -p[:, 0:1] * q[:, 0:1],
                 -p[:, 1:2] * q[:, 0:1],
                 -p[:, 2:3] * q[:, 0:1],
             ],
-            dim=1,
+            1,
         )
 
     if axis == 'y':
-        return torch.cat(
+        return concatenate(
             [
                 zeros,
                 zeros,
                 zeros,
                 zeros,
                 p[:, 0:1],
                 p[:, 1:2],
@@ -1119,19 +1117,19 @@
                 zeros,
                 zeros,
                 zeros,
                 -p[:, 0:1] * q[:, 1:2],
                 -p[:, 1:2] * q[:, 1:2],
                 -p[:, 2:3] * q[:, 1:2],
             ],
-            dim=1,
+            1,
         )
 
     if axis == 'z':
-        return torch.cat(
+        return concatenate(
             [
                 zeros,
                 zeros,
                 zeros,
                 zeros,
                 zeros,
                 zeros,
@@ -1141,28 +1139,28 @@
                 p[:, 1:2],
                 p[:, 2:3],
                 ones,
                 -p[:, 0:1] * q[:, 2:3],
                 -p[:, 1:2] * q[:, 2:3],
                 -p[:, 2:3] * q[:, 2:3],
             ],
-            dim=1,
+            1,
         )
 
     raise NotImplementedError(f"perspective params for axis `{axis}` is not implemented.")
 
 
 def warp_perspective3d(
-    src: torch.Tensor,
-    M: torch.Tensor,
+    src: Tensor,
+    M: Tensor,
     dsize: Tuple[int, int, int],
     flags: str = 'bilinear',
     border_mode: str = 'zeros',
     align_corners: bool = False,
-) -> torch.Tensor:
+) -> Tensor:
     r"""Apply a perspective transformation to an image.
 
     The function warp_perspective transforms the source image using
     the specified matrix:
 
     .. math::
         \text{dst} (x, y) = \text{src} \left(
@@ -1182,41 +1180,41 @@
 
     Returns:
         the warped input image :math:`(B, C, D, H, W)`.
 
     .. note::
         This function is often used in conjunction with :func:`get_perspective_transform3d`.
     """
-    if not isinstance(src, torch.Tensor):
-        raise TypeError(f"Input src type is not a torch.Tensor. Got {type(src)}")
+    if not isinstance(src, Tensor):
+        raise TypeError(f"Input src type is not a Tensor. Got {type(src)}")
 
-    if not isinstance(M, torch.Tensor):
-        raise TypeError(f"Input M type is not a torch.Tensor. Got {type(M)}")
+    if not isinstance(M, Tensor):
+        raise TypeError(f"Input M type is not a Tensor. Got {type(M)}")
 
     if not len(src.shape) == 5:
         raise ValueError(f"Input src must be a BxCxDxHxW tensor. Got {src.shape}")
 
     if not (len(M.shape) == 3 or M.shape[-2:] == (4, 4)):
         raise ValueError(f"Input M must be a Bx4x4 tensor. Got {M.shape}")
 
     # launches the warper
     d, h, w = src.shape[-3:]
     return _transform_warp_impl3d(src, M, (d, h, w), dsize, flags, border_mode, align_corners)
 
 
 def homography_warp(
-    patch_src: torch.Tensor,
-    src_homo_dst: torch.Tensor,
+    patch_src: Tensor,
+    src_homo_dst: Tensor,
     dsize: Tuple[int, int],
     mode: str = 'bilinear',
     padding_mode: str = 'zeros',
     align_corners: bool = False,
     normalized_coordinates: bool = True,
     normalized_homography: bool = True,
-) -> torch.Tensor:
+) -> Tensor:
     r"""Warp image patches or tensors by normalized 2D homographies.
 
     See :class:`~kornia.geometry.warp.HomographyWarper` for details.
 
     Args:
         patch_src: The image or tensor to warp. Should be from source of shape :math:`(N, C, H, W)`.
         src_homo_dst: The homography or stack of homographies from destination to source of shape :math:`(N, 3, 3)`.
@@ -1261,38 +1259,38 @@
         return F.grid_sample(patch_src, warped_grid, mode=mode, padding_mode=padding_mode, align_corners=align_corners)
     return warp_perspective(
         patch_src, src_homo_dst, dsize, mode='bilinear', padding_mode=padding_mode, align_corners=True
     )
 
 
 def _transform_warp_impl3d(
-    src: torch.Tensor,
-    dst_pix_trans_src_pix: torch.Tensor,
+    src: Tensor,
+    dst_pix_trans_src_pix: Tensor,
     dsize_src: Tuple[int, int, int],
     dsize_dst: Tuple[int, int, int],
     grid_mode: str,
     padding_mode: str,
     align_corners: bool,
-) -> torch.Tensor:
+) -> Tensor:
     """Compute the transform in normalized coordinates and perform the warping."""
-    dst_norm_trans_src_norm: torch.Tensor = normalize_homography3d(dst_pix_trans_src_pix, dsize_src, dsize_dst)
+    dst_norm_trans_src_norm: Tensor = normalize_homography3d(dst_pix_trans_src_pix, dsize_src, dsize_dst)
 
     src_norm_trans_dst_norm = torch.inverse(dst_norm_trans_src_norm)
     return homography_warp3d(src, src_norm_trans_dst_norm, dsize_dst, grid_mode, padding_mode, align_corners, True)
 
 
 def homography_warp3d(
-    patch_src: torch.Tensor,
-    src_homo_dst: torch.Tensor,
+    patch_src: Tensor,
+    src_homo_dst: Tensor,
     dsize: Tuple[int, int, int],
     mode: str = 'bilinear',
     padding_mode: str = 'zeros',
     align_corners: bool = False,
     normalized_coordinates: bool = True,
-) -> torch.Tensor:
+) -> Tensor:
     r"""Warp image patches or tensors by normalized 3D homographies.
 
     Args:
         patch_src: The image or tensor to warp. Should be from source of shape :math:`(N, C, D, H, W)`.
         src_homo_dst: The homography or stack of homographies from destination to source of shape
           :math:`(N, 4, 4)`.
         dsize: The height and width of the image to warp.
```

### Comparing `kornia-0.6.8/kornia/geometry/transform/pyramid.py` & `kornia-0.6.9/kornia/geometry/transform/pyramid.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,40 +1,39 @@
 import math
 from typing import List, Tuple
 
 import torch
-import torch.nn as nn
 import torch.nn.functional as F
 
-from kornia.core import Tensor
+from kornia.core import Module, Tensor, pad, stack, tensor
 from kornia.filters import filter2d, gaussian_blur2d
 from kornia.testing import KORNIA_CHECK, KORNIA_CHECK_SHAPE
 
 __all__ = ["PyrDown", "PyrUp", "ScalePyramid", "pyrdown", "pyrup", "build_pyramid", "build_laplacian_pyramid"]
 
 
 def _get_pyramid_gaussian_kernel() -> Tensor:
     """Utility function that return a pre-computed gaussian kernel."""
     return (
-        torch.tensor(
+        tensor(
             [
                 [
                     [1.0, 4.0, 6.0, 4.0, 1.0],
                     [4.0, 16.0, 24.0, 16.0, 4.0],
                     [6.0, 24.0, 36.0, 24.0, 6.0],
                     [4.0, 16.0, 24.0, 16.0, 4.0],
                     [1.0, 4.0, 6.0, 4.0, 1.0],
                 ]
             ]
         )
         / 256.0
     )
 
 
-class PyrDown(nn.Module):
+class PyrDown(Module):
     r"""Blur a tensor and downsamples it.
 
     Args:
         border_type: the padding mode to be applied before convolving.
           The expected modes are: ``'constant'``, ``'reflect'``,
           ``'replicate'`` or ``'circular'``.
         align_corners: interpolation flag.
@@ -58,15 +57,15 @@
         self.align_corners: bool = align_corners
         self.factor: float = factor
 
     def forward(self, input: Tensor) -> Tensor:
         return pyrdown(input, self.border_type, self.align_corners, self.factor)
 
 
-class PyrUp(nn.Module):
+class PyrUp(Module):
     r"""Upsample a tensor and then blurs it.
 
     Args:
         borde_type: the padding mode to be applied before convolving.
           The expected modes are: ``'constant'``, ``'reflect'``,
           ``'replicate'`` or ``'circular'``.
         align_corners: interpolation flag.
@@ -88,15 +87,15 @@
         self.border_type: str = border_type
         self.align_corners: bool = align_corners
 
     def forward(self, input: Tensor) -> Tensor:
         return pyrup(input, self.border_type, self.align_corners)
 
 
-class ScalePyramid(nn.Module):
+class ScalePyramid(Module):
     r"""Create an scale pyramid of image, usually used for local feature detection.
 
     Images are consequently smoothed with Gaussian blur and downscaled.
 
     Args:
         n_levels: number of the levels in octave.
         init_sigma: initial blur level.
@@ -163,34 +162,35 @@
         #  PyTorch does not allow to pad more than original size.
         #  Therefore there is a hack in forward function
 
         if ksize % 2 == 0:
             ksize += 1
         return ksize
 
-    def get_first_level(self, input):
+    def get_first_level(self, input: Tensor) -> Tuple[Tensor, float, float]:
         pixel_distance = 1.0
         cur_sigma = 0.5
         # Same as in OpenCV up to interpolation difference
         if self.double_image:
             x = F.interpolate(input, scale_factor=2.0, mode='bilinear', align_corners=False)
             pixel_distance = 0.5
             cur_sigma *= 2.0
         else:
             x = input
+
         if self.init_sigma > cur_sigma:
             sigma = max(math.sqrt(self.init_sigma**2 - cur_sigma**2), 0.01)
             ksize = self.get_kernel_size(sigma)
             cur_level = gaussian_blur2d(x, (ksize, ksize), (sigma, sigma))
             cur_sigma = self.init_sigma
         else:
             cur_level = x
         return cur_level, cur_sigma, pixel_distance
 
-    def forward(self, x: Tensor) -> Tuple[List, List, List]:  # type: ignore
+    def forward(self, x: Tensor) -> Tuple[List[Tensor], List[Tensor], List[Tensor]]:
         bs, _, _, _ = x.size()
         cur_level, cur_sigma, pixel_distance = self.get_first_level(x)
 
         sigmas = [cur_sigma * torch.ones(bs, self.n_levels + self.extra_levels).to(x.device).to(x.dtype)]
         pixel_dists = [pixel_distance * torch.ones(bs, self.n_levels + self.extra_levels).to(x.device).to(x.dtype)]
         pyr = [[cur_level]]
         oct_idx = 0
@@ -220,17 +220,18 @@
             cur_sigma = self.init_sigma
             if min(nextOctaveFirstLevel.size(2), nextOctaveFirstLevel.size(3)) <= self.min_size:
                 break
             pyr.append([nextOctaveFirstLevel])
             sigmas.append(cur_sigma * torch.ones(bs, self.n_levels + self.extra_levels).to(x.device))
             pixel_dists.append(pixel_distance * torch.ones(bs, self.n_levels + self.extra_levels).to(x.device))
             oct_idx += 1
-        for i in range(len(pyr)):
-            pyr[i] = torch.stack(pyr[i], dim=2)  # type: ignore
-        return pyr, sigmas, pixel_dists
+
+        output_pyr = [stack(i, 2) for i in pyr]
+
+        return output_pyr, sigmas, pixel_dists
 
 
 def pyrdown(input: Tensor, border_type: str = 'reflect', align_corners: bool = False, factor: float = 2.0) -> Tensor:
     r"""Blur a tensor and downsamples it.
 
     .. image:: _static/img/pyrdown.png
 
@@ -390,16 +391,16 @@
     h = input.size()[2]
     w = input.size()[3]
     require_padding = not (is_powerof_two(w) or is_powerof_two(h))
 
     if require_padding:
         # in case of arbitrary shape tensor image need to be padded.
         # Reference: https://stackoverflow.com/a/29967555
-        pad = (0, find_next_powerof_two(w) - w, 0, find_next_powerof_two(h) - h)
-        input = F.pad(input, pad, "reflect")
+        padding = (0, find_next_powerof_two(w) - w, 0, find_next_powerof_two(h) - h)
+        input = pad(input, padding, "reflect")
 
     # create gaussian pyramid
     gaussian_pyramid: List[Tensor] = build_pyramid(input, max_level, border_type, align_corners)
     # create empty list
     laplacian_pyramid: List[Tensor] = []
 
     # iterate and compute difference of adjacent layers in a gaussian pyramid
```

### Comparing `kornia-0.6.8/kornia/geometry/transform/thin_plate_spline.py` & `kornia-0.6.9/kornia/geometry/transform/thin_plate_spline.py`

 * *Files identical despite different names*

### Comparing `kornia-0.6.8/kornia/grad_estimator/ste.py` & `kornia-0.6.9/kornia/grad_estimator/ste.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-from typing import Callable, Optional, Tuple
+from typing import Any, Callable, Optional, Tuple
 
 import torch.nn as nn
 from torch import Tensor
 from torch.autograd import Function
 
 __all__ = ["STEFunction", "StraightThroughEstimator"]
 
@@ -32,24 +32,24 @@
         >>> loss = out_est.mean()
         >>> loss.backward()
         >>> input.grad
         tensor([0.2500, 0.2500, 0.2500, 0.2500])
     """
 
     @staticmethod
-    def forward(  # type:ignore
-        ctx, input: Tensor, output: Tensor, grad_fn: Optional[Callable] = None
+    def forward(  # type: ignore[override]
+        ctx: Any, input: Tensor, output: Tensor, grad_fn: Optional[Callable[..., Any]] = None
     ) -> Tensor:
         ctx.in_shape = input.shape
         ctx.out_shape = output.shape
         ctx.grad_fn = grad_fn
         return output
 
     @staticmethod
-    def backward(ctx, grad_output: Tensor) -> Tuple[Tensor, Tensor, None]:  # type:ignore
+    def backward(ctx: Any, grad_output: Tensor) -> Tuple[Tensor, Tensor, None]:  # type: ignore[override]
         if ctx.grad_fn is None:
             return grad_output.sum_to_size(ctx.in_shape), grad_output.sum_to_size(ctx.out_shape), None
         return (
             ctx.grad_fn(grad_output.sum_to_size(ctx.in_shape)),
             ctx.grad_fn(grad_output.sum_to_size(ctx.out_shape)),
             None,
         )
@@ -101,15 +101,15 @@
         >>> input.grad
         tensor([[[[0.0422, 0.0626, 0.0566, 0.0422],
                   [0.0566, 0.0626, 0.0626, 0.0626],
                   [0.0626, 0.0626, 0.0626, 0.0566],
                   [0.0422, 0.0566, 0.0626, 0.0422]]]])
     """
 
-    def __init__(self, target_fn: nn.Module, grad_fn: Optional[Callable] = None):
+    def __init__(self, target_fn: nn.Module, grad_fn: Optional[Callable[..., Any]] = None):
         super().__init__()
         self.target_fn = target_fn
         self.grad_fn = grad_fn
 
     def __repr__(self):
         return f"{self.__class__.__name__}(target_fn={self.target_fn}, grad_fn={self.grad_fn})"
```

### Comparing `kornia-0.6.8/kornia/io/io.py` & `kornia-0.6.9/kornia/io/io.py`

 * *Files 2% similar despite different names*

```diff
@@ -3,14 +3,15 @@
 except ImportError:
     kornia_rs = None
 
 import os
 from enum import Enum
 
 import torch
+from torch.utils import dlpack  # TODO: remove this  if kornia relies on torch>=1.10
 
 from kornia.color import rgb_to_grayscale, rgba_to_rgb
 from kornia.color.gray import grayscale_to_rgb
 from kornia.color.rgb import rgb_to_rgba
 from kornia.core import Tensor
 from kornia.testing import KORNIA_CHECK
 
@@ -30,15 +31,15 @@
     # combines image-rs a self maintained version of the dlpack-rs. After the decoding,
     # the obtained stream bits are encapusalted to a cv::Tensor data structure without
     # memory ownership and passed as PyCapsule from rust to python.
     cv_tensor = kornia_rs.read_image_rs(path_file)
     # for convenience use the torch dlpack parser to get a zero copy torch.Tensor
     # TODO: evaluate other potential API so that we can return in numpy, jax, mxnet since
     # the kornia_rs cv::Tensor has this ability.
-    th_tensor = torch.utils.dlpack.from_dlpack(cv_tensor)  # type: ignore # HxWx3
+    th_tensor = dlpack.from_dlpack(cv_tensor)  # HxWx3
     # move the tensor to the desired device, move the data layout to CHW and clone
     # to return an owned data tensor.
     return th_tensor.to(torch.device(device)).permute(2, 0, 1).clone()  # CxHxW
 
 
 def to_float32(image: Tensor) -> Tensor:
     KORNIA_CHECK(image.dtype == torch.uint8)
```

### Comparing `kornia-0.6.8/kornia/losses/__init__.py` & `kornia-0.6.9/kornia/losses/__init__.py`

 * *Files identical despite different names*

### Comparing `kornia-0.6.8/kornia/losses/depth_smooth.py` & `kornia-0.6.9/kornia/losses/depth_smooth.py`

 * *Files identical despite different names*

### Comparing `kornia-0.6.8/kornia/losses/dice.py` & `kornia-0.6.9/kornia/losses/dice.py`

 * *Files 1% similar despite different names*

```diff
@@ -11,15 +11,15 @@
 def dice_loss(input: torch.Tensor, target: torch.Tensor, eps: float = 1e-8) -> torch.Tensor:
     r"""Criterion that computes Srensen-Dice Coefficient loss.
 
     According to [1], we compute the Srensen-Dice Coefficient as follows:
 
     .. math::
 
-        \text{Dice}(x, class) = \frac{2 |X| \cap |Y|}{|X| + |Y|}
+        \text{Dice}(x, class) = \frac{2 |X \cap Y|}{|X| + |Y|}
 
     Where:
        - :math:`X` expects to be the scores of each class.
        - :math:`Y` expects to be the one-hot tensor with the class labels.
 
     the loss, is finally computed as:
```

### Comparing `kornia-0.6.8/kornia/losses/divergence.py` & `kornia-0.6.9/kornia/losses/divergence.py`

 * *Files identical despite different names*

### Comparing `kornia-0.6.8/kornia/losses/focal.py` & `kornia-0.6.9/kornia/losses/focal.py`

 * *Files identical despite different names*

### Comparing `kornia-0.6.8/kornia/losses/hausdorff.py` & `kornia-0.6.9/kornia/losses/hausdorff.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,12 +1,14 @@
 from typing import Callable
 
 import torch
 import torch.nn as nn
 
+from kornia.core import Tensor, as_tensor, stack, tensor, where, zeros_like
+
 
 class _HausdorffERLossBase(torch.jit.ScriptModule):
     """Base class for binary Hausdorff loss based on morphological erosion.
 
     This is an Hausdorff Distance (HD) Loss that based on morphological erosion,which provided
     a differentiable approximation of Hausdorff distance as stated in :cite:`karimi2019reducing`.
     The code is refactored on top of `here <https://github.com/PatRyg99/HausdorffLoss/
@@ -19,33 +21,33 @@
             'none': no reduction will be applied, 'mean': the weighted mean of the output is taken,
             'sum': the output will be summed.
 
     Returns:
         Estimated Hausdorff Loss.
     """
 
-    conv: Callable
-    max_pool: Callable
+    conv: Callable[..., Tensor]
+    max_pool: Callable[..., Tensor]
 
     def __init__(self, alpha: float = 2.0, k: int = 10, reduction: str = 'mean') -> None:
         super().__init__()
         self.alpha = alpha
         self.k = k
         self.reduction = reduction
         self.register_buffer("kernel", self.get_kernel())
 
-    def get_kernel(self) -> torch.Tensor:
+    def get_kernel(self) -> Tensor:
         """Get kernel for image morphology convolution."""
         raise NotImplementedError
 
-    def perform_erosion(self, pred: torch.Tensor, target: torch.Tensor) -> torch.Tensor:
+    def perform_erosion(self, pred: Tensor, target: Tensor) -> Tensor:
         bound = (pred - target) ** 2
 
-        kernel = torch.as_tensor(self.kernel, device=pred.device, dtype=pred.dtype)
-        eroded = torch.zeros_like(bound, device=pred.device, dtype=pred.dtype)
+        kernel = as_tensor(self.kernel, device=pred.device, dtype=pred.dtype)
+        eroded = zeros_like(bound, device=pred.device, dtype=pred.dtype)
         mask = torch.ones_like(bound, device=pred.device, dtype=torch.bool)
 
         # Same padding, assuming kernel is odd and square (cube) shaped.
         # NOTE: int() has to be added for enabling JIT.
         padding = int((kernel.size(-1) - 1) // 2)
         for k in range(self.k):
             # compute convolution with kernel
@@ -61,24 +63,24 @@
             _to_norm = (erosion_max - erosion_min) != 0
             to_norm = _to_norm.squeeze()
             if to_norm.any():
                 # NOTE: avoid in-place ops like below, which will not pass gradcheck:
                 #       erosion[to_norm] = (erosion[to_norm] - erosion_min[to_norm]) / (
                 #           erosion_max[to_norm] - erosion_min[to_norm])
                 _erosion_to_fill = (erosion - erosion_min) / (erosion_max - erosion_min)
-                erosion = torch.where(mask * _to_norm, _erosion_to_fill, erosion)
+                erosion = where(mask * _to_norm, _erosion_to_fill, erosion)
 
             # save erosion and add to loss
             eroded = eroded + erosion * (k + 1) ** self.alpha
             bound = erosion
 
         return eroded
 
     # NOTE: we add type ignore because the forward pass does not work well with subclassing
-    def forward(self, pred: torch.Tensor, target: torch.Tensor) -> torch.Tensor:  # type: ignore
+    def forward(self, pred: Tensor, target: Tensor) -> Tensor:  # type: ignore[override]
         """Compute Hausdorff loss.
 
         Args:
             pred: predicted tensor with a shape of :math:`(B, C, H, W)` or :math:`(B, C, D, H, W)`.
                 Each channel is as binary as: 1 -> fg, 0 -> bg.
             target: target tensor with a shape of :math:`(B, 1, H, W)` or :math:`(B, C, D, H, W)`.
 
@@ -90,22 +92,22 @@
                 "Prediction and target need to be of same size, and target should not be one-hot."
                 f"Got {pred.shape} and {target.shape}."
             )
 
         if pred.size(1) < target.max().item():
             raise ValueError("Invalid target value.")
 
-        out = torch.stack(
+        out = stack(
             [
                 self.perform_erosion(
                     pred[:, i : i + 1],
-                    torch.where(
+                    where(
                         target == i,
-                        torch.tensor(1, device=target.device, dtype=target.dtype),
-                        torch.tensor(0, device=target.device, dtype=target.dtype),
+                        tensor(1, device=target.device, dtype=target.dtype),
+                        tensor(0, device=target.device, dtype=target.dtype),
                     ),
                 )
                 for i in range(pred.size(1))
             ]
         )
 
         if self.reduction == 'mean':
@@ -155,22 +157,22 @@
         >>> target = (torch.rand(5, 1, 20, 20) * 2).long()
         >>> res = hdloss(input, target)
     """
 
     conv = torch.conv2d
     max_pool = nn.AdaptiveMaxPool2d(1)
 
-    def get_kernel(self) -> torch.Tensor:
+    def get_kernel(self) -> Tensor:
         """Get kernel for image morphology convolution."""
-        cross = torch.tensor([[[0, 1, 0], [1, 1, 1], [0, 1, 0]]])
+        cross = tensor([[[0, 1, 0], [1, 1, 1], [0, 1, 0]]])
         kernel = cross * 0.2
         return kernel[None]
 
     # NOTE: we add type ignore because the forward pass does not work well with subclassing
-    def forward(self, pred: torch.Tensor, target: torch.Tensor) -> torch.Tensor:  # type: ignore
+    def forward(self, pred: Tensor, target: Tensor) -> Tensor:  # type: ignore[override]
         """Compute Hausdorff loss.
 
         Args:
             pred: predicted tensor with a shape of :math:`(B, C, H, W)`.
                 Each channel is as binary as: 1 -> fg, 0 -> bg.
             target: target tensor with a shape of :math:`(B, 1, H, W)`.
 
@@ -222,26 +224,26 @@
         >>> target = (torch.rand(5, 1, 20, 20, 20) * 2).long()
         >>> res = hdloss(input, target)
     """
 
     conv = torch.conv3d
     max_pool = nn.AdaptiveMaxPool3d(1)
 
-    def get_kernel(self) -> torch.Tensor:
+    def get_kernel(self) -> Tensor:
         """Get kernel for image morphology convolution."""
-        cross = torch.tensor([[[0, 1, 0], [1, 1, 1], [0, 1, 0]]])
-        bound = torch.tensor([[[0, 0, 0], [0, 1, 0], [0, 0, 0]]])
+        cross = tensor([[[0, 1, 0], [1, 1, 1], [0, 1, 0]]])
+        bound = tensor([[[0, 0, 0], [0, 1, 0], [0, 0, 0]]])
         # NOTE: The original repo claimed it shaped as (3, 1, 3, 3)
         #    which Jian suspect it is wrongly implemented.
         # https://github.com/PatRyg99/HausdorffLoss/blob/9f580acd421af648e74b45d46555ccb7a876c27c/hausdorff_loss.py#L94
-        kernel = torch.stack([bound, cross, bound], dim=1) * (1 / 7)
+        kernel = stack([bound, cross, bound], 1) * (1 / 7)
         return kernel[None]
 
     # NOTE: we add type ignore because the forward pass does not work well with subclassing
-    def forward(self, pred: torch.Tensor, target: torch.Tensor) -> torch.Tensor:  # type: ignore
+    def forward(self, pred: Tensor, target: Tensor) -> Tensor:  # type: ignore[override]
         """Compute 3D Hausdorff loss.
 
         Args:
             pred: predicted tensor with a shape of :math:`(B, C, D, H, W)`.
                 Each channel is as binary as: 1 -> fg, 0 -> bg.
             target: target tensor with a shape of :math:`(B, 1, D, H, W)`.
```

### Comparing `kornia-0.6.8/kornia/losses/lovasz_hinge.py` & `kornia-0.6.9/kornia/losses/lovasz_hinge.py`

 * *Files identical despite different names*

### Comparing `kornia-0.6.8/kornia/losses/lovasz_softmax.py` & `kornia-0.6.9/kornia/losses/lovasz_softmax.py`

 * *Files identical despite different names*

### Comparing `kornia-0.6.8/kornia/losses/ms_ssim.py` & `kornia-0.6.9/kornia/losses/ms_ssim.py`

 * *Files identical despite different names*

### Comparing `kornia-0.6.8/kornia/losses/psnr.py` & `kornia-0.6.9/kornia/losses/psnr.py`

 * *Files identical despite different names*

### Comparing `kornia-0.6.8/kornia/losses/ssim.py` & `kornia-0.6.9/kornia/losses/ssim.py`

 * *Files identical despite different names*

### Comparing `kornia-0.6.8/kornia/losses/total_variation.py` & `kornia-0.6.9/kornia/losses/total_variation.py`

 * *Files 9% similar despite different names*

```diff
@@ -25,14 +25,18 @@
        total_variation_denoising.html>`__.
        Total Variation is formulated with summation, however this is not resolution invariant.
        Thus, `reduction='mean'` was added as an optional reduction method.
 
     Reference:
         [1] https://en.wikipedia.org/wiki/Total_variation
     """
+    # TODO: here torchscript doesn't like KORNIA_CHECK_TYPE
+    if not isinstance(img, Tensor):
+        raise TypeError(f"Not a Tensor type. Got: {type(img)}")
+
     KORNIA_CHECK_SHAPE(img, ["*", "H", "W"])
     KORNIA_CHECK(reduction in ("mean", "sum"), f"Expected reduction to be one of 'mean'/'sum', but got '{reduction}'.")
 
     pixel_dif1 = img[..., 1:, :] - img[..., :-1, :]
     pixel_dif2 = img[..., :, 1:] - img[..., :, :-1]
 
     res1 = pixel_dif1.abs()
```

### Comparing `kornia-0.6.8/kornia/losses/tversky.py` & `kornia-0.6.9/kornia/losses/tversky.py`

 * *Files identical despite different names*

### Comparing `kornia-0.6.8/kornia/metrics/accuracy.py` & `kornia-0.6.9/kornia/metrics/accuracy.py`

 * *Files identical despite different names*

### Comparing `kornia-0.6.8/kornia/metrics/average_meter.py` & `kornia-0.6.9/kornia/metrics/average_meter.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,30 +1,35 @@
 from typing import Union
 
-import torch
+from kornia.core import Tensor
 
 
 class AverageMeter:
     """Computes and stores the average and current value.
 
     Example:
         >>> stats = AverageMeter()
         >>> acc1 = torch.tensor(0.99) # coming from K.metrics.accuracy
         >>> stats.update(acc1, n=1)  # where n is batch size usually
         >>> stats.avg
         tensor(0.9900)
     """
 
+    val: Union[int, float, bool, Tensor]
+    avg: Union[int, float, Tensor]
+    sum: Union[int, float, Tensor]
+    count: int
+
     def __init__(self) -> None:
         self.reset()
 
     def reset(self):
         self.val = 0
         self.avg = 0
         self.sum = 0
         self.count = 0
 
-    def update(self, val: Union[int, float, bool, torch.Tensor], n: int = 1) -> None:
+    def update(self, val: Union[int, float, bool, Tensor], n: int = 1) -> None:
         self.val = val
         self.sum += val * n
         self.count += n
         self.avg = self.sum / self.count
```

### Comparing `kornia-0.6.8/kornia/metrics/confusion_matrix.py` & `kornia-0.6.9/kornia/metrics/confusion_matrix.py`

 * *Files identical despite different names*

### Comparing `kornia-0.6.8/kornia/metrics/mean_average_precision.py` & `kornia-0.6.9/kornia/metrics/mean_average_precision.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,23 +1,25 @@
-from typing import List, Tuple
+from typing import Dict, List, Tuple
 
 import torch
 
+from kornia.core import Tensor, concatenate, tensor, zeros
+
 from .mean_iou import mean_iou_bbox
 
 
 def mean_average_precision(
-    pred_boxes: List[torch.Tensor],
-    pred_labels: List[torch.Tensor],
-    pred_scores: List[torch.Tensor],
-    gt_boxes: List[torch.Tensor],
-    gt_labels: List[torch.Tensor],
+    pred_boxes: List[Tensor],
+    pred_labels: List[Tensor],
+    pred_scores: List[Tensor],
+    gt_boxes: List[Tensor],
+    gt_labels: List[Tensor],
     n_classes: int,
     threshold: float = 0.5,
-) -> Tuple[torch.Tensor, dict]:
+) -> Tuple[Tensor, Dict[int, float]]:
     """Calculate the Mean Average Precision (mAP) of detected objects.
 
     Code altered from https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Object-Detection/blob/master/utils.py#L271.
     Background class (0 index) is excluded.
 
     Args:
         pred_boxes: a tensor list of predicted bounding boxes.
@@ -42,47 +44,43 @@
         raise AssertionError
 
     # Store all (true) objects in a single continuous tensor while keeping track of the image it is from
     gt_images = []
     for i, labels in enumerate(gt_labels):
         gt_images.extend([i] * labels.size(0))
     # (n_objects), n_objects is the total no. of objects across all images
-    _gt_boxes = torch.cat(gt_boxes, dim=0)  # (n_objects, 4)
-    _gt_labels = torch.cat(gt_labels, dim=0)  # (n_objects)
-    _gt_images = torch.tensor(gt_images, device=_gt_boxes.device, dtype=torch.long)
+    _gt_boxes = concatenate(gt_boxes, 0)  # (n_objects, 4)
+    _gt_labels = concatenate(gt_labels, 0)  # (n_objects)
+    _gt_images = tensor(gt_images, device=_gt_boxes.device, dtype=torch.long)
 
     if not _gt_images.size(0) == _gt_boxes.size(0) == _gt_labels.size(0):
         raise AssertionError
 
     # Store all detections in a single continuous tensor while keeping track of the image it is from
     pred_images = []
     for i, labels in enumerate(pred_labels):
         pred_images.extend([i] * labels.size(0))
-    _pred_boxes = torch.cat(pred_boxes, dim=0)  # (n_detections, 4)
-    _pred_labels = torch.cat(pred_labels, dim=0)  # (n_detections)
-    _pred_scores = torch.cat(pred_scores, dim=0)  # (n_detections)
-    _pred_images = torch.tensor(pred_images, device=_pred_boxes.device, dtype=torch.long)  # (n_detections)
+    _pred_boxes = concatenate(pred_boxes, 0)  # (n_detections, 4)
+    _pred_labels = concatenate(pred_labels, 0)  # (n_detections)
+    _pred_scores = concatenate(pred_scores, 0)  # (n_detections)
+    _pred_images = tensor(pred_images, device=_pred_boxes.device, dtype=torch.long)  # (n_detections)
 
     if not _pred_images.size(0) == _pred_boxes.size(0) == _pred_labels.size(0) == _pred_scores.size(0):
         raise AssertionError
 
     # Calculate APs for each class (except background)
-    average_precisions = torch.zeros(
-        (n_classes - 1), device=_pred_boxes.device, dtype=_pred_boxes.dtype
-    )  # (n_classes - 1)
+    average_precisions = zeros((n_classes - 1), device=_pred_boxes.device, dtype=_pred_boxes.dtype)  # (n_classes - 1)
     for c in range(1, n_classes):
         # Extract only objects with this class
         gt_class_images = _gt_images[_gt_labels == c]  # (n_class_objects)
         gt_class_boxes = _gt_boxes[_gt_labels == c]  # (n_class_objects, 4)
 
         # Keep track of which true objects with this class have already been 'detected'
         # (n_class_objects)
-        gt_class_boxes_detected = torch.zeros(
-            (gt_class_images.size(0)), dtype=torch.uint8, device=gt_class_images.device
-        )
+        gt_class_boxes_detected = zeros((gt_class_images.size(0)), dtype=torch.uint8, device=gt_class_images.device)
 
         # Extract only detections with this class
         pred_class_images = _pred_images[_pred_labels == c]  # (n_class_detections)
         pred_class_boxes = _pred_boxes[_pred_labels == c]  # (n_class_detections, 4)
         pred_class_scores = _pred_scores[_pred_labels == c]  # (n_class_detections)
         n_class_detections = pred_class_boxes.size(0)
         if n_class_detections == 0:
@@ -90,18 +88,18 @@
 
         # Sort detections in decreasing order of confidence/scores
         pred_class_scores, sort_ind = torch.sort(pred_class_scores, dim=0, descending=True)  # (n_class_detections)
         pred_class_images = pred_class_images[sort_ind]  # (n_class_detections)
         pred_class_boxes = pred_class_boxes[sort_ind]  # (n_class_detections, 4)
 
         # In the order of decreasing scores, check if true or false positive
-        gt_positives = torch.zeros(
+        gt_positives = zeros(
             (n_class_detections,), dtype=pred_class_boxes.dtype, device=pred_class_boxes.device
         )  # (n_class_detections)
-        false_positives = torch.zeros(
+        false_positives = zeros(
             (n_class_detections,), dtype=pred_class_boxes.dtype, device=pred_class_boxes.device
         )  # (n_class_detections)
         for d in range(n_class_detections):
             this_detection_box = pred_class_boxes[d].unsqueeze(0)  # (1, 4)
             this_image = pred_class_images[d]  # (), scalar
 
             # Find objects in the image with this class, their difficulties, and whether they have been detected before
@@ -113,15 +111,15 @@
 
             # Find maximum overlap of this detection with objects in this image of this class
             overlaps = mean_iou_bbox(this_detection_box, object_boxes)  # (1, n_class_objects_in_img)
             max_overlap, ind = torch.max(overlaps.squeeze(0), dim=0)  # (), () - scalars
 
             # 'ind' is the index of the object in these image-level tensors 'object_boxes', 'object_difficulties'
             # In the original class-level tensors 'gt_class_boxes', etc., 'ind' corresponds to object with index...
-            original_ind = torch.tensor(
+            original_ind = tensor(
                 range(gt_class_boxes.size(0)), device=gt_class_boxes_detected.device, dtype=torch.long
             )[gt_class_images == this_image][ind]
             # We need 'original_ind' to update 'gt_class_boxes_detected'
 
             # If the maximum overlap is greater than the threshold of 0.5, it's a match
             if max_overlap.item() > threshold:
                 # If this object has already not been detected, it's a true positive
@@ -141,23 +139,23 @@
         cumul_precision = cumul_gt_positives / (
             cumul_gt_positives + cumul_false_positives + 1e-10
         )  # (n_class_detections)
         cumul_recall = cumul_gt_positives / _gt_boxes.size(0)  # (n_class_detections)
 
         # Find the mean of the maximum of the precisions corresponding to recalls above the threshold 't'
         recall_thresholds = torch.arange(start=0, end=1.1, step=0.1).tolist()  # (11)
-        precisions = torch.zeros((len(recall_thresholds)), device=_gt_boxes.device, dtype=_gt_boxes.dtype)  # (11)
+        precisions = zeros((len(recall_thresholds)), device=_gt_boxes.device, dtype=_gt_boxes.dtype)  # (11)
         for i, t in enumerate(recall_thresholds):
             recalls_above_t = cumul_recall >= t
             if recalls_above_t.any():
                 precisions[i] = cumul_precision[recalls_above_t].max()
             else:
                 precisions[i] = 0.0
         average_precisions[c - 1] = precisions.mean()  # c is in [1, n_classes - 1]
 
     # Calculate Mean Average Precision (mAP)
     mean_ap = average_precisions.mean()
 
     # Keep class-wise average precisions in a dictionary
-    ap_dict = {c + 1: v for c, v in enumerate(average_precisions.tolist())}
+    ap_dict = {c + 1: float(v) for c, v in enumerate(average_precisions.tolist())}
 
     return mean_ap, ap_dict
```

### Comparing `kornia-0.6.8/kornia/metrics/mean_iou.py` & `kornia-0.6.9/kornia/metrics/mean_iou.py`

 * *Files identical despite different names*

### Comparing `kornia-0.6.8/kornia/metrics/psnr.py` & `kornia-0.6.9/kornia/metrics/psnr.py`

 * *Files identical despite different names*

### Comparing `kornia-0.6.8/kornia/metrics/ssim.py` & `kornia-0.6.9/kornia/metrics/ssim.py`

 * *Files identical despite different names*

### Comparing `kornia-0.6.8/kornia/morphology/morphology.py` & `kornia-0.6.9/kornia/morphology/morphology.py`

 * *Files identical despite different names*

### Comparing `kornia-0.6.8/kornia/tracking/planar_tracker.py` & `kornia-0.6.9/kornia/tracking/planar_tracker.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,20 +1,20 @@
 from typing import Dict, Optional, Tuple
 
 import torch
-import torch.nn as nn
 
+from kornia.core import Module, Tensor
 from kornia.feature import DescriptorMatcher, GFTTAffNetHardNet, LocalFeatureMatcher, LoFTR
 from kornia.feature.integrated import LocalFeature
 from kornia.geometry.linalg import transform_points
 from kornia.geometry.ransac import RANSAC
 from kornia.geometry.transform import warp_perspective
 
 
-class HomographyTracker(nn.Module):
+class HomographyTracker(Module):
     r"""Module, which performs local-feature-based tracking of the target planar object in the sequence of the
     frames.
 
     Args:
         initial_matcher: image matching module, e.g. :class:`~kornia.feature.LocalFeatureMatcher`
                           or :class:`~kornia.feature.LoFTR`. Default: :class:`~kornia.feature.GFTTAffNetHardNet`.
         fast_matcher: fast image matching module, e.g. :class:`~kornia.feature.LocalFeatureMatcher`
@@ -22,31 +22,31 @@
         ransac: homography estimation module. Default: :class:`~kornia.geometry.RANSAC`.
         minimum_inliers_num: threshold for number inliers for matching to be successful.
     """
 
     def __init__(
         self,
         initial_matcher: Optional[LocalFeature] = None,
-        fast_matcher: Optional[nn.Module] = None,
-        ransac: Optional[nn.Module] = None,
+        fast_matcher: Optional[Module] = None,
+        ransac: Optional[Module] = None,
         minimum_inliers_num: int = 30,
     ) -> None:
         super().__init__()
         self.initial_matcher = initial_matcher or (
             LocalFeatureMatcher(GFTTAffNetHardNet(3000), DescriptorMatcher('smnn', 0.95))
         )
         self.fast_matcher = fast_matcher or LoFTR('outdoor')
         self.ransac = ransac or RANSAC('homography', inl_th=5.0, batch_size=4096, max_iter=10, max_lo_iters=10)
         self.minimum_inliers_num = minimum_inliers_num
 
         # placeholders
-        self.target: torch.Tensor
-        self.target_initial_representation: Dict[str, torch.Tensor] = {}
-        self.target_fast_representation: Dict[str, torch.Tensor] = {}
-        self.previous_homography: Optional[torch.Tensor] = None
+        self.target: Tensor
+        self.target_initial_representation: Dict[str, Tensor] = {}
+        self.target_fast_representation: Dict[str, Tensor] = {}
+        self.previous_homography: Optional[Tensor] = None
 
         self.inliers_num: int = 0
         self.keypoints0_num: int = 0
         self.keypoints1_num: int = 0
 
         self.reset_tracking()
 
@@ -55,40 +55,42 @@
         return self.target.device
 
     @property
     def dtype(self) -> torch.dtype:
         return self.target.dtype
 
     @torch.no_grad()
-    def set_target(self, target: torch.Tensor) -> None:
+    def set_target(self, target: Tensor) -> None:
         self.target = target
         self.target_initial_representation = {}
         self.target_fast_representation = {}
-        if hasattr(self.initial_matcher, 'extract_features'):
-            self.target_initial_representation = self.initial_matcher.extract_features(target)  # type: ignore
-        if hasattr(self.fast_matcher, 'extract_features'):
-            self.target_fast_representation = self.fast_matcher.extract_features(target)  # type: ignore
+        if hasattr(self.initial_matcher, 'extract_features') and isinstance(
+            self.initial_matcher.extract_features, Module
+        ):
+            self.target_initial_representation = self.initial_matcher.extract_features(target)
+        if hasattr(self.fast_matcher, 'extract_features') and isinstance(self.fast_matcher.extract_features, Module):
+            self.target_fast_representation = self.fast_matcher.extract_features(target)
 
     def reset_tracking(self) -> None:
         self.previous_homography = None
 
-    def no_match(self) -> Tuple[torch.Tensor, bool]:
+    def no_match(self) -> Tuple[Tensor, bool]:
         self.inliers_num = 0
         self.keypoints0_num = 0
         self.keypoints1_num = 0
         return torch.empty(3, 3, device=self.device, dtype=self.dtype), False
 
-    def match_initial(self, x: torch.Tensor) -> Tuple[torch.Tensor, bool]:
+    def match_initial(self, x: Tensor) -> Tuple[Tensor, bool]:
         """The frame `x` is matched with initial_matcher and  verified with ransac."""
-        input_dict: Dict[str, torch.Tensor] = {"image0": self.target, "image1": x}
+        input_dict: Dict[str, Tensor] = {"image0": self.target, "image1": x}
 
         for k, v in self.target_initial_representation.items():
             input_dict[f'{k}0'] = v
 
-        match_dict: Dict[str, torch.Tensor] = self.initial_matcher(input_dict)
+        match_dict: Dict[str, Tensor] = self.initial_matcher(input_dict)
         keypoints0 = match_dict['keypoints0'][match_dict['batch_indexes'] == 0]
         keypoints1 = match_dict['keypoints1'][match_dict['batch_indexes'] == 0]
 
         self.keypoints0_num = len(keypoints0)
         self.keypoints1_num = len(keypoints1)
 
         if self.keypoints0_num < self.minimum_inliers_num:
@@ -99,26 +101,26 @@
 
         if self.inliers_num < self.minimum_inliers_num:
             return self.no_match()
         self.previous_homography = H.clone()
 
         return H, True
 
-    def track_next_frame(self, x: torch.Tensor) -> Tuple[torch.Tensor, bool]:
+    def track_next_frame(self, x: Tensor) -> Tuple[Tensor, bool]:
         """The frame `x` is prewarped according to the previous frame homography, matched with fast_matcher
         verified with ransac."""
         if self.previous_homography is not None:  # mypy, shut up
             Hwarp = self.previous_homography.clone()[None]
         # make a bit of border for safety
         Hwarp[:, 0:2, 0:2] = Hwarp[:, 0:2, 0:2] / 0.8
         Hwarp[:, 0:2, 2] -= 10.0
         Hinv = torch.inverse(Hwarp)
         h, w = self.target.shape[2:]
         frame_warped = warp_perspective(x, Hinv, (h, w))
-        input_dict: Dict[str, torch.Tensor] = {"image0": self.target, "image1": frame_warped}
+        input_dict: Dict[str, Tensor] = {"image0": self.target, "image1": frame_warped}
         for k, v in self.target_fast_representation.items():
             input_dict[f'{k}0'] = v
 
         match_dict = self.fast_matcher(input_dict)
         keypoints0 = match_dict['keypoints0'][match_dict['batch_indexes'] == 0]
         keypoints1 = match_dict['keypoints1'][match_dict['batch_indexes'] == 0]
         keypoints1 = transform_points(Hwarp, keypoints1)
@@ -136,11 +138,11 @@
         if self.inliers_num < self.minimum_inliers_num:
             self.reset_tracking()
             return self.no_match()
 
         self.previous_homography = H.clone()
         return H, True
 
-    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, bool]:
+    def forward(self, x: Tensor) -> Tuple[Tensor, bool]:
         if self.previous_homography is not None:
             return self.track_next_frame(x)
         return self.match_initial(x)
```

### Comparing `kornia-0.6.8/kornia/utils/__init__.py` & `kornia-0.6.9/kornia/utils/__init__.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,10 +1,17 @@
+from ._compat import torch_meshgrid
 from .draw import draw_convex_polygon, draw_line, draw_rectangle
 from .grid import create_meshgrid, create_meshgrid3d
-from .helpers import _extract_device_dtype, get_cuda_device_if_available, safe_inverse_with_mask, safe_solve_with_mask
+from .helpers import (
+    _extract_device_dtype,
+    get_cuda_device_if_available,
+    map_location_to_cpu,
+    safe_inverse_with_mask,
+    safe_solve_with_mask,
+)
 from .image import ImageToTensor, image_list_to_tensor, image_to_tensor, tensor_to_image
 from .memory import batched_forward
 from .misc import eye_like, vec_like
 from .one_hot import one_hot
 from .pointcloud_io import load_pointcloud_ply, save_pointcloud_ply
 
 __all__ = [
@@ -23,8 +30,10 @@
     "draw_line",
     "_extract_device_dtype",
     "safe_inverse_with_mask",
     "safe_solve_with_mask",
     "ImageToTensor",
     "eye_like",
     "vec_like",
+    "torch_meshgrid",
+    "map_location_to_cpu",
 ]
```

### Comparing `kornia-0.6.8/kornia/utils/draw.py` & `kornia-0.6.9/kornia/utils/draw.py`

 * *Files identical despite different names*

### Comparing `kornia-0.6.8/kornia/utils/grid.py` & `kornia-0.6.9/kornia/utils/grid.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,12 +1,12 @@
 from typing import Optional
 
 import torch
-from torch import Tensor, stack
 
+from kornia.core import Tensor, stack
 from kornia.utils._compat import torch_meshgrid
 
 
 def create_meshgrid(
     height: int,
     width: int,
     normalized_coordinates: bool = True,
@@ -56,14 +56,19 @@
     # if normalized_coordinates:
     #     base_grid = K.geometry.normalize_pixel_coordinates(base_grid, height, width)
     # return torch.unsqueeze(base_grid.transpose(0, 1), dim=0)
     if normalized_coordinates:
         xs = (xs / (width - 1) - 0.5) * 2
         ys = (ys / (height - 1) - 0.5) * 2
     # generate grid by stacking coordinates
+    # TODO: torchscript doesn't like `torch_version_ge`
+    # if torch_version_ge(1, 13, 0):
+    #     x, y = torch_meshgrid([xs, ys], indexing="xy")
+    #     return stack([x, y], -1).unsqueeze(0)  # 1xHxWx2
+    # TODO: remove after we drop support of old versions
     base_grid: Tensor = stack(torch_meshgrid([xs, ys], indexing="ij"), dim=-1)  # WxHx2
     return base_grid.permute(1, 0, 2).unsqueeze(0)  # 1xHxWx2
 
 
 def create_meshgrid3d(
     depth: int,
     height: int,
```

### Comparing `kornia-0.6.8/kornia/utils/helpers.py` & `kornia-0.6.9/kornia/utils/helpers.py`

 * *Files 11% similar despite different names*

```diff
@@ -1,13 +1,14 @@
 import warnings
-from functools import partial, wraps
+from functools import wraps
 from inspect import isclass, isfunction
-from typing import Any, Callable, List, Optional, Tuple
+from typing import TYPE_CHECKING, Any, Callable, List, Optional, Tuple, Union, overload
 
 import torch
+from torch.linalg import inv_ex
 
 from kornia.core import Tensor
 from kornia.utils._compat import torch_version_geq
 
 
 def get_cuda_device_if_available(index: int = 0) -> torch.device:
     """Tries to get cuda device, if fail, returns cpu.
@@ -24,21 +25,32 @@
         else:
             dev = torch.device('cpu')
     except BaseException as e:  # noqa: F841
         dev = torch.device('cpu')
     return dev
 
 
-def _deprecated(func: Optional[Callable] = None, replace_with: Optional[str] = None):
-    if func is None:
-        return partial(_deprecated, replace_with=replace_with)
+@overload
+def map_location_to_cpu(storage: Tensor, location: str) -> Tensor:
+    ...
 
+
+@overload
+def map_location_to_cpu(storage: str) -> str:
+    ...
+
+
+def map_location_to_cpu(storage: Union[str, Tensor], *args: Any, **kwargs: Any) -> Union[str, Tensor]:
+    return storage
+
+
+def _deprecated(func: Callable[..., Any], replace_with: Optional[str] = None):
     @wraps(func)
     def wrapper(*args, **kwargs):
-        name: str = ""
+        name = ""
         if isclass(func):
             name = func.__class__.__name__
         if isfunction(func):
             name = func.__name__
         if replace_with is not None:
             warnings.warn(f"`{name}` is deprecated in favor of `{replace_with}`.", category=DeprecationWarning)
         else:
@@ -47,25 +59,25 @@
             )
         return func(*args, **kwargs)
 
     return wrapper
 
 
 def _extract_device_dtype(tensor_list: List[Optional[Any]]) -> Tuple[torch.device, torch.dtype]:
-    """Check if all the input are in the same device (only if when they are torch.Tensor).
+    """Check if all the input are in the same device (only if when they are Tensor).
 
     If so, it would return a tuple of (device, dtype). Default: (cpu, ``get_default_dtype()``).
 
     Returns:
         [torch.device, torch.dtype]
     """
     device, dtype = None, None
     for tensor in tensor_list:
         if tensor is not None:
-            if not isinstance(tensor, (torch.Tensor,)):
+            if not isinstance(tensor, (Tensor,)):
                 continue
             _device = tensor.device
             _dtype = tensor.dtype
             if device is None and dtype is None:
                 device = _device
                 dtype = _dtype
             elif device != _device or dtype != _dtype:
@@ -77,87 +89,93 @@
         # TODO: update this when having torch.get_default_device()
         device = torch.device('cpu')
     if dtype is None:
         dtype = torch.get_default_dtype()
     return (device, dtype)
 
 
-def _torch_inverse_cast(input: torch.Tensor) -> torch.Tensor:
+def _torch_inverse_cast(input: Tensor) -> Tensor:
     """Helper function to make torch.inverse work with other than fp32/64.
 
     The function torch.inverse is only implemented for fp32/64 which makes impossible to be used by fp16 or others. What
     this function does, is cast input data type to fp32, apply torch.inverse, and cast back to the input dtype.
     """
-    if not isinstance(input, torch.Tensor):
-        raise AssertionError(f"Input must be torch.Tensor. Got: {type(input)}.")
+    if not isinstance(input, Tensor):
+        raise AssertionError(f"Input must be Tensor. Got: {type(input)}.")
     dtype: torch.dtype = input.dtype
     if dtype not in (torch.float32, torch.float64):
         dtype = torch.float32
     return torch.inverse(input.to(dtype)).to(input.dtype)
 
 
-def _torch_histc_cast(input: torch.Tensor, bins: int, min: int, max: int) -> torch.Tensor:
+def _torch_histc_cast(input: Tensor, bins: int, min: int, max: int) -> Tensor:
     """Helper function to make torch.histc work with other than fp32/64.
 
     The function torch.histc is only implemented for fp32/64 which makes impossible to be used by fp16 or others. What
     this function does, is cast input data type to fp32, apply torch.inverse, and cast back to the input dtype.
     """
-    if not isinstance(input, torch.Tensor):
-        raise AssertionError(f"Input must be torch.Tensor. Got: {type(input)}.")
+    if not isinstance(input, Tensor):
+        raise AssertionError(f"Input must be Tensor. Got: {type(input)}.")
     dtype: torch.dtype = input.dtype
     if dtype not in (torch.float32, torch.float64):
         dtype = torch.float32
     return torch.histc(input.to(dtype), bins, min, max).to(input.dtype)
 
 
-def _torch_svd_cast(input: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
+def _torch_svd_cast(input: Tensor) -> Tuple[Tensor, Tensor, Tensor]:
     """Helper function to make torch.svd work with other than fp32/64.
 
     The function torch.svd is only implemented for fp32/64 which makes
     impossible to be used by fp16 or others. What this function does, is cast
     input data type to fp32, apply torch.svd, and cast back to the input dtype.
 
     NOTE: in torch 1.8.1 this function is recommended to use as torch.linalg.svd
     """
-    if not isinstance(input, torch.Tensor):
-        raise AssertionError(f"Input must be torch.Tensor. Got: {type(input)}.")
-    dtype: torch.dtype = input.dtype
+    # if not isinstance(input, torch.Tensor):
+    #    raise AssertionError(f"Input must be torch.Tensor. Got: {type(input)}.")
+    dtype = input.dtype
     if dtype not in (torch.float32, torch.float64):
         dtype = torch.float32
 
     out1, out2, out3H = torch.linalg.svd(input.to(dtype))
     if torch_version_geq(1, 11):
         out3 = out3H.mH
     else:
         out3 = out3H.transpose(-1, -2)
     return (out1.to(input.dtype), out2.to(input.dtype), out3.to(input.dtype))
 
 
-def _torch_linalg_svdvals(input: torch.Tensor) -> torch.Tensor:
+def _torch_linalg_svdvals(input: Tensor) -> Tensor:
     """Helper function to make torch.linalg.svdvals work with other than fp32/64.
 
     The function torch.svd is only implemented for fp32/64 which makes
     impossible to be used by fp16 or others. What this function does, is cast
     input data type to fp32, apply torch.svd, and cast back to the input dtype.
 
     NOTE: in torch 1.8.1 this function is recommended to use as torch.linalg.svd
     """
-    if not isinstance(input, torch.Tensor):
-        raise AssertionError(f"Input must be torch.Tensor. Got: {type(input)}.")
+    if not isinstance(input, Tensor):
+        raise AssertionError(f"Input must be Tensor. Got: {type(input)}.")
     dtype: torch.dtype = input.dtype
     if dtype not in (torch.float32, torch.float64):
         dtype = torch.float32
-    if torch_version_geq(1, 10):
-        out = torch.linalg.svdvals(input.to(dtype))
+
+    if TYPE_CHECKING:
+        # TODO: remove this branch when kornia relies on torch >= 1.10
+        out: Tensor
     else:
-        _, out, _ = torch.linalg.svd(input.to(dtype))
+        if torch_version_geq(1, 10):
+            out = torch.linalg.svdvals(input.to(dtype))
+        else:
+            # TODO: remove this branch when kornia relies on torch >= 1.10
+            _, out, _ = torch.linalg.svd(input.to(dtype))
     return out.to(input.dtype)
 
 
-# TODO: return only `torch.Tensor` and review all the calls to adjust
+# TODO: return only `Tensor` and review all the calls to adjust
 def _torch_solve_cast(A: Tensor, B: Tensor) -> Tensor:
     """Helper function to make torch.solve work with other than fp32/64.
 
     The function torch.solve is only implemented for fp32/64 which makes impossible to be used by fp16 or others. What
     this function does, is cast input data type to fp32, apply torch.svd, and cast back to the input dtype.
     """
     dtype: torch.dtype = A.dtype
@@ -165,58 +183,68 @@
         dtype = torch.float32
 
     out = torch.linalg.solve(A.to(dtype), B.to(dtype))
 
     return out.to(A.dtype)
 
 
-def safe_solve_with_mask(B: torch.Tensor, A: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
+def safe_solve_with_mask(B: Tensor, A: Tensor) -> Tuple[Tensor, Tensor, Tensor]:
     r"""Helper function, which avoids crashing because of singular matrix input and outputs the mask of valid
     solution."""
     if not torch_version_geq(1, 10):
         sol = _torch_solve_cast(A, B)
         warnings.warn('PyTorch version < 1.10, solve validness mask maybe not correct', RuntimeWarning)
         return sol, sol, torch.ones(len(A), dtype=torch.bool, device=A.device)
     # Based on https://github.com/pytorch/pytorch/issues/31546#issuecomment-694135622
-    if not isinstance(B, torch.Tensor):
-        raise AssertionError(f"B must be torch.Tensor. Got: {type(B)}.")
+    if not isinstance(B, Tensor):
+        raise AssertionError(f"B must be Tensor. Got: {type(B)}.")
     dtype: torch.dtype = B.dtype
     if dtype not in (torch.float32, torch.float64):
         dtype = torch.float32
-    if not torch_version_geq(1, 13):
-        A_LU, pivots, info = torch.lu(A.to(dtype), True, get_infos=True)
+
+    if TYPE_CHECKING:
+        # TODO: remove this branch when kornia relies on torch >= 1.13
+        A_LU: Tensor
+        pivots: Tensor
+        info: Tensor
     else:
-        A_LU, pivots, info = torch.linalg.lu_factor_ex(A.to(dtype))
+        if torch_version_geq(1, 13):
+            A_LU, pivots, info = torch.linalg.lu_factor_ex(A.to(dtype))
+        else:
+            # TODO: remove this branch when kornia relies on torch >= 1.13
+            A_LU, pivots, info = torch.lu(A.to(dtype), True, get_infos=True)
 
-    valid_mask: torch.Tensor = info == 0
+    valid_mask: Tensor = info == 0
     n_dim_B = len(B.shape)
     n_dim_A = len(A.shape)
     if n_dim_A - n_dim_B == 1:
         B = B.unsqueeze(-1)
 
-    if not torch_version_geq(1, 13):
-        X = torch.lu_solve(B.to(dtype), A_LU, pivots)
+    if TYPE_CHECKING:
+        # TODO: remove this branch when kornia relies on torch >= 1.13
+        X: Tensor
     else:
-        X = torch.linalg.lu_solve(A_LU, pivots, B.to(dtype))
+        if torch_version_geq(1, 13):
+            X = torch.linalg.lu_solve(A_LU, pivots, B.to(dtype))
+        else:
+            # TODO: remove this branch when kornia relies on torch >= 1.13
+            X = torch.lu_solve(B.to(dtype), A_LU, pivots)
+
     return X.to(B.dtype), A_LU.to(A.dtype), valid_mask
 
 
-def safe_inverse_with_mask(A: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
+def safe_inverse_with_mask(A: Tensor) -> Tuple[Tensor, Tensor]:
     r"""Helper function, which avoids crashing because of non-invertable matrix input and outputs the mask of valid
     solution."""
-    # Based on https://github.com/pytorch/pytorch/issues/31546#issuecomment-694135622
-    if not torch_version_geq(1, 9):
-        inv = _torch_inverse_cast(A)
-        warnings.warn('PyTorch version < 1.9, inverse validness mask maybe not correct', RuntimeWarning)
-        return inv, torch.ones(len(A), dtype=torch.bool, device=A.device)
-    if not isinstance(A, torch.Tensor):
-        raise AssertionError(f"A must be torch.Tensor. Got: {type(A)}.")
-    dtype_original: torch.dtype = A.dtype
+
+    if not isinstance(A, Tensor):
+        raise AssertionError(f"A must be Tensor. Got: {type(A)}.")
+
+    dtype_original = A.dtype
     if dtype_original not in (torch.float32, torch.float64):
         dtype = torch.float32
     else:
         dtype = dtype_original
-    from torch.linalg import inv_ex  # type: ignore # (not available in 1.8.1)
 
     inverse, info = inv_ex(A.to(dtype))
     mask = info == 0
     return inverse.to(dtype_original), mask
```

### Comparing `kornia-0.6.8/kornia/utils/image.py` & `kornia-0.6.9/kornia/utils/image.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,18 +1,20 @@
 from functools import wraps
-from typing import TYPE_CHECKING, Callable, List
+from typing import TYPE_CHECKING, Any, Callable, List
 
 import torch
 import torch.nn as nn
 
+from kornia.core import Tensor
+
 if TYPE_CHECKING:
-    import numpy as np
+    import numpy.typing as npt
 
 
-def image_to_tensor(image: "np.ndarray", keepdim: bool = True) -> torch.Tensor:
+def image_to_tensor(image: "npt.NDArray[Any]", keepdim: bool = True) -> Tensor:
     """Convert a numpy image to a PyTorch 4d tensor image.
 
     Args:
         image: image of the form :math:`(H, W, C)`, :math:`(H, W)` or
             :math:`(B, H, W, C)`.
         keepdim: If ``False`` unsqueeze the input image to match the shape
             :math:`(B, H, W, C)`.
@@ -34,15 +36,15 @@
         >>> image_to_tensor(img, keepdim=False).shape
         torch.Size([1, 3, 4, 4])
     """
     if len(image.shape) > 4 or len(image.shape) < 2:
         raise ValueError("Input size must be a two, three or four dimensional array")
 
     input_shape = image.shape
-    tensor: torch.Tensor = torch.from_numpy(image)
+    tensor: Tensor = torch.from_numpy(image)
 
     if len(input_shape) == 2:
         # (H, W) -> (1, H, W)
         tensor = tensor.unsqueeze(0)
     elif len(input_shape) == 3:
         # (H, W, C) -> (C, H, W)
         tensor = tensor.permute(2, 0, 1)
@@ -52,15 +54,15 @@
         keepdim = True  # no need to unsqueeze
     else:
         raise ValueError(f"Cannot process image with shape {input_shape}")
 
     return tensor.unsqueeze(0) if not keepdim else tensor
 
 
-def image_list_to_tensor(images: List["np.ndarray"]) -> torch.Tensor:
+def image_list_to_tensor(images: List["npt.NDArray[Any]"]) -> Tensor:
     """Converts a list of numpy images to a PyTorch 4d tensor image.
 
     Args:
         images: list of images, each of the form :math:`(H, W, C)`.
         Image shapes must be consistent
 
     Returns:
@@ -72,32 +74,32 @@
         torch.Size([2, 1, 4, 4])
     """
     if not images:
         raise ValueError("Input list of numpy images is empty")
     if len(images[0].shape) != 3:
         raise ValueError("Input images must be three dimensional arrays")
 
-    list_of_tensors: List[torch.Tensor] = []
+    list_of_tensors: List[Tensor] = []
     for image in images:
         list_of_tensors.append(image_to_tensor(image))
-    tensor: torch.Tensor = torch.stack(list_of_tensors)
+    tensor: Tensor = torch.stack(list_of_tensors)
     return tensor
 
 
-def _to_bchw(tensor: torch.Tensor) -> torch.Tensor:
+def _to_bchw(tensor: Tensor) -> Tensor:
     """Convert a PyTorch tensor image to BCHW format.
 
     Args:
         tensor (torch.Tensor): image of the form :math:`(*, H, W)`.
 
     Returns:
         input tensor of the form :math:`(B, C, H, W)`.
     """
-    if not isinstance(tensor, torch.Tensor):
-        raise TypeError(f"Input type is not a torch.Tensor. Got {type(tensor)}")
+    if not isinstance(tensor, Tensor):
+        raise TypeError(f"Input type is not a Tensor. Got {type(tensor)}")
 
     if len(tensor.shape) < 2:
         raise ValueError(f"Input size must be a two, three or four dimensional tensor. Got {tensor.shape}")
 
     if len(tensor.shape) == 2:
         tensor = tensor.unsqueeze(0)
 
@@ -106,25 +108,25 @@
 
     if len(tensor.shape) > 4:
         tensor = tensor.view(-1, tensor.shape[-3], tensor.shape[-2], tensor.shape[-1])
 
     return tensor
 
 
-def _to_bcdhw(tensor: torch.Tensor) -> torch.Tensor:
+def _to_bcdhw(tensor: Tensor) -> Tensor:
     """Convert a PyTorch tensor image to BCDHW format.
 
     Args:
         tensor (torch.Tensor): image of the form :math:`(*, D, H, W)`.
 
     Returns:
         input tensor of the form :math:`(B, C, D, H, W)`.
     """
-    if not isinstance(tensor, torch.Tensor):
-        raise TypeError(f"Input type is not a torch.Tensor. Got {type(tensor)}")
+    if not isinstance(tensor, Tensor):
+        raise TypeError(f"Input type is not a Tensor. Got {type(tensor)}")
 
     if len(tensor.shape) < 3:
         raise ValueError(f"Input size must be a three, four or five dimensional tensor. Got {tensor.shape}")
 
     if len(tensor.shape) == 3:
         tensor = tensor.unsqueeze(0)
 
@@ -133,15 +135,15 @@
 
     if len(tensor.shape) > 5:
         tensor = tensor.view(-1, tensor.shape[-4], tensor.shape[-3], tensor.shape[-2], tensor.shape[-1])
 
     return tensor
 
 
-def tensor_to_image(tensor: torch.Tensor, keepdim: bool = False) -> "np.ndarray":
+def tensor_to_image(tensor: Tensor, keepdim: bool = False) -> "npt.NDArray[Any]":
     """Converts a PyTorch tensor image to a numpy image.
 
     In case the tensor is in the GPU, it will be copied back to CPU.
 
     Args:
         tensor: image of the form :math:`(H, W)`, :math:`(C, H, W)` or
             :math:`(B, C, H, W)`.
@@ -156,22 +158,22 @@
         >>> tensor_to_image(img).shape
         (3, 3)
 
         >>> img = torch.ones(3, 4, 4)
         >>> tensor_to_image(img).shape
         (4, 4, 3)
     """
-    if not isinstance(tensor, torch.Tensor):
-        raise TypeError(f"Input type is not a torch.Tensor. Got {type(tensor)}")
+    if not isinstance(tensor, Tensor):
+        raise TypeError(f"Input type is not a Tensor. Got {type(tensor)}")
 
     if len(tensor.shape) > 4 or len(tensor.shape) < 2:
         raise ValueError("Input size must be a two, three or four dimensional tensor")
 
     input_shape = tensor.shape
-    image: "np.ndarray" = tensor.cpu().detach().numpy()
+    image: "npt.NDArray[Any]" = tensor.cpu().detach().numpy()
 
     if len(input_shape) == 2:
         # (H, W) -> (H, W)
         pass
     elif len(input_shape) == 3:
         # (C, H, W) -> (H, W, C)
         if input_shape[0] == 1:
@@ -199,68 +201,68 @@
         keepdim: If ``False`` unsqueeze the input image to match the shape :math:`(B, H, W, C)`.
     """
 
     def __init__(self, keepdim: bool = False):
         super().__init__()
         self.keepdim = keepdim
 
-    def forward(self, x: "np.ndarray") -> torch.Tensor:
+    def forward(self, x: "npt.NDArray[Any]") -> Tensor:
         return image_to_tensor(x, keepdim=self.keepdim)
 
 
-def perform_keep_shape_image(f: Callable) -> Callable:
+def perform_keep_shape_image(f: Callable[..., Tensor]) -> Callable[..., Tensor]:
     """A decorator that enable `f` to be applied to an image of arbitrary leading dimensions `(*, C, H, W)`.
 
     It works by first viewing the image as `(B, C, H, W)`, applying the function and re-viewing the image as original
     shape.
     """
 
     @wraps(f)
-    def _wrapper(input: torch.Tensor, *args, **kwargs):
-        if not isinstance(input, torch.Tensor):
-            raise TypeError(f"Input input type is not a torch.Tensor. Got {type(input)}")
+    def _wrapper(input: Tensor, *args, **kwargs) -> Tensor:
+        if not isinstance(input, Tensor):
+            raise TypeError(f"Input input type is not a Tensor. Got {type(input)}")
 
         if input.numel() == 0:
             raise ValueError("Invalid input tensor, it is empty.")
 
         input_shape = input.shape
         input = _to_bchw(input)  # view input as (B, C, H, W)
-        output: torch.Tensor = f(input, *args, **kwargs)
+        output = f(input, *args, **kwargs)
         if len(input_shape) == 3:
             output = output[0]
 
         if len(input_shape) == 2:
             output = output[0, 0]
 
         if len(input_shape) > 4:
             output = output.view(*(input_shape[:-3] + output.shape[-3:]))
 
         return output
 
     return _wrapper
 
 
-def perform_keep_shape_video(f: Callable) -> Callable:
+def perform_keep_shape_video(f: Callable[..., Tensor]) -> Callable[..., Tensor]:
     """A decorator that enable `f` to be applied to an image of arbitrary leading dimensions `(*, C, D, H, W)`.
 
     It works by first viewing the image as `(B, C, D, H, W)`, applying the function and re-viewing the image as original
     shape.
     """
 
     @wraps(f)
-    def _wrapper(input: torch.Tensor, *args, **kwargs):
-        if not isinstance(input, torch.Tensor):
-            raise TypeError(f"Input input type is not a torch.Tensor. Got {type(input)}")
+    def _wrapper(input: Tensor, *args, **kwargs) -> Tensor:
+        if not isinstance(input, Tensor):
+            raise TypeError(f"Input input type is not a Tensor. Got {type(input)}")
 
         if input.numel() == 0:
             raise ValueError("Invalid input tensor, it is empty.")
 
         input_shape = input.shape
         input = _to_bcdhw(input)  # view input as (B, C, D, H, W)
-        output: torch.Tensor = f(input, *args, **kwargs)
+        output = f(input, *args, **kwargs)
         if len(input_shape) == 4:
             output = output[0]
 
         if len(input_shape) == 3:
             output = output[0, 0]
 
         if len(input_shape) > 5:
```

### Comparing `kornia-0.6.8/kornia/utils/memory.py` & `kornia-0.6.9/kornia/utils/memory.py`

 * *Files identical despite different names*

### Comparing `kornia-0.6.8/kornia/utils/one_hot.py` & `kornia-0.6.9/kornia/utils/one_hot.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,49 +1,43 @@
-from typing import Optional
-
 import torch
 
+from kornia.core import Tensor, zeros
+
 
-def one_hot(
-    labels: torch.Tensor,
-    num_classes: int,
-    device: Optional[torch.device] = None,
-    dtype: Optional[torch.dtype] = None,
-    eps: float = 1e-6,
-) -> torch.Tensor:
+def one_hot(labels: Tensor, num_classes: int, device: torch.device, dtype: torch.dtype, eps: float = 1e-6) -> Tensor:
     r"""Convert an integer label x-D tensor to a one-hot (x+1)-D tensor.
 
     Args:
         labels: tensor with labels of shape :math:`(N, *)`, where N is batch size.
           Each value is an integer representing correct classification.
         num_classes: number of classes in labels.
         device: the desired device of returned tensor.
         dtype: the desired data type of returned tensor.
 
     Returns:
         the labels in one hot tensor of shape :math:`(N, C, *)`,
 
     Examples:
         >>> labels = torch.LongTensor([[[0, 1], [2, 0]]])
-        >>> one_hot(labels, num_classes=3)
+        >>> one_hot(labels, num_classes=3, device=torch.device('cpu'), dtype=torch.int64)
         tensor([[[[1.0000e+00, 1.0000e-06],
                   [1.0000e-06, 1.0000e+00]],
         <BLANKLINE>
                  [[1.0000e-06, 1.0000e+00],
                   [1.0000e-06, 1.0000e-06]],
         <BLANKLINE>
                  [[1.0000e-06, 1.0000e-06],
                   [1.0000e+00, 1.0000e-06]]]])
     """
-    if not isinstance(labels, torch.Tensor):
-        raise TypeError(f"Input labels type is not a torch.Tensor. Got {type(labels)}")
+    if not isinstance(labels, Tensor):
+        raise TypeError(f"Input labels type is not a Tensor. Got {type(labels)}")
 
     if not labels.dtype == torch.int64:
         raise ValueError(f"labels must be of the same dtype torch.int64. Got: {labels.dtype}")
 
     if num_classes < 1:
         raise ValueError("The number of classes must be bigger than one." " Got: {}".format(num_classes))
 
     shape = labels.shape
-    one_hot = torch.zeros((shape[0], num_classes) + shape[1:], device=device, dtype=dtype)
+    one_hot = zeros((shape[0], num_classes) + shape[1:], device=device, dtype=dtype)
 
     return one_hot.scatter_(1, labels.unsqueeze(1), 1.0) + eps
```

### Comparing `kornia-0.6.8/kornia/utils/pointcloud_io.py` & `kornia-0.6.9/kornia/utils/pointcloud_io.py`

 * *Files identical despite different names*

### Comparing `kornia-0.6.8/kornia/x/callbacks.py` & `kornia-0.6.9/kornia/x/callbacks.py`

 * *Files 4% similar despite different names*

```diff
@@ -80,15 +80,15 @@
         )
 
         trainer = ImageClassifierTrainer(...,
             callbacks={"on_checkpoint", model_checkpoint}
         )
     """
 
-    def __init__(self, filepath: str, monitor: str, filename_fcn: Optional[Callable] = None) -> None:
+    def __init__(self, filepath: str, monitor: str, filename_fcn: Optional[Callable[..., str]] = None) -> None:
         self.filepath = filepath
         self.monitor = monitor
         self._filename_fcn = filename_fcn or default_filename_fcn
 
         # track best model
         self.best_metric: float = 0.0
```

### Comparing `kornia-0.6.8/kornia/x/trainer.py` & `kornia-0.6.9/kornia/x/trainer.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,21 +1,22 @@
 import logging
-from typing import Callable, Dict, Optional
+from typing import Any, Callable, Dict, Optional
 
 # the accelerator library is a requirement for the Trainer
 # but it is optional for grousnd base user of kornia.
 import torch
-import torch.nn as nn
+from torch.optim import Optimizer, lr_scheduler
 from torch.utils.data import DataLoader
 
 try:
     from accelerate import Accelerator
 except ImportError:
     Accelerator = None
 
+from kornia.core import Module, Tensor
 from kornia.metrics import AverageMeter
 
 from .utils import Configuration, StatsTracker, TrainerState
 
 callbacks_whitelist = [
     # high level functions
     "preprocess",
@@ -57,22 +58,22 @@
     .. seealso::
         Learn how to use the API in our documentation
         `here <https://kornia.readthedocs.io/en/latest/get-started/training.html>`_.
     """
 
     def __init__(
         self,
-        model: nn.Module,
-        train_dataloader: DataLoader,
-        valid_dataloader: DataLoader,
-        criterion: Optional[nn.Module],
-        optimizer: torch.optim.Optimizer,
-        scheduler: torch.optim.lr_scheduler.CosineAnnealingLR,
+        model: Module,
+        train_dataloader: DataLoader[Any],
+        valid_dataloader: DataLoader[Any],
+        criterion: Optional[Module],
+        optimizer: Optimizer,
+        scheduler: lr_scheduler.CosineAnnealingLR,
         config: Configuration,
-        callbacks: Dict[str, Callable] = {},
+        callbacks: Dict[str, Callable[..., None]] = {},
     ) -> None:
         # setup the accelerator
         if Accelerator is None:
             raise ModuleNotFoundError('accelerate library is not installed: pip install "kornia[x]"')
         self.accelerator = Accelerator()
 
         # setup the data related objects
@@ -97,15 +98,15 @@
 
         self._logger = logging.getLogger('train')
 
     @property
     def device(self) -> torch.device:
         return self.accelerator.device
 
-    def backward(self, loss: torch.Tensor) -> None:
+    def backward(self, loss: Tensor) -> None:
         self.accelerator.backward(loss)
 
     def fit_epoch(self, epoch: int) -> None:
         # train loop
         self.model.train()
         losses = AverageMeter()
         for sample_id, sample in enumerate(self.train_dataloader):
@@ -155,15 +156,15 @@
             self.scheduler.step()
 
         ...
 
     # events stubs
 
     @torch.no_grad()
-    def evaluate(self) -> dict:
+    def evaluate(self) -> Dict[str, AverageMeter]:
         self.model.eval()
         stats = StatsTracker()
         for sample_id, sample in enumerate(self.valid_dataloader):
             sample = {"input": sample[0], "target": sample[1]}  # new dataset api will come like this
             # perform the preprocess and augmentations in batch
             sample = self.preprocess(sample)
             sample = self.on_before_model(sample)
@@ -183,36 +184,36 @@
                 self._logger.info(f"Test: {sample_id}/{len(self.valid_dataloader)} {stats}")
 
         return stats.as_dict()
 
     def on_epoch_start(self, *args, **kwargs):
         ...
 
-    def preprocess(self, x: dict) -> dict:
+    def preprocess(self, x: Dict[str, Tensor]) -> Dict[str, Tensor]:
         return x
 
-    def augmentations(self, x: dict) -> dict:
+    def augmentations(self, x: Dict[str, Tensor]) -> Dict[str, Tensor]:
         return x
 
-    def compute_metrics(self, *args: torch.Tensor) -> Dict[str, float]:
+    def compute_metrics(self, *args: Tensor) -> Dict[str, float]:
         """Compute metrics during the evaluation."""
         return {}
 
-    def compute_loss(self, *args: torch.Tensor) -> torch.Tensor:
+    def compute_loss(self, *args: Tensor) -> Tensor:
         if self.criterion is None:
             raise RuntimeError("`criterion` should not be None.")
         return self.criterion(*args)
 
-    def on_before_model(self, x: dict) -> dict:
+    def on_before_model(self, x: Dict[str, Tensor]) -> Dict[str, Tensor]:
         return x
 
-    def on_model(self, model, sample: dict):
+    def on_model(self, model, sample: Dict[str, Tensor]):
         return model(sample["input"])
 
-    def on_after_model(self, output: torch.Tensor, sample: dict):
+    def on_after_model(self, output: Tensor, sample: Dict[str, Tensor]):
         ...
 
     def on_checkpoint(self, *args, **kwargs):
         ...
 
     def on_epoch_end(self, *args, **kwargs):
         ...
```

### Comparing `kornia-0.6.8/kornia/x/trainers.py` & `kornia-0.6.9/kornia/x/trainers.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,13 +1,13 @@
-from typing import Callable, Dict, Optional
+from typing import Any, Callable, Dict, Optional
 
-import torch
-import torch.nn as nn
+from torch.optim import Optimizer, lr_scheduler
 from torch.utils.data import DataLoader
 
+from kornia.core import Module, Tensor, stack
 from kornia.metrics import accuracy, mean_average_precision, mean_iou
 
 from .trainer import Trainer
 from .utils import Configuration
 
 
 class ImageClassifierTrainer(Trainer):
@@ -18,15 +18,15 @@
     :py:func:`~kornia.metrics.accuracy` topk@[1, 5].
 
     .. seealso::
         Learn how to use this class in the following
         `example <https://github.com/kornia/kornia/blob/master/examples/train/image_classifier/>`__.
     """
 
-    def compute_metrics(self, *args: torch.Tensor) -> Dict[str, float]:
+    def compute_metrics(self, *args: Tensor) -> Dict[str, float]:
         if len(args) != 2:
             raise AssertionError
         out, target = args
         acc1, acc5 = accuracy(out, target, topk=(1, 5))
         return dict(top1=acc1.item(), top5=acc5.item())
 
 
@@ -37,15 +37,15 @@
     :py:func:`~kornia.x.Trainer.evaluate` function implementing IoU :py:func:`~kornia.metrics.mean_iou`.
 
     .. seealso::
         Learn how to use this class in the following
         `example <https://github.com/kornia/kornia/blob/master/examples/train/semantic_segmentation/>`__.
     """
 
-    def compute_metrics(self, *args: torch.Tensor) -> Dict[str, float]:
+    def compute_metrics(self, *args: Tensor) -> Dict[str, float]:
         if len(args) != 2:
             raise AssertionError
         out, target = args
         iou = mean_iou(out.argmax(1), target, out.shape[1]).mean()
         return dict(iou=iou.item())
 
 
@@ -58,48 +58,48 @@
     .. seealso::
         Learn how to use this class in the following
         `example <https://github.com/kornia/kornia/blob/master/examples/train/object_detection/>`__.
     """
 
     def __init__(
         self,
-        model: nn.Module,
-        train_dataloader: DataLoader,
-        valid_dataloader: DataLoader,
-        criterion: Optional[nn.Module],
-        optimizer: torch.optim.Optimizer,
-        scheduler: torch.optim.lr_scheduler.CosineAnnealingLR,
+        model: Module,
+        train_dataloader: DataLoader[Any],
+        valid_dataloader: DataLoader[Any],
+        criterion: Optional[Module],
+        optimizer: Optimizer,
+        scheduler: lr_scheduler.CosineAnnealingLR,
         config: Configuration,
         num_classes: int,
-        callbacks: Optional[Dict[str, Callable]] = None,
+        callbacks: Optional[Dict[str, Callable[..., None]]] = None,
         loss_computed_by_model: Optional[bool] = None,
     ) -> None:
         if callbacks is None:
             callbacks = {}
         super().__init__(model, train_dataloader, valid_dataloader, criterion, optimizer, scheduler, config, callbacks)
         # TODO: auto-detect if the model is from TorchVision
         self.loss_computed_by_model = loss_computed_by_model
         self.num_classes = num_classes
 
-    def on_model(self, model: nn.Module, sample: dict):
+    def on_model(self, model: Module, sample: Dict[str, Tensor]):
         if self.loss_computed_by_model and model.training:
             return model(sample["input"], sample["target"])
         return model(sample["input"])
 
-    def compute_loss(self, *args: torch.Tensor) -> torch.Tensor:
+    def compute_loss(self, *args: Tensor) -> Tensor:
         if self.loss_computed_by_model:
             # Note: in case of dict losses obtained
             if isinstance(args[0], dict):
-                return torch.stack([v for _, v in args[0].items()]).mean()
-            return torch.stack(list(args[0])).sum()
+                return stack([v for _, v in args[0].items()]).mean()
+            return stack(list(args[0])).sum()
         if self.criterion is None:
             raise RuntimeError("`criterion` should not be None if `loss_computed_by_model` is False.")
         return self.criterion(*args)
 
-    def compute_metrics(self, *args: torch.Tensor) -> Dict[str, float]:
+    def compute_metrics(self, *args: Tensor) -> Dict[str, float]:
         if (
             isinstance(args[0], dict)
             and "boxes" in args[0]
             and "labels" in args[0]
             and "scores" in args[0]
             and isinstance(args[1], dict)
             and "boxes" in args[1]
```

### Comparing `kornia-0.6.8/kornia/x/utils.py` & `kornia-0.6.9/kornia/x/utils.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,17 +1,15 @@
-# TODO: remove the type: ignore in below after deprecating python 3.6
-from dataclasses import dataclass, field  # type: ignore
+from dataclasses import dataclass, field
 from enum import Enum
-from typing import Dict
-
-import torch.nn as nn
+from typing import Dict, Tuple
 
+from kornia.core import Module
 from kornia.metrics.average_meter import AverageMeter
 
-# import yaml  # type: ignore
+# import yaml
 
 
 class TrainerState(Enum):
     STARTING = 0
     TRAINING = 1
     VALIDATE = 2
     TERMINATE = 3
@@ -21,31 +19,31 @@
 @dataclass
 class Configuration:
     data_path: str = field(default="./", metadata={"help": "The input data directory."})
     batch_size: int = field(default=1, metadata={"help": "The number of batches for the training dataloader."})
     num_epochs: int = field(default=1, metadata={"help": "The number of epochs to run the training."})
     lr: float = field(default=1e-3, metadata={"help": "The learning rate to be used for the optimize."})
     output_path: str = field(default="./output", metadata={"help": "The output data directory."})
-    image_size: tuple = field(default=(224, 224), metadata={"help": "The input image size."})
+    image_size: Tuple[int, int] = field(default=(224, 224), metadata={"help": "The input image size."})
 
     # TODO: possibly remove because hydra already do this
     # def __init__(self, **entries):
     #     for k, v in entries.items():
     #         self.__dict__[k] = Configuration(**v) if isinstance(v, dict) else v
 
     # @classmethod
     # def from_yaml(cls, config_file: str):
     #     """Create an instance of the configuration from a yaml file."""
     #     with open(config_file) as f:
     #         data = yaml.safe_load(f)
     #     return cls(**data)
 
 
-class Lambda(nn.Module):
-    """Module to create a lambda function as nn.Module.
+class Lambda(Module):
+    """Module to create a lambda function as Module.
 
     Args:
         fcn: a pointer to any function.
 
     Example:
         >>> import torch
         >>> import kornia as K
```

### Comparing `kornia-0.6.8/kornia.egg-info/PKG-INFO` & `kornia-0.6.9/kornia.egg-info/PKG-INFO`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: kornia
-Version: 0.6.8
+Version: 0.6.9
 Summary: Open Source Differentiable Computer Vision Library for PyTorch
 Home-page: https://www.kornia.org
 Download-URL: https://github.com/kornia/kornia
 Author: Edgar Riba
 Author-email: edgar@kornia.org
 License: Apache-2.0
 Project-URL: Bug Tracker, https://github.com/kornia/kornia/issues
@@ -22,15 +22,15 @@
 Classifier: Natural Language :: English
 Classifier: Operating System :: OS Independent
 Classifier: Programming Language :: Python :: 3
 Classifier: Programming Language :: Python :: 3 :: Only
 Classifier: Topic :: Scientific/Engineering :: Artificial Intelligence
 Classifier: Topic :: Scientific/Engineering :: Image Processing
 Classifier: Topic :: Software Development :: Libraries
-Requires-Python: >=3.6
+Requires-Python: >=3.7
 Description-Content-Type: text/markdown
 Provides-Extra: dev
 Provides-Extra: docs
 Provides-Extra: x
 License-File: LICENSE
 
 <div align="center">
```

#### html2text {}

```diff
@@ -1,8 +1,8 @@
-Metadata-Version: 2.1 Name: kornia Version: 0.6.8 Summary: Open Source
+Metadata-Version: 2.1 Name: kornia Version: 0.6.9 Summary: Open Source
 Differentiable Computer Vision Library for PyTorch Home-page: https://
 www.kornia.org Download-URL: https://github.com/kornia/kornia Author: Edgar
 Riba Author-email: edgar@kornia.org License: Apache-2.0 Project-URL: Bug
 Tracker, https://github.com/kornia/kornia/issues Project-URL: Documentation,
 https://kornia.readthedocs.io/en/latest Project-URL: Source Code, https://
 github.com/kornia/kornia Keywords: computer vision,deep learning,pytorch
 Classifier: Development Status :: 4 - Beta Classifier: Environment :: Console
@@ -11,15 +11,15 @@
 Information Technology Classifier: Intended Audience :: Science/Research
 Classifier: License :: OSI Approved :: Apache Software License Classifier:
 Natural Language :: English Classifier: Operating System :: OS Independent
 Classifier: Programming Language :: Python :: 3 Classifier: Programming
 Language :: Python :: 3 :: Only Classifier: Topic :: Scientific/Engineering ::
 Artificial Intelligence Classifier: Topic :: Scientific/Engineering :: Image
 Processing Classifier: Topic :: Software Development :: Libraries Requires-
-Python: >=3.6 Description-Content-Type: text/markdown Provides-Extra: dev
+Python: >=3.7 Description-Content-Type: text/markdown Provides-Extra: dev
 Provides-Extra: docs Provides-Extra: x License-File: LICENSE
        [https://github.com/kornia/data/raw/main/kornia_banner_pixie.png]
 --- English | [](README_zh-CN.md)  Website  Docs  Try_it_Now
    Tutorials  Examples  Blog  Community [![PyPI python](https://
   img.shields.io/pypi/pyversions/kornia)](https://pypi.org/project/kornia) [!
 [PyPI version](https://badge.fury.io/py/kornia.svg)](https://pypi.org/project/
    kornia) [![Downloads](https://pepy.tech/badge/kornia)](https://pepy.tech/
```

### Comparing `kornia-0.6.8/kornia.egg-info/SOURCES.txt` & `kornia-0.6.9/kornia.egg-info/SOURCES.txt`

 * *Files 2% similar despite different names*

```diff
@@ -1,15 +1,15 @@
 LICENSE
 README.md
 pyproject.toml
 setup.cfg
 setup.py
 kornia/__init__.py
-kornia/__tmp__.py
 kornia/constants.py
+kornia/py.typed
 kornia.egg-info/PKG-INFO
 kornia.egg-info/SOURCES.txt
 kornia.egg-info/dependency_links.txt
 kornia.egg-info/requires.txt
 kornia.egg-info/top_level.txt
 kornia.egg-info/zip-safe
 kornia/augmentation/__init__.py
@@ -30,31 +30,35 @@
 kornia/augmentation/_2d/geometric/resized_crop.py
 kornia/augmentation/_2d/geometric/rotation.py
 kornia/augmentation/_2d/geometric/thin_plate_spline.py
 kornia/augmentation/_2d/geometric/vertical_flip.py
 kornia/augmentation/_2d/intensity/__init__.py
 kornia/augmentation/_2d/intensity/base.py
 kornia/augmentation/_2d/intensity/box_blur.py
+kornia/augmentation/_2d/intensity/brightness.py
 kornia/augmentation/_2d/intensity/channel_shuffle.py
 kornia/augmentation/_2d/intensity/color_jiggle.py
 kornia/augmentation/_2d/intensity/color_jitter.py
+kornia/augmentation/_2d/intensity/contrast.py
 kornia/augmentation/_2d/intensity/denormalize.py
 kornia/augmentation/_2d/intensity/equalize.py
 kornia/augmentation/_2d/intensity/erasing.py
 kornia/augmentation/_2d/intensity/gamma.py
 kornia/augmentation/_2d/intensity/gaussian_blur.py
 kornia/augmentation/_2d/intensity/gaussian_noise.py
 kornia/augmentation/_2d/intensity/grayscale.py
+kornia/augmentation/_2d/intensity/hue.py
 kornia/augmentation/_2d/intensity/invert.py
 kornia/augmentation/_2d/intensity/motion_blur.py
 kornia/augmentation/_2d/intensity/normalize.py
 kornia/augmentation/_2d/intensity/planckian_jitter.py
 kornia/augmentation/_2d/intensity/plasma.py
 kornia/augmentation/_2d/intensity/posterize.py
 kornia/augmentation/_2d/intensity/random_rgb_shift.py
+kornia/augmentation/_2d/intensity/saturation.py
 kornia/augmentation/_2d/intensity/sharpness.py
 kornia/augmentation/_2d/intensity/solarize.py
 kornia/augmentation/_2d/mix/__init__.py
 kornia/augmentation/_2d/mix/base.py
 kornia/augmentation/_2d/mix/cutmix.py
 kornia/augmentation/_2d/mix/jigsaw.py
 kornia/augmentation/_2d/mix/mixup.py
@@ -86,15 +90,14 @@
 kornia/augmentation/random_generator/utils.py
 kornia/augmentation/random_generator/_2d/__init__.py
 kornia/augmentation/random_generator/_2d/affine.py
 kornia/augmentation/random_generator/_2d/color_jiggle.py
 kornia/augmentation/random_generator/_2d/color_jitter.py
 kornia/augmentation/random_generator/_2d/crop.py
 kornia/augmentation/random_generator/_2d/cutmix.py
-kornia/augmentation/random_generator/_2d/func.py
 kornia/augmentation/random_generator/_2d/jigsaw.py
 kornia/augmentation/random_generator/_2d/mixup.py
 kornia/augmentation/random_generator/_2d/mosaic.py
 kornia/augmentation/random_generator/_2d/motion_blur.py
 kornia/augmentation/random_generator/_2d/perspective.py
 kornia/augmentation/random_generator/_2d/plain_uniform.py
 kornia/augmentation/random_generator/_2d/planckian_jitter.py
@@ -115,14 +118,15 @@
 kornia/color/gray.py
 kornia/color/hls.py
 kornia/color/hsv.py
 kornia/color/lab.py
 kornia/color/luv.py
 kornia/color/raw.py
 kornia/color/rgb.py
+kornia/color/sepia.py
 kornia/color/xyz.py
 kornia/color/ycbcr.py
 kornia/color/yuv.py
 kornia/contrib/__init__.py
 kornia/contrib/classification.py
 kornia/contrib/connected_components.py
 kornia/contrib/diamond_square.py
@@ -133,14 +137,15 @@
 kornia/contrib/histogram_matching.py
 kornia/contrib/image_stitching.py
 kornia/contrib/lambda_module.py
 kornia/contrib/vit.py
 kornia/contrib/vit_mobile.py
 kornia/core/__init__.py
 kornia/core/_backend.py
+kornia/core/tensor_wrapper.py
 kornia/enhance/__init__.py
 kornia/enhance/adjust.py
 kornia/enhance/core.py
 kornia/enhance/equalization.py
 kornia/enhance/histogram.py
 kornia/enhance/normalize.py
 kornia/enhance/shift_rgb.py
@@ -202,16 +207,19 @@
 kornia/geometry/bbox.py
 kornia/geometry/boxes.py
 kornia/geometry/conversions.py
 kornia/geometry/depth.py
 kornia/geometry/homography.py
 kornia/geometry/linalg.py
 kornia/geometry/line.py
+kornia/geometry/plane.py
 kornia/geometry/quaternion.py
 kornia/geometry/ransac.py
+kornia/geometry/ray.py
+kornia/geometry/vector.py
 kornia/geometry/calibration/__init__.py
 kornia/geometry/calibration/distort.py
 kornia/geometry/calibration/pnp.py
 kornia/geometry/calibration/undistort.py
 kornia/geometry/camera/__init__.py
 kornia/geometry/camera/perspective.py
 kornia/geometry/camera/pinhole.py
@@ -220,14 +228,20 @@
 kornia/geometry/epipolar/_metrics.py
 kornia/geometry/epipolar/essential.py
 kornia/geometry/epipolar/fundamental.py
 kornia/geometry/epipolar/numeric.py
 kornia/geometry/epipolar/projection.py
 kornia/geometry/epipolar/scene.py
 kornia/geometry/epipolar/triangulation.py
+kornia/geometry/liegroup/__init__.py
+kornia/geometry/liegroup/_utils.py
+kornia/geometry/liegroup/se2.py
+kornia/geometry/liegroup/se3.py
+kornia/geometry/liegroup/so2.py
+kornia/geometry/liegroup/so3.py
 kornia/geometry/subpix/__init__.py
 kornia/geometry/subpix/dsnt.py
 kornia/geometry/subpix/nms.py
 kornia/geometry/subpix/spatial_soft_argmax.py
 kornia/geometry/transform/__init__.py
 kornia/geometry/transform/affwarp.py
 kornia/geometry/transform/crop2d.py
```

### Comparing `kornia-0.6.8/setup.cfg` & `kornia-0.6.9/setup.cfg`

 * *Files 16% similar despite different names*

```diff
@@ -1,18 +1,17 @@
 [metadata]
 name = kornia
-version = 0.6.8
+version = 0.6.9
 description = Open Source Differentiable Computer Vision Library for PyTorch
 long_description = file: README.md
 long_description_content_type = text/markdown
 url = https://www.kornia.org
 author = Edgar Riba
 author_email = edgar@kornia.org
 license = Apache-2.0
-license_file = LICENSE
 license_files = LICENSE
 classifiers = 
 	Development Status :: 4 - Beta
 	Environment :: Console
 	Environment :: GPU
 	Intended Audience :: Developers
 	Intended Audience :: Education
@@ -33,16 +32,16 @@
 	Documentation = https://kornia.readthedocs.io/en/latest
 	Source Code =  https://github.com/kornia/kornia
 
 [options]
 packages = find:
 install_requires = 
 	packaging
-	torch>=1.8.1
-python_requires = >=3.6
+	torch>=1.9.1
+python_requires = >=3.7
 setup_requires = 
 	pytest-runner
 tests_require = 
 	pytest
 zip_safe = True
 
 [options.packages.find]
@@ -50,25 +49,24 @@
 	docs*
 	test
 	test.*
 	examples*
 
 [options.extras_require]
 dev = 
-	flake8==5.0.4
 	isort
+	kornia-rs==0.0.5
+	mypy[reports]
 	numpy
 	opencv-python
 	pre-commit>=2.0
 	pydocstyle
-	pytest==7.1.3
+	pytest==7.2.0
 	pytest-cov==4.0.0
-	pytest-mypy==0.10.0
 	scipy
-	kornia-rs==0.0.5;python_version>="3.7"
 docs = 
 	PyYAML>=5.1,<6.1.0
 	furo
 	matplotlib
 	opencv-python
 	sphinx>=4.0
 	sphinx-autodoc-defaultargs
@@ -78,56 +76,59 @@
 	sphinx-rtd-theme>0.5
 	sphinxcontrib-bibtex
 	sphinxcontrib-gtagjs
 	sphinxcontrib-youtube
 	torch
 	torchvision
 x = 
-	accelerate==0.13.1
-	dataclasses;python_version<"3.7"
+	accelerate==0.15.0
+
+[options.package_data]
+kornia = py.typed
 
 [bdist_wheel]
 universal = True
 
 [aliases]
 test = pytest
 
 [tool:pytest]
 norecursedirs = 
 	.git
 	.github
 	dist
 	build
 addopts = 
-	--strict
 	--color=yes
 markers = 
 	jit: mark a test as torchscript test
 	grad: mark a test as gradcheck test
 	nn: mark a test as module test
 
 [flake8]
 max-line-length = 120
 exclude = 
 	docs
-	versioneer.py
-	kornia/_version.py
 per-file-ignores = 
 	__init__.py: F401, F403
 verbose = 2
 ignore = 
-	W503  # Ignore "Line break occurred before a binary operator"
-	E203  # Ignore "whitespace before ':'"
+	W503
+	E203
 
 [mypy]
-files = kornia, test
+files = kornia/
 pretty = True
+check_untyped_defs = True
+disallow_any_generics = True
 show_error_codes = True
 ignore_missing_imports = True
 no_implicit_optional = True
+warn_unused_ignores = True
+warn_redundant_casts = True
 
 [pydocstyle]
 match = .*\.py
 ignore = 
 	D105
 	D107
 	D203
```

### Comparing `kornia-0.6.8/test/test_contrib.py` & `kornia-0.6.9/test/test_contrib.py`

 * *Files 7% similar despite different names*

```diff
@@ -4,15 +4,14 @@
 import torch
 from torch.autograd import gradcheck
 
 import kornia
 import kornia.testing as utils  # test utils
 from kornia.contrib.face_detection import FaceKeypoint
 from kornia.testing import assert_close
-from packaging import version
 
 
 class TestDiamondSquare:
     def test_smoke(self, device, dtype):
         torch.manual_seed(0)
         output_size = (1, 1, 3, 4)
         roughness = 0.5
@@ -137,21 +136,18 @@
             device=device,
             dtype=dtype,
         )
 
         out = kornia.contrib.connected_components(img, num_iterations=10)
         assert_close(out, expected)
 
-    @pytest.mark.skipif(
-        version.parse(torch.__version__) < version.parse("1.9"), reason="Tuple cannot be used with PyTorch < v1.9"
-    )
     def test_gradcheck(self, device, dtype):
         B, C, H, W = 2, 1, 4, 4
         img = torch.ones(B, C, H, W, device=device, dtype=torch.float64, requires_grad=True)
-        assert gradcheck(kornia.contrib.connected_components, (img,), raise_exception=True)
+        assert gradcheck(kornia.contrib.connected_components, (img,), raise_exception=True, fast_mode=True)
 
     def test_jit(self, device, dtype):
         B, C, H, W = 2, 1, 4, 4
         img = torch.ones(B, C, H, W, device=device, dtype=dtype)
         op = kornia.contrib.connected_components
         op_jit = torch.jit.script(op)
         assert_close(op(img), op_jit(img))
@@ -274,15 +270,15 @@
         expected = kornia.denormalize_pixel_coordinates(grid, height, width)
 
         assert_close(actual, expected)
 
     def test_gradcheck(self, device):
         img = torch.rand(2, 3, 4, 4).to(device)
         img = utils.tensor_to_gradcheck_var(img)  # to var
-        assert gradcheck(kornia.contrib.extract_tensor_patches, (img, 3), raise_exception=True)
+        assert gradcheck(kornia.contrib.extract_tensor_patches, (img, 3), raise_exception=True, fast_mode=True)
 
 
 class TestCombineTensorPatches:
     def test_smoke(self, device, dtype):
         img = torch.arange(16, device=device, dtype=dtype).view(1, 1, 4, 4)
         m = kornia.contrib.CombineTensorPatches((4, 4), (2, 2))
         patches = kornia.contrib.extract_tensor_patches(img, window_size=(2, 2), stride=(2, 2))
@@ -365,15 +361,17 @@
         assert (img == m(patches)).all()
 
     def test_gradcheck(self, device, dtype):
         patches = kornia.contrib.extract_tensor_patches(
             torch.arange(16.0, device=device, dtype=dtype).view(1, 1, 4, 4), window_size=(2, 2), stride=(2, 2)
         )
         img = utils.tensor_to_gradcheck_var(patches)  # to var
-        assert gradcheck(kornia.contrib.combine_tensor_patches, (img, (4, 4), (2, 2), (2, 2)), raise_exception=True)
+        assert gradcheck(
+            kornia.contrib.combine_tensor_patches, (img, (4, 4), (2, 2), (2, 2)), raise_exception=True, fast_mode=True
+        )
 
 
 class TestLambdaModule:
     def add_2_layer(self, tensor):
         return tensor + 2
 
     def add_x_mul_y(self, tensor, x, y=2):
@@ -405,39 +403,124 @@
         out = lambda_module(img)
         assert isinstance(out, torch.Tensor)
 
     def test_gradcheck(self, device, dtype):
         B, C, H, W = 1, 3, 4, 5
         img = torch.rand(B, C, H, W, device=device, dtype=torch.float64, requires_grad=True)
         func = kornia.color.bgr_to_grayscale
-        assert gradcheck(kornia.contrib.Lambda(func), (img,), raise_exception=True)
+        assert gradcheck(kornia.contrib.Lambda(func), (img,), raise_exception=True, fast_mode=True)
 
 
 class TestImageStitcher:
     @pytest.mark.parametrize("estimator", ['ransac', 'vanilla'])
     def test_smoke(self, estimator, device, dtype):
-        B, C, H, W = 1, 3, 224, 224
-        sample1 = torch.rand(B, C, H, W, device=device, dtype=dtype)
-        sample2 = torch.rand(B, C, H, W, device=device, dtype=dtype)
+        B, C, H, W = 1, 3, 6, 6
+        sample1 = torch.tensor(
+            [
+                [0.5349, 0.1988, 0.6592, 0.6569, 0.2328, 0.4251],
+                [0.2071, 0.6297, 0.3653, 0.8513, 0.8549, 0.5509],
+                [0.2868, 0.2063, 0.4451, 0.3593, 0.7204, 0.0731],
+                [0.9699, 0.1078, 0.8829, 0.4132, 0.7572, 0.6948],
+                [0.5209, 0.5932, 0.8797, 0.6286, 0.7653, 0.1132],
+                [0.8559, 0.6721, 0.6267, 0.5691, 0.7437, 0.9592],
+            ],
+            dtype=dtype,
+            device=device,
+        )
+        sample2 = torch.tensor(
+            [
+                [0.3887, 0.2214, 0.3742, 0.1953, 0.7405, 0.2529],
+                [0.2332, 0.9314, 0.9575, 0.5575, 0.4134, 0.4355],
+                [0.7369, 0.0331, 0.0914, 0.8994, 0.9936, 0.4703],
+                [0.1049, 0.5137, 0.2674, 0.4990, 0.7447, 0.7213],
+                [0.4414, 0.5550, 0.6361, 0.1081, 0.3305, 0.5196],
+                [0.2147, 0.2816, 0.6679, 0.7878, 0.5070, 0.3055],
+            ],
+            dtype=dtype,
+            device=device,
+        )
+        sample1 = sample1.expand((B, C, H, W))
+        sample2 = sample2.expand((B, C, H, W))
         return_value = {
-            "keypoints0": torch.rand((15, 2), device=device, dtype=dtype),
-            "keypoints1": torch.rand((15, 2), device=device, dtype=dtype),
-            "confidence": torch.rand((15,), device=device, dtype=dtype),
+            "keypoints0": torch.tensor(
+                [
+                    [0.1546, 0.9391],
+                    [0.8077, 0.1051],
+                    [0.6768, 0.5596],
+                    [0.5092, 0.7195],
+                    [0.2856, 0.8889],
+                    [0.4342, 0.0203],
+                    [0.6701, 0.0585],
+                    [0.3828, 0.9038],
+                    [0.7301, 0.0762],
+                    [0.7864, 0.4490],
+                    [0.3509, 0.0756],
+                    [0.6782, 0.9297],
+                    [0.4132, 0.3664],
+                    [0.3134, 0.5039],
+                    [0.2073, 0.2552],
+                ],
+                device=device,
+                dtype=dtype,
+            ),
+            "keypoints1": torch.tensor(
+                [
+                    [0.2076, 0.2669],
+                    [0.9679, 0.8137],
+                    [0.9536, 0.8317],
+                    [0.3718, 0.2456],
+                    [0.3875, 0.8450],
+                    [0.7592, 0.1687],
+                    [0.5173, 0.6760],
+                    [0.9446, 0.4570],
+                    [0.6164, 0.1867],
+                    [0.4732, 0.1786],
+                    [0.4090, 0.8089],
+                    [0.9742, 0.8943],
+                    [0.5996, 0.7427],
+                    [0.7038, 0.9210],
+                    [0.6272, 0.0796],
+                ],
+                device=device,
+                dtype=dtype,
+            ),
+            "confidence": torch.tensor(
+                [
+                    0.9314,
+                    0.5951,
+                    0.4187,
+                    0.0318,
+                    0.1434,
+                    0.7952,
+                    0.8306,
+                    0.7511,
+                    0.6407,
+                    0.7379,
+                    0.4363,
+                    0.9220,
+                    0.8453,
+                    0.5075,
+                    0.8141,
+                ],
+                device=device,
+                dtype=dtype,
+            ),
             "batch_indexes": torch.zeros((15,), device=device, dtype=dtype),
         }
         with patch(
             'kornia.contrib.ImageStitcher.on_matcher', new_callable=PropertyMock, return_value=lambda x: return_value
         ):
             # NOTE: This will need to download the pretrained weights.
             # To avoid that, we mock as below
             matcher = kornia.feature.LoFTR(None)
             stitcher = kornia.contrib.ImageStitcher(matcher, estimator=estimator).to(device=device, dtype=dtype)
+            torch.manual_seed(1)  # issue kornia#2027
             out = stitcher(sample1, sample2)
-            assert out.shape[:-1] == torch.Size([1, 3, 224])
-            assert out.shape[-1] <= 448
+            assert out.shape[:-1] == torch.Size([1, 3, 6])
+            assert out.shape[-1] <= 12
 
     def test_exception(self, device, dtype):
         B, C, H, W = 1, 3, 224, 224
         sample1 = torch.rand(B, C, H, W, device=device, dtype=dtype)
         sample2 = torch.rand(B, C, H, W, device=device, dtype=dtype)
         # NOTE: This will need to download the pretrained weights.
         matcher = kornia.feature.LoFTR(None)
@@ -517,15 +600,15 @@
         )
         output1 = kornia.contrib.distance_transform(sample1, kernel_size, h)
         assert_close(expected_output1, output1)
 
     def test_gradcheck(self, device):
         B, C, H, W = 1, 1, 32, 32
         sample1 = torch.ones(B, C, H, W, device=device, dtype=torch.float64, requires_grad=True)
-        assert gradcheck(kornia.contrib.distance_transform, (sample1), raise_exception=True)
+        assert gradcheck(kornia.contrib.distance_transform, (sample1), raise_exception=True, fast_mode=True)
 
     def test_loss_grad(self, device, dtype):
         B, C, H, W = 1, 1, 32, 32
         sample1 = torch.rand(B, C, H, W, device=device, dtype=dtype, requires_grad=True)
         sample2 = torch.rand(B, C, H, W, device=device, dtype=dtype, requires_grad=True)
         tiny_module = torch.nn.Conv2d(1, 1, (3, 3), (1, 1), (1, 1)).to(device=device, dtype=dtype)
         sample1 = kornia.contrib.distance_transform(tiny_module(sample1))
@@ -565,27 +648,32 @@
         assert_close(out, exp, rtol=1e-4, atol=1e-4)
 
     @pytest.mark.skip(reason="not differentiable now.")
     def test_grad(self, device):
         B, C, H, W = 1, 3, 32, 32
         src = torch.rand(B, C, H, W, device=device, dtype=torch.float64, requires_grad=True)
         dst = torch.rand(B, C, H, W, device=device, dtype=torch.float64, requires_grad=True)
-        assert gradcheck(kornia.contrib.histogram_matching, (src, dst), raise_exception=True)
+        assert gradcheck(kornia.contrib.histogram_matching, (src, dst), raise_exception=True, fast_mode=True)
 
 
 class TestFaceDetection:
     def test_smoke(self, device, dtype):
         assert kornia.contrib.FaceDetector().to(device, dtype) is not None
 
-    def test_valid(self, device, dtype):
+    @pytest.mark.parametrize("batch_size", [1, 2, 4])
+    def test_valid(self, batch_size, device, dtype):
         torch.manual_seed(44)
-        img = torch.rand(1, 3, 320, 320, device=device, dtype=dtype)
+        img = torch.rand(batch_size, 3, 320, 320, device=device, dtype=dtype)
         face_detection = kornia.contrib.FaceDetector().to(device, dtype)
         dets = face_detection(img)
-        assert len(dets) == 1
+        assert isinstance(dets, list)
+        assert len(dets) == batch_size  # same as the number of images
+        assert isinstance(dets[0], torch.Tensor)
+        assert dets[0].shape[0] >= 0  # number of detections
+        assert dets[0].shape[1] == 15  # dims of each detection
 
     def test_jit(self, device, dtype):
         op = kornia.contrib.FaceDetector().to(device, dtype)
         op_jit = torch.jit.script(op)
         assert op_jit is not None
 
     def test_results(self, device, dtype):
```

### Comparing `kornia-0.6.8/test/test_metrics.py` & `kornia-0.6.9/test/test_metrics.py`

 * *Files identical despite different names*

