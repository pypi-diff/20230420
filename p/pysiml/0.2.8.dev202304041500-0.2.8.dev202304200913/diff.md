# Comparing `tmp/pysiml-0.2.8.dev202304041500-py3-none-any.whl.zip` & `tmp/pysiml-0.2.8.dev202304200913-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,28 +1,28 @@
-Zip file size: 135015 bytes, number of entries: 87
+Zip file size: 138987 bytes, number of entries: 92
 -rw-r--r--  2.0 unx     1499 b- defN 80-Jan-01 00:00 pyproject.toml
--rw-r--r--  2.0 unx      568 b- defN 80-Jan-01 00:00 siml/__init__.py
+-rw-r--r--  2.0 unx      638 b- defN 80-Jan-01 00:00 siml/__init__.py
 -rw-r--r--  2.0 unx        0 b- defN 80-Jan-01 00:00 siml/__main__/__init__.py
 -rw-r--r--  2.0 unx      970 b- defN 80-Jan-01 00:00 siml/__main__/convert_interim_data.py
--rw-r--r--  2.0 unx     1374 b- defN 80-Jan-01 00:00 siml/__main__/convert_raw_data.py
+-rw-r--r--  2.0 unx     1400 b- defN 80-Jan-01 00:00 siml/__main__/convert_raw_data.py
 -rw-r--r--  2.0 unx     1742 b- defN 80-Jan-01 00:00 siml/__main__/optimize.py
 -rw-r--r--  2.0 unx     8663 b- defN 80-Jan-01 00:00 siml/__main__/plot_losses.py
 -rw-r--r--  2.0 unx      929 b- defN 80-Jan-01 00:00 siml/__main__/prepare_preprocess_converters.py
 -rw-r--r--  2.0 unx      712 b- defN 80-Jan-01 00:00 siml/__main__/preprocess_interim_data.py
 -rw-r--r--  2.0 unx     1376 b- defN 80-Jan-01 00:00 siml/__main__/train.py
 -rw-r--r--  2.0 unx     3961 b- defN 80-Jan-01 00:00 siml/__main__/visualize_graph.py
 -rw-r--r--  2.0 unx       57 b- defN 80-Jan-01 00:00 siml/config.py
 -rw-r--r--  2.0 unx     3520 b- defN 80-Jan-01 00:00 siml/data_parallel.py
--rw-r--r--  2.0 unx    26594 b- defN 80-Jan-01 00:00 siml/datasets.py
--rw-r--r--  2.0 unx    26955 b- defN 80-Jan-01 00:00 siml/inferer.py
+-rw-r--r--  2.0 unx    26681 b- defN 80-Jan-01 00:00 siml/datasets.py
+-rw-r--r--  2.0 unx    27000 b- defN 80-Jan-01 00:00 siml/inferer.py
 -rw-r--r--  2.0 unx     1466 b- defN 80-Jan-01 00:00 siml/loss_operations/loss_assignment.py
 -rw-r--r--  2.0 unx     3734 b- defN 80-Jan-01 00:00 siml/loss_operations/loss_calculator.py
 -rw-r--r--  2.0 unx     2064 b- defN 80-Jan-01 00:00 siml/loss_operations/loss_selector.py
 -rw-r--r--  2.0 unx       58 b- defN 80-Jan-01 00:00 siml/loss_operations/loss_type.py
--rw-r--r--  2.0 unx     1347 b- defN 80-Jan-01 00:00 siml/mains.py
+-rw-r--r--  2.0 unx     1364 b- defN 80-Jan-01 00:00 siml/mains.py
 -rw-r--r--  2.0 unx     2724 b- defN 80-Jan-01 00:00 siml/networks/__init__.py
 -rw-r--r--  2.0 unx    11337 b- defN 80-Jan-01 00:00 siml/networks/abstract_equivariant_gnn.py
 -rw-r--r--  2.0 unx     6505 b- defN 80-Jan-01 00:00 siml/networks/abstract_gcn.py
 -rw-r--r--  2.0 unx     1007 b- defN 80-Jan-01 00:00 siml/networks/activation.py
 -rw-r--r--  2.0 unx     4565 b- defN 80-Jan-01 00:00 siml/networks/activations.py
 -rw-r--r--  2.0 unx      715 b- defN 80-Jan-01 00:00 siml/networks/array2diagmat.py
 -rw-r--r--  2.0 unx     1576 b- defN 80-Jan-01 00:00 siml/networks/array2symmat.py
@@ -64,26 +64,31 @@
 -rw-r--r--  2.0 unx     5969 b- defN 80-Jan-01 00:00 siml/networks/tensor_operations.py
 -rw-r--r--  2.0 unx     1415 b- defN 80-Jan-01 00:00 siml/networks/threshold.py
 -rw-r--r--  2.0 unx      782 b- defN 80-Jan-01 00:00 siml/networks/time_norm.py
 -rw-r--r--  2.0 unx     1882 b- defN 80-Jan-01 00:00 siml/networks/translator.py
 -rw-r--r--  2.0 unx     1034 b- defN 80-Jan-01 00:00 siml/networks/upper_limit.py
 -rw-r--r--  2.0 unx     6940 b- defN 80-Jan-01 00:00 siml/optimize.py
 -rw-r--r--  2.0 unx     5523 b- defN 80-Jan-01 00:00 siml/postprocessor.py
--rw-r--r--  2.0 unx    63227 b- defN 80-Jan-01 00:00 siml/prepost.py
--rw-r--r--  2.0 unx    48874 b- defN 80-Jan-01 00:00 siml/setting.py
+-rw-r--r--  2.0 unx    45390 b- defN 80-Jan-01 00:00 siml/prepost.py
+-rw-r--r--  2.0 unx       33 b- defN 80-Jan-01 00:00 siml/preprocessing/__init__.py
+-rw-r--r--  2.0 unx    20676 b- defN 80-Jan-01 00:00 siml/preprocessing/converter.py
+-rw-r--r--  2.0 unx    49064 b- defN 80-Jan-01 00:00 siml/setting.py
 -rw-r--r--  2.0 unx    11036 b- defN 80-Jan-01 00:00 siml/siml_manager.py
 -rw-r--r--  2.0 unx    13021 b- defN 80-Jan-01 00:00 siml/study.py
 -rw-r--r--  2.0 unx    34860 b- defN 80-Jan-01 00:00 siml/trainer.py
 -rw-r--r--  2.0 unx      211 b- defN 80-Jan-01 00:00 siml/update_functions/__init__.py
 -rw-r--r--  2.0 unx     1774 b- defN 80-Jan-01 00:00 siml/update_functions/element_batch_update.py
 -rw-r--r--  2.0 unx     3590 b- defN 80-Jan-01 00:00 siml/update_functions/pseudo_batch_update.py
 -rw-r--r--  2.0 unx     2669 b- defN 80-Jan-01 00:00 siml/update_functions/standard_update.py
 -rw-r--r--  2.0 unx      429 b- defN 80-Jan-01 00:00 siml/update_functions/update_interface.py
 -rw-r--r--  2.0 unx    37201 b- defN 80-Jan-01 00:00 siml/util.py
+-rw-r--r--  2.0 unx       71 b- defN 80-Jan-01 00:00 siml/utils/__init__.py
+-rw-r--r--  2.0 unx     6090 b- defN 80-Jan-01 00:00 siml/utils/fem_data_utils.py
+-rw-r--r--  2.0 unx     2147 b- defN 80-Jan-01 00:00 siml/utils/path_utils.py
 -rw-r--r--  2.0 unx       58 b- defN 80-Jan-01 00:00 siml/variables/__init__.py
 -rw-r--r--  2.0 unx     3487 b- defN 80-Jan-01 00:00 siml/variables/siml_variables.py
-?rw-r--r--  2.0 unx      388 b- defN 16-Jan-01 00:00 pysiml-0.2.8.dev202304041500.dist-info/entry_points.txt
--rw-r--r--  2.0 unx    11431 b- defN 80-Jan-01 00:00 pysiml-0.2.8.dev202304041500.dist-info/LICENSE
-?rw-r--r--  2.0 unx       83 b- defN 16-Jan-01 00:00 pysiml-0.2.8.dev202304041500.dist-info/WHEEL
-?rw-r--r--  2.0 unx     1595 b- defN 16-Jan-01 00:00 pysiml-0.2.8.dev202304041500.dist-info/METADATA
-?rw-r--r--  2.0 unx     7327 b- defN 16-Jan-01 00:00 pysiml-0.2.8.dev202304041500.dist-info/RECORD
-87 files, 522313 bytes uncompressed, 123515 bytes compressed:  76.4%
+?rw-r--r--  2.0 unx      388 b- defN 16-Jan-01 00:00 pysiml-0.2.8.dev202304200913.dist-info/entry_points.txt
+-rw-r--r--  2.0 unx    11431 b- defN 80-Jan-01 00:00 pysiml-0.2.8.dev202304200913.dist-info/LICENSE
+?rw-r--r--  2.0 unx       83 b- defN 16-Jan-01 00:00 pysiml-0.2.8.dev202304200913.dist-info/WHEEL
+?rw-r--r--  2.0 unx     1595 b- defN 16-Jan-01 00:00 pysiml-0.2.8.dev202304200913.dist-info/METADATA
+?rw-r--r--  2.0 unx     7744 b- defN 16-Jan-01 00:00 pysiml-0.2.8.dev202304200913.dist-info/RECORD
+92 files, 534345 bytes uncompressed, 126837 bytes compressed:  76.3%
```

## zipnote {}

```diff
@@ -204,14 +204,20 @@
 
 Filename: siml/postprocessor.py
 Comment: 
 
 Filename: siml/prepost.py
 Comment: 
 
+Filename: siml/preprocessing/__init__.py
+Comment: 
+
+Filename: siml/preprocessing/converter.py
+Comment: 
+
 Filename: siml/setting.py
 Comment: 
 
 Filename: siml/siml_manager.py
 Comment: 
 
 Filename: siml/study.py
@@ -234,29 +240,38 @@
 
 Filename: siml/update_functions/update_interface.py
 Comment: 
 
 Filename: siml/util.py
 Comment: 
 
+Filename: siml/utils/__init__.py
+Comment: 
+
+Filename: siml/utils/fem_data_utils.py
+Comment: 
+
+Filename: siml/utils/path_utils.py
+Comment: 
+
 Filename: siml/variables/__init__.py
 Comment: 
 
 Filename: siml/variables/siml_variables.py
 Comment: 
 
-Filename: pysiml-0.2.8.dev202304041500.dist-info/entry_points.txt
+Filename: pysiml-0.2.8.dev202304200913.dist-info/entry_points.txt
 Comment: 
 
-Filename: pysiml-0.2.8.dev202304041500.dist-info/LICENSE
+Filename: pysiml-0.2.8.dev202304200913.dist-info/LICENSE
 Comment: 
 
-Filename: pysiml-0.2.8.dev202304041500.dist-info/WHEEL
+Filename: pysiml-0.2.8.dev202304200913.dist-info/WHEEL
 Comment: 
 
-Filename: pysiml-0.2.8.dev202304041500.dist-info/METADATA
+Filename: pysiml-0.2.8.dev202304200913.dist-info/METADATA
 Comment: 
 
-Filename: pysiml-0.2.8.dev202304041500.dist-info/RECORD
+Filename: pysiml-0.2.8.dev202304200913.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## pyproject.toml

```diff
@@ -1,10 +1,10 @@
 [tool.poetry]
 name = "pysiml"
-version = "0.2.8.dev202304041500"
+version = "0.2.8.dev202304200913"
 description = "SiML - a Simulation ML library"
 license = "Apache-2.0"
 authors = ["RICOS Co. Ltd."]
 readme = "README.md"
 repository = "https://github.com/ricosjp/pysiml"
 documentation = "https://ricosjp.github.io/pysiml/"
 packages = [
```

## siml/__init__.py

```diff
@@ -10,14 +10,16 @@
 from siml import networks  # NOQA
 from siml import optimize  # NOQA
 from siml import prepost  # NOQA
 from siml import setting  # NOQA
 from siml import study  # NOQA
 from siml import trainer  # NOQA
 from siml import util  # NOQA
+from siml import preprocessing  # NOQA
+from siml import utils  # NOQA
 
 
 def get_version():
     path = Path(__file__).resolve().parent.parent / 'pyproject.toml'
     pyproject = toml.loads(open(str(path)).read())
     return pyproject['tool']['poetry']['version']
```

## siml/__main__/convert_raw_data.py

```diff
@@ -1,12 +1,12 @@
 import argparse
 from distutils.util import strtobool
 import pathlib
 
-import siml
+from siml.preprocessing import converter
 
 
 def main(
         add_argument=None, conversion_function=None, filter_function=None,
         load_function=None):
     parser = argparse.ArgumentParser()
     parser.add_argument(
@@ -27,15 +27,15 @@
     parser.add_argument(
         '-r', '--recursive',
         type=strtobool,
         default=1,
         help='If True, process directory recursively [True]')
     args = parser.parse_args()
 
-    raw_converter = siml.prepost.RawConverter.read_settings(
+    raw_converter = converter.RawConverter.read_settings(
         args.settings_yaml,
         conversion_function=conversion_function,
         filter_function=filter_function,
         load_function=load_function,
         force_renew=args.force_renew,
         recursive=args.recursive,
         to_first_order=True,
```

## siml/datasets.py

```diff
@@ -2,15 +2,15 @@
 
 import femio
 from ignite.utils import convert_tensor
 import numpy as np
 import torch
 from tqdm import tqdm
 
-from . import prepost
+from .utils import fem_data_utils
 from . import util
 
 
 class BaseDataset(torch.utils.data.Dataset):
 
     def __init__(
             self, x_variable_names, y_variable_names, directories, *,
@@ -192,17 +192,19 @@
         if self.conversion_setting.skip_femio:
             dict_data = {}
             fem_data = None
         else:
             fem_data = femio.FEMData.read_directory(
                 self.conversion_setting.file_type, raw_data_directory,
                 save=False)
-            dict_data = prepost.extract_variables(
-                fem_data, self.conversion_setting.mandatory,
-                optional_variables=self.conversion_setting.optional)
+            wrapped_data = fem_data_utils.FemDataWrapper(fem_data)
+            dict_data = wrapped_data.extract_variables(
+                self.conversion_setting.mandatory,
+                optional_variables=self.conversion_setting.optional
+            )
 
         if self.conversion_function is not None:
             dict_data.update(self.conversion_function(
                 fem_data, raw_data_directory))
 
         if self.load_function is not None:
             data_files = util.collect_files(
```

## siml/inferer.py

```diff
@@ -13,14 +13,15 @@
 from . import datasets
 from . import networks
 from . import postprocessor
 from . import prepost
 from . import setting
 from . import siml_manager
 from . import util
+from .utils import path_utils
 
 
 class Inferer(siml_manager.SimlManager):
 
     @classmethod
     def read_settings(cls, settings_yaml, **kwargs):
         """Read settings.yaml to generate Inferer object.
@@ -611,21 +612,21 @@
             data_directory = ''
         if self.setting.inferer.output_directory is not None:
             return self.setting.inferer.output_directory
 
         subdirectory = self._determine_subdirectory()
         base = self.setting.inferer.output_directory_base / subdirectory
         if 'preprocessed' in str(data_directory):
-            output_directory = prepost.determine_output_directory(
+            output_directory = path_utils.determine_output_directory(
                 data_directory, base, 'preprocessed')
         elif 'interim' in str(data_directory):
-            output_directory = prepost.determine_output_directory(
+            output_directory = path_utils.determine_output_directory(
                 data_directory, base, 'interim')
         elif 'raw' in str(data_directory):
-            output_directory = prepost.determine_output_directory(
+            output_directory = path_utils.determine_output_directory(
                 data_directory, base, 'raw')
         else:
             output_directory = base
         return output_directory
 
     def _determine_subdirectory(self):
         if self.setting.inferer.model is not None:
@@ -659,21 +660,21 @@
                             return interim_candidate
                         else:
                             return None
                 else:
                     return None
 
         if 'preprocessed' in str(data_directory):
-            write_simulation_base = prepost.determine_output_directory(
+            write_simulation_base = path_utils.determine_output_directory(
                 data_directory,
                 self.setting.inferer.write_simulation_base,
                 'preprocessed')
 
         elif 'interim' in str(data_directory):
-            write_simulation_base = prepost.determine_output_directory(
+            write_simulation_base = path_utils.determine_output_directory(
                 data_directory,
                 self.setting.inferer.write_simulation_base,
                 'interim')
         elif 'raw' in str(data_directory):
             write_simulation_base = data_directory
         else:
             write_simulation_base \
```

## siml/mains.py

```diff
@@ -1,12 +1,12 @@
 import argparse
 from distutils.util import strtobool
 import pathlib
 
-from . import prepost
+from .preprocessing import converter
 
 
 def convert_raw_data(
         add_argument=None, conversion_function=None, filter_function=None,
         load_function=None, **kwargs):
     parser = argparse.ArgumentParser()
     parser.add_argument(
@@ -28,15 +28,15 @@
         '-r', '--recursive',
         type=strtobool,
         default=1,
         help='If True, process directory recursively [True]')
     parser = add_argument(parser)
     args = parser.parse_args()
 
-    raw_converter = prepost.RawConverter.read_settings(
+    raw_converter = converter.RawConverter.read_settings(
         args.settings_yaml,
         conversion_function=conversion_function,
         filter_function=filter_function,
         load_function=load_function,
         force_renew=args.force_renew,
         recursive=args.recursive,
         read_npy=args.read_npy, **kwargs)
```

## siml/prepost.py

```diff
@@ -1,360 +1,24 @@
 """Module for preprocessing."""
 
 import datetime as dt
-from functools import reduce
 import glob
 import io
 import itertools as it
 import multiprocessing as multi
-from operator import or_
-import os
-from pathlib import Path
 import pickle
-from typing import Dict
+from pathlib import Path
 
 import femio
 import matplotlib.pyplot as plt
 import numpy as np
 import scipy.sparse as sp
 
-from . import util
-from . import setting
-
-
-FEMIO_FILE = 'femio_npy_saved.npy'
-
-
-class RawConverter():
-
-    @classmethod
-    def read_settings(cls, settings_yaml, **args):
-        main_setting = setting.MainSetting.read_settings_yaml(
-            settings_yaml, replace_preprocessed=False)
-        return cls(main_setting, **args)
-
-    def __init__(
-            self, main_setting, *,
-            recursive=True,
-            conversion_function=None, filter_function=None, load_function=None,
-            save_function=None,
-            force_renew=False, read_npy=False, write_ucd=True, read_res=True,
-            max_process=None, to_first_order=False):
-        """Initialize converter of raw data and save them in interim directory.
-
-        Parameters
-        ----------
-        main_setting: siml.setting.MainSetting
-            MainSetting object.
-        recursive: bool, optional
-            If True, recursively convert data.
-        conversion_function: callable, optional
-            Conversion function which takes femio.FEMData object and
-            pathlib.Path (data directory) as only arguments and returns data
-            dict to be saved.
-        filter_function: callable, optional
-            Function to filter the data which can be converted. It should take
-            femio.FEMData object, pathlib.Path (data directory), and dict_data
-            as only arguments and returns True (for convertable data) or False
-            (for unconvertable data).
-        load_function: callable, optional
-            Function to load data, which take list of pathlib.Path objects
-            (as required files) and pathlib.Path object (as data directory)
-            and returns data_dictionary and fem_data (can be None) to be saved.
-        save_function: callable, optional
-            Function to save data, which take femio.FEMData object,
-            data_dict, pathliub.Path object as output directory,
-            and bool represents force renew.
-        force_renew: bool, optional
-            If True, renew npy files even if they are alerady exist.
-        read_npy: bool, optional
-            If True, read .npy files instead of original files if exists.
-        write_ucd: bool, optional
-            If True, write AVS UCD file with preprocessed variables.
-        read_res: bool, optional
-            If True, read res file of FrontISTR.
-        max_process: int, optional
-            The maximum number of processes to perform conversion.
-        """
-        self.setting: setting.MainSetting = main_setting
-        self.recursive = recursive
-        self.conversion_function = conversion_function
-        self.filter_function = filter_function
-        self.load_function = load_function
-        self.save_function = save_function
-        self.force_renew = force_renew
-        self.read_npy = read_npy
-        self.write_ucd = write_ucd
-        self.to_first_order = to_first_order
-        self.read_res = read_res
-        self.max_process = min(
-            main_setting.conversion.max_process,
-            util.determine_max_process(max_process))
-        self.setting.conversion.output_base_directory \
-            = self.setting.data.interim_root
-
-    def convert(self, raw_directory=None):
-        """Perform conversion.
-
-        Parameters
-        ----------
-        raw_directory: str or pathlib.Path, optional
-            Raw data directory name. If not fed, self.setting.data.raw is used
-            instead.
-        """
-        print(f"# process: {self.max_process}")
-        if raw_directory is None:
-            raw_directory = self.setting.data.raw
-
-        # Process all subdirectories when recursice is True
-        if self.recursive:
-            if isinstance(raw_directory, (list, tuple, set)):
-                raw_directories = reduce(or_, [
-                    set(util.collect_data_directories(
-                        Path(d), print_state=True))
-                    for d in raw_directory])
-            else:
-                raw_directories = util.collect_data_directories(
-                    Path(raw_directory), print_state=True)
-        else:
-            if isinstance(raw_directory, (list, tuple, set)):
-                raw_directories = raw_directory
-            else:
-                raw_directories = [raw_directory]
-
-        chunksize = max(len(raw_directories) // self.max_process // 16, 1)
-
-        with multi.Pool(self.max_process) as pool:
-            pool.map(
-                self.convert_single_data, raw_directories,
-                chunksize=chunksize)
-
-        return
-
-    def convert_single_data(
-            self, raw_path, *, output_directory=None,
-            raise_when_overwrite=False):
-        """Convert single directory.
-
-        Parameters
-        ----------
-        raw_path: pathlib.Path
-            Input data path of raw data.
-        output_directory: pathlib.Path, optional
-            If fed, use the fed path as the output directory.
-        raise_when_overwrite: bool, optional
-            If True, raise when the output directory exists. The default is
-            False.
-
-        Returns
-        -------
-        None
-        """
-        conversion_setting = self.setting.conversion
-
-        # Determine output directory
-        raw_path = Path(raw_path)
-        if output_directory is None:
-            output_directory = determine_output_directory(
-                raw_path, conversion_setting.output_base_directory, 'raw')
-
-        # Guard
-        if raw_path.is_dir():
-            if not util.files_exist(
-                    raw_path, conversion_setting.required_file_names):
-                return
-        elif raw_path.is_file():
-            pass
-        else:
-            raise ValueError(f"raw_path not understandable: {raw_path}")
-
-        if (output_directory / conversion_setting.finished_file).exists():
-            if raise_when_overwrite:
-                raise ValueError(f"{output_directory} already exists.")
-            if not self.force_renew:
-                print(
-                    f"Already converted. Skipped conversion: {raw_path}")
-                return
-
-        # Main process
-        print(f"Processing: {raw_path}")
-        dict_data = {}
-        if conversion_setting.skip_femio:
-            fem_data = None
-        else:
-            if self.read_npy and (output_directory / FEMIO_FILE).exists():
-                fem_data = femio.FEMData.read_npy_directory(
-                    output_directory)
-            else:
-                try:
-                    if raw_path.is_dir():
-                        fem_data = femio.FEMData.read_directory(
-                            conversion_setting.file_type, raw_path,
-                            read_npy=self.read_npy, save=False,
-                            read_res=self.read_res,
-                            time_series=conversion_setting.time_series)
-                    else:
-                        fem_data = femio.FEMData.read_files(
-                            conversion_setting.file_type, raw_path,
-                            time_series=conversion_setting.time_series)
-                except ValueError:
-                    print("femio read failed. Skipped.")
-                    output_directory.mkdir(parents=True, exist_ok=True)
-                    (output_directory / 'failed').touch()
-                    return
-
-        if self.conversion_function is not None:
-            try:
-                dict_data.update(
-                    self.conversion_function(fem_data, raw_path))
-            except BaseException as e:
-                raise ValueError(
-                    f"{e}\nconversion_function failed for: {raw_path}")
-
-        if self.load_function is not None:
-            data_files = util.collect_files(
-                raw_path, conversion_setting.required_file_names)
-            try:
-                loaded_dict_data, fem_data = self.load_function(
-                    data_files, raw_path)
-            except BaseException as e:
-                raise ValueError(f"{e}\nload_function failed for: {raw_path}")
-            dict_data.update(loaded_dict_data)
-
-        if self.filter_function is not None and not self.filter_function(
-                fem_data, raw_path, dict_data):
-            return
-
-        if fem_data is not None:
-            if conversion_setting.mandatory_variables is not None \
-                    and len(conversion_setting.mandatory_variables) > 0:
-                dict_data.update(extract_variables(
-                    fem_data, conversion_setting.mandatory_variables,
-                    optional_variables=conversion_setting.optional_variables
-                ))
-
-        # Save data
-        output_directory.mkdir(parents=True, exist_ok=True)
-        if fem_data is not None:
-            if self.setting.conversion.save_femio:
-                fem_data.save(output_directory)
-
-            if self.write_ucd:
-                if self.to_first_order:
-                    fem_data_to_save = fem_data.to_first_order()
-                else:
-                    fem_data_to_save = fem_data
-                fem_data_to_save = update_fem_data(
-                    fem_data_to_save, dict_data, allow_overwrite=True)
-                fem_data_to_save.to_first_order().write(
-                    'ucd', output_directory / 'mesh.inp',
-                    overwrite=self.force_renew)
-        if self.save_function is not None:
-            self.save_function(
-                fem_data, dict_data, output_directory, self.force_renew)
-
-        if not self.setting.conversion.skip_save:
-            save_dict_data(
-                output_directory, dict_data,
-                encrypt_key=self.setting.data.encrypt_key,
-                finished_file=self.setting.conversion.finished_file,
-                save_dtype_dict=self.setting.misc.get("save_dtype_dict"))
-
-        return
-
-
-def update_fem_data(
-        fem_data, dict_data, prefix='', *, allow_overwrite=False,
-        answer_keys=None, answer_prefix=''):
-    for key, value in dict_data.items():
-
-        if answer_keys is not None:
-            if key in answer_keys:
-                variable_name = answer_prefix + key
-            else:
-                variable_name = prefix + key
-        else:
-            variable_name = prefix + key
-        if isinstance(value, np.ndarray):
-            value = reshape_data_if_needed(value)
-            shape = value.shape
-
-            if shape[0] == len(fem_data.nodes.ids):
-                # Nodal data
-                dict_data_to_update = {
-                    variable_name: value}
-                fem_data.nodal_data.update_data(
-                    fem_data.nodes.ids, dict_data_to_update,
-                    allow_overwrite=allow_overwrite)
-            elif shape[1] == len(fem_data.nodes.ids):
-                # Nodal data with time series
-                if shape[0] == 1:
-                    dict_data_to_update = {
-                        variable_name: reshape_data_if_needed(value[0])}
-                else:
-                    dict_data_to_update = {
-                        f"{variable_name}_{i}": reshape_data_if_needed(v)
-                        for i, v in enumerate(value)}
-                fem_data.nodal_data.update_data(
-                    fem_data.nodes.ids, dict_data_to_update,
-                    allow_overwrite=allow_overwrite)
-            elif shape[0] == len(fem_data.elements.ids):
-                # Elemental data
-                dict_data_to_update = {
-                    variable_name: value}
-                fem_data.elemental_data.update_data(
-                    fem_data.elements.ids, dict_data_to_update,
-                    allow_overwrite=allow_overwrite)
-            elif shape[1] == len(fem_data.elements.ids):
-                # Elemental data with time series
-                if shape[0] == 1:
-                    dict_data_to_update = {
-                        variable_name: reshape_data_if_needed(value[0])}
-                else:
-                    dict_data_to_update = {
-                        f"{variable_name}_{i}": reshape_data_if_needed(v)
-                        for i, v in enumerate(value)}
-                fem_data.elemental_data.update_data(
-                    fem_data.nodes.ids, dict_data_to_update,
-                    allow_overwrite=allow_overwrite)
-            else:
-                print(f"{variable_name} is skipped to include in fem_data")
-                continue
-        else:
-            print(f"{variable_name} is skipped to include in fem_data")
-
-    return fem_data
-
-
-def reshape_data_if_needed(value):
-    """Reshape numpy.ndarray-like to be writable to visualization files.
-
-    Parameters
-    ----------
-    value: numpy.ndarray
-        Data to be processed.
-
-    Returns
-    -------
-    reshaped_data: numpy.ndarray
-    """
-    if len(value.shape) > 2 and value.shape[-1] == 1:
-        if len(value.shape) == 4 and value.shape[1] == 3 \
-                and value.shape[2] == 3:
-            # NOTE: Assume this is symmetric matrix
-            reshaped_value \
-                = femio.functions.convert_symmetric_matrix2array(
-                    value[..., 0])
-        else:
-            reshaped_value = value[..., 0]
-    elif len(value.shape) == 1:
-        reshaped_value = value[:, None]
-    else:
-        reshaped_value = value
-    return reshaped_value
+from . import setting, util
+from siml.utils import fem_data_utils, path_utils
 
 
 def add_difference(
         fem_data, dict_data, reference_dict_data, prefix='difference'):
     if reference_dict_data is None:
         return fem_data
     intersections = set(
@@ -364,17 +28,21 @@
 
     difference_dict_data = {
         intersection:
         np.reshape(
             dict_data[intersection], reference_dict_data[intersection].shape)
         - reference_dict_data[intersection]
         for intersection in intersections}
-    fem_data = update_fem_data(fem_data, difference_dict_data, prefix=prefix)
 
-    return fem_data
+    wrapped_data = fem_data_utils.FemDataWrapper(fem_data)
+    wrapped_data.update_fem_data(
+        difference_dict_data, prefix=prefix
+    )
+
+    return wrapped_data.fem_data
 
 
 def add_abs_difference(
         fem_data, dict_data, reference_dict_data, prefix='difference_abs'):
     if reference_dict_data is None:
         return fem_data
     intersections = set(
@@ -386,17 +54,22 @@
         intersection:
         np.abs(
             np.reshape(
                 dict_data[intersection],
                 reference_dict_data[intersection].shape)
             - reference_dict_data[intersection])
         for intersection in intersections}
-    fem_data = update_fem_data(fem_data, difference_dict_data, prefix=prefix)
 
-    return fem_data
+    wrapped_data = fem_data_utils.FemDataWrapper(fem_data)
+    wrapped_data.update_fem_data(
+        difference_dict_data,
+        prefix=prefix
+    )
+
+    return wrapped_data.fem_data
 
 
 def concatenate_preprocessed_data(
         preprocessed_base_directories, output_directory_base, variable_names,
         *, ratios=(.9, .05, .05), overwrite=False):
     """Concatenate preprocessed data in the element direction.
 
@@ -645,15 +318,15 @@
         with multi.Pool(self.max_process) as pool:
             pool.starmap(
                 self.transform_single_variable, preprocess_converter_inputs,
                 chunksize=1)
 
         # Touch finished files
         for data_directory in self.interim_directories:
-            output_directory = determine_output_directory(
+            output_directory = path_utils.determine_output_directory(
                 data_directory, self.setting.data.preprocessed_root,
                 self.str_replace)
             (output_directory / self.FINISHED_FILE).touch()
 
         yaml_file = self.setting.data.preprocessed_root / 'settings.yml'
         if not yaml_file.exists():
             setting.write_yaml(self.setting, yaml_file)
@@ -844,15 +517,15 @@
         -------
         None
         """
         if isinstance(preprocess_converter.converter, util.Identity):
             # Shortcut preprocessing
 
             for data_directory in self.interim_directories:
-                output_directory = determine_output_directory(
+                output_directory = path_utils.determine_output_directory(
                     data_directory, self.setting.data.preprocessed_root,
                     self.str_replace)
                 if not self.force_renew \
                         and self._file_exists(output_directory, variable_name):
                     print(
                         f"{output_directory} / {variable_name} "
                         'already exists. Skipped.')
@@ -860,15 +533,15 @@
 
                 util.copy_variable_file(
                     data_directory, variable_name, output_directory,
                     allow_missing=self.allow_missing)
             return
 
         for data_directory in self.interim_directories:
-            output_directory = determine_output_directory(
+            output_directory = path_utils.determine_output_directory(
                 data_directory, self.setting.data.preprocessed_root,
                 self.str_replace)
             if not self.force_renew \
                     and self._file_exists(output_directory, variable_name):
                 print(
                     f"{output_directory} / {variable_name} "
                     'already exists. Skipped.')
@@ -1123,29 +796,33 @@
                 raise ValueError(
                     'Please specify required_file_names when skip_femio '
                     'is True.')
             data_files = util.collect_files(
                 write_simulation_base, required_file_names)
             data_dict, fem_data = load_function(
                 data_files, write_simulation_base)
-            fem_data = update_fem_data(
-                fem_data, data_dict, allow_overwrite=True)
+            wrapped_fem_data = fem_data_utils.FemDataWrapper(fem_data)
+            wrapped_fem_data.update_fem_data(data_dict, allow_overwrite=True)
+            fem_data = wrapped_fem_data.fem_data
         else:
             raise ValueError(
                 'When skip_femio is True, please feed load_function.')
 
         if convert_to_order1:
             fem_data = fem_data.to_first_order()
 
-        fem_data = update_fem_data(
-            fem_data, dict_data_x, prefix='input_')
+        wrapped_fem_data = fem_data_utils.FemDataWrapper(fem_data)
+        wrapped_fem_data.update_fem_data(dict_data_x, prefix='input_')
         if dict_data_answer is not None:
-            fem_data = update_fem_data(
-                fem_data, dict_data_answer, prefix='answer_')
-        fem_data = update_fem_data(fem_data, dict_data_y, prefix='predicted_')
+            wrapped_fem_data.update_fem_data(
+                dict_data_answer, prefix='answer_'
+            )
+        wrapped_fem_data.update_fem_data(dict_data_y, prefix='predicted_')
+
+        fem_data = wrapped_fem_data.fem_data
         fem_data = add_difference(
             fem_data, dict_data_y, dict_data_answer, prefix='difference_')
         fem_data = add_abs_difference(
             fem_data, dict_data_y, dict_data_answer, prefix='difference_abs_')
         if data_addition_function is not None:
             fem_data = data_addition_function(fem_data, write_simulation_base)
 
@@ -1205,176 +882,14 @@
         if not output_directory.exists():
             output_directory.mkdir(parents=True, exist_ok=True)
         for variable_name, data in data_dict.items():
             np.save(output_directory / f"{variable_name}.npy", data)
         return
 
 
-def extract_variables(
-        fem_data, mandatory_variables, *, optional_variables=None):
-    """Extract variables from FEMData object to convert to data dictionary.
-
-    Parameters
-    ----------
-    fem_data: femio.FEMData
-        FEMData object to be extracted variables from.
-    mandatory_variables: list[str]
-        Mandatory variable names.
-    optional_variables: list[str], optional
-        Optional variable names.
-
-    Returns
-    -------
-        dict_data: dict
-            Data dictionary.
-    """
-    dict_data = {
-        mandatory_variable: _extract_single_variable(
-            fem_data, mandatory_variable, mandatory=True, ravel=True)
-        for mandatory_variable in mandatory_variables}
-
-    if optional_variables is not None and len(optional_variables) > 0:
-        for optional_variable in optional_variables:
-            optional_variable_data = _extract_single_variable(
-                fem_data, optional_variable, mandatory=False, ravel=True)
-            if optional_variable_data is not None:
-                dict_data.update({optional_variable: optional_variable_data})
-    return dict_data
-
-
-def _extract_single_variable(
-        fem_data, variable_name, *, mandatory=True, ravel=True):
-    if variable_name in fem_data.nodal_data:
-        return fem_data.nodal_data.get_attribute_data(variable_name)
-    elif variable_name in fem_data.elemental_data:
-        return fem_data.elemental_data.get_attribute_data(variable_name)
-    else:
-        if mandatory:
-            raise ValueError(
-                f"{variable_name} not found in {fem_data.nodal_data.keys()}, "
-                f"{fem_data.elemental_data.keys()}")
-        else:
-            return None
-
-
-def save_dict_data(
-        output_directory, dict_data, *, dtype=np.float32, encrypt_key=None,
-        finished_file='converted', save_dtype_dict: Dict = None):
-    """Save dict_data.
-
-    Parameters
-    ----------
-    output_directory: pathlib.Path
-        Output directory path.
-    dict_data: dict
-        Data dictionary to be saved.
-    dtype: type, optional
-        Data type to be saved.
-    encrypt_key: bytes, optional
-        Data for encryption.
-
-    Returns
-    -------
-        None
-    """
-    for key, value in dict_data.items():
-        save_dtype = _get_save_dtype(key,
-                                     default_dtype=dtype,
-                                     save_dtype_dict=save_dtype_dict)
-        util.save_variable(
-            output_directory,
-            key, value, dtype=save_dtype, encrypt_key=encrypt_key)
-    (output_directory / finished_file).touch()
-    return
-
-
-def _get_save_dtype(variable_name: str,
-                    default_dtype: np.dtype,
-                    save_dtype_dict: Dict = None):
-    if save_dtype_dict is None:
-        return default_dtype
-    if variable_name in save_dtype_dict:
-        return save_dtype_dict[variable_name]
-    else:
-        return default_dtype
-
-
-def determine_output_directory(
-        input_directory, output_base_directory, str_replace):
-    """Determine output directory by replacing a string (str_replace) in the
-    input_directory.
-
-    Parameters
-    ----------
-    input_directory: pathlib.Path
-        Input directory path.
-    output_base_directory: pathlib.Path
-        Output base directory path. The output directry name is under that
-        directory.
-    str_replace: str
-        The string to be replaced.
-
-    Returns
-    -------
-    output_directory: pathlib.Path
-        Detemined output directory path.
-    """
-    common_prefix = common_parent(
-        input_directory,
-        output_base_directory
-    )
-    relative_input_path = Path(os.path.relpath(input_directory, common_prefix))
-    parts = list(relative_input_path.parts)
-
-    replace_indices = np.where(
-        np.array(relative_input_path.parts) == str_replace)[0]
-    if len(replace_indices) == 0:
-        pass
-    elif len(replace_indices) == 1:
-        replace_index = replace_indices[0]
-        parts[replace_index] = ''
-    else:
-        raise ValueError(
-            f"Input directory {input_directory} contains several "
-            f"{str_replace} parts thus ambiguous.")
-    output_directory = output_base_directory / '/'.join(parts).lstrip('/')
-
-    return output_directory
-
-
-def common_parent(
-        directory_1: Path,
-        directory_2: Path) -> Path:
-    """Search common parent directory
-
-    Parameters
-    ----------
-    directory_1 : pathlib.Path
-    directory_2 : pathlib.Path
-
-    Returns
-    -------
-    common_parent: pathlib.Path
-        Path to common parent directory
-    """
-    parents_1 = directory_1.parents
-    parents_2 = directory_2.parents
-    min_idx_1 = len(parents_1) - 1
-    min_idx_2 = len(parents_2) - 1
-    min_idx = min(len(parents_1), len(parents_2))
-
-    common_parent = Path("")
-    for i in range(min_idx):
-        if parents_1[min_idx_1 - i] == parents_2[min_idx_2 - i]:
-            common_parent = parents_1[min_idx_1 - i]
-        else:
-            break
-    return common_parent
-
-
 def normalize_adjacency_matrix(adj):
     """Symmetrically normalize adjacency matrix.
 
     Parameters
     ----------
     adj: scipy.sparse.coo_matrix
         Adjacency matrix in COO expression.
```

## siml/setting.py

```diff
@@ -1084,14 +1084,21 @@
             self.optional = self.optional_variables
         else:
             pass
 
         super().__post_init__()
         return
 
+    @property
+    def should_load_mandatory_variables(self) -> bool:
+        if self.mandatory_variables is None:
+            return False
+
+        return len(self.mandatory_variables) > 0
+
 
 @dc.dataclass
 class PreprocessSetting:
     preprocess: dict = dc.field(default_factory=dict)
 
     @classmethod
     def read_settings_yaml(cls, settings_yaml):
```

## Comparing `pysiml-0.2.8.dev202304041500.dist-info/LICENSE` & `pysiml-0.2.8.dev202304200913.dist-info/LICENSE`

 * *Files identical despite different names*

## Comparing `pysiml-0.2.8.dev202304041500.dist-info/METADATA` & `pysiml-0.2.8.dev202304200913.dist-info/METADATA`

 * *Files 6% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: pysiml
-Version: 0.2.8.dev202304041500
+Version: 0.2.8.dev202304200913
 Summary: SiML - a Simulation ML library
 Home-page: https://github.com/ricosjp/pysiml
 License: Apache-2.0
 Author: RICOS Co. Ltd.
 Requires-Python: >=3.9,<3.10
 Classifier: License :: OSI Approved :: Apache Software License
 Classifier: Programming Language :: Python :: 3
```

## Comparing `pysiml-0.2.8.dev202304041500.dist-info/RECORD` & `pysiml-0.2.8.dev202304200913.dist-info/RECORD`

 * *Files 6% similar despite different names*

```diff
@@ -1,27 +1,27 @@
-pyproject.toml,sha256=StJ_2cyp6Su8Bq8aG26F0K9I7NM9OmRPzFzvOdn4Uvg,1499
-siml/__init__.py,sha256=WvWxHKFZftUEZ9CjBIq11jDMpdrKAwAKxPtbAhI7Y20,568
+pyproject.toml,sha256=BGTtWpF5gkubVixtujxhQeYX9rQeNyK01kF8g5RUC4w,1499
+siml/__init__.py,sha256=mC61oSLHO5F6zXHhOrzH51R-vkH0mL4MShhSBiRRe6s,638
 siml/__main__/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 siml/__main__/convert_interim_data.py,sha256=63owXqB0-VG0fZn0f-1LE6FbxHirCHJ1d0twZlDRjRs,970
-siml/__main__/convert_raw_data.py,sha256=i2AuUoic2WZ_egeCcX-nDObdx3lM9Nwele2VS6Q0vas,1374
+siml/__main__/convert_raw_data.py,sha256=VNVOyx9lChopF2Q-9OXtN_-MtfBsyjS_JvFwCjdkv0c,1400
 siml/__main__/optimize.py,sha256=6HALn60Mxh4GUiPbSXjvrdpkoQ4NgYSzCHCBRbli4W0,1742
 siml/__main__/plot_losses.py,sha256=0uMsoPN-xR0JhPg0WzSQL5GbD80ovuFO12eVpc0x6Xc,8663
 siml/__main__/prepare_preprocess_converters.py,sha256=wdr7jA690BMyuTYxZYmLnBZV10_0rKvqt98p7l6wU94,929
 siml/__main__/preprocess_interim_data.py,sha256=D74mGTZcAk-4kQQtq5BN2dc7rAIxwx49oYBFtpJfupc,712
 siml/__main__/train.py,sha256=7cLknw6yx4Ehm_K9UkUGIbg3QPAXJpr_HpNWP60MvPs,1376
 siml/__main__/visualize_graph.py,sha256=xyPqVZHA0p3xnB41w4PsKCfy7feicBRMXZveQdo_lpM,3961
 siml/config.py,sha256=_Cc2w65g05Y5QDuh8b8UIY_XFZbwKmXx_Z40ATxVw_I,57
 siml/data_parallel.py,sha256=b4DU0QCVVbFWB02QdvtMLuLuTIz57npYSlbmKfJVjVo,3520
-siml/datasets.py,sha256=uJCQsC7GgbV0i3FFxDopKZjOyG38duQvctXkupSMGB4,26594
-siml/inferer.py,sha256=CUgvug3tncPiipyc8uV6LIikEyrsHBTzziITrJYbsY4,26955
+siml/datasets.py,sha256=8nYbgADi86eqzy95pXUlNHs9iBxMDbk1oyIuL7DPOqg,26681
+siml/inferer.py,sha256=SfoblDVNBF3V1U5j7nKUI5sZRLJDvUuacMAwsFzFNFw,27000
 siml/loss_operations/loss_assignment.py,sha256=LuGyNfM_y0agzwSTfdMqSGwB25HmgziqXz1araAmEbE,1466
 siml/loss_operations/loss_calculator.py,sha256=ooL19kFfy3OizPfCScT8CfX12V4YR7-mAhK52mjKvv0,3734
 siml/loss_operations/loss_selector.py,sha256=2tgLKwTiM6NEJkVTYixt5bNO-iQneYrWNnprOt63LY0,2064
 siml/loss_operations/loss_type.py,sha256=Pt1TF7OjUfES47BpmbQNSVulpGtBktNFmV1YFk_ZmCY,58
-siml/mains.py,sha256=6MMmHiTKhHASjwbKMbCqyIgSL6pb7MgSoOJONx0FbUY,1347
+siml/mains.py,sha256=2Bah9LWn8YPmmXfianE-_nhl9581nUlgwDtA0xUy30o,1364
 siml/networks/__init__.py,sha256=8lF58NgkEeR1cl_h5fELLyIOMeYZUS-L4wWChGSMCH8,2724
 siml/networks/abstract_equivariant_gnn.py,sha256=plXB7OLBDxyXLpdIt9xGLFCambyxUNX_OeY0tMouI4E,11337
 siml/networks/abstract_gcn.py,sha256=9KjRZipFwA02kNRkiRPF8t-_oD5asQZZD_08ZHZl0Hs,6505
 siml/networks/activation.py,sha256=lQYzv6VBDRkYtQsJdSEEx-e-5RMwJgjN-W1uc-fx-PU,1007
 siml/networks/activations.py,sha256=i_bO5nuIa22GBZqFppheEd1j6himhYMKa4SdSwUQ8Xw,4565
 siml/networks/array2diagmat.py,sha256=VCPwLvAOp3pa76v2CDs0AUC1wVdX9H-3lhCoIRAQSxo,715
 siml/networks/array2symmat.py,sha256=dpMo6uZb29UOKYVD0KRIJnVvaoKHnt41HRIsthTD9Go,1576
@@ -63,25 +63,30 @@
 siml/networks/tensor_operations.py,sha256=bzzxvWswGKj5zx9xOPmpQuY-r2KORtodKbvRB-fsJfM,5969
 siml/networks/threshold.py,sha256=oy-7D58Z68uZBn8SZ5XJHzo7GXbUiiBkW5MZL0qq5EE,1415
 siml/networks/time_norm.py,sha256=fM0A7WF6CjmAcvvBlbziinSQEjQlAIHRkQEQ0Y8tLr0,782
 siml/networks/translator.py,sha256=-X4wO4QmdtF6byUcHdNRmCCds-c2F-UUvprgW6msCCE,1882
 siml/networks/upper_limit.py,sha256=OUi7vXtqf0r83ZebSX98Jlo0N6C_v2KpdcF1pccIfGw,1034
 siml/optimize.py,sha256=KJAkJSvb4s4XF2y86fAt5MZafYrbW9SGXPPsJFqkqkA,6940
 siml/postprocessor.py,sha256=5DXPxY3Xdutc5WSLkJ6FAo8lZXMPyG29XsSHn_mB3kI,5523
-siml/prepost.py,sha256=EtlzUxLeKqf7VBx83zvRpPpWT8Sg0gg1Wg7XgdL1Yj0,63227
-siml/setting.py,sha256=Y4YC4Y4EMCgoBnDqJi9zPmunL2JqWI60KHJwZspVLXU,48874
+siml/prepost.py,sha256=QTMoM4JJUKYFqYQg9_bfGRoS-n1sgbbKyEM4vXx4--0,45390
+siml/preprocessing/__init__.py,sha256=uGj2n-EecOO5XxBoVCmp96skKw-mM43QHMEKC179GE4,33
+siml/preprocessing/converter.py,sha256=AYzhEaB1wc_usbqENhQMsQSn9HMHVqhqKfHBfNfwxUM,20676
+siml/setting.py,sha256=ntrNdSYLh_QOowJVZT0fMzfQKxz4hTbPFTwB73-Rk6g,49064
 siml/siml_manager.py,sha256=K6Umxl8JFMLWhkSyrHk9dJ7r0luNw-FWXpw6Ue__hIA,11036
 siml/study.py,sha256=VsiEuAiwcC0a1voORjic9RcZcnPVJiBowV-mobS6tAQ,13021
 siml/trainer.py,sha256=StgBFmZaJHdMeCdulluL_qRJzD7K_0Sl3uWccfjJSBY,34860
 siml/update_functions/__init__.py,sha256=fQY19xMEhZv7k6flZPBmhIgLl3uHuBWkay-YCFfaY64,211
 siml/update_functions/element_batch_update.py,sha256=93_L9jl4D9lFQXmEX1Lo8l4n3rDf1wdZljPJIQd9rUs,1774
 siml/update_functions/pseudo_batch_update.py,sha256=rmkCSATfWm-0JGaPGNbdBbmizZ7IoXXxr4pVczLwTPo,3590
 siml/update_functions/standard_update.py,sha256=8FRbEEkpI4qwRuRLLqNtbtesSkB5xzx1820zqH2qBhQ,2669
 siml/update_functions/update_interface.py,sha256=O1fGrE21iXckz7NTOTDNlyQ4rll1SQDehdc_Kk5we0Q,429
 siml/util.py,sha256=OI3ve02GZ1iy-H53vI74WGRVXgSYYp7Ibap7BWw6DrU,37201
+siml/utils/__init__.py,sha256=tVmXHjjriMvU91mYn_MF20FEevUs7QvzHtuiOYttIxY,71
+siml/utils/fem_data_utils.py,sha256=u3mCaTHP2BSvBGBQojmF1TYtwH5XWoefeivgP-iAdcg,6090
+siml/utils/path_utils.py,sha256=nBR68zANQQDUANjvl7noE6aZBMXg7HxErTh3bDJQJko,2147
 siml/variables/__init__.py,sha256=827EnWPECNl5CepRa19l_p7KvR1rx1xk2xgoJFfltVg,58
 siml/variables/siml_variables.py,sha256=sr2ZyrNJuZy-0SQrz173a0qBPcFMNEymQlUXaUscMVo,3487
-pysiml-0.2.8.dev202304041500.dist-info/entry_points.txt,sha256=zXU0Gd56XeUfgaeG-CGXcYaTpORSU6kVeQ_vVvjIFPs,388
-pysiml-0.2.8.dev202304041500.dist-info/LICENSE,sha256=Ok4O8jxCIpLhGZjTNzgBW8V8xITF_SUVEp1juLijZpA,11431
-pysiml-0.2.8.dev202304041500.dist-info/WHEEL,sha256=y3eDiaFVSNTPbgzfNn0nYn5tEn1cX6WrdetDlQM4xWw,83
-pysiml-0.2.8.dev202304041500.dist-info/METADATA,sha256=IUh1E_cXAQ02gU01_oMivmX__WblGhvsuO_rMEjWlfM,1595
-pysiml-0.2.8.dev202304041500.dist-info/RECORD,,
+pysiml-0.2.8.dev202304200913.dist-info/entry_points.txt,sha256=zXU0Gd56XeUfgaeG-CGXcYaTpORSU6kVeQ_vVvjIFPs,388
+pysiml-0.2.8.dev202304200913.dist-info/LICENSE,sha256=Ok4O8jxCIpLhGZjTNzgBW8V8xITF_SUVEp1juLijZpA,11431
+pysiml-0.2.8.dev202304200913.dist-info/WHEEL,sha256=y3eDiaFVSNTPbgzfNn0nYn5tEn1cX6WrdetDlQM4xWw,83
+pysiml-0.2.8.dev202304200913.dist-info/METADATA,sha256=W5i6LgQOF7u5D1ZDAAuFNLTYJksZHcW0r4KgWISoL9I,1595
+pysiml-0.2.8.dev202304200913.dist-info/RECORD,,
```

