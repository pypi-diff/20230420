# Comparing `tmp/deep_training-0.1.2.post0-py3-none-any.whl.zip` & `tmp/deep_training-0.1.2.post2-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,16 +1,16 @@
-Zip file size: 244519 bytes, number of entries: 145
+Zip file size: 244738 bytes, number of entries: 145
 -rw-rw-rw-  2.0 fat       47 b- defN 23-Mar-22 07:43 deep_training/__init__.py
--rw-rw-rw-  2.0 fat      903 b- defN 23-Apr-12 08:36 deep_training/setup.py
+-rw-rw-rw-  2.0 fat      903 b- defN 23-Apr-20 02:26 deep_training/setup.py
 -rw-rw-rw-  2.0 fat       55 b- defN 22-Dec-09 05:30 deep_training/cv/__init__.py
 -rw-rw-rw-  2.0 fat      195 b- defN 23-Jan-29 01:07 deep_training/data_helper/__init__.py
 -rw-rw-rw-  2.0 fat    29088 b- defN 23-Apr-06 04:11 deep_training/data_helper/data_helper.py
 -rw-rw-rw-  2.0 fat     4926 b- defN 23-Feb-17 02:41 deep_training/data_helper/data_module.py
 -rw-rw-rw-  2.0 fat     1383 b- defN 23-Jan-29 01:07 deep_training/data_helper/data_writer.py
--rw-rw-rw-  2.0 fat    12552 b- defN 23-Apr-11 05:55 deep_training/data_helper/training_args.py
+-rw-rw-rw-  2.0 fat    12557 b- defN 23-Apr-19 09:13 deep_training/data_helper/training_args.py
 -rw-rw-rw-  2.0 fat       70 b- defN 22-Dec-13 03:17 deep_training/nlp/__init__.py
 -rw-rw-rw-  2.0 fat       56 b- defN 22-Nov-10 08:28 deep_training/nlp/layers/__init__.py
 -rw-rw-rw-  2.0 fat      241 b- defN 23-Mar-13 05:48 deep_training/nlp/layers/activate.py
 -rw-rw-rw-  2.0 fat    13271 b- defN 22-Nov-14 00:17 deep_training/nlp/layers/crf.py
 -rw-rw-rw-  2.0 fat     4653 b- defN 22-Dec-12 00:12 deep_training/nlp/layers/handshakingkernel.py
 -rw-rw-rw-  2.0 fat      435 b- defN 22-Dec-02 00:22 deep_training/nlp/layers/mask.py
 -rw-rw-rw-  2.0 fat     1319 b- defN 22-Dec-12 00:12 deep_training/nlp/layers/mhslayer.py
@@ -93,28 +93,28 @@
 -rw-rw-rw-  2.0 fat     9025 b- defN 23-Apr-03 00:32 deep_training/nlp/models/w2ner.py
 -rw-rw-rw-  2.0 fat    16500 b- defN 23-Mar-27 00:33 deep_training/nlp/models/LLaMA/__init__.py
 -rw-rw-rw-  2.0 fat     5087 b- defN 23-Mar-13 01:04 deep_training/nlp/models/LLaMA/configuration.py
 -rw-rw-rw-  2.0 fat    19183 b- defN 23-Mar-27 00:33 deep_training/nlp/models/LLaMA_parallel/__init__.py
 -rw-rw-rw-  2.0 fat     5087 b- defN 23-Mar-10 00:30 deep_training/nlp/models/LLaMA_parallel/configuration.py
 -rw-rw-rw-  2.0 fat    31627 b- defN 23-Mar-13 06:15 deep_training/nlp/models/PaLM/__init__.py
 -rw-rw-rw-  2.0 fat     5890 b- defN 23-Mar-13 06:15 deep_training/nlp/models/PaLM/configuration.py
--rw-rw-rw-  2.0 fat    60188 b- defN 23-Apr-10 00:42 deep_training/nlp/models/chatglm/__init__.py
+-rw-rw-rw-  2.0 fat    60508 b- defN 23-Apr-19 01:01 deep_training/nlp/models/chatglm/__init__.py
 -rw-rw-rw-  2.0 fat     4575 b- defN 23-Apr-10 00:42 deep_training/nlp/models/chatglm/configuration.py
 -rw-rw-rw-  2.0 fat    15150 b- defN 23-Apr-03 00:32 deep_training/nlp/models/chatglm/quantization.py
--rw-rw-rw-  2.0 fat    16460 b- defN 23-Apr-10 00:42 deep_training/nlp/models/chatglm/tokenization.py
+-rw-rw-rw-  2.0 fat    16642 b- defN 23-Apr-17 00:24 deep_training/nlp/models/chatglm/tokenization.py
 -rw-rw-rw-  2.0 fat    34123 b- defN 23-Mar-13 06:18 deep_training/nlp/models/laMDA/__init__.py
 -rw-rw-rw-  2.0 fat     5981 b- defN 23-Mar-13 06:15 deep_training/nlp/models/laMDA/configuration.py
 -rw-rw-rw-  2.0 fat      181 b- defN 23-Apr-11 07:19 deep_training/nlp/models/lora/__init__.py
 -rw-rw-rw-  2.0 fat      123 b- defN 23-Apr-11 00:30 deep_training/nlp/models/lora/v1/__init__.py
 -rw-rw-rw-  2.0 fat     7054 b- defN 23-Apr-11 00:30 deep_training/nlp/models/lora/v1/configuration.py
 -rw-rw-rw-  2.0 fat    13576 b- defN 23-Apr-12 08:35 deep_training/nlp/models/lora/v1/lora_wrapper.py
 -rw-rw-rw-  2.0 fat      206 b- defN 23-Apr-11 01:58 deep_training/nlp/models/lora/v2/__init__.py
 -rw-rw-rw-  2.0 fat    13112 b- defN 23-Apr-11 00:30 deep_training/nlp/models/lora/v2/adalora_model.py
 -rw-rw-rw-  2.0 fat    11281 b- defN 23-Apr-11 06:27 deep_training/nlp/models/lora/v2/configuration.py
--rw-rw-rw-  2.0 fat    11332 b- defN 23-Apr-12 08:32 deep_training/nlp/models/lora/v2/lora_model.py
+-rw-rw-rw-  2.0 fat    11745 b- defN 23-Apr-18 01:24 deep_training/nlp/models/lora/v2/lora_model.py
 -rw-rw-rw-  2.0 fat    10265 b- defN 23-Apr-11 01:58 deep_training/nlp/models/lora/v2/lora_wrapper.py
 -rw-rw-rw-  2.0 fat     4889 b- defN 23-Apr-11 00:30 deep_training/nlp/models/lora/v2/save_and_load.py
 -rw-rw-rw-  2.0 fat      102 b- defN 22-Nov-22 08:00 deep_training/nlp/models/splinker/__init__.py
 -rw-rw-rw-  2.0 fat     2851 b- defN 22-Dec-22 08:14 deep_training/nlp/models/splinker/splinker.py
 -rw-rw-rw-  2.0 fat    14478 b- defN 23-Feb-11 09:07 deep_training/nlp/models/t5decoder/__init__.py
 -rw-rw-rw-  2.0 fat     6646 b- defN 23-Feb-09 00:28 deep_training/nlp/models/t5encoder/__init__.py
 -rw-rw-rw-  2.0 fat       56 b- defN 22-Dec-14 08:02 deep_training/nlp/optimizer/__init__.py
@@ -136,12 +136,12 @@
 -rw-rw-rw-  2.0 fat       53 b- defN 23-Mar-07 01:25 deep_training/tfnlp/scheduler/__init__.py
 -rw-rw-rw-  2.0 fat       53 b- defN 23-Mar-07 01:26 deep_training/tfnlp/utils/__init__.py
 -rw-rw-rw-  2.0 fat       55 b- defN 23-Mar-07 01:20 deep_training/utils/__init__.py
 -rw-rw-rw-  2.0 fat     1941 b- defN 23-Mar-07 01:20 deep_training/utils/distributed.py
 -rw-rw-rw-  2.0 fat     1724 b- defN 23-Mar-07 01:22 deep_training/utils/func.py
 -rw-rw-rw-  2.0 fat     5117 b- defN 23-Feb-21 09:01 deep_training/utils/maskedlm.py
 -rw-rw-rw-  2.0 fat     7469 b- defN 23-Mar-20 00:27 deep_training/utils/trainer.py
--rw-rw-rw-  2.0 fat      629 b- defN 23-Apr-12 08:40 deep_training-0.1.2.post0.dist-info/METADATA
--rw-rw-rw-  2.0 fat       92 b- defN 23-Apr-12 08:40 deep_training-0.1.2.post0.dist-info/WHEEL
--rw-rw-rw-  2.0 fat       14 b- defN 23-Apr-12 08:40 deep_training-0.1.2.post0.dist-info/top_level.txt
-?rw-rw-r--  2.0 fat    14010 b- defN 23-Apr-12 08:40 deep_training-0.1.2.post0.dist-info/RECORD
-145 files, 833785 bytes uncompressed, 221807 bytes compressed:  73.4%
+-rw-rw-rw-  2.0 fat      629 b- defN 23-Apr-20 02:27 deep_training-0.1.2.post2.dist-info/METADATA
+-rw-rw-rw-  2.0 fat       92 b- defN 23-Apr-20 02:27 deep_training-0.1.2.post2.dist-info/WHEEL
+-rw-rw-rw-  2.0 fat       14 b- defN 23-Apr-20 02:27 deep_training-0.1.2.post2.dist-info/top_level.txt
+?rw-rw-r--  2.0 fat    14010 b- defN 23-Apr-20 02:27 deep_training-0.1.2.post2.dist-info/RECORD
+145 files, 834705 bytes uncompressed, 222026 bytes compressed:  73.4%
```

## zipnote {}

```diff
@@ -417,20 +417,20 @@
 
 Filename: deep_training/utils/maskedlm.py
 Comment: 
 
 Filename: deep_training/utils/trainer.py
 Comment: 
 
-Filename: deep_training-0.1.2.post0.dist-info/METADATA
+Filename: deep_training-0.1.2.post2.dist-info/METADATA
 Comment: 
 
-Filename: deep_training-0.1.2.post0.dist-info/WHEEL
+Filename: deep_training-0.1.2.post2.dist-info/WHEEL
 Comment: 
 
-Filename: deep_training-0.1.2.post0.dist-info/top_level.txt
+Filename: deep_training-0.1.2.post2.dist-info/top_level.txt
 Comment: 
 
-Filename: deep_training-0.1.2.post0.dist-info/RECORD
+Filename: deep_training-0.1.2.post2.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## deep_training/setup.py

```diff
@@ -1,15 +1,15 @@
 #! -*- coding: utf-8 -*-
 
 from setuptools import setup, find_packages
 
 ignore = ['test','tests']
 setup(
     name='deep_training',
-    version='0.1.2@post0',
+    version='0.1.2@post2',
     description='an easy training architecture',
     long_description='torch_training: https://github.com/ssbuild/deep_training.git',
     license='Apache License 2.0',
     url='https://github.com/ssbuild/deep_training',
     author='ssbuild',
     author_email='9727464@qq.com',
     install_requires=['pytorch-lightning>=2',
```

## deep_training/data_helper/training_args.py

```diff
@@ -223,19 +223,19 @@
 
     def __post_init__(self):
         if self.learning_rate_for_task is None:
             self.learning_rate_for_task = self.learning_rate
 
         if self.seed is not None:
             try:
-                from pytorch_lightning.utilities.seed import seed_everything
-                seed_everything(self.seed)
+                from lightning_fabric.utilities.seed import seed_everything
+                seed_everything(int(self.seed))
             except:
                 try:
-                    from lightning_fabric.utilities.seed import seed_everything
+                    from pytorch_lightning.utilities.seed import seed_everything
                     seed_everything(self.seed)
                 except:
                     warnings.warn('missing seed_everything')
 
 
         assert self.hierarchical_position is None or (self.hierarchical_position >0 and self.hierarchical_position <1)
```

## deep_training/nlp/models/chatglm/__init__.py

```diff
@@ -51,23 +51,33 @@
             scores.zero_()
             scores[..., 5] = 5e4
         return scores
 
 
 
 
+def module_init_function(cls,*args,**kwargs):
+    return cls(*args,**kwargs)
 
+skip_init_function = skip_init
 
-def setup_model_profile():
+def setup_model_profile(skip_init_flag=True):
     # flags required to enable jit fusion kernels
     torch._C._jit_set_profiling_mode(False)
     torch._C._jit_set_profiling_executor(False)
     torch._C._jit_override_can_fuse_on_cpu(True)
     torch._C._jit_override_can_fuse_on_gpu(True)
 
+    global skip_init_function
+    if skip_init_flag:
+        skip_init_function = skip_init
+    else:
+        skip_init_function = module_init_function
+
+
 
 def load_tf_weights_in_chatglm_6b(model, config, tf_checkpoint_path):
     """Load tf checkpoints in a pytorch model."""
     try:
         import re
 
         import numpy as np
@@ -284,18 +294,16 @@
     output_size = (query_layer.size(1), query_layer.size(2), query_layer.size(0), key_layer.size(0))
 
     # [sq, b, np, hn] -> [sq, b * np, hn]
     query_layer = query_layer.view(output_size[2], output_size[0] * output_size[1], -1)
     # [sk, b, np, hn] -> [sk, b * np, hn]
     key_layer = key_layer.view(output_size[3], output_size[0] * output_size[1], -1)
 
-    matmul_result = torch.empty(
-        output_size[0] * output_size[1],
-        output_size[2],
-        output_size[3],
+    matmul_result = torch.zeros(
+        1, 1, 1,
         dtype=query_layer.dtype,
         device=query_layer.device,
     )
 
     matmul_result = torch.baddbmm(
         matmul_result,
         query_layer.transpose(0, 1),  # [b * np, sq, hn]
@@ -383,23 +391,23 @@
             self.hidden_size_per_attention_head = hidden_size // num_attention_heads
         else:
             self.hidden_size_per_attention_head = hidden_size_per_attention_head
 
         self.inner_hidden_size = num_attention_heads * self.hidden_size_per_attention_head
 
         # Strided linear layer.
-        self.query_key_value = skip_init(
+        self.query_key_value = skip_init_function(
             torch.nn.Linear,
             hidden_size,
             3 * self.inner_hidden_size,
             bias=bias,
             dtype=params_dtype,
         )
 
-        self.dense = skip_init(
+        self.dense = skip_init_function(
             torch.nn.Linear,
             self.inner_hidden_size,
             hidden_size,
             bias=bias,
             dtype=params_dtype,
         )
 
@@ -513,23 +521,23 @@
         self.activation_func = activation_func
 
         # Project to 4h.
         self.hidden_size = hidden_size
         if inner_hidden_size is None:
             inner_hidden_size = 4 * hidden_size
         self.inner_hidden_size = inner_hidden_size
-        self.dense_h_to_4h = skip_init(
+        self.dense_h_to_4h = skip_init_function(
             torch.nn.Linear,
             self.hidden_size,
             self.inner_hidden_size,
             bias=bias,
             dtype=params_dtype,
         )
         # Project back to h.
-        self.dense_4h_to_h = skip_init(
+        self.dense_4h_to_h = skip_init_function(
             torch.nn.Linear,
             self.inner_hidden_size,
             self.hidden_size,
             bias=bias,
             dtype=params_dtype,
         )
 
@@ -696,31 +704,34 @@
         for i, context_length in enumerate(context_lengths):
             attention_mask[i, :, :context_length] = 1
         attention_mask.unsqueeze_(1)
         attention_mask = (attention_mask < 0.5).bool()
 
         return attention_mask
 
-    def get_position_ids(self, input_ids, mask_positions, device, gmask=False):
+    def get_position_ids(self, input_ids, mask_positions, device, use_gmasks=None):
         batch_size, seq_length = input_ids.shape
+        if use_gmasks is None:
+            use_gmasks = [False] * batch_size
         context_lengths = [seq.tolist().index(self.config.bos_token_id) for seq in input_ids]
         if self.position_encoding_2d:
             position_ids = torch.arange(seq_length, dtype=torch.long, device=device).unsqueeze(0).repeat(batch_size, 1)
             for i, context_length in enumerate(context_lengths):
                 position_ids[i, context_length:] = mask_positions[i]
             block_position_ids = [torch.cat((
                 torch.zeros(context_length, dtype=torch.long, device=device),
                 torch.arange(seq_length - context_length, dtype=torch.long, device=device) + 1
             )) for context_length in context_lengths]
             block_position_ids = torch.stack(block_position_ids, dim=0)
             position_ids = torch.stack((position_ids, block_position_ids), dim=1)
         else:
             position_ids = torch.arange(seq_length, dtype=torch.long, device=device).unsqueeze(0).repeat(batch_size, 1)
-            if not gmask:
-                for i, context_length in enumerate(context_lengths):
+
+            for i, context_length in enumerate(context_lengths):
+                if not use_gmasks[i]:
                     position_ids[context_length:] = mask_positions[i]
 
         return position_ids
 
 CHATGLM_6B_START_DOCSTRING = r"""
     This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) sub-class.
     Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general
@@ -826,15 +837,15 @@
         self.layernorm_epsilon = config.layernorm_epsilon
         self.inner_hidden_size = config.inner_hidden_size
         self.hidden_size_per_attention_head = self.hidden_size // self.num_attention_heads
         self.position_encoding_2d = config.position_encoding_2d
         self.pre_seq_len = config.pre_seq_len
         self.prefix_projection = config.prefix_projection
 
-        self.word_embeddings = skip_init(
+        self.word_embeddings = skip_init_function(
             torch.nn.Embedding,
             num_embeddings=self.vocab_size, embedding_dim=self.hidden_size,
             dtype=self.params_dtype or torch.half
         )
         self.gradient_checkpointing = False
         def get_layer(layer_id):
             return GLMBlock(
@@ -924,15 +935,15 @@
                 )
                 use_cache = False
         if input_ids is not None and inputs_embeds is not None:
             raise ValueError("You cannot specify both input_ids and inputs_embeds at the same time")
         elif input_ids is not None:
             batch_size, seq_length = input_ids.shape[:2]
         elif inputs_embeds is not None:
-            batch_size, seq_length, _ = inputs_embeds.shape[:2]
+            batch_size, seq_length = inputs_embeds.shape[:2]
         else:
             raise ValueError("You have to specify either input_ids or inputs_embeds")
 
         
         
         if inputs_embeds is None:
             inputs_embeds = self.word_embeddings(input_ids)
@@ -947,51 +958,49 @@
 
             if attention_mask is None:
                 attention_mask = self.get_masks(
                     input_ids,
                     device=input_ids.device
                 )
 
-            
             if position_ids is None:
                 MASK, gMASK = self.config.mask_token_id, self.config.gmask_token_id
-                mask_token = gMASK if gMASK in input_ids else MASK
-                use_gmask = True if gMASK in input_ids else False
+                seqs = input_ids.tolist()
+
+                mask_positions, use_gmasks = [], []
+                for seq in seqs:
+                    mask_token = gMASK if gMASK in seq else MASK
+                    use_gmask = mask_token == gMASK
+                    mask_positions.append(seq.index(mask_token))
+                    use_gmasks.append(use_gmask)
 
-                mask_positions = [seq.tolist().index(mask_token) for seq in input_ids]
                 position_ids = self.get_position_ids(
                     input_ids,
                     mask_positions=mask_positions,
                     device=input_ids.device,
-                    gmask=use_gmask
+                    use_gmasks=use_gmasks
                 )
 
         if self.pre_seq_len is not None and attention_mask is not None:
             prefix_attention_mask = torch.ones(batch_size, 1, input_ids.size(-1), self.pre_seq_len).to(
                 attention_mask.device)
             prefix_attention_mask = (prefix_attention_mask < 0.5).bool()
             attention_mask = torch.cat((prefix_attention_mask, attention_mask), dim=3)
 
         # [seq_len, batch, hidden_size]
         hidden_states = inputs_embeds.transpose(0, 1)
 
         presents = () if use_cache else None
         all_self_attentions = () if output_attentions else None
         all_hidden_states = () if output_hidden_states else None
-
-                                         
-                                  
-                                          
-                                                                   
-                                                                                
         if attention_mask is None:
             attention_mask = torch.zeros(1, 1, device=input_ids.device).bool()
 
         else:
-            attention_mask = attention_mask.to(input_ids.device)
+            attention_mask = attention_mask.to(hidden_states.device)
 
         for i, layer in enumerate(self.layers):
 
             if output_hidden_states:
                 all_hidden_states = all_hidden_states + (hidden_states,)
             layer_past = past_key_values[i]
 
@@ -1045,15 +1054,15 @@
 class ChatGLMForConditionalGeneration(ChatGLMPreTrainedModel):
     _keys_to_ignore_on_load_missing = [ r"lm_head.weight"]
     def __init__(self, config: ChatGLMConfig):
         super().__init__(config)
         self.max_sequence_length = config.max_sequence_length
         self.position_encoding_2d = config.position_encoding_2d
         self.transformer = ChatGLMModel(config)
-        self.lm_head = skip_init(
+        self.lm_head = skip_init_function(
             nn.Linear,
             config.hidden_size,
             config.vocab_size,
             bias=False,
             dtype=self.transformer.params_dtype or torch.half
         )
         self.config = config
@@ -1111,18 +1120,22 @@
             attention_mask: Optional[torch.Tensor] = None,
             position_ids: Optional[torch.Tensor] = None,
             **kwargs
     ) -> dict:
 
         batch_size, seq_length = input_ids.shape
         MASK, gMASK = self.config.mask_token_id, self.config.gmask_token_id
-        mask_token = gMASK if gMASK in input_ids else MASK
-        use_gmask = True if gMASK in input_ids else False
+
         seqs = input_ids.tolist()
-        mask_positions = [seq.index(mask_token) for seq in seqs]
+        mask_positions, use_gmasks = [], []
+        for seq in seqs:
+            mask_token = gMASK if gMASK in seq else MASK
+            use_gmask = mask_token == gMASK
+            mask_positions.append(seq.index(mask_token))
+            use_gmasks.append(use_gmask)
 
         # only last token for input_ids if past is not None
         if past is not None or past_key_values is not None:
 
             last_token = input_ids[:, -1].unsqueeze(-1)
             if attention_mask is not None and attention_mask.dtype == torch.bool:
                 attention_mask = attention_mask[:, :, -1:]
@@ -1146,27 +1159,27 @@
                 "input_ids": last_token,
                 "past_key_values": past,
                 "position_ids": position_ids,
                 "attention_mask": attention_mask
             }
         else:
             if attention_mask is not None and attention_mask.dtype != torch.bool:
-                # logger.warning_once(f"The dtype of attention mask ({attention_mask.dtype}) is not bool")
+                logger.warning_once(f"The dtype of attention mask ({attention_mask.dtype}) is not bool")
                 attention_mask = None
             if attention_mask is None:
                 attention_mask = self.get_masks(
                     input_ids,
                     device=input_ids.device
                 )
             if position_ids is None:
                 position_ids = self.get_position_ids(
                     input_ids,
                     device=input_ids.device,
                     mask_positions=mask_positions,
-                    gmask=use_gmask
+                    use_gmasks=use_gmasks
                 )
 
             return {
                 "input_ids": input_ids,
                 "past_key_values": past,
                 "position_ids": position_ids,
                 "attention_mask": attention_mask
```

## deep_training/nlp/models/chatglm/tokenization.py

```diff
@@ -165,59 +165,69 @@
     model_input_names = ["input_ids", "attention_mask", "position_ids"]
 
     def __init__(
             self,
             vocab_file,
             do_lower_case=False,
             remove_space=False,
-            bos_token='sop',
-            eos_token='eos',
-            eop_token='eop',
+            bos_token='<sop>',
+            eos_token='<eop>',
+            end_token='</s>',
             mask_token='[MASK]',
             gmask_token='[gMASK]',
             padding_side="left",
+            pad_token="<pad>",
+            unk_token="<unk>",
             num_image_tokens=20000,
             **kwargs
     ) -> None:
         super().__init__(
             do_lower_case=do_lower_case,
             remove_space=remove_space,
             padding_side=padding_side,
+            bos_token=bos_token,
+            eos_token=eos_token,
+            end_token=end_token,
+            mask_token=mask_token,
+            gmask_token=gmask_token,
+            pad_token=pad_token,
+            unk_token=unk_token,
+            num_image_tokens=num_image_tokens,
             **kwargs
         )
 
         self.do_lower_case = do_lower_case
         self.remove_space = remove_space
         self.vocab_file = vocab_file
 
         self.bos_token = bos_token
         self.eos_token = eos_token
-        self.eop_token = eop_token
+        self.end_token = end_token
         self.mask_token = mask_token
         self.gmask_token = gmask_token
 
         self.sp_tokenizer = SPTokenizer(vocab_file, num_image_tokens=num_image_tokens)
 
         """ Initialisation """
 
     @property
     def gmask_token_id(self) -> Optional[int]:
         if self.gmask_token is None:
             return None
         return self.convert_tokens_to_ids(self.gmask_token)
 
     @property
-    def eop_token_id(self) -> Optional[int]:
+    def end_token_id(self) -> Optional[int]:
         """
-        `Optional[int]`: Id of the end of sentence token in the vocabulary. Returns `None` if the token has not been
+        `Optional[int]`: Id of the end of context token in the vocabulary. Returns `None` if the token has not been
         set.
         """
-        if self.eop_token is None:
+        if self.end_token is None:
             return None
-        return self.convert_tokens_to_ids(self.eop_token)
+        return self.convert_tokens_to_ids(self.end_token)
 
     @property
     def vocab_size(self):
         """ Returns vocab size """
         return self.sp_tokenizer.num_tokens
 
     def get_vocab(self):
@@ -306,30 +316,19 @@
             token_ids_0 (`List[int]`):
                 List of IDs to which the special tokens will be added.
             token_ids_1 (`List[int]`, *optional*):
                 Optional second list of IDs for sequence pairs.
         Returns:
             `List[int]`: List of [input IDs](../glossary#input-ids) with the appropriate special tokens.
         """
-        mask_ids = self.sp_tokenizer[self.mask_token]
-        gmask_ids = self.sp_tokenizer[self.gmask_token]
-        eop_id = self.sp_tokenizer[self.eop_token]
-        if mask_ids not in token_ids_0 and gmask_ids not in token_ids_0:
-            token_ids_0 += [gmask_ids]
-
-        if token_ids_0[-1] != mask_ids and token_ids_0[-1] != gmask_ids:
-            token_ids_0 += [self.sp_tokenizer[self.eos_token]]
-
-        token_ids_0 += [self.sp_tokenizer[self.bos_token]]
-
+        gmask_id = self.sp_tokenizer[self.gmask_token]
+        eos_id = self.sp_tokenizer[self.eos_token]
+        token_ids_0 = token_ids_0 + [gmask_id, self.sp_tokenizer[self.bos_token]]
         if token_ids_1 is not None:
-            if not token_ids_1 or token_ids_1[-1] != eop_id:
-                token_ids_1 += [eop_id]
-            token_ids_0 += token_ids_1
-
+            token_ids_0 = token_ids_0 + token_ids_1 + [eos_id]
         return token_ids_0
 
     def _pad(
             self,
             encoded_inputs: Union[Dict[str, EncodedInput], BatchEncoding],
             max_length: Optional[int] = None,
             padding_strategy: PaddingStrategy = PaddingStrategy.DO_NOT_PAD,
@@ -383,14 +382,18 @@
                 attention_mask = np.ones((1, seq_length, seq_length))
                 attention_mask = np.tril(attention_mask)
                 attention_mask[:, :, :context_length] = 1
                 attention_mask = np.bool_(attention_mask < 0.5)
                 encoded_inputs["attention_mask"] = attention_mask
 
             if "position_ids" not in encoded_inputs:
+                if bos_token_id in required_input:
+                    context_length = required_input.index(bos_token_id)
+                else:
+                    context_length = seq_length
                 position_ids = np.arange(seq_length, dtype=np.int64)
                 mask_token = mask_token_id if mask_token_id in required_input else gmask_token_id
                 if mask_token in required_input:
                     mask_position = required_input.index(mask_token)
                     position_ids[context_length:] = mask_position
                 block_position_ids = np.concatenate(
                     [np.zeros(context_length, dtype=np.int64),
```

## deep_training/nlp/models/lora/v2/lora_model.py

```diff
@@ -10,15 +10,15 @@
 from dataclasses import asdict
 from enum import Enum
 import torch
 from torch import nn
 from transformers import Conv1D
 
 from ....layers.lora_v2.layers import mark_only_lora_as_trainable, is_bnb_available, LoraLayer, Linear
-from ....layers.lora_v2.utils import _freeze_adapter, _get_submodules
+from ....layers.lora_v2.utils import _freeze_adapter, _get_submodules, ModulesToSaveWrapper
 
 __all__ = [
     'is_bnb_available',
     'LoraModel',
 ]
 
 
@@ -114,24 +114,25 @@
                         lora_config.lora_alpha,
                         lora_config.lora_dropout,
                         lora_config.init_lora_weights,
                         dtype=kwargs.get('dtype',None)
                     )
                 else:
                     if loaded_in_8bit and isinstance(target, bnb.nn.Linear8bitLt):
-                        kwargs.update(
+                        eightbit_kwargs = kwargs.copy()
+                        eightbit_kwargs.update(
                             {
                                 "has_fp16_weights": target.state.has_fp16_weights,
                                 "memory_efficient_backward": target.state.memory_efficient_backward,
                                 "threshold": target.state.threshold,
                                 "index": target.index,
                             }
                         )
                         new_module = Linear8bitLt(
-                            adapter_name, target.in_features, target.out_features, bias=bias, **kwargs
+                            adapter_name, target.in_features, target.out_features, bias=bias, **eightbit_kwargs
                         )
                     else:
                         if isinstance(target, torch.nn.Linear):
                             in_features, out_features = target.in_features, target.out_features
                             if kwargs["fan_in_fan_out"]:
                                 warnings.warn(
                                     "fan_in_fan_out is set to True but the target module is `torch.nn.Linear`. "
@@ -240,14 +241,21 @@
             raise ValueError("GPT2 models are not supported for merging LORA layers")
 
         if getattr(self.model, "is_loaded_in_8bit", False):
             raise ValueError("Cannot merge LORA layers when the model is loaded in 8-bit mode")
 
         key_list = [key for key, _ in self.model.named_modules() if "lora" not in key]
         for key in key_list:
-            parent, target, target_name = _get_submodules(self.model, key)
+            try:
+                parent, target, target_name = _get_submodules(self.model, key)
+            except AttributeError:
+                continue
             if isinstance(target, LoraLayer):
                 bias = target.bias is not None
                 new_module = torch.nn.Linear(target.in_features, target.out_features, bias=bias)
                 target.merge()
                 self._replace_module(parent, target_name, new_module, target)
+
+            # save any additional trainable modules part of `modules_to_save`
+            if isinstance(target, ModulesToSaveWrapper):
+                setattr(parent, target_name, target.modules_to_save[target.active_adapter])
         return self.model
```

## Comparing `deep_training-0.1.2.post0.dist-info/METADATA` & `deep_training-0.1.2.post2.dist-info/METADATA`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: deep-training
-Version: 0.1.2-post0
+Version: 0.1.2-post2
 Summary: an easy training architecture
 Home-page: https://github.com/ssbuild/deep_training
 Author: ssbuild
 Author-email: 9727464@qq.com
 License: Apache License 2.0
 Platform: UNKNOWN
 Requires-Dist: pytorch-lightning (>=2)
```

## Comparing `deep_training-0.1.2.post0.dist-info/RECORD` & `deep_training-0.1.2.post2.dist-info/RECORD`

 * *Files 2% similar despite different names*

```diff
@@ -1,15 +1,15 @@
 deep_training/__init__.py,sha256=bhATnUT4VEzwvA8_8IwxspnDRKf32ZgEeHYCN2E5Dd4,47
-deep_training/setup.py,sha256=Gz4IDp0RAgCkkuzEG3_uYR_STeSbY55JHVs6Bs5MLOU,903
+deep_training/setup.py,sha256=uLO1dDML4prp6R2XGy5XlYXPfe-I7vgxToq8ekav7Tw,903
 deep_training/cv/__init__.py,sha256=J-zlKxMsAfAgoO0vSAzgYJXSuMSJcJ7NKAPKeaeC3TM,55
 deep_training/data_helper/__init__.py,sha256=P8rAMalR6xNepAf-9ldGoOSsEiUtur8Px6gUpTXQhd8,195
 deep_training/data_helper/data_helper.py,sha256=wamHVXHpCIcG-COkVLRmwIjmcQ0MQy99xjago0ulkkY,29088
 deep_training/data_helper/data_module.py,sha256=cRqwzcKMpFaE2HhdbAEwrIC9KxOE0rLkeQ25VCJh2W8,4926
 deep_training/data_helper/data_writer.py,sha256=BnxUmMuR60Wawd1SH9TTEBsQuDLozxWJdmLozuO-Wv4,1383
-deep_training/data_helper/training_args.py,sha256=pxNrCoUDfHa1LoC7sEUPxT1bnY3WFMlq_7KYbmuQUOo,12552
+deep_training/data_helper/training_args.py,sha256=xAjTMEaNvXXzgdV9oNvSCaR1lKX4wdeIJoqus1VTN7w,12557
 deep_training/nlp/__init__.py,sha256=L4_ltrwpG8mrgN1hZRKimefLHgjhRYyXVtLMFzr1grw,70
 deep_training/nlp/layers/__init__.py,sha256=zbd9GfR02_YVgsTJSXjfyIcQwj8PmG4PscMdA0p6ONI,56
 deep_training/nlp/layers/activate.py,sha256=0q7htFl9Az2fdUjrjv-QMUCE5oenYPVTLZ3lRemIKzA,241
 deep_training/nlp/layers/crf.py,sha256=JTihPuJuBBp83I9UZzVg0wogwwpdJrs0VKtuLPBSCDM,13271
 deep_training/nlp/layers/handshakingkernel.py,sha256=BRJZbEjKM347q8zEMEtJXxXjmqhegmQgqebhqMy4UkI,4653
 deep_training/nlp/layers/mask.py,sha256=8SB_Hl9X48-yuJMCPjLDabDXvgWvH4VPqUOSVDmePFs,435
 deep_training/nlp/layers/mhslayer.py,sha256=Ky6xW3hNe0x4WWPPoWa8pZkCp_-MR5VE0yKHYPzzpd0,1319
@@ -92,28 +92,28 @@
 deep_training/nlp/models/w2ner.py,sha256=-KtfdpYY68s142CMcWHJbfiEHFW-qbSEENqtRgxHjBw,9025
 deep_training/nlp/models/LLaMA/__init__.py,sha256=n_M2atEv-G2i3oy_YkLCVu5QiLLyfxEj2xr91h1dWEw,16500
 deep_training/nlp/models/LLaMA/configuration.py,sha256=HNzzhIIdR9HBN9Y4Oavv6cGgIf0ExcphwsbVkltJ2ZM,5087
 deep_training/nlp/models/LLaMA_parallel/__init__.py,sha256=5lsrh09pBTzRRKwc1bJtJs37Qn5qKqif_5S0pOoD1zc,19183
 deep_training/nlp/models/LLaMA_parallel/configuration.py,sha256=HNzzhIIdR9HBN9Y4Oavv6cGgIf0ExcphwsbVkltJ2ZM,5087
 deep_training/nlp/models/PaLM/__init__.py,sha256=P1qwWPUycRmZ6I48tov6janJUNpp4L-iMoVN54ykcQw,31627
 deep_training/nlp/models/PaLM/configuration.py,sha256=kIb3nj-2pQB2wyNrYHSZqr_ta1F0Cg-VbGEbnM5icPc,5890
-deep_training/nlp/models/chatglm/__init__.py,sha256=VU859zidTz3HD5gT8xt2ZQr2NNU8wAnSjdY3TUz3sIk,60188
+deep_training/nlp/models/chatglm/__init__.py,sha256=t9u927aSHY0mVpxtGrY7_jMEx25g9rsHKcE0HR3GWGc,60508
 deep_training/nlp/models/chatglm/configuration.py,sha256=4w-Kbp_FJ2crIQVyu6kie9lbMSuE3U4nnjwjVPos2E8,4575
 deep_training/nlp/models/chatglm/quantization.py,sha256=sqX_poTcYNLJLDPbCwfRllDCF0enhshjX_dw7yZa604,15150
-deep_training/nlp/models/chatglm/tokenization.py,sha256=8K8W7b2ciL8rAYlq-XEYM9p0H2NtyLn9oIUYO8zHlQo,16460
+deep_training/nlp/models/chatglm/tokenization.py,sha256=XPdz8DFekZ3qsYakyP2iZbk0Iu1cCXgnXjrBKHDO_s8,16642
 deep_training/nlp/models/laMDA/__init__.py,sha256=fvxTQQ8jfU-msPRdC8KsGlCwzM6u8-WBmayu6gE-s0E,34123
 deep_training/nlp/models/laMDA/configuration.py,sha256=8ZvPEl1C1KUGYWw7a8XcgIgl3gWH9WXa_-ZNDqz34PE,5981
 deep_training/nlp/models/lora/__init__.py,sha256=YyYmxdwz0IdyE4QXMPWhjUNmvOccqO8kJzPblfPwGJI,181
 deep_training/nlp/models/lora/v1/__init__.py,sha256=zwGdNKqudVj7c8sMWbmZ9CnnncWXuEapAucWY-VEhLs,123
 deep_training/nlp/models/lora/v1/configuration.py,sha256=MAkg2BQCqL6XvHd2SRXsijtPToNMuN-4Hmm_02HVCmU,7054
 deep_training/nlp/models/lora/v1/lora_wrapper.py,sha256=449xBRufJ8Tt4p_OLsp7TZ3v8Y3iIBdpccgReGDzUng,13576
 deep_training/nlp/models/lora/v2/__init__.py,sha256=2XorjeFlyNuH6xTXiyNO1A8A3P5acBApOjxVv3YEon0,206
 deep_training/nlp/models/lora/v2/adalora_model.py,sha256=iKfKWnW--iY2gmXkMcBv6QWJr9vu-uSll57r1UNvRrY,13112
 deep_training/nlp/models/lora/v2/configuration.py,sha256=RXTsOjRKUHGFKpSrrFywjV41kT2liEPj1C6Li-ogJpI,11281
-deep_training/nlp/models/lora/v2/lora_model.py,sha256=RSOldvGoGfarFpxok8fvf_Ng-1490ZiX4s7Kgl35U1o,11332
+deep_training/nlp/models/lora/v2/lora_model.py,sha256=5k09kzj8bSvU-CQm5GJWtg5isXUEUYPmvKuxdLPc_V4,11745
 deep_training/nlp/models/lora/v2/lora_wrapper.py,sha256=tzDCWUiGYC81cbfNiu4ZKo3VZsftM_SF8NwH8r56BO0,10265
 deep_training/nlp/models/lora/v2/save_and_load.py,sha256=U7_ZaPm8gpg8gQhZei6UG5KvsJXDtSNZfZk1gWo6nWc,4889
 deep_training/nlp/models/splinker/__init__.py,sha256=QtgnpJa78vAq9bzfjN67NmHU3dXU6WH84jeyZoD1sBs,102
 deep_training/nlp/models/splinker/splinker.py,sha256=cAXQoPucM3ULYUhBw9z-OxPK_b9hHKEZPgMb4vFfQwc,2851
 deep_training/nlp/models/t5decoder/__init__.py,sha256=R9Op4Ysli9isootQQ2FcjhpbG13fNESlmUROu6cfGH0,14478
 deep_training/nlp/models/t5encoder/__init__.py,sha256=692ChfLf2sZWgzhBM37g1PdpmEmsU1R9RRl_uTHRET0,6646
 deep_training/nlp/optimizer/__init__.py,sha256=c4cmx9ebIdqwXBu3N9QbcNNHb32t2MV6fTK9aC2VBGQ,56
@@ -135,11 +135,11 @@
 deep_training/tfnlp/scheduler/__init__.py,sha256=69flKnae4cQQyWUDwuYE0w0iaPonvH0P_WjBd_t-IqU,53
 deep_training/tfnlp/utils/__init__.py,sha256=kAmlOWNSpQCHbtT-mAsKGQzQFoWKp2jQf3neCJ0cCRY,53
 deep_training/utils/__init__.py,sha256=JFm7m_LPsS9Oavyxn9rbWqllCmV_zBho19rISlHNX4c,55
 deep_training/utils/distributed.py,sha256=-dhvJ6YHpRxvtZ1_on50IE33fUFW3zKXBKqqK-L1HGM,1941
 deep_training/utils/func.py,sha256=1p8hiQDCyk_gQGKrF7y6Dt66k3jLXSAt2IQeJuHQEl8,1724
 deep_training/utils/maskedlm.py,sha256=o8EB2BbDdh7wdgqz9Oi6SsVr1uBWxV15qfTk2VPjWsU,5117
 deep_training/utils/trainer.py,sha256=Rit5xEC2r9MvQdDR4A5A6E4_gzNCVNmW9m55kC83O50,7469
-deep_training-0.1.2.post0.dist-info/METADATA,sha256=7tvOzHtlpKlshtZR5LlbKXIo8JTENv8WBjOB7oStm9w,629
-deep_training-0.1.2.post0.dist-info/WHEEL,sha256=OqRkF0eY5GHssMorFjlbTIq072vpHpF60fIQA6lS9xA,92
-deep_training-0.1.2.post0.dist-info/top_level.txt,sha256=P4qengiW56PZRm1VvlGcseSUCmAaBCsalCviUABZtO0,14
-deep_training-0.1.2.post0.dist-info/RECORD,,
+deep_training-0.1.2.post2.dist-info/METADATA,sha256=7DPlXexw6vD32Bc_gEcsvA0I3Su07O4FHL29vH6dJ2g,629
+deep_training-0.1.2.post2.dist-info/WHEEL,sha256=OqRkF0eY5GHssMorFjlbTIq072vpHpF60fIQA6lS9xA,92
+deep_training-0.1.2.post2.dist-info/top_level.txt,sha256=P4qengiW56PZRm1VvlGcseSUCmAaBCsalCviUABZtO0,14
+deep_training-0.1.2.post2.dist-info/RECORD,,
```

