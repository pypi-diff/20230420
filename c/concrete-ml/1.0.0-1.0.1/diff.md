# Comparing `tmp/concrete_ml-1.0.0-py3-none-any.whl.zip` & `tmp/concrete_ml-1.0.1-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,57 +1,57 @@
-Zip file size: 168782 bytes, number of entries: 55
+Zip file size: 168436 bytes, number of entries: 55
 -rw-r--r--  2.0 unx      284 b- defN 80-Jan-01 00:00 concrete/__init__.py
 -rw-r--r--  2.0 unx     1722 b- defN 80-Jan-01 00:00 concrete/ml/__init__.py
 -rw-r--r--  2.0 unx       95 b- defN 80-Jan-01 00:00 concrete/ml/common/__init__.py
 -rw-r--r--  2.0 unx     2519 b- defN 80-Jan-01 00:00 concrete/ml/common/check_inputs.py
 -rw-r--r--  2.0 unx      101 b- defN 80-Jan-01 00:00 concrete/ml/common/debugging/__init__.py
 -rw-r--r--  2.0 unx     2377 b- defN 80-Jan-01 00:00 concrete/ml/common/debugging/custom_assert.py
 -rw-r--r--  2.0 unx        0 b- defN 80-Jan-01 00:00 concrete/ml/common/serialization/__init__.py
--rw-r--r--  2.0 unx     1310 b- defN 80-Jan-01 00:00 concrete/ml/common/serialization/dumpers.py
+-rw-r--r--  2.0 unx     1318 b- defN 80-Jan-01 00:00 concrete/ml/common/serialization/dumpers.py
 -rw-r--r--  2.0 unx     2018 b- defN 80-Jan-01 00:00 concrete/ml/common/serialization/encoder.py
--rw-r--r--  2.0 unx     2856 b- defN 80-Jan-01 00:00 concrete/ml/common/serialization/loaders.py
--rw-r--r--  2.0 unx    17216 b- defN 80-Jan-01 00:00 concrete/ml/common/utils.py
+-rw-r--r--  2.0 unx     2888 b- defN 80-Jan-01 00:00 concrete/ml/common/serialization/loaders.py
+-rw-r--r--  2.0 unx    17217 b- defN 80-Jan-01 00:00 concrete/ml/common/utils.py
 -rw-r--r--  2.0 unx      235 b- defN 80-Jan-01 00:00 concrete/ml/deployment/Dockerfile.server
 -rw-r--r--  2.0 unx      121 b- defN 80-Jan-01 00:00 concrete/ml/deployment/__init__.py
--rw-r--r--  2.0 unx    17568 b- defN 80-Jan-01 00:00 concrete/ml/deployment/deploy_to_aws.py
+-rw-r--r--  2.0 unx    17843 b- defN 80-Jan-01 00:00 concrete/ml/deployment/deploy_to_aws.py
 -rw-r--r--  2.0 unx     3800 b- defN 80-Jan-01 00:00 concrete/ml/deployment/deploy_to_docker.py
--rw-r--r--  2.0 unx    15080 b- defN 80-Jan-01 00:00 concrete/ml/deployment/fhe_client_server.py
--rw-r--r--  2.0 unx     4321 b- defN 80-Jan-01 00:00 concrete/ml/deployment/server.py
+-rw-r--r--  2.0 unx    15084 b- defN 80-Jan-01 00:00 concrete/ml/deployment/fhe_client_server.py
+-rw-r--r--  2.0 unx     2590 b- defN 80-Jan-01 00:00 concrete/ml/deployment/server.py
 -rw-r--r--  2.0 unx       33 b- defN 80-Jan-01 00:00 concrete/ml/deployment/server_requirements.txt
--rw-r--r--  2.0 unx     3285 b- defN 80-Jan-01 00:00 concrete/ml/deployment/utils.py
+-rw-r--r--  2.0 unx     3293 b- defN 80-Jan-01 00:00 concrete/ml/deployment/utils.py
 -rw-r--r--  2.0 unx       19 b- defN 80-Jan-01 00:00 concrete/ml/onnx/__init__.py
 -rw-r--r--  2.0 unx     3472 b- defN 80-Jan-01 00:00 concrete/ml/onnx/convert.py
 -rw-r--r--  2.0 unx     9401 b- defN 80-Jan-01 00:00 concrete/ml/onnx/onnx_impl_utils.py
--rw-r--r--  2.0 unx     9684 b- defN 80-Jan-01 00:00 concrete/ml/onnx/onnx_model_manipulations.py
+-rw-r--r--  2.0 unx     9685 b- defN 80-Jan-01 00:00 concrete/ml/onnx/onnx_model_manipulations.py
 -rw-r--r--  2.0 unx    20192 b- defN 80-Jan-01 00:00 concrete/ml/onnx/onnx_utils.py
--rw-r--r--  2.0 unx    58040 b- defN 80-Jan-01 00:00 concrete/ml/onnx/ops_impl.py
+-rw-r--r--  2.0 unx    58045 b- defN 80-Jan-01 00:00 concrete/ml/onnx/ops_impl.py
 -rw-r--r--  2.0 unx      101 b- defN 80-Jan-01 00:00 concrete/ml/pytest/__init__.py
--rw-r--r--  2.0 unx    42028 b- defN 80-Jan-01 00:00 concrete/ml/pytest/torch_models.py
--rw-r--r--  2.0 unx     8787 b- defN 80-Jan-01 00:00 concrete/ml/pytest/utils.py
+-rw-r--r--  2.0 unx    42036 b- defN 80-Jan-01 00:00 concrete/ml/pytest/torch_models.py
+-rw-r--r--  2.0 unx     8786 b- defN 80-Jan-01 00:00 concrete/ml/pytest/utils.py
 -rw-r--r--  2.0 unx     1081 b- defN 80-Jan-01 00:00 concrete/ml/quantization/__init__.py
--rw-r--r--  2.0 unx    35250 b- defN 80-Jan-01 00:00 concrete/ml/quantization/base_quantized_op.py
--rw-r--r--  2.0 unx    43507 b- defN 80-Jan-01 00:00 concrete/ml/quantization/post_training.py
--rw-r--r--  2.0 unx    24344 b- defN 80-Jan-01 00:00 concrete/ml/quantization/quantized_module.py
--rw-r--r--  2.0 unx    80967 b- defN 80-Jan-01 00:00 concrete/ml/quantization/quantized_ops.py
--rw-r--r--  2.0 unx    37503 b- defN 80-Jan-01 00:00 concrete/ml/quantization/quantizers.py
+-rw-r--r--  2.0 unx    35262 b- defN 80-Jan-01 00:00 concrete/ml/quantization/base_quantized_op.py
+-rw-r--r--  2.0 unx    43525 b- defN 80-Jan-01 00:00 concrete/ml/quantization/post_training.py
+-rw-r--r--  2.0 unx    24352 b- defN 80-Jan-01 00:00 concrete/ml/quantization/quantized_module.py
+-rw-r--r--  2.0 unx    80991 b- defN 80-Jan-01 00:00 concrete/ml/quantization/quantized_ops.py
+-rw-r--r--  2.0 unx    37549 b- defN 80-Jan-01 00:00 concrete/ml/quantization/quantizers.py
 -rw-r--r--  2.0 unx       77 b- defN 80-Jan-01 00:00 concrete/ml/search_parameters/__init__.py
--rw-r--r--  2.0 unx    21176 b- defN 80-Jan-01 00:00 concrete/ml/search_parameters/p_error_search.py
+-rw-r--r--  2.0 unx    21193 b- defN 80-Jan-01 00:00 concrete/ml/search_parameters/p_error_search.py
 -rw-r--r--  2.0 unx     4679 b- defN 80-Jan-01 00:00 concrete/ml/sklearn/__init__.py
--rw-r--r--  2.0 unx    62686 b- defN 80-Jan-01 00:00 concrete/ml/sklearn/base.py
+-rw-r--r--  2.0 unx    62708 b- defN 80-Jan-01 00:00 concrete/ml/sklearn/base.py
 -rw-r--r--  2.0 unx    13633 b- defN 80-Jan-01 00:00 concrete/ml/sklearn/glm.py
 -rw-r--r--  2.0 unx    25212 b- defN 80-Jan-01 00:00 concrete/ml/sklearn/linear_model.py
--rw-r--r--  2.0 unx    13339 b- defN 80-Jan-01 00:00 concrete/ml/sklearn/qnn.py
--rw-r--r--  2.0 unx    12567 b- defN 80-Jan-01 00:00 concrete/ml/sklearn/qnn_module.py
+-rw-r--r--  2.0 unx    13342 b- defN 80-Jan-01 00:00 concrete/ml/sklearn/qnn.py
+-rw-r--r--  2.0 unx    12572 b- defN 80-Jan-01 00:00 concrete/ml/sklearn/qnn_module.py
 -rw-r--r--  2.0 unx    12243 b- defN 80-Jan-01 00:00 concrete/ml/sklearn/rf.py
 -rw-r--r--  2.0 unx    10614 b- defN 80-Jan-01 00:00 concrete/ml/sklearn/svm.py
 -rw-r--r--  2.0 unx    10394 b- defN 80-Jan-01 00:00 concrete/ml/sklearn/tree.py
--rw-r--r--  2.0 unx    10920 b- defN 80-Jan-01 00:00 concrete/ml/sklearn/tree_to_numpy.py
+-rw-r--r--  2.0 unx    10923 b- defN 80-Jan-01 00:00 concrete/ml/sklearn/tree_to_numpy.py
 -rw-r--r--  2.0 unx    20074 b- defN 80-Jan-01 00:00 concrete/ml/sklearn/xgb.py
 -rw-r--r--  2.0 unx       83 b- defN 80-Jan-01 00:00 concrete/ml/torch/__init__.py
--rw-r--r--  2.0 unx    17532 b- defN 80-Jan-01 00:00 concrete/ml/torch/compile.py
+-rw-r--r--  2.0 unx    17531 b- defN 80-Jan-01 00:00 concrete/ml/torch/compile.py
 -rw-r--r--  2.0 unx     3174 b- defN 80-Jan-01 00:00 concrete/ml/torch/numpy_module.py
 -rw-r--r--  2.0 unx      124 b- defN 80-Jan-01 00:00 concrete/ml/version.py
--rw-r--r--  2.0 unx     1546 b- defN 80-Jan-01 00:00 concrete_ml-1.0.0.dist-info/LICENSE
-?rw-r--r--  2.0 unx       88 b- defN 16-Jan-01 00:00 concrete_ml-1.0.0.dist-info/WHEEL
-?rw-r--r--  2.0 unx    11662 b- defN 16-Jan-01 00:00 concrete_ml-1.0.0.dist-info/METADATA
-?rw-r--r--  2.0 unx     4967 b- defN 16-Jan-01 00:00 concrete_ml-1.0.0.dist-info/RECORD
-55 files, 706107 bytes uncompressed, 160814 bytes compressed:  77.2%
+-rw-r--r--  2.0 unx     1546 b- defN 80-Jan-01 00:00 concrete_ml-1.0.1.dist-info/LICENSE
+?rw-r--r--  2.0 unx       88 b- defN 16-Jan-01 00:00 concrete_ml-1.0.1.dist-info/WHEEL
+?rw-r--r--  2.0 unx    11617 b- defN 16-Jan-01 00:00 concrete_ml-1.0.1.dist-info/METADATA
+?rw-r--r--  2.0 unx     4967 b- defN 16-Jan-01 00:00 concrete_ml-1.0.1.dist-info/RECORD
+55 files, 704829 bytes uncompressed, 160468 bytes compressed:  77.2%
```

## zipnote {}

```diff
@@ -147,20 +147,20 @@
 
 Filename: concrete/ml/torch/numpy_module.py
 Comment: 
 
 Filename: concrete/ml/version.py
 Comment: 
 
-Filename: concrete_ml-1.0.0.dist-info/LICENSE
+Filename: concrete_ml-1.0.1.dist-info/LICENSE
 Comment: 
 
-Filename: concrete_ml-1.0.0.dist-info/WHEEL
+Filename: concrete_ml-1.0.1.dist-info/WHEEL
 Comment: 
 
-Filename: concrete_ml-1.0.0.dist-info/METADATA
+Filename: concrete_ml-1.0.1.dist-info/METADATA
 Comment: 
 
-Filename: concrete_ml-1.0.0.dist-info/RECORD
+Filename: concrete_ml-1.0.1.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## concrete/ml/__init__.py

```diff
@@ -1,14 +1,14 @@
 """ML module."""
 import os
 
 from .version import __version__
 
 # If the use of skops needs to be disabled.
-# This could be useful when loading a model with python 3.7 with a higher version
+# This could be useful when loading a model with Python 3.7 with a higher version
 USE_SKOPS = int(os.environ.get("USE_SKOPS", 1))
 
 # These are all the trusted types that are considered by skops
 TRUSTED_SKOPS = [
     "numpy.int64",
     "numpy.float64",
     "numpy.int32",
```

## concrete/ml/common/debugging/custom_assert.py

```diff
@@ -1,15 +1,15 @@
 """Provide some variants of assert."""
 from typing import Type
 
 
 def _custom_assert(
     condition: bool, on_error_msg: str = "", error_type: Type[Exception] = AssertionError
 ):
-    """Provide a custom assert which is kept even if the optimized python mode is used.
+    """Provide a custom assert which is kept even if the optimized Python mode is used.
 
     See https://docs.python.org/3/reference/simple_stmts.html#assert for the documentation
     on the classical assert function
 
     Args:
         condition(bool): the condition. If False, raise AssertionError
         on_error_msg(str): optional message for precising the error, in case of error
```

## concrete/ml/common/serialization/dumpers.py

```diff
@@ -22,15 +22,15 @@
         return str(random_state)
     if random_state is None:
         return "null"
     return json.dumps(random_state.get_state(), cls=CustomEncoder)
 
 
 def dump(obj: Any, file: TextIO):
-    """Dump any CML object that has a dump method.
+    """Dump any Concrete ML object that has a dump method.
 
     Arguments:
         obj (Any): the object to dump.
         file (TextIO): a file containing the serialized object.
     """
     file.write(dumps(obj))
```

## concrete/ml/common/serialization/loaders.py

```diff
@@ -10,15 +10,15 @@
 from ...sklearn.base import _ALL_SKLEARN_MODELS
 
 # pylint: disable=invalid-name
 LOADS_METHODS: Dict[str, Callable] = {}
 
 
 def load_dict(metadata: Dict[str, Any]) -> Any:
-    """Load any CML object that has a dump method.
+    """Load any Concrete ML object that has a dump method.
 
     Arguments:
         metadata (Dict[str, Any]): a dict of a serialized object.
 
     Returns:
         Any: the object itself.
 
@@ -35,34 +35,34 @@
         }
         LOADS_METHODS["QuantizedArray"] = QuantizedArray.load_dict
         LOADS_METHODS["UniformQuantizer"] = UniformQuantizer.load_dict
 
     try:
         class_name = metadata["cml_dumped_class_name"]
     except KeyError as exception:
-        raise ValueError("The content provided is not a CML dumped model.") from exception
+        raise ValueError("The content provided is not a Concrete ML dumped model.") from exception
 
     return LOADS_METHODS[class_name](metadata)
 
 
 def loads(content: str) -> Any:
-    """Load any CML object that has a dump method.
+    """Load any Concrete ML object that has a dump method.
 
     Arguments:
         content (str): a serialized object.
 
     Returns:
         Any: the object itself.
     """
     parsed = json.loads(content)
     return load_dict(parsed)
 
 
 def load(file: TextIO):
-    """Load any CML object that has a dump method.
+    """Load any Concrete ML object that has a dump method.
 
     Arguments:
         file (TextIO): a file containing the serialized object.
 
     Returns:
         Any: the object itself.
     """
@@ -90,15 +90,15 @@
         serialized_random_state (str): a serialized version of the random state
 
     Returns:
         random_state (Union[RandomState, int, None]): a random state
 
 
     """
-    unserialized = json.loads(serialized_random_state)
-    if isinstance(unserialized, int) or (unserialized is None):
-        return unserialized
-    assert isinstance(unserialized, list)
-    unserialized = tuple(unserialized)
+    deserialized = json.loads(serialized_random_state)
+    if isinstance(deserialized, int) or (deserialized is None):
+        return deserialized
+    assert isinstance(deserialized, list)
+    deserialized = tuple(deserialized)
     random_state = RandomState()
-    random_state.set_state(unserialized)
+    random_state.set_state(deserialized)
     return random_state
```

## concrete/ml/common/utils.py

```diff
@@ -112,23 +112,23 @@
             returned in a dictionary. Only the sanitized names will work for a call to the proxy
             function.
 
     Returns:
         Tuple[Callable, Dict[str, str]]: the proxy function and the mapping of the original arg name
             to the new and sanitized arg names.
     """
-    # Some input names can be invalid arg names (e.g. coming from torch input.0) so sanitize them
-    # to be valid python arg names.
+    # Some input names can be invalid arg names (eg coming from torch input.0) so sanitize them
+    # to be valid Python arg names.
     orig_args_to_proxy_func_args = {
         arg_name: f"_{replace_invalid_arg_name_chars(arg_name)}"
         for arg_name in desired_functions_arg_names
     }
     proxy_func_arg_string = ", ".join(orig_args_to_proxy_func_args.values())
     proxy_func_name = replace_invalid_arg_name_chars(f"{function_to_proxy.__name__}_proxy")
-    # compile is the built-in python compile to generate code at runtime.
+    # compile is the built-in Python compile to generate code at runtime.
     function_proxy_code = compile(
         f"def {proxy_func_name}({proxy_func_arg_string}): "
         f"return function_to_proxy({proxy_func_arg_string})",
         __file__,
         mode="exec",
     )
     function_proxy = FunctionType(function_proxy_code.co_consts[0], locals(), proxy_func_name)
@@ -331,15 +331,15 @@
     Returns:
         bool: If the input container is a DataFrame orSeries
     """
     return is_pandas_dataframe(input_container) or is_pandas_series(input_container)
 
 
 def _get_dtype(values: Any):
-    """Get a set of values' dtype in a string format to facilitate opetations between sets.
+    """Get a set of values' dtype in a string format to facilitate operations between sets.
 
     Args:
         values (Any): The values to consider
 
     Returns:
         set[str]: The values' dtype(s) in string format.
 
@@ -378,15 +378,15 @@
     """Indicate if the values' dtype(s) matches the given one(s).
 
     Args:
         values (Any): The values to consider.
         valid_dtypes (Dict): The only dtype(s) to consider.
 
     Returns:
-        bool: If the values' dtype(s) matche the given ones.
+        bool: If the values' dtype(s) matches the given ones.
 
     Examples:
         values_types = _get_dtype(pandas.DataFrame([[1, 0.5, '1']]))
         valid_dtypes = _get_dtype(SUPPORTED_INT_TYPE)
 
         >>> values_types
         {'float64', 'int64', 'object'}
@@ -518,24 +518,24 @@
 def all_values_are_integers(*values: Any) -> bool:
     """Indicate that all unpacked values are of a supported integer dtype.
 
     Args:
         *values (Any): The values to consider.
 
     Returns:
-        bool: Wether all values are supported integers or not.
+        bool: Whether all values are supported integers or not.
 
     """
     return all(_is_of_dtype(value, SUPPORTED_INT_TYPES) for value in values)
 
 
 def all_values_are_floats(*values: Any) -> bool:
     """Indicate that all unpacked values are of a supported float dtype.
 
     Args:
         *values (Any): The values to consider.
 
     Returns:
-        bool: Wether all values are supported floating points or not.
+        bool: Whether all values are supported floating points or not.
 
     """
     return all(_is_of_dtype(value, SUPPORTED_FLOAT_TYPES) for value in values)
```

## concrete/ml/deployment/deploy_to_aws.py

```diff
@@ -2,15 +2,15 @@
 
 It takes as input a folder with:
     - client.zip
     - server.zip
     - processing.json
 
 It spawns a AWS EC2 instance with proper security groups.
-Then SSHs to it to rsync the files and update python dependencies.
+Then SSHs to it to rsync the files and update Python dependencies.
 It then launches the server.
 """
 
 import argparse
 import json
 import subprocess
 import time
@@ -18,17 +18,19 @@
 from contextlib import closing
 from datetime import datetime
 from pathlib import Path
 from typing import Any, Dict, Optional
 
 import boto3
 
-from ..deployment.utils import filter_logs, wait_for_connexion_to_be_available
+from ..deployment.utils import filter_logs, wait_for_connection_to_be_available
 
 DATE_FORMAT: str = "%Y_%m_%d_%H_%M_%S"
+# More up to date public Concrete ML AWS AMI
+DEFAULT_CML_AMI_ID: str = "ami-0d7427e883fa00ff3"
 
 
 class AWSInstance:
     """AWSInstance.
 
     Context manager for AWS instance that supports ssh and http over one port.
     """
@@ -39,21 +41,23 @@
         self,
         instance_type: str = "c5.large",
         open_port=5000,
         instance_name: Optional[str] = None,
         verbose: bool = False,
         terminate_on_shutdown: bool = True,
         region_name: Optional[str] = None,
+        ami_id: str = DEFAULT_CML_AMI_ID,
     ):
         metadata = create_instance(
             instance_type=instance_type,
             open_port=open_port,
             instance_name=instance_name,
             verbose=verbose,
             region_name=region_name,
+            ami_id=ami_id,
         )
         self.instance_metadata = metadata
         self.terminate_on_shutdown = terminate_on_shutdown
         self.region_name = region_name
 
     def __enter__(
         self,
@@ -91,23 +95,25 @@
 
 def create_instance(
     instance_type: str = "c5.large",
     open_port=5000,
     instance_name: Optional[str] = None,
     verbose: bool = False,
     region_name: Optional[str] = None,
+    ami_id=DEFAULT_CML_AMI_ID,
 ) -> Dict[str, Any]:
     """Create a EC2 instance.
 
     Arguments:
         instance_type (str): the type of AWS EC2 instance.
         open_port (int): the port to open.
         instance_name (Optional[str]): the name to use for AWS created objects
         verbose (bool): show logs or not
         region_name (Optional[str]): AWS region
+        ami_id (str): AMI to use
 
     Returns:
         Dict[str, Any]: some information about the newly created instance.
             - ip
             - private_key
             - instance_id
             - key_path
@@ -119,16 +125,15 @@
     # Create client/resource objects
     with closing(boto3.client("ec2", region_name=region_name)) as client:
         resources = boto3.resource("ec2", region_name=region_name)
         str_now = datetime.now().strftime(DATE_FORMAT)
         name = f"deploy-cml-{str_now}" if instance_name is None else f"{instance_name}-{str_now}"
 
         # Get VPC
-        response: Dict = client.describe_vpcs()
-        vpc_id: str = response.get("Vpcs", [{}])[0].get("VpcId", "")
+        vpc_id: str = client.describe_vpcs().get("Vpcs", [{}])[0].get("VpcId", "")
         # OPTION 1: get fist vpc available
         if vpc_id:
             vpc = resources.Vpc(vpc_id)
         # OPTION 2: create VPC (not possible if too many VPCs)
         else:  # pragma:no cover
             vpc = resources.create_vpc(CidrBlock="0.0.0.0/0")
             vpc.wait_until_available()
@@ -139,18 +144,17 @@
         if not subnets:  # pragma: no cover
             subnet = vpc.create_subnet(CidrBlock=vpc.cidr_block)
         # OPTION 2: get first subnet
         else:
             subnet = subnets[0]
 
         # Create security group
-        response = client.create_security_group(
-            GroupName=name, Description=f"Deploy CML {str_now}", VpcId=vpc_id
-        )
-        security_group_id = response["GroupId"]
+        security_group_id = client.create_security_group(
+            GroupName=name, Description=f"Deploy Concrete ML {str_now}", VpcId=vpc_id
+        )["GroupId"]
         if verbose:
             print(f"Security Group Created {security_group_id} in vpc {vpc_id}.")
 
         client.authorize_security_group_ingress(
             GroupId=security_group_id,
             IpPermissions=[
                 # Server port
@@ -168,29 +172,28 @@
                     "IpRanges": [{"CidrIp": "0.0.0.0/0"}],
                 },
             ],
         )
 
         # Create key-pair
         keypair_name = f"{name}-keypair"
-        response = client.create_key_pair(KeyName=keypair_name)
-        private_key: str = response["KeyMaterial"]
+        private_key: str = client.create_key_pair(KeyName=keypair_name)["KeyMaterial"]
 
-        # Keep the key if we want to ssh to check what happenned on the instance
+        # Keep the key if we want to ssh to check what happened on the instance
         key_folder = (Path(__file__).parent / "ssh_keys").resolve()
         key_folder.mkdir(exist_ok=True)
         key_path = key_folder / f"{keypair_name}.pem"
         with open(key_path, "w", encoding="utf-8") as file:
             file.write(private_key)
         key_path.chmod(0o400)
 
         # Create instance
         instances = resources.create_instances(
-            # CML official AMI ID to make sure to have everything needed
-            ImageId="ami-0d7427e883fa00ff3",
+            # Concrete ML official AMI ID to make sure to have everything needed
+            ImageId=ami_id,
             InstanceType=instance_type,  # Instance type
             DryRun=False,
             InstanceInitiatedShutdownBehavior="terminate",
             # Associate keypair to instance
             KeyName=keypair_name,
             # Some tags
             TagSpecifications=[
@@ -252,16 +255,16 @@
     """Deploy a model to a EC2 AWS instance.
 
     Arguments:
         instance_metadata (Dict[str, Any]): the metadata of AWS EC2 instance
             created using AWSInstance or create_instance
         path_to_model (Path): the path to the serialized model
         number_of_ssh_retries (int): the number of ssh retries (-1 is no limit)
-        wait_bar (bool): wheter to show a wait bar when waiting for ssh connexion to be available
-        verbose (bool): wheter to show a logs
+        wait_bar (bool): whether to show a wait bar when waiting for ssh connection to be available
+        verbose (bool): whether to show a logs
 
     Returns:
         instance_metadata (Dict[str, Any])
 
     Raises:
         RuntimeError: if launching the server crashed
     """
@@ -282,28 +285,28 @@
         file.write(
             f"""#! /bin/env bash
 aws ec2 terminate-instances --instance-ids {instance_metadata['instance_id']}
 aws ec2 delete-security-group --group-id {instance_metadata['security_group_id']}"""
         )
 
     if verbose:
-        print("Waiting for SSH connexion to be available...")
+        print("Waiting for SSH connection to be available...")
 
     # Connect to instance
-    wait_for_connexion_to_be_available(
+    wait_for_connection_to_be_available(
         hostname=hostname,
         ip_address=ip_address,
         path_to_private_key=key_path,
         timeout=1,
         max_retries=number_of_ssh_retries,
         wait_bar=wait_bar,
     )
 
     if verbose:
-        print("SSH connexion available.")
+        print("SSH connection available.")
 
     path_to_server_file = Path(__file__).parent / "server.py"
     path_to_server_requirements = Path(__file__).parent / "server_requirements.txt"
 
     if verbose:
         print("upload files...")
 
@@ -334,17 +337,19 @@
         f"sudo chmod -R 777 /home/{hostname}/venv",
         f"sudo apt install -y python{python_version} python{python_version}-distutils make cmake",
         f"virtualenv deployment_venv --python=python{python_version}",
         # The venv is not activated by default
         "source deployment_venv/bin/activate",
         # Install server requirements
         "python -m pip install -r server_requirements.txt",
-        f"python -m pip install concrete-ml=={concrete_ml_version}",
+        # We need to relax the constraint on the version for internal testing
+        f"python -m pip install concrete-ml=={concrete_ml_version}"
+        " || python -m pip install concrete-ml",
         # We still need to force concrete-python version to be exactly the same as the file
-        f"python -m pip install concrete-python=={concrete_python_version}",
+        f"python -m pip install concrete-python=={concrete_python_version} || :",
         # Launch server
         f'PORT={port} PATH_TO_MODEL="./{path_to_model.name}" python ./server.py',
     ]
 
     # + f"-o RemoteCommand=\"tmux new -A -s {instance_metadata['instance_name']}\" "
     # Join commands
     ssh_command = (
@@ -456,15 +461,15 @@
 
     Arguments:
         path_to_model (Path): path to serialized model to serve.
         port (int): port to use.
         instance_type (str): type of AWS EC2 instance to use.
         instance_name (Optional[str]): the name to use for AWS created objects
         verbose (bool): show logs or not
-        wait_bar (bool): show progress bar when waiting for ssh connexion
+        wait_bar (bool): show progress bar when waiting for ssh connection
         terminate_on_shutdown (bool): terminate instance when script is over
     """
 
     with AWSInstance(
         instance_type=instance_type,
         open_port=port,
         instance_name=instance_name,
```

## concrete/ml/deployment/deploy_to_docker.py

```diff
@@ -1,17 +1,17 @@
-"""Methods to deploy a server using docker.
+"""Methods to deploy a server using Docker.
 
 It takes as input a folder with:
     - client.zip
     - server.zip
     - processing.json
 
-It builds a docker image and spawns a docker container that runs the server.
+It builds a Docker image and spawns a Docker container that runs the server.
 
-This module is untested as it would require to first build the release docker image.
+This module is untested as it would require to first build the release Docker image.
 FIXME: https://github.com/zama-ai/concrete-ml-internal/issues/3347
 """
 
 import argparse
 import os
 import shutil
 import subprocess
@@ -19,15 +19,15 @@
 from pathlib import Path
 from tempfile import TemporaryDirectory
 
 DATE_FORMAT: str = "%Y_%m_%d_%H_%M_%S"
 
 
 def delete_image(image_name: str):
-    """Delete a docker image.
+    """Delete a Docker image.
 
     Arguments:
         image_name (str): to name of the image to delete.
     """
     to_delete = subprocess.check_output(
         f"docker ps -a --filter name={image_name} -q", shell=True
     ).decode("utf-8")
@@ -35,25 +35,25 @@
         subprocess.check_output(f"docker rmi {to_delete}", shell=True)
 
 
 def stop_container(image_name: str):
     """Kill all containers that use a given image.
 
     Arguments:
-        image_name (str): name of docker image for which to stop docker containers.
+        image_name (str): name of Docker image for which to stop Docker containers.
     """
     to_delete = subprocess.check_output(
         f"docker ps -q --filter ancestor={image_name}", shell=True
     ).decode("utf-8")
     if to_delete:
         subprocess.check_output(f"docker kill {to_delete}", shell=True)
 
 
 def build_docker_image(path_to_model: Path, image_name: str):
-    """Build server docker image.
+    """Build server Docker image.
 
     Arguments:
         path_to_model (Path): path to serialized model to serve.
         image_name (str): name to give to the image.
     """
     delete_image(image_name)
 
@@ -79,29 +79,29 @@
         subprocess.check_output(command, shell=True)
     os.chdir(cwd)
 
 
 def main(path_to_model: Path, image_name: str):
     """Deploy function.
 
-    - Builds docker image.
-    - Runs docker server.
+    - Builds Docker image.
+    - Runs Docker server.
     - Stop container and delete image.
 
     Arguments:
         path_to_model (Path): path to model to server
-        image_name (str): name of the docker image
+        image_name (str): name of the Docker image
     """
 
     build_docker_image(path_to_model, image_name)
 
     if args.only_build:
         return
 
-    # Run newly created docker server
+    # Run newly created Docker server
     try:
         with open("./url.txt", mode="w", encoding="utf-8") as file:
             file.write("http://localhost:5000")
         subprocess.check_output(f"docker run -p 5000:5000 {image_name}", shell=True)
     except KeyboardInterrupt:
         message = "Terminate container? (y/n) "
         shutdown_instance = input(message).lower()
```

## concrete/ml/deployment/fhe_client_server.py

```diff
@@ -110,15 +110,15 @@
                 "Version mismatch for packages: \n"
                 + "\n".join(f"{error[0]}: {error[1]} != {error[2]}" for error in errors)
             )
 
         if not versions["python"].startswith(
             f"{sys.version_info.major}.{sys.version_info.minor}"
         ):  # pragma: no cover
-            raise ValueError("Not the same python version between the compiler and the server.")
+            raise ValueError("Not the same Python version between the compiler and the server.")
 
         self.server = fhe.Server.load(Path(self.path_dir).joinpath("server.zip"))
 
     def run(
         self,
         serialized_encrypted_quantized_data: bytes,
         serialized_evaluation_keys: bytes,
@@ -394,32 +394,32 @@
         )
         assert isinstance(deserialized_decrypted_quantized_result, numpy.ndarray)
         return deserialized_decrypted_quantized_result
 
     def deserialize_decrypt_dequantize(
         self, serialized_encrypted_quantized_result: bytes
     ) -> numpy.ndarray:
-        """Deserialize, decrypt and dequantize the values.
+        """Deserialize, decrypt and de-quantize the values.
 
         Args:
             serialized_encrypted_quantized_result (bytes): the serialized, encrypted
                 and quantized result
 
         Returns:
-            numpy.ndarray: the decrypted (dequantized) values
+            numpy.ndarray: the decrypted (de-quantized) values
         """
         # Decrypt and deserialize the values
         deserialized_decrypted_quantized_result = self.deserialize_decrypt(
             serialized_encrypted_quantized_result
         )
 
-        # Dequantize the values
+        # De-quantize the values
         deserialized_decrypted_dequantized_result = self.model.dequantize_output(
             deserialized_decrypted_quantized_result
         )
 
-        # Apply post-processing the to dequantized values
+        # Apply post-processing the to de-quantized values
         deserialized_decrypted_dequantized_result = self.model.post_processing(
             deserialized_decrypted_dequantized_result
         )
 
         return deserialized_decrypted_dequantized_result
```

## concrete/ml/deployment/server.py

```diff
@@ -5,25 +5,23 @@
     - Add a key
     - Compute
 """
 
 import io
 import os
 import uuid
-
-# pylint: disable=import-error
-import zipfile
 from pathlib import Path
 from typing import Dict
 
 import uvicorn
 from fastapi import FastAPI, Form, HTTPException, UploadFile
 from fastapi.responses import FileResponse, StreamingResponse
 
-from ..deployment import FHEModelServer
+# No relative import here because when not used in the package itself
+from concrete.ml.deployment import FHEModelServer
 
 if __name__ == "__main__":
     app = FastAPI(debug=False)
 
     FILE_FOLDER = Path(__file__).parent
 
     KEY_PATH = Path(os.environ.get("KEY_PATH", FILE_FOLDER / Path("server_keys")))
@@ -36,30 +34,14 @@
 
     PATH_TO_CLIENT = (CLIENT_SERVER_PATH / "client.zip").resolve()
     PATH_TO_SERVER = (CLIENT_SERVER_PATH / "server.zip").resolve()
 
     assert PATH_TO_CLIENT.exists()
     assert PATH_TO_SERVER.exists()
 
-    # remove this hack once 1.x is released
-    # FIXME: https://github.com/zama-ai/concrete-ml-internal/issues/3251
-    PATH_TO_PROCESSING = CLIENT_SERVER_PATH / "serialized_processing.json"
-    if not PATH_TO_PROCESSING.exists():  # then try to extract it from client
-        with zipfile.ZipFile(PATH_TO_CLIENT) as client_zip:
-            path_in_client = zipfile.Path(client_zip) / "serialized_processing.json"
-            if not path_in_client.exists():
-                raise FileNotFoundError(f"{path_in_client} does not exists")
-            with client_zip.open("serialized_processing.json", mode="r") as file_in_json, open(
-                PATH_TO_PROCESSING, mode="wb"
-            ) as file_out:
-                file_out.write(file_in_json.read())
-    # populate client with processing too
-    with zipfile.ZipFile(PATH_TO_CLIENT, "a") as zip_file:
-        zip_file.write(filename=PATH_TO_PROCESSING, arcname="serialized_processing.json")
-
     @app.get("/get_client")
     def get_client():
         """Get client.
 
         Returns:
             FileResponse: client.zip
 
@@ -67,37 +49,14 @@
             HTTPException: if the file can't be find locally
         """
         path_to_client = (CLIENT_SERVER_PATH / "client.zip").resolve()
         if not path_to_client.exists():
             raise HTTPException(status_code=500, detail="Could not find client.")
         return FileResponse(path_to_client, media_type="application/zip")
 
-    # Needed for legacy reasons (to remove once 1.x is released)
-    # FIXME: https://github.com/zama-ai/concrete-ml-internal/issues/3225
-    @app.get("/get_processing")
-    def get_processing():
-        """Get processing.
-
-        Returns:
-            FileResponse: serialized_processing.json
-
-        Raises:
-            HTTPException: if the file can't be find locally
-        """
-        # Backward compatibility issue
-        # This root has to be removed once 1.x is released
-
-        path_to_processing = (CLIENT_SERVER_PATH / "serialized_processing.json").resolve()
-        if not path_to_processing.exists():
-            raise HTTPException(status_code=500, detail="Could not find client.")
-        return FileResponse(
-            path_to_processing,
-            media_type="application/json",
-        )
-
     @app.post("/add_key")
     async def add_key(key: UploadFile):
         """Add public key.
 
         Arguments:
             key (UploadFile): public key
```

## concrete/ml/deployment/utils.py

```diff
@@ -25,24 +25,24 @@
 
     for current_index, current_line in enumerate(current_splitted):
         if current_line not in previous_splitted:
             return "\n".join(current_splitted[current_index:])
     return ""
 
 
-def wait_for_connexion_to_be_available(
+def wait_for_connection_to_be_available(
     hostname: str,
     ip_address: str,
     path_to_private_key: Path,
     timeout: int = 1,
     wait_time: int = 1,
     max_retries: int = 20,
     wait_bar: bool = False,
 ):
-    """Wait for connexion to be available.
+    """Wait for connection to be available.
 
     Arguments:
         hostname (str): host name
         ip_address (str): ip address
         path_to_private_key (Path): path to private key
         timeout (int): ssh timeout option
         wait_time (int): time to wait between retries
@@ -52,54 +52,54 @@
     Raises:
         TimeoutError: if it wasn't able connect to ssh with the given constraints
     """
     with tqdm(disable=not wait_bar) as pbar:
         # We can't cover infinite retry without risking an infinite loop
         if max_retries < 0:  # pragma: no cover
             while True:
-                if is_connexion_available(
+                if is_connection_available(
                     hostname=hostname,
                     ip_address=ip_address,
                     timeout=timeout,
                     path_to_private_key=path_to_private_key,
                 ):
                     return
                 time.sleep(wait_time)
                 pbar.update(1)
         else:
             for _ in range(max_retries):
-                if is_connexion_available(
+                if is_connection_available(
                     hostname=hostname,
                     ip_address=ip_address,
                     timeout=timeout,
                     path_to_private_key=path_to_private_key,
                 ):
                     return
                 time.sleep(wait_time)
                 pbar.update(1)
 
     raise TimeoutError(
-        "Timeout reached while trying to check for connexion "
+        "Timeout reached while trying to check for connection "
         f"availability on {hostname}@{ip_address}"
     )
 
 
-def is_connexion_available(
+def is_connection_available(
     hostname: str, ip_address: str, path_to_private_key: Path, timeout: int = 1
 ):
-    """Check if ssh connexion is available.
+    """Check if ssh connection is available.
 
     Arguments:
         hostname (str): host name
         ip_address (str): ip address
         path_to_private_key (Path): path to private key
         timeout: ssh timeout option
 
     Returns:
-        bool: True if connexion succeeded
+        bool: True if connection succeeded
     """
 
     command = (
         f"ssh -i {path_to_private_key.resolve()} "
         + f"-q -o ConnectTimeout={timeout} -o BatchMode=yes -o "
         + f"\"StrictHostKeyChecking=no\" {hostname}@{ip_address} 'exit 0'"
     )
```

## concrete/ml/onnx/onnx_impl_utils.py

```diff
@@ -112,15 +112,15 @@
     kernel_shape: Tuple[int, ...],
     pads: Tuple[int, ...],
     strides: Tuple[int, ...],
     ceil_mode: int,
 ) -> Tuple[int, ...]:
     """Compute any additional padding needed to compute pooling layers.
 
-    The ONNX standard uses ceil_mode=1 to match tensorflow style pooling output computation.
+    The ONNX standard uses ceil_mode=1 to match TensorFlow style pooling output computation.
     In this setting, the kernel can be placed at a valid position even though it contains values
     outside of the input shape including padding. The ceil_mode parameter controls whether
     this mode is enabled. If the mode is not enabled, the output shape follows PyTorch rules.
 
     Args:
         input_shape (Tuple[int, ...]): shape of the input to be padded as N x C x H x W
         kernel_shape (Tuple[int, ...]): shape of the conv or pool kernel, as Kh x Kw (or n-d)
@@ -133,15 +133,15 @@
 
     Returns:
         res (Tuple[int, ...]): shape of the output of a conv or pool operator with given parameters
     """
 
     pads2 = list(pads)
     if ceil_mode == 1:
-        # We will pad the input with additional rows to respect tensorflow style
+        # We will pad the input with additional rows to respect TensorFlow style
         # padding (ceil_mode == 1)
 
         # Compute the dimensions for floor/ceil output computation modes
         dims_floor = compute_conv_output_dims(input_shape, kernel_shape, pads, strides, 0)
         dims_ceil = compute_conv_output_dims(input_shape, kernel_shape, pads, strides, 1)
 
         # Compute the amount of additional padding necessary
@@ -174,23 +174,23 @@
         ceil_mode (int): set to 1 to use the `ceil` function to compute the output shape, as
             described in the PyTorch doc
 
     Returns:
         res (float): tensor or scalar, corresponding to normalization factors to apply for the
             average pool computation for each valid kernel position
     """
-    # Handle the tensorflow pooling mode
+    # Handle the TensorFlow pooling mode
     if ceil_mode == 1:
         n_in_channels = input_shape[1]
         kernel = numpy.ones(
             (n_in_channels, 1, kernel_shape[0], kernel_shape[1]),
             dtype=numpy.int64,
         )
 
-        # Tensorflow (and ONNX pool with ceil_mode==1) allow the kernel of the pooling op
+        # TensorFlow (and ONNX pool with ceil_mode==1) allow the kernel of the pooling op
         # to be placed in positions that include out-of-bounds indices.
         # For example an input of size 2 containing values V,
         # with padding P of 1 to the left and right:
         #      P V V P
         # The pooling of size 2 can be applied at positions: 0,1,2,3:
         #    (P+V)/2   (V+V)/2  (V+P)/2   P
         # Even though at position 3 it is out of bounds.
@@ -199,15 +199,15 @@
         # and the averaging is done counting only the valid values (P or V) in its support
 
         # We thus need to find the number of valid indices for each kernel position
 
         # Compute the padded input tensor in Floor mode (PyTorch)
         pool_pads_floor = compute_onnx_pool_padding(input_shape, kernel_shape, pads, strides, 0)
 
-        # Compute it again in tensorflow mode
+        # Compute it again in TensorFlow mode
         pool_pads_ceil = compute_onnx_pool_padding(input_shape, kernel_shape, pads, strides, 1)
 
         # Create a tensor of ones for PyTorch mode and one of zeros for TF mode
         padded_flr = numpy_onnx_pad(numpy.ones(input_shape, dtype=numpy.int64), pool_pads_floor, 1)
         padded_ceil = numpy_onnx_pad(numpy.zeros(input_shape, dtype=numpy.int64), pool_pads_ceil, 0)
 
         # Initialize a final tensor that has 1s in valid indices and 0s in invalid ones
```

## concrete/ml/onnx/onnx_model_manipulations.py

```diff
@@ -25,15 +25,15 @@
         onnx_model (onnx.ModelProto): the model for which we want to remove unused Constant nodes.
     """
 
     constants_to_remove = {}
 
     for node in onnx_model.graph.node:
         if node.op_type == "Constant":
-            # Initially we don't know if a constant is used, so it's to be removed by default
+            # Initially we don't know if a constant is used, so it is to be removed by default
             constants_to_remove[node.output[0]] = node
             continue
 
         for input_ in node.input:
             if input_ in constants_to_remove:
                 # If we find a constant that is used, then it is not a constant to remove anymore
                 constants_to_remove.pop(input_)
```

## concrete/ml/onnx/ops_impl.py

```diff
@@ -1,8 +1,8 @@
-"""ONNX ops implementation in python + numpy."""
+"""ONNX ops implementation in Python + NumPy."""
 
 # pylint: disable=too-many-lines
 from inspect import signature
 from typing import Iterable, List, Optional, Sequence, Set, Tuple, Union
 
 import numpy
 import onnx
@@ -125,26 +125,26 @@
     c: numpy.ndarray,
     t: numpy.ndarray,
     f: Union[numpy.ndarray, int],
 ) -> numpy.ndarray:
     """Compute the equivalent of numpy.where.
 
     This function is not mapped to any ONNX operator (as opposed to numpy_where). It is usable by
-    functions which are mapped to ONNX operators, e.g. numpy_div or numpy_where.
+    functions which are mapped to ONNX operators, eg numpy_div or numpy_where.
 
     Args:
         c (numpy.ndarray): Condition operand.
         t (numpy.ndarray): True operand.
         f (numpy.ndarray): False operand.
 
     Returns:
         numpy.ndarray: numpy.where(c, t, f)
 
     """
-    # Use numpy.where (it is currently supported by CN) once we investigate why it outputs a
+    # Use numpy.where (it is currently supported by Concrete) once we investigate why it outputs a
     # a different dtype then the following workaround
     # FIXME: https://github.com/zama-ai/concrete-ml-internal/issues/2738
     return c * t + (1.0 - c) * f
 
 
 def numpy_where(
     c: numpy.ndarray,
@@ -179,15 +179,15 @@
 
     Returns:
         Tuple[numpy.ndarray]: Result, has same element type as two inputs
     """
     return (a + b,)
 
 
-# input, min and max are python built-in but we need to match the ONNX naming, ignore the lint
+# input, min and max are Python built-in but we need to match the ONNX naming, ignore the lint
 # pylint: disable=redefined-builtin
 @onnx_func_raw_args("min", "max")
 def numpy_clip(a: numpy.ndarray, min=None, max=None) -> Tuple[numpy.ndarray]:
     """Compute clip in numpy according to ONNX spec.
 
     See https://github.com/onnx/onnx/blob/main/docs/Changelog.md#Clip-13
 
@@ -264,19 +264,19 @@
             computation is done as if C is a scalar 0. The shape of C should be unidirectional
             broadcastable to (M, N).
             Defaults to None.
         alpha (float): Scalar multiplier for the product of input tensors A * B.
             Defaults to 1.
         beta (float): Scalar multiplier for input tensor C.
             Defaults to 1.
-        transA (int): Whether A should be transposed. The type is kept as int as it's the
-            type used by ONNX and it can easily be interpreted by python as a boolean.
+        transA (int): Whether A should be transposed. The type is kept as int as it is the
+            type used by ONNX and it can easily be interpreted by Python as a boolean.
             Defaults to 0.
-        transB (int): Whether B should be transposed. The type is kept as int as it's the
-            type used by ONNX and it can easily be interpreted by python as a boolean.
+        transB (int): Whether B should be transposed. The type is kept as int as it is the
+            type used by ONNX and it can easily be interpreted by Python as a boolean.
             Defaults to 0.
 
     Returns:
         Tuple[numpy.ndarray]: The tuple containing the result tensor
     """
     # If alpha and beta are integer, apply the int type for Concrete to see they are integers
     processed_alpha = int(alpha) if round(alpha) == alpha else alpha
@@ -286,15 +286,15 @@
     b_prime = numpy.transpose(b) if transB else b
     c_prime: Union[numpy.ndarray, float] = c if c is not None else 0
 
     # Do
     #
     #       y = processed_alpha * numpy.matmul(a_prime, b_prime) + processed_beta * c_prime
     #
-    # in an efficient way, i.e. to make tracing directly optimized, without expecting any opt from
+    # in an efficient way, i.e., to make tracing directly optimized, without expecting any opt from
     # the compiler here
 
     y = numpy.matmul(a_prime, b_prime)
 
     if processed_alpha != 1:
         y = y * processed_alpha
 
@@ -1627,15 +1627,15 @@
         x (numpy.ndarray): Tensor to be quantized
         scale (float): Quantizer scale
         zero_point (float): Quantizer zero-point
         bit_width (int): Number of bits of the integer representation
         rounding_mode (str): Rounding mode (default and only accepted option is "ROUND")
         signed (int): Whether this op quantizes to signed integers (default 1),
         narrow (int): Whether this op quantizes to a narrow range of integers
-            e.g. [-2**n_bits-1 .. 2**n_bits-1] (default 0),
+            eg [-2**n_bits-1 .. 2**n_bits-1] (default 0),
 
     Returns:
         result (numpy.ndarray): Tensor with float quantized values
     """
 
     assert_true(rounding_mode == "ROUND", "Only rounding quantization is supported for Brevitas")
     assert_true(signed in (1, 0), "Signed flag in Brevitas quantizer must be 0/1")
```

## concrete/ml/pytest/torch_models.py

```diff
@@ -621,18 +621,18 @@
         """
         x = self.act(self.avg_pool1(self.conv1(x)))
         x = self.act(self.conv2(x))
         return x
 
 
 class TinyQATCNN(nn.Module):
-    """A very small QAT CNN to classify the sklearn digits dataset.
+    """A very small QAT CNN to classify the sklearn digits data-set.
 
     This class also allows pruning to a maximum of 10 active neurons, which
-    should help keep the accumulator bit width low.
+    should help keep the accumulator bit-width low.
     """
 
     def __init__(self, n_classes, n_bits, n_active, signed, narrow) -> None:
         """Construct the CNN with a configurable number of classes.
 
         Args:
             n_classes (int): number of outputs of the neural net
@@ -673,15 +673,15 @@
         """Enable or remove pruning.
 
         Args:
             enable: if we enable the pruning or not
 
         """
 
-        # Maximum number of active neurons (i.e. corresponding weight != 0)
+        # Maximum number of active neurons (i.e., corresponding weight != 0)
 
         # Go through all the convolution layers
         for layer in (self.conv1, self.conv2, self.conv3):
             s = layer.weight.shape
 
             # Compute fan-in (number of inputs to a neuron)
             # and fan-out (number of neurons in the layer)
@@ -886,15 +886,15 @@
 
         x = step(x, 0.5) * 2.0
         x = self.act(x)
         return x
 
 
 class SingleMixNet(nn.Module):
-    """Torch model that with a single conv layer that produces the output, e.g. a blur filter."""
+    """Torch model that with a single conv layer that produces the output, eg a blur filter."""
 
     mixing_layer: Union[nn.Module, nn.Sequential]
 
     def __init__(self, use_conv, use_qat, inp_size, n_bits):
         super().__init__()
 
         if use_conv:
@@ -1130,15 +1130,15 @@
 
 class PaddingNet(nn.Module):
     """Torch QAT model that applies various padding patterns."""
 
     def __init__(self):
         super().__init__()
 
-        # Use a QAT network to allow the torch result to be the same as the CML result
+        # Use a QAT network to allow the torch result to be the same as the Concrete ML result
         self.input_quant = qnn.QuantIdentity(bit_width=8)
 
     def forward(self, x):
         """Forward pass.
 
         Args:
             x (torch.tensor): The input of the model
```

## concrete/ml/pytest/utils.py

```diff
@@ -48,15 +48,15 @@
     DecisionTreeRegressor,
     RandomForestRegressor,
     partial(
         NeuralNetRegressor,
         module__n_layers=3,
         module__n_w_bits=2,
         module__n_a_bits=2,
-        module__n_accum_bits=7,  # Let's stay with 7 bits for test exec time
+        module__n_accum_bits=7,  # Stay with 7 bits for test exec time
         module__n_hidden_neurons_multiplier=1,
         module__activation_function=nn.ReLU,
         max_epochs=10,
         verbose=0,
     ),
 ]
 
@@ -71,15 +71,15 @@
         module__n_layers=3,
         module__activation_function=nn.ReLU,
         max_epochs=10,
         verbose=0,
     ),
 ]
 
-# Get the datasets. The data generation is seeded in load_data.
+# Get the data-sets. The data generation is seeded in load_data.
 _classifiers_and_datasets = [
     pytest.param(
         model,
         {
             "n_samples": 1000,
             "n_features": 10,
             "n_classes": n_classes,
@@ -88,15 +88,15 @@
         },
         id=get_model_name(model),
     )
     for model in _classifier_models
     for n_classes in [2, 4]
 ]
 
-# Get the datasets. The data generation is seeded in load_data.
+# Get the data-sets. The data generation is seeded in load_data.
 # Only LinearRegression supports multi targets
 # GammaRegressor, PoissonRegressor and TweedieRegressor only handle positive target values
 _regressors_and_datasets = [
     pytest.param(
         model,
         {
             "n_samples": 200,
@@ -193,15 +193,15 @@
     Args:
         param (Dict): Set of hyper-parameters to use based on the selected torchvision data-set.
             It must contain: data-set transformations (torchvision.transforms.Compose), and the
             data-set_size (Optional[int]).
         train_set (bool): Use train data-set if True, else testing data-set
 
     Returns:
-        A torchvision datasets.
+        A torchvision data-sets.
     """
 
     transform = param["train_transform"] if train_set else param["test_transform"]
     dataset = param["dataset"](download=True, root="./data", train=train_set, transform=transform)
 
     if param.get("dataset_size", None):
         dataset = torch.utils.data.random_split(
@@ -209,15 +209,15 @@
             [param["dataset_size"], len(dataset) - param["dataset_size"]],
         )[0]
 
     return dataset
 
 
 def data_calibration_processing(data, n_sample: int, targets=None):
-    """Reduce size of the given dataset.
+    """Reduce size of the given data-set.
 
     Args:
         data: The input container to consider
         n_sample (int): Number of samples to keep if the given data-set
         targets: If `dataset` is a `torch.utils.data.Dataset`, it typically contains both the data
             and the corresponding targets. In this case, `targets` must be set to `None`.
             If `data` is instance of `torch.Tensor` or 'numpy.ndarray`, `targets` is expected.
@@ -250,15 +250,15 @@
         and isinstance(targets, (List, numpy.ndarray, torch.Tensor))  # type: ignore[arg-type]
         and isinstance(data, (numpy.ndarray, torch.Tensor))
     ):
         x = numpy.array(data)
         y = numpy.array(targets)
     else:
         raise TypeError(
-            "Only numpy arrays, torch tensors and torchvision datasets are supported. "
+            "Only numpy arrays, torch tensors and torchvision data-sets are supported. "
             f"Got `{type(data)}` as input type and `{type(targets)}` as target type"
         )
 
     x = x[random_sample]
     y = y[random_sample]
 
     return x, y
@@ -269,21 +269,21 @@
     state_dict_or_path: Optional[Union[str, Path, Dict[str, Any]]],
     params: Dict,
     device: str = "cpu",
 ) -> torch.nn.Module:
     """Load an object saved with torch.save() from a file or dict.
 
     Args:
-        model_class (torch.nn.Module): A Pytorch or Brevitas network.
+        model_class (torch.nn.Module): A PyTorch or Brevitas network.
         state_dict_or_path (Optional[Union[str, Path, Dict[str, Any]]]): Path or state_dict
         params (Dict): Model's parameters
         device (str):  Device type.
 
     Returns:
-        torch.nn.Module: A Pytorch or Brevitas network.
+        torch.nn.Module: A PyTorch or Brevitas network.
     """
     model = model_class(**params)
 
     if state_dict_or_path is not None:
         if isinstance(state_dict_or_path, (str, Path)):
             state_dict = torch.load(state_dict_or_path, map_location=device)
         else:
```

## concrete/ml/quantization/base_quantized_op.py

```diff
@@ -22,30 +22,31 @@
 ONNXOpInputOutputType = Union[numpy.ndarray, QuantizedArray, None]
 
 ALL_QUANTIZED_OPS: Set[Type] = set()
 
 ONNX_OPS_TO_QUANTIZED_IMPL: Dict[str, Type["QuantizedOp"]] = {}
 
 # This constant determines the number of bits for the quantization of input and output values
-# of an ML model. This is not necessarily the maximum bitwidth in the network, as Gemm/Conv ops
-# have output bitwidth that is related to their weights and inputs.
+# of an ML model. This is not necessarily the maximum bit-width in the network, as Gemm/Conv ops
+# have output bit-width that is related to their weights and inputs.
 # Run time in FHE is strongly impacted by the number of bits, with increases of 5-20x for
 # each additional bit. However, strong quantization of inputs and outputs can negatively impact
 # accuracy. This value is chosen as a compromise between run time and model accuracy. This default
-# value is used only if the user does not specifically specify a value for input or output bitwidth.
+# value is used only if the user does not specifically specify a value for input or output
+# bit-width.
 DEFAULT_MODEL_BITS = 5
 
 
 class QuantizedOp:
     """Base class for quantized ONNX ops implemented in numpy.
 
     Args:
         n_bits_output (int): The number of bits to use for the quantization of the output
         op_instance_name (str): The name that should be assigned to this operation, used
-            to retrieve it later or get debugging information about this op (bitwidth, value range,
+            to retrieve it later or get debugging information about this op (bit-width, value range,
             integer intermediary values, op-specific error messages). Usually this name is the same
             as the ONNX operation name for which this operation is constructed.
         int_input_names (Set[str]): The set of names of integer tensors that are inputs to this op
         constant_inputs (Optional[Union[Dict[str, Any], Dict[int, Any]]]): The constant tensors
             that are inputs to this op
         input_quant_opts (QuantizationOptions): Input quantizer options, determine the quantization
             that is applied to input tensors (that are not constants)
@@ -76,19 +77,19 @@
     _params_that_are_onnx_inputs: Set[str] = set()
     _params_that_are_onnx_var_inputs: Set[str] = set()
     _params_that_are_required_onnx_inputs: Set[str] = set()
     _has_attr: bool
     _inputs_not_quantized: Set[str] = set()
     quantize_inputs_with_model_outputs_precision: bool = False
 
-    # The ONNX name of this op instance (e.g. "Conv_9", "MatMul_5" etc.)
+    # The ONNX name of this op instance (eg "Conv_9", "MatMul_5" etc.)
     op_instance_name: str
 
-    # Determines if this op computes a tensor that is a graph output, i.e. a tensor
-    # that will be decrypted and dequantized in the clear
+    # Determines if this op computes a tensor that is a graph output, i.e., a tensor
+    # that will be decrypted and de-quantized in the clear
     produces_graph_output = False
 
     # Determines if the op produces a raw output (not quantized). This can
     # be a float or integer tensor that contains non-encrypted values
     produces_raw_output = False
 
     error_tracker: Optional[List[int]] = None
@@ -122,15 +123,15 @@
             self.input_quant_opts = QuantizationOptions(self.n_bits, is_signed=True)
 
         self.output_quant_params = None
         self.output_quant_stats = None
 
         # By default, such as for operators that only have a float implementation,
         # we assume a single integer input tensor. Since we can't use {"0"} as a default value in
-        # python, we use None and we initialize the set to {"0"}. When constructing the ops
+        # Python, we use None and we initialize the set to {"0"}. When constructing the ops
         # through ONNX -> NumpyModule conversion, a value should always be provided
         # for int_input_names. This default is only for instantiating ops manually, which is not
         # recommended usage. We use "0" since this is a common ONNX tensor name for inputs.
         self._int_input_names = {"0"} if int_input_names is None else int_input_names
 
         constant_inputs_per_name: Dict[str, Any] = {}
 
@@ -176,15 +177,15 @@
 
         self.attrs = dict(self._default_attrs, **deepcopy(attrs))
 
         # Only use QAT for layers that need it (that mix encrypted values: conv, dense, add, etc...)
         if self.can_fuse():
             self.input_quant_opts.is_qat = False
 
-        # Set the operation's name, which is used to identify this op in the CML op graph
+        # Set the operation's name, which is used to identify this op in the Concrete ML op graph
         # with respect to the ONNX graph (usually we keep use ONNX op name)
         self.op_instance_name = op_instance_name
 
     @classmethod
     def op_type(cls):
         """Get the type of this operation.
 
@@ -436,15 +437,15 @@
         *inputs: ONNXOpInputOutputType,
         calibrate: bool,
         quantize_actual_values: bool,
     ):
         """Retrieve all the inputs of an operator in the computational graph.
 
         This helper method will prepare a list of inputs to an operator. Inputs can be variables,
-        i.e. encrypted tensors, or constants (in the clear). Inputs to an operator are set-up in
+        i.e., encrypted tensors, or constants (in the clear). Inputs to an operator are set-up in
         the slots of a list, as the order of inputs is important.
 
         Usually the input list is populated with QuantizedArrays. Operators that require the
         original float (operators that only produce or contribute to TLUs) values will just read
         out the .values of  these quantized arrays. Operators that do matrix multiplication will
         read out the quantized integer values of the arrays.  The method can be called during
         calibration, in which case the variable inputs are just float numpy tensors.
@@ -492,15 +493,15 @@
         # QuantizedArrays, else we return the float32 values directly.
 
         curr_input_fill_idx = 0
         for input_idx, input_ in enumerate(inputs):
             while prepared_inputs[curr_input_fill_idx] is not None:
                 curr_input_fill_idx += 1
 
-            # This is an integer scalar (e.g. tensor shape). This is not an encrypted
+            # This is an integer scalar (eg tensor shape). This is not an encrypted
             # value, it is not traced
             is_clear_value = isinstance(input_, RawOpOutput)
 
             if input_ is None:
                 # Do nothing if the input is not set, the underlying ops will handle the None
                 pass
             elif calibrate or is_clear_value:
@@ -509,15 +510,15 @@
                 prepared_inputs[curr_input_fill_idx] = input_
             elif quantize_actual_values:
                 # This is used by mixing (conv/gemm) or value re-arranging ops (reshape)
                 input_ = cast(QuantizedArray, input_)
                 new_input = self._prepare_quantized_input(input_)
 
                 # Check that the input quantizer is correct - that it can de-quantize
-                # values correctly. If it's not, it is added to the list of invalid tensors
+                # values correctly. If it is not, it is added to the list of invalid tensors
                 # for which an error is raised
                 if (
                     new_input.quantizer.is_qat
                     and not input_.quantizer.is_precomputed_qat
                     and self.error_tracker is not None
                     and not new_input.quantizer.check_is_uniform_quantized(
                         new_input.quantizer.quant_options
@@ -663,15 +664,15 @@
         return output_quant_opts
 
 
 class QuantizedOpUnivariateOfEncrypted(QuantizedOp, is_utility=True):
     """An univariate operator of an encrypted value.
 
     This operation is not really operating as a quantized operation. It is useful when the
-    computations get fused into a TLU, as in e.g. Act(x) = x || (x + 42)).
+    computations get fused into a TLU, as in eg Act(x) = x || (x + 42)).
     """
 
     def __init__(
         self,
         n_bits_output: int,
         op_instance_name: str,
         int_input_names: Optional[Set[str]] = None,
```

## concrete/ml/quantization/post_training.py

```diff
@@ -263,15 +263,15 @@
             {"calibrate_rounding": True} if isinstance(quantized_op, QuantizedMixingOp) else {}
         )
 
         # Add calibrate_attr to list of attr for q_impl method
         q_impl_attr = quantized_op.attrs.copy()
         q_impl_attr.update(calibrate_attr)
 
-        # Dequantize to have the value in clear and ready for next calibration
+        # De-quantize to have the value in clear and ready for next calibration
         quant_result = quantized_op.q_impl(*q_calibration_data, **q_impl_attr)
         if quantized_op.produces_graph_output:
 
             assert isinstance(quant_result, QuantizedArray), (
                 "The PyTorch module can not return a raw value, "
                 "such as a clear constant or the shape of a tensor."
             )
@@ -477,26 +477,26 @@
 
                 assert_true(
                     op_type in ONNX_OPS_TO_QUANTIZED_IMPL,
                     f"{op_type} can't be found in {ONNX_OPS_TO_QUANTIZED_IMPL}",
                 )
 
                 # Note that the output of a quantized op could be a network output
-                # Thus the quantized op outputs are quantized to the network output bitwidth
+                # Thus the quantized op outputs are quantized to the network output bit-width
                 quantized_op_instance = quantized_op_class(
                     self.n_bits_model_outputs,
                     node.name,
                     node_integer_inputs,
                     curr_cst_inputs,
                     self._get_input_quant_opts(curr_calibration_data, quantized_op_class),
                     **attributes,
                 )
 
-                # Determine if this op computes a tensor that is a graph output, i.e. a tensor
-                # that will be decrypted and dequantized in the clear
+                # Determine if this op computes a tensor that is a graph output, i.e., a tensor
+                # that will be decrypted and de-quantized in the clear
                 quantized_op_instance.produces_graph_output = output_name in graph_output_names
 
                 # Store the output tensor's integer producers
                 tensor_int_producers[output_name] = set()
                 if not quantized_op_instance.can_fuse():
                     # This tensor is produced by a non fusable op
                     # Thus this tensor is marked as produced by itself
@@ -610,15 +610,16 @@
 
         Args:
             quantized_module (QuantizedModule): the quantized module containing the ops of the model
             calibration_data: calibration data for each input tensor
         """
 
         # Create several lists:
-        # - a list of layers that use each input directly (i.e. have this input as an integer input)
+        # - a list of layers that use each input directly
+        #   (i.e., have this input as an integer input)
         # - a list of quantizers that are applied to each input node
         # - a list of inputs that have TLUs, for which these TLUs cannot be removed
         layer_using_input: Dict[int, List[QuantizedOp]] = {}
         quantizers_for_input: Dict[int, List[QuantizedBrevitasQuant]] = {}
         inputs_not_optimizable: List[int] = []
 
         # Determine, for each input, whether it is used by a single non-fusable layer
@@ -824,15 +825,15 @@
     def _check_distribution_is_symmetric_around_zero(values: numpy.ndarray) -> bool:
         """Check if the distribution of the values is somewhat symmetric around 0.
 
         Neural network weights are usually symmetric, while regression coefficients
         are usually non-symmetric
 
         Symmetric quantization will have a zero zero-point, which avoids the computation
-        of a term in the quantized Gemm, leading to lower overall circuit bitwidth
+        of a term in the quantized Gemm, leading to lower overall circuit bit-width
         and faster speed. However, symmetric quantization can lose precision if the distribution
         of the original values is not symmetric
 
         Args:
             values (numpy.ndarray): a sample from the distribution to check
 
         Returns:
```

## concrete/ml/quantization/quantized_module.py

```diff
@@ -57,15 +57,15 @@
         q_inputs (Union[numpy.ndarray, Tuple[numpy.ndarray, ...]]): The quantized inputs.
 
     Returns:
         Generator: The input set generator with proper dimensions.
     """
     q_inputs = to_tuple(q_inputs)
 
-    assert len(q_inputs) > 0, "Inputset cannot be empty"
+    assert len(q_inputs) > 0, "The input-set cannot be empty"
 
     if len(q_inputs) > 1:
         return (
             tuple(numpy.expand_dims(q_input[idx], 0) for q_input in q_inputs)
             for idx in range(q_inputs[0].shape[0])
         )
 
@@ -158,21 +158,21 @@
         Args:
             post_processing_params (dict): the post-processing parameters
         """
         self._post_processing_params = post_processing_params
 
     # pylint: disable-next=no-self-use
     def post_processing(self, values: numpy.ndarray) -> numpy.ndarray:
-        """Apply post-processing to the dequantized values.
+        """Apply post-processing to the de-quantized values.
 
         For quantized modules, there is no post-processing step but the method is kept to make the
         API consistent for the client-server API.
 
         Args:
-            values (numpy.ndarray): The dequantized values to post-process.
+            values (numpy.ndarray): The de-quantized values to post-process.
 
         Returns:
             numpy.ndarray: The post-processed values.
         """
         return values
 
     def _set_output_quantizers(self) -> List[UniformQuantizer]:
@@ -236,15 +236,15 @@
         This method executes the forward pass in the clear, with simulation or in FHE. Input values
         are expected to be floating points, as the method handles the quantization step. The
         returned values are floating points as well.
 
         Args:
             *x (numpy.ndarray): Input float values to consider.
             fhe (Union[FheMode, str]): The mode to use for prediction. Can be FheMode.DISABLE for
-                Concrete ML python inference, FheMode.SIMULATE for FHE simulation and
+                Concrete ML Python inference, FheMode.SIMULATE for FHE simulation and
                 FheMode.EXECUTE for actual FHE execution. Can also be the string representation of
                 any of these values. Default to FheMode.DISABLE.
             debug (bool): In debug mode, returns quantized intermediary values of the computation.
                 This is useful when a model's intermediary values in Concrete ML need to be
                 compared with the intermediary values obtained in pytorch/onnx. When set, the
                 second return value is a dictionary containing ONNX operation names as keys and,
                 as values, their input QuantizedArray or ndarray. The use can thus extract the
@@ -282,28 +282,28 @@
             result = self.quantized_forward(*q_x, fhe="disable")
             for (_, layer) in self.quant_layers_dict.values():
                 layer.debug_value_tracker = None
             return result, debug_value_tracker
 
         q_y_pred = self.quantized_forward(*q_x, fhe=fhe)
 
-        # Dequantize the output predicted values
+        # De-quantize the output predicted values
         y_pred = self.dequantize_output(q_y_pred)
 
         return y_pred
 
     def quantized_forward(
         self, *q_x: numpy.ndarray, fhe: Union[FheMode, str] = FheMode.DISABLE
     ) -> numpy.ndarray:
         """Forward function for the FHE circuit.
 
         Args:
             *q_x (numpy.ndarray): Input integer values to consider.
             fhe (Union[FheMode, str]): The mode to use for prediction. Can be FheMode.DISABLE for
-                Concrete ML python inference, FheMode.SIMULATE for FHE simulation and
+                Concrete ML Python inference, FheMode.SIMULATE for FHE simulation and
                 FheMode.EXECUTE for actual FHE execution. Can also be the string representation of
                 any of these values. Default to FheMode.DISABLE.
 
         Returns:
             (numpy.ndarray): Predictions of the quantized model, with integer values.
 
         """
@@ -421,15 +421,15 @@
             "Rounding is not currently optimized for execution in FHE. "
             "Only simulation is allowed with a rounding operator.",
         )
 
         results_cnp_circuit_list = []
         for i in range(q_x[0].shape[0]):
 
-            # Extract the i th example from every element in the tuple q_x
+            # Extract example i from every element in the tuple q_x
             q_input = tuple(q_x[input][[i]] for input in range(len(q_x)))
 
             # For mypy
             assert self.fhe_circuit is not None
 
             # Execute the forward pass in FHE or with simulation
             q_result = (
@@ -464,21 +464,21 @@
         q_x = tuple(self.input_quantizers[idx].quant(x[idx]) for idx in range(len(x)))
 
         assert numpy.array(q_x).dtype == numpy.int64, "Inputs were not quantized to int64 x"
 
         return q_x[0] if len(q_x) == 1 else q_x
 
     def dequantize_output(self, q_y_preds: numpy.ndarray) -> numpy.ndarray:
-        """Take the last layer q_out and use its dequant function.
+        """Take the last layer q_out and use its de-quant function.
 
         Args:
             q_y_preds (numpy.ndarray): Quantized output values of the last layer.
 
         Returns:
-            numpy.ndarray: Dequantized output values of the last layer.
+            numpy.ndarray: De-quantized output values of the last layer.
         """
         y_preds = tuple(
             output_quantizer.dequant(q_y_preds) for output_quantizer in self.output_quantizers
         )
 
         assert_true(len(y_preds) == 1)
 
@@ -559,15 +559,15 @@
             forward_proxy,
             {arg_name: "encrypted" for arg_name in orig_args_to_proxy_func_args.values()},
         )
 
         # Quantize the inputs
         q_inputs = self.quantize_input(*inputs)
 
-        # Generate the inputset with proper dimensions
+        # Generate the input-set with proper dimensions
         inputset = _get_inputset_generator(q_inputs)
 
         # Don't let the user shoot in her foot, by having p_error or global_p_error set in both
         # configuration and in direct arguments
         check_there_is_no_p_error_options_in_configuration(configuration)
 
         # Find the right way to set parameters for compiler, depending on the way we want to default
@@ -586,21 +586,21 @@
         self._is_compiled = True
 
         return self.fhe_circuit
 
     def bitwidth_and_range_report(
         self,
     ) -> Optional[Dict[str, Dict[str, Union[Tuple[int, ...], int]]]]:
-        """Report the ranges and bitwidths for layers that mix encrypted integer values.
+        """Report the ranges and bit-widths for layers that mix encrypted integer values.
 
         Returns:
             op_names_to_report (Dict): a dictionary with operation names as keys. For each
-                operation, (e.g. conv/gemm/add/avgpool ops), a range and a bitwidth are returned.
+                operation, (e.g., conv/gemm/add/avgpool ops), a range and a bit-width are returned.
                 The range contains the min/max values encountered when computing the operation and
-                the bitwidth gives the number of bits needed to represent this range.
+                the bit-width gives the number of bits needed to represent this range.
         """
 
         if self.fhe_circuit is None:
             return None
 
         op_names_to_report: Dict[str, Dict[str, Union[Tuple[int, ...], int]]] = {}
         for (_, op_inst) in self.quant_layers_dict.values():
```

## concrete/ml/quantization/quantized_ops.py

```diff
@@ -1,12 +1,12 @@
 """Quantized versions of the ONNX operators for post training quantization."""
 
 # pylint: disable=too-many-lines
 
-# This file is too long and should be splitted
+# This file is too long and should be split
 # FIXME: https://github.com/zama-ai/concrete-ml-internal/issues/1018
 
 from typing import Any, Dict, Optional, Sequence, Set, Union
 
 import numpy
 from concrete.fhe import conv as cnp_conv
 from concrete.fhe import maxpool as cnp_maxpool
@@ -183,15 +183,15 @@
 
         q_input: QuantizedArray = prepared_inputs[0]
         q_weights: QuantizedArray = prepared_inputs[1]
         q_bias: Optional[numpy.ndarray] = (
             None if len(prepared_inputs) == 2 or beta == 0 else prepared_inputs[2]
         )
 
-        # Using snake case here to please the python format, the original attrs don't have the '_'
+        # Using snake case here to please the Python format, the original attrs don't have the '_'
         # Use default false so we also support MatMul impl, MatMul does not have these flags
         transpose_inputs = attrs.get("transA", False)
         transpose_w = attrs.get("transB", False)
 
         with tag(self.op_instance_name + ".input"):
             input_q_values = (
                 numpy.transpose(q_input.qvalues) if transpose_inputs else q_input.qvalues
@@ -220,15 +220,15 @@
 
         # Core matmul operation in full integers with a shape change (INTEGERS)
         with tag(self.op_instance_name + ".matmul"):
             matmul = input_q_values @ weights_q_values
 
         # If the weights have symmetric quantization, their zero point will be 0
         # The following check avoids the computation of the sum of the inputs, which may have
-        # large bitwidth, in the case where it would be multiplied by zero
+        # large bit-width, in the case where it would be multiplied by zero
         if q_weights.quantizer.zero_point != 0:
             # Sum operation in full integers resulting in large integers (INTEGERS)
             with tag(self.op_instance_name + ".matmul_inputsum"):
                 sum_input = -q_weights.quantizer.zero_point * numpy.sum(
                     input_q_values, axis=1, keepdims=True
                 )
 
@@ -246,32 +246,32 @@
         sum_weights = q_input.quantizer.zero_point * numpy.sum(
             weights_q_values, axis=0, keepdims=True
         )
 
         final_term = p * q_input.quantizer.zero_point * q_weights.quantizer.zero_point
 
         # Note that here we do not rescale to the output_scale and we do not add a zero-point
-        # Any following Gemm/MatMul/Conv layers will do the rescaling (during requantization)
+        # Any following Gemm/MatMul/Conv layers will do the rescaling (during re-quantization)
         # by calling _prepare_inputs_with_constants(...quantize_real_values=True)
         m_matmul = q_input.quantizer.scale * q_weights.quantizer.scale
 
         # If this operation's result are network outputs, return
         # directly the integer values and a appropriate quantization parameters that
-        # allow direct in-the-clear dequantization, including the bias
+        # allow direct in-the-clear de-quantization, including the bias
         if self.produces_graph_output:
             out_zp: Union[int, numpy.ndarray] = sum_weights - final_term
             if q_bias is not None:
                 # Make mypy happy
                 assert q_bias is not None
                 # Reshape the biases to broadcast them to each neuron
                 out_zp = out_zp + q_bias / (-m_matmul)
 
             # We identify terms in the above equation to determine what
             # the scale/zero-point of the in-the-clear quantizer should be
-            # to properly dequantize numpy_q_out
+            # to properly de-quantize numpy_q_out
             return self.make_output_quant_parameters(numpy_q_out, m_matmul, out_zp)
 
         with tag(self.op_instance_name + ".matmul_rounding"):
             # Apply Concrete rounding (if relevant)
             numpy_q_out = self.cnp_round(numpy_q_out, calibrate_rounding)
 
         # Quantization scales and zero points (FLOATS involved)
@@ -361,17 +361,17 @@
 
         assert q_input_0.quantizer.scale is not None
         assert q_input_0.quantizer.zero_point is not None
 
         assert q_input_1.quantizer.scale is not None
         assert q_input_1.quantizer.zero_point is not None
 
-        # Dequantize with input params and requantize with output parameters
+        # De-quantize with input params and re-quantize with output parameters
         # This will use TLUs over each element of the two inputs
-        # We do the dequantization directly, instead of q_inputs[0].dequant(),
+        # We do the de-quantization directly, instead of q_inputs[0].dequant(),
         # So that we do not lose precision in the computation
 
         rescale_q0 = numpy.rint(
             q_input_0.quantizer.scale
             / self.output_quant_params.scale
             * (q_input_0.qvalues + (-q_input_0.quantizer.zero_point))
         ).astype(numpy.int64)
@@ -391,15 +391,15 @@
         #       sum_q = rescale_q0 + self.b_sign * rescale_q1
         # when zama-ai/concrete-numpy-internal#1749 is done
         if self.b_sign == 1:
             sum_q = rescale_q0 + rescale_q1
         elif self.b_sign == -1:
             sum_q = rescale_q0 - rescale_q1
 
-        # But we would like the output to have n_bits, so we dequantize
+        # But we would like the output to have n_bits, so we de-quantize
         dequant_sum = self.output_quant_params.scale * sum_q
 
         # Return the raw float values without re-quantizing them to the new scale, as any
         # following Gemm/Add/Conv will quantize them with _prepare_inputs_with_constants(...)
         return QuantizedArray(
             self.n_bits,
             dequant_sum,
@@ -540,15 +540,15 @@
         **attrs,
     ) -> None:
         """Construct the quantized convolution operator and retrieve parameters.
 
         Args:
             n_bits_output: number of bits for the quantization of the outputs of this operator
             op_instance_name (str): The name that should be assigned to this operation, used
-                to retrieve it later or get debugging information about this op (bitwidth, value
+                to retrieve it later or get debugging information about this op (bit-width, value
                 range, integer intermediary values, op-specific error messages). Usually this name
                 is the same as the ONNX operation name for which this operation is constructed.
             int_input_names: names of integer tensors that are taken as input for this operation
             constant_inputs: the weights and activations
             input_quant_opts: options for the input quantizer
             attrs: convolution options
                 dilations (Tuple[int]): dilation of the kernel. Default to 1 on all dimensions.
@@ -681,15 +681,15 @@
             )
 
         # The total number of elements that are convolved by the application of a single kernel
         n_weights = numpy.prod(q_weights.qvalues.shape[1:])
 
         # If the weights have symmetric quantization, their zero point will be 0
         # The following check avoids the computation of the sum of the inputs, which may have
-        # large bitwidth, in the case where it would be multiplied by zero
+        # large bit-width, in the case where it would be multiplied by zero
         if q_weights.quantizer.zero_point != 0:
             # Compute the sum of the inputs (second encrypted term)
             assert_true(
                 isinstance(q_weights.quantizer.zero_point, (int, numpy.int_)),
                 f"Zero point of weights tensor in {self.op_type} "
                 f"op {self.op_instance_name} must be integer",
             )
@@ -717,23 +717,23 @@
         sum_weights = q_input.quantizer.zero_point * numpy.sum(
             q_weights.qvalues, axis=(1, 2, 3), keepdims=True
         ).transpose(1, 0, 2, 3)
 
         # Compute the forth term which is a constant
         final_term = n_weights * q_input.quantizer.zero_point * q_weights.quantizer.zero_point
 
-        # Compute the rescaling factor that dequantizes the input
+        # Compute the rescaling factor that de-quantizes the input
         # This is going to be compiled with a PBS (along with the following activation function)
-        # Note that we don't requantize the output of the conv, this will be done by
+        # Note that we don't re-quantize the output of the conv, this will be done by
         # any Gemm/Add/Conv layers that follow
         m_matmul = q_input.quantizer.scale * q_weights.quantizer.scale
 
         # If this operation's result are network outputs, return
         # directly the integer values and an appropriate quantization parameters that
-        # allow direct in-the-clear dequantization, including the bias
+        # allow direct in-the-clear de-quantization, including the bias
         if self.produces_graph_output:
             # Note that to use the bias, we need to rescale it to the output scale
             # For Eq. 7 in  https://arxiv.org/abs/1712.05877, we can write:
             # S_out(q_out - zp_out) = S_x * S_w (multisum + bias / (S_x * S_w))
             # where multisum is the dot product of quantized inputs and quantized weights
             # Then we identify terms:
             #   S_out = S_x * S_w
@@ -742,15 +742,15 @@
             out_zp: Union[int, numpy.ndarray] = sum_weights - final_term
             if q_bias is not None:
                 # Reshape the biases to broadcast them to each channel
                 out_zp = out_zp - q_bias.reshape((1, -1, 1, 1)) / m_matmul
 
             # We identify terms in the above equation to determine what
             # the scale/zero-point of the in-the-clear quantizer should be
-            # to properly dequantize numpy_q_out
+            # to properly de-quantize numpy_q_out
             return self.make_output_quant_parameters(numpy_q_out, m_matmul, out_zp)
 
         with tag(self.op_instance_name + ".conv_rounding"):
             # Apply Concrete rounding (if relevant)
             numpy_q_out = self.cnp_round(numpy_q_out, calibrate_rounding)
 
         # Now compute the whole sum (sum of the four terms)
@@ -854,15 +854,15 @@
             self.kernel_shape,
             self.pads,
             self.strides,
             self.ceil_mode,
         )
 
         # for mypy: The Quantized ops can only run on QuantizedArray that have quantization
-        # parameters (i.e. were fully constructed). This should always be the case, except
+        # parameters (i.e., were fully constructed). This should always be the case, except
         # during the UniformQuantizer initialization when the zero_point can exist as None
         assert q_input.quantizer.zero_point is not None
 
         # Compute padding with floor and apply it to the input, pad with the input zero-point
         pool_pads = compute_onnx_pool_padding(
             q_input.qvalues.shape, self.kernel_shape, self.pads, self.strides, 0
         )
@@ -870,15 +870,15 @@
         # Can only pad with scalar zero-points, but zero-points can be float in special cases
         # for output layers
         _check_op_input_zero_point(q_input.quantizer.zero_point, self.op_instance_name)
         pad_value = int(q_input.quantizer.zero_point)
         q_input_pad = numpy_onnx_pad(q_input.qvalues, pool_pads, pad_value, int_only=True)
 
         if self.ceil_mode == 1:
-            # Padding for tensorflow style
+            # Padding for TensorFlow style
 
             # Compute padding with ceil and apply it to the input, pad with zeros, the zeros
             # will be ignored in the computation
             pool_pads_ceil = compute_onnx_pool_padding(
                 q_input.qvalues.shape, self.kernel_shape, self.pads, self.strides, 1
             )
 
@@ -982,15 +982,15 @@
         # Retrieve the quantized inputs
         prepared_inputs = self._prepare_inputs_with_constants(
             *q_inputs, calibrate=False, quantize_actual_values=True
         )
         q_input: QuantizedArray = prepared_inputs[0]
 
         # for mypy: The Quantized ops can only run on QuantizedArray that have quantization
-        # parameters (i.e. were fully constructed). This should always be the case, except
+        # parameters (i.e., were fully constructed). This should always be the case, except
         # during the UniformQuantizer initialization when the zero_point can exist as None
         assert q_input.quantizer.zero_point is not None
 
         assert_true(
             self.ceil_mode == 0,
             "Only ceil_mode = 0 is supported by Concrete for now",
         )
@@ -1308,25 +1308,25 @@
         assert_true(constant_inputs is not None and len(constant_inputs) >= 1)
 
 
 class QuantizedOr(QuantizedOpUnivariateOfEncrypted, QuantizedOp):
     """Or operator ||.
 
     This operation is not really working as a quantized operation. It just works when things got
-    fused, as in e.g. Act(x) = x || (x + 42))
+    fused, as in eg Act(x) = x || (x + 42))
     """
 
     _impl_for_op_named: str = "Or"
 
 
 class QuantizedDiv(QuantizedOpUnivariateOfEncrypted, QuantizedOp):
     """Div operator /.
 
     This operation is not really working as a quantized operation. It just works when things got
-    fused, as in e.g. Act(x) = 1000 / (x + 42))
+    fused, as in eg Act(x) = 1000 / (x + 42))
     """
 
     _impl_for_op_named: str = "Div"
 
 
 class QuantizedMul(QuantizedOpUnivariateOfEncrypted, QuantizedOp):
     """Multiplication operator.
@@ -1428,15 +1428,15 @@
         **attrs,
     ) -> None:
         """Construct the quantized ReduceSum operator and retrieve parameters.
 
         Args:
             n_bits_output (int): Number of bits for the operator's quantization of outputs.
             op_instance_name (str): The name that should be assigned to this operation, used
-                to retrieve it later or get debugging information about this op (bitwidth, value
+                to retrieve it later or get debugging information about this op (bit-width, value
                 range, integer intermediary values, op-specific error messages). Usually this name
                 is the same as the ONNX operation name for which this operation is constructed.
             int_input_names (Optional[Set[str]]): Names of input integer tensors. Default to None.
             constant_inputs (Optional[Dict]): Input constant tensor.
                 axes (Optional[numpy.ndarray]): Array of integers along which to reduce.
                     The default is to reduce over all the dimensions of the input tensor if
                     'noop_with_empty_axes' is false, else act as an Identity op when
@@ -1514,30 +1514,30 @@
         # Retrieve values and axes parameters
         q_values = prepared_inputs[0].qvalues
         axes = prepared_inputs[1]
 
         # Sum all the quantized values
         q_sum = numpy.sum(q_values, axis=axes, keepdims=self.keepdims)
 
-        # Determining the number of output zero_points to use for dequantization with the total
+        # Determining the number of output zero_points to use for de-quantization with the total
         # number of elements summed all together, which is the product of all the number of elements
         # found within the given axes
         n_elem = numpy.prod([q_values.shape[axis] for axis in axes])
 
         # Determining the output scale and zero_point
         input_quantizer = prepared_inputs[0].quantizer
         scale = input_quantizer.scale
         zero_point = n_elem * input_quantizer.zero_point
 
         # If this operator is the graph's last operator, there's is no need to created additional
         # TLUs
         if self.produces_graph_output:
             return self.make_output_quant_parameters(q_sum, scale, zero_point)
 
-        # Dequantize the sum
+        # De-quantize the sum
         f_sum = scale * (q_sum - zero_point)
 
         sum_qarray = QuantizedArray(
             self.n_bits,
             f_sum,
             value_is_float=True,
             options=self._get_output_quant_opts(),
@@ -1552,15 +1552,15 @@
         *inputs: ONNXOpInputOutputType,
         calibrate: bool,
         quantize_actual_values: bool,
     ):
         """Retrieve all the inputs of an operator in the computational graph.
 
         This helper method will prepare a list of inputs to an operator. Inputs can be variables,
-        i.e. encrypted tensors, or constants (in the clear). Inputs to an operator are set-up in
+        i.e., encrypted tensors, or constants (in the clear). Inputs to an operator are set-up in
         the slots of a list, as the order of inputs is important.
 
         Usually the input list is populated with QuantizedArrays. Operators that require the
         original float (operators that only produce or contribute to TLUs) values will just read
         out the .values of  these quantized arrays. Operators that do matrix multiplication will
         read out the quantized integer values of the arrays.  The method can be called during
         calibration, in which case the variable inputs are just float numpy tensors.
@@ -1601,17 +1601,17 @@
             "ReduceSum axis parameter should either be a Numpy array or None. "
             f"Got {type(prepared_inputs[1])}",
         )
 
         # Retrieve the axis parameter
         axes = prepared_inputs[1]
 
-        # As the calibration inputset and inputs are ran over several samples, we need to apply the
+        # As the calibration input-set and inputs are ran over several samples, we need to apply the
         # sum on all the given axes except the first one (the sample axis), including when axes is
-        # set to None (i.e. sum over all axes).
+        # set to None (i.e., sum over all axes).
         prepared_inputs[1] = (
             tuple(axes + 1) if axes is not None else tuple(numpy.arange(1, len(shape)))
         )
 
         return prepared_inputs
 
 
@@ -1628,15 +1628,15 @@
 
 
 class QuantizedBrevitasQuant(QuantizedOp):
     """Brevitas uniform quantization with encrypted input."""
 
     _impl_for_op_named: str = "onnx.brevitas.Quant"
     # Note that this should be reset when the correctness test that finds
-    # all mismatches between CML and Brevitas is fixed
+    # all mismatches between Concrete ML and Brevitas is fixed
     # FIXME: https://github.com/zama-ai/concrete-ml-internal/issues/2373
     quantize_inputs_with_model_outputs_precision = True
     output_quant_opts: QuantizationOptions
 
     def __init__(
         self,
         n_bits_output: int,
@@ -1648,29 +1648,29 @@
     ) -> None:
         """Construct the Brevitas quantization operator.
 
         Args:
             n_bits_output (int): Number of bits for the operator's quantization of outputs.
                 Not used, will be overridden by the bit_width in ONNX
             op_instance_name (str): The name that should be assigned to this operation, used
-                to retrieve it later or get debugging information about this op (bitwidth, value
+                to retrieve it later or get debugging information about this op (bit-width, value
                 range, integer intermediary values, op-specific error messages). Usually this name
                 is the same as the ONNX operation name for which this operation is constructed.
             int_input_names (Optional[Set[str]]): Names of input integer tensors. Default to None.
             constant_inputs (Optional[Dict]): Input constant tensor.
                 scale (float): Quantizer scale
                 zero_point (float): Quantizer zero-point
                 bit_width (int): Number of bits of the integer representation
             input_quant_opts (Optional[QuantizationOptions]): Options for the input quantizer.
                 Default to None.
             attrs (dict):
                 rounding_mode (str): Rounding mode (default and only accepted option is "ROUND")
                 signed (int): Whether this op quantizes to signed integers (default 1),
                 narrow (int): Whether this op quantizes to a narrow range of integers
-                    e.g. [-2**n_bits-1 .. 2**n_bits-1] (default 0),
+                    eg [-2**n_bits-1 .. 2**n_bits-1] (default 0),
         """
 
         super().__init__(
             n_bits_output,
             op_instance_name,
             int_input_names,
             constant_inputs,
@@ -1702,15 +1702,15 @@
         self.is_narrow = bool(attrs["narrow"])
 
         assert_false(
             not self.is_signed and self.is_narrow,
             "Can not use narrow range for non-signed Brevitas quantizers",
         )
 
-        # To ensure dequantization produces floats, the following parameters must be float.
+        # To ensure de-quantization produces floats, the following parameters must be float.
         # This should be default export setting in Brevitas
         check_float(
             constant_inputs is not None
             and self.constant_inputs[self._params_name_to_input_idx["scale"]],
             "Scale of Brevitas Quant op must be float",
         )
         check_float(
@@ -1718,15 +1718,15 @@
             and self.constant_inputs[self._params_name_to_input_idx["zero_point"]],
             "Zero Point of Brevitas Quant op must be float",
         )
 
         # For mypy
         assert constant_inputs is not None
 
-        # The constant inputs can have either int or str keys, here it's int
+        # The constant inputs can have either int or str keys, here it is int
         n_bits = constant_inputs[3]  # type: ignore
 
         # Set the QAT flag on the output of this operation, so that the
         # following operation in the graph is aware of this flag and can
         # just copy the quantization
         self.output_quant_opts = self._get_output_quant_opts()
         self.output_quant_opts.n_bits = n_bits
```

## concrete/ml/quantization/quantizers.py

```diff
@@ -92,15 +92,16 @@
     is_qat: bool = False
 
     # Determine if the quantized integer values should only be in [2^(n-1)+1 .. 2^(n-1)-1]
     # or whether we use the full range. Only implemented for QAT
     is_narrow: bool = False
 
     # Determines whether the values handled by the quantizer were produced by a custom
-    # quantization layer that has pre-computed scale and zero-point (i.e. ONNX brevitas quant layer)
+    # quantization layer that has pre-computed scale and zero-point
+    #   (i.e., ONNX brevitas quant layer)
     is_precomputed_qat: bool = False
 
     def __init__(
         self, n_bits: int, is_signed: bool = False, is_symmetric: bool = False, is_qat: bool = False
     ):
         self.n_bits = n_bits
         self.is_signed = is_signed
@@ -110,15 +111,15 @@
         # QAT quantization is not symmetric
         assert_true(not self.is_qat or not self.is_symmetric)
 
         # Symmetric quantization is signed
         assert_true(not self.is_symmetric or self.is_signed)
 
     def dump_dict(self) -> Dict:
-        """Dump itelf to a dict.
+        """Dump itself to a dict.
 
         Returns:
             metadata (Dict): dict of serialized object
         """
         metadata: Dict[str, Any] = {}
         metadata["cml_dumped_class_name"] = str(type(self).__name__)
         metadata["n_bits"] = self.n_bits
@@ -126,35 +127,35 @@
         metadata["is_symmetric"] = self.is_symmetric
         metadata["is_qat"] = self.is_qat
         metadata["is_narrow"] = self.is_narrow
         metadata["is_precomputed_qat"] = self.is_precomputed_qat
         return metadata
 
     def dumps(self) -> str:
-        """Dump itelf to a string.
+        """Dump itself to a string.
 
         Returns:
             metadata (str): string of serialized object
         """
 
         return json.dumps(self.dump_dict(), cls=CustomEncoder)
 
     def dump(self, file: IO[str]):
-        """Dump itelf to a file.
+        """Dump itself to a file.
 
         Args:
             file (IO[str]): file of where to dump.
         """
         metadata = self.dump_dict()
         json.dump(metadata, file, cls=CustomEncoder)
 
     # Loading
     @staticmethod
     def load_dict(metadata: Dict):
-        """Load itelf from a string.
+        """Load itself from a string.
 
         Args:
             metadata (Dict): dict of serialized object
 
         Returns:
             QuantizationOptions: the loaded object
         """
@@ -166,28 +167,28 @@
         )
         for attr_name in ["is_narrow", "is_precomputed_qat"]:
             setattr(to_return, attr_name, metadata[attr_name])
         return to_return
 
     @staticmethod
     def load(file: IO[str]) -> QuantizationOptions:
-        """Load itelf from a file.
+        """Load itself from a file.
 
         Args:
             file (IO[str]): file of serialized object
 
         Returns:
             QuantizationOptions: the loaded object
         """
         metadata = json.load(file)
         return QuantizationOptions.load_dict(metadata=metadata)
 
     @staticmethod
     def loads(metadata: str) -> QuantizationOptions:
-        """Load itelf from a string.
+        """Load itself from a string.
 
         Args:
             metadata (str): serialized object
 
         Returns:
             QuantizationOptions: the loaded object
         """
@@ -259,48 +260,48 @@
         uvalues: Optional[numpy.ndarray] = None,
     ):
         self.rmax = rmax
         self.rmin = rmin
         self.uvalues = uvalues
 
     def dump_dict(self) -> Dict:
-        """Dump itelf to a dict.
+        """Dump itself to a dict.
 
         Returns:
             metadata (Dict): dict of serialized object
         """
         metadata: Dict[str, Any] = {}
         metadata["cml_dumped_class_name"] = str(type(self).__name__)
         metadata["rmax"] = self.rmax
         metadata["rmin"] = self.rmin
         metadata["uvalues"] = self.uvalues
         return metadata
 
     def dumps(self) -> str:
-        """Dump itelf to a string.
+        """Dump itself to a string.
 
         Returns:
             metadata (str): string of serialized object
         """
 
         return json.dumps(self.dump_dict(), cls=CustomEncoder)
 
     def dump(self, file: IO[str]):
-        """Dump itelf to a file.
+        """Dump itself to a file.
 
         Args:
             file (IO[str]): file of where to dump.
         """
         metadata = self.dump_dict()
         json.dump(metadata, file, cls=CustomEncoder)
 
     # Loading
     @staticmethod
     def load_dict(metadata: Dict):
-        """Load itelf from a string.
+        """Load itself from a string.
 
         Args:
             metadata (Dict): dict of serialized object
 
         Returns:
             QuantizationOptions: the loaded object
         """
@@ -310,28 +311,28 @@
             uvalues=numpy.array(metadata["uvalues"]),
         )
 
         return to_return
 
     @staticmethod
     def load(file: IO[str]) -> MinMaxQuantizationStats:
-        """Load itelf from a file.
+        """Load itself from a file.
 
         Args:
             file (IO[str]): file of serialized object
 
         Returns:
             MinMaxQuantizationStats: the loaded object
         """
         metadata = json.load(file)
         return MinMaxQuantizationStats.load_dict(metadata=metadata)
 
     @staticmethod
     def loads(metadata: str) -> MinMaxQuantizationStats:
-        """Load itelf from a string.
+        """Load itself from a string.
 
         Args:
             metadata (str): serialized object
 
         Returns:
             MinMaxQuantizationStats: the loaded object
         """
@@ -429,48 +430,48 @@
         offset: Optional[int] = None,
     ):
         self.scale = scale
         self.zero_point = zero_point
         self.offset = offset
 
     def dump_dict(self) -> Dict:
-        """Dump itelf to a dict.
+        """Dump itself to a dict.
 
         Returns:
             metadata (Dict): dict of serialized object
         """
         metadata: Dict[str, Any] = {}
         metadata["cml_dumped_class_name"] = type(self).__name__
         metadata["scale"] = self.scale
         metadata["zero_point"] = self.zero_point
         metadata["offset"] = self.offset
         return metadata
 
     def dumps(self) -> str:
-        """Dump itelf to a string.
+        """Dump itself to a string.
 
         Returns:
             metadata (str): string of serialized object
         """
 
         return json.dumps(self.dump_dict(), cls=CustomEncoder)
 
     def dump(self, file: IO[str]):
-        """Dump itelf to a file.
+        """Dump itself to a file.
 
         Args:
             file (IO[str]): file of where to dump.
         """
         metadata = self.dump_dict()
         json.dump(metadata, file, cls=CustomEncoder)
 
     # Loading
     @staticmethod
     def load_dict(metadata: Dict) -> UniformQuantizationParameters:
-        """Load itelf from a string.
+        """Load itself from a string.
 
         Args:
             metadata (Dict): dict of serialized object
 
         Returns:
             UniformQuantizationParameters: the loaded object
         """
@@ -479,28 +480,28 @@
             zero_point=metadata["zero_point"],
             offset=metadata["offset"],
         )
         return to_return
 
     @staticmethod
     def load(file: IO[str]) -> UniformQuantizationParameters:
-        """Load itelf from a file.
+        """Load itself from a file.
 
         Args:
             file (IO[str]): file of serialized object
 
         Returns:
             UniformQuantizationParameters: the loaded object
         """
         metadata = json.load(file)
         return UniformQuantizationParameters.load_dict(metadata=metadata)
 
     @staticmethod
     def loads(metadata: str) -> UniformQuantizationParameters:
-        """Load itelf from a string.
+        """Load itself from a string.
 
         Args:
             metadata (str): serialized object
 
         Returns:
             UniformQuantizationParameters: the loaded object
         """
@@ -639,15 +640,15 @@
 
 # Change UniformQuantizer inheritance from UniformQuantizationParameters to composition.
 # FIXME: https://github.com/zama-ai/concrete-ml-internal/issues/1434
 class UniformQuantizer(UniformQuantizationParameters, QuantizationOptions, MinMaxQuantizationStats):
     """Uniform quantizer.
 
     Contains all information necessary for uniform quantization and provides
-    quantization/dequantization functionality on numpy arrays.
+    quantization/de-quantization functionality on numpy arrays.
 
     Args:
         options (QuantizationOptions): Quantization options set
         stats (Optional[MinMaxQuantizationStats]): Quantization batch statistics set
         params (Optional[UniformQuantizationParameters]): Quantization parameters set
             (scale, zero-point)
     """
@@ -709,21 +710,21 @@
                 min_value += 1
 
             qvalues = qvalues.clip(min_value, 2 ** (self.n_bits) - 1 - self.offset)
 
         return qvalues.astype(numpy.int64)
 
     def dequant(self, qvalues: numpy.ndarray) -> Union[Any, numpy.ndarray]:
-        """Dequantize values.
+        """De-quantize values.
 
         Args:
-            qvalues (numpy.ndarray): integer values to dequantize
+            qvalues (numpy.ndarray): integer values to de-quantize
 
         Returns:
-            Union[Any, numpy.ndarray]: Dequantized float values.
+            Union[Any, numpy.ndarray]: De-quantized float values.
         """
 
         # for mypy
         assert self.zero_point is not None
         assert self.scale is not None
 
         assert_true(
@@ -735,49 +736,49 @@
         )
 
         ans = self.scale * (qvalues - numpy.asarray(self.zero_point, dtype=numpy.float64))
 
         return ans
 
     def dump_dict(self) -> Dict:
-        """Dump itelf to a dict.
+        """Dump itself to a dict.
 
         Returns:
             metadata (Dict): dict of serialized object
         """
         metadata: Dict[str, Any] = {}
         metadata["cml_dumped_class_name"] = str(type(self).__name__)
 
         for attribute in ["zero_point", "scale", "offset", "n_bits"]:
             if hasattr(self, attribute):
                 metadata[attribute] = getattr(self, attribute)
         return metadata
 
     def dumps(self) -> str:
-        """Dump itelf to a string.
+        """Dump itself to a string.
 
         Returns:
             metadata (str): string of serialized object
         """
         metadata = self.dump_dict()
         return json.dumps(metadata, cls=CustomEncoder)
 
     def dump(self, file: IO[str]) -> None:
-        """Dump itelf to a file.
+        """Dump itself to a file.
 
         Args:
             file (IO[str]): file of where to dump.
         """
         metadata = self.dump_dict()
         json.dump(metadata, file, cls=CustomEncoder)
 
     # Loading
     @staticmethod
     def load_dict(metadata: Dict) -> UniformQuantizer:
-        """Load itelf from a string.
+        """Load itself from a string.
 
         Args:
             metadata (Dict): dict of serialized object
 
         Returns:
             UniformQuantizer: the loaded object
         """
@@ -786,28 +787,28 @@
         for attribute in ["zero_point", "scale", "offset", "n_bits"]:
             if attribute in metadata:
                 setattr(obj, attribute, metadata[attribute])
         return obj
 
     @staticmethod
     def load(file: IO[str]) -> UniformQuantizer:
-        """Load itelf from a file.
+        """Load itself from a file.
 
         Args:
             file (IO[str]): file of serialized object
 
         Returns:
             UniformQuantizer: the loaded object
         """
         metadata = json.load(file)
         return UniformQuantizer.load_dict(metadata=metadata)
 
     @staticmethod
     def loads(metadata: str) -> UniformQuantizer:
-        """Load itelf from a string.
+        """Load itself from a string.
 
         Args:
             metadata (str): serialized object
 
         Returns:
             UniformQuantizer: the loaded object
         """
@@ -979,28 +980,28 @@
             numpy.ndarray: Quantized values.
         """
 
         self.qvalues = self.quantizer.quant(self.values)
         return self.qvalues
 
     def dequant(self) -> numpy.ndarray:
-        """Dequantize self.qvalues.
+        """De-quantize self.qvalues.
 
         Returns:
-            numpy.ndarray: Dequantized values.
+            numpy.ndarray: De-quantized values.
         """
         self.values = self.quantizer.dequant(self.qvalues)
         assert_true(
             not isinstance(self.values, numpy.ndarray) or self.values.dtype == numpy.float64,
-            "Dequantized values must be float64",
+            "De-quantized values must be float64",
         )
         return self.values
 
     def dump_dict(self) -> Dict:
-        """Dump itelf to a dict.
+        """Dump itself to a dict.
 
         Returns:
             metadata (Dict): dict of serialized object
         """
         metadata: Dict[str, Any] = {}
         metadata["cml_dumped_class_name"] = str(type(self).__name__)
         metadata["STABILITY_CONST"] = self.STABILITY_CONST
@@ -1008,35 +1009,35 @@
         metadata["n_bits"] = self.n_bits
         metadata["values"] = self.values
         metadata["qvalues"] = self.qvalues
         metadata["values"] = self.values
         return metadata
 
     def dumps(self) -> str:
-        """Dump itelf to a string.
+        """Dump itself to a string.
 
         Returns:
             metadata (str): string of serialized object
         """
         metadata = self.dump_dict()
         return json.dumps(metadata, cls=CustomEncoder)
 
     def dump(self, file: IO[str]) -> None:
-        """Dump itelf to a file.
+        """Dump itself to a file.
 
         Args:
             file (IO[str]): file of where to dump.
         """
         metadata = self.dump_dict()
         json.dump(metadata, file, cls=CustomEncoder)
 
     # Loading
     @staticmethod
     def load_dict(metadata: Dict) -> QuantizedArray:
-        """Load itelf from a string.
+        """Load itself from a string.
 
         Args:
             metadata (Dict): dict of serialized object
 
         Returns:
             QuantizedArray: the loaded object
         """
@@ -1047,28 +1048,28 @@
         obj.values = metadata["values"]
         obj.qvalues = metadata["qvalues"]
 
         return obj
 
     @staticmethod
     def load(file: IO[str]) -> QuantizedArray:
-        """Load itelf from a file.
+        """Load itself from a file.
 
         Args:
             file (IO[str]): file of serialized object
 
         Returns:
             QuantizedArray: the loaded object
         """
         metadata = json.load(file)
         return QuantizedArray.load_dict(metadata=metadata)
 
     @staticmethod
     def loads(metadata: str) -> QuantizedArray:
-        """Load itelf from a string.
+        """Load itself from a string.
 
         Args:
             metadata (str): serialized object
 
         Returns:
             QuantizedArray: the loaded object
         """
```

## concrete/ml/search_parameters/p_error_search.py

```diff
@@ -1,22 +1,22 @@
 """p_error binary search for classification and regression tasks.
 
-Only Pytorch neural networks and Concrete built-in models are supported.
+Only PyTorch neural networks and Concrete built-in models are supported.
 - Concrete built-in models include trees and QNN
 - Quantized aware trained model are supported using Brevitas framework
 - Torch models can be converted into post-trained quantized models
 
 The `p_error` represents an essential hyper-parameter in the FHE computation at Zama.
 As it impacts the speed of the FHE computations and the model's performance.
 
 In this script, we provide an approach to find out an optimal `p_error`, which would offer
 an interesting compromise between speed and efficiency.
 
 The `p_error` represents the probability of a single PBS being incorrect. Know that the FHE
-scheme allows to perfom 2 types of operations
+scheme allows to perform 2 types of operations
 - Linear operations: additions and multiplications
 - Non-linear operation: uni-variate activation functions
 
 At Zama, non-linear operations are represented by table lookup (TLU), which are implemented
 through the Programmable Bootstrapping technology (PBS).
 A single PBS operation has `p_error` chances of being incorrect.
 
@@ -89,18 +89,18 @@
         ground_truth (numpy.ndarray): The ground truth
         p_error (float): Concrete ML uses table lookup (TLU) to represent any non-linear
         n_bits (int): Quantization bits
         is_qat (bool): True, if the NN has been trained through QAT.
             If `False` it is converted into post-trained quantized model.
         metric (Callable): Classification or regression evaluation metric.
         predict (str): The predict method to use.
-        kwargs (Dict): Metric's hyper-parameters.
+        kwargs (Dict): Hyper-parameters to use for the metric.
 
     Returns:
-        Tuple[numpy.ndarray, float]: Dequantized or quantized output model depending on
+        Tuple[numpy.ndarray, float]: De-quantized or quantized output model depending on
         `is_benchmark_test` and the score.
 
     Raises:
         ValueError: If the model is neither a built-in model nor a torch neural network.
     """
 
     compile_params: Dict = {}
@@ -172,15 +172,15 @@
         directory: str = None,
         verbose: bool = False,
         **kwargs: dict,
     ):
         """`p_error` binary search algorithm.
 
         Args:
-            estimator : Custom model (Brevitas or Pytorch) or built-in models (trees or QNNs).
+            estimator : Custom model (Brevitas or PyTorch) or built-in models (trees or QNNs).
             predict (str): The prediction method to use for built-in tree models.
             metric (Callable): Evaluation metric for classification or regression tasks.
             n_bits (int): Quantization bits, for PTQ models. Default is 4.
             is_qat (bool): Flag that indicates whether the `estimator` has been trained through
                 QAT (quantization-aware training). Default is True.
             lower (float): The lower bound of the search space for the `p_error`. Default is 0.0.
             upper (float): The upper bound of the search space for the `p_error`. Default is 0.9.
@@ -365,15 +365,15 @@
     def _update_attr(self, **kwargs: dict) -> None:
         """Update the hyper-parameters then check if the values are valid.
 
         Args:
             kwargs (Dict): Hyper-parameters
 
         Raises:
-            AttributeError: If the attribut is not in the class.
+            AttributeError: If the attribute is not in the class.
         """
         for attr, new_value in kwargs.items():
             if hasattr(self, attr):
                 setattr(self, attr, new_value)
             else:
                 raise AttributeError(f"{attr=} does not belong to this class")
 
@@ -409,15 +409,15 @@
         update of the `p_error` is below at a given threshold.
 
         Args:
             x (numpy.ndarray): Data-set which is used for calibration and evaluation
             ground_truth (numpy.ndarray): The ground truth
             kwargs (Dict): Class parameters
             strategy (Callable): A uni-variate function that defines a "match". It can be: a
-                built-in functions provided in python, like: any or all or a custom function, like:
+                built-in functions provided in Python, like: any or all or a custom function, like:
                 mean = lambda all_matches: numpy.mean(all_matches) >= 0.5
                 median = lambda all_matches: numpy.median(all_matches) == 1
                 Default is `all`.
 
         Returns:
             float: The optimal `p_error` that aims to speedup computations while maintaining good
                 performance.
@@ -502,13 +502,13 @@
         if numpy.mean(self.history[-1]["metric_difference"]) > self.max_metric_loss:
             # pylint: disable=pointless-statement
             warning_message = (
                 "ConvergenceWarning: The convergence is not reached for the "
                 f"`{self.estimator.__class__}` model, you can:\n"
                 "  - Increase the number of iterations `max_iter` or `n_simulation`.\n"
                 "  - Choose another strategy.\n"
-                "  - Increase the size of the calibration dataset."
+                "  - Increase the size of the calibration data-set."
             )
             warnings.simplefilter("always")
             warnings.warn(warning_message, category=UserWarning, stacklevel=2)
 
         return self.p_error
```

## concrete/ml/sklearn/base.py

```diff
@@ -1,14 +1,14 @@
 """Base classes for all estimators."""
 from __future__ import annotations
 
 import json
 import tempfile
 
-# Disable pylint as some names like X and q_X are used, following Scikit-Learn's standard. The file
+# Disable pylint as some names like X and q_X are used, following scikit-Learn's standard. The file
 # is also more than 1000 lines long.
 # pylint: disable=too-many-lines,invalid-name
 import warnings
 from abc import ABC, abstractmethod
 from pathlib import Path
 from typing import IO, Any, Callable, Dict, List, Optional, Set, Type, Union
 
@@ -48,17 +48,17 @@
     UniformQuantizationParameters,
     UniformQuantizer,
 )
 from ..torch import NumpyModule
 from .qnn_module import SparseQuantNeuralNetwork
 from .tree_to_numpy import tree_to_numpy
 
-# Disable pylint to import hummingbird while ignoring the warnings
+# Disable pylint to import Hummingbird while ignoring the warnings
 # pylint: disable=wrong-import-position,wrong-import-order
-# Silence hummingbird warnings
+# Silence Hummingbird warnings
 warnings.filterwarnings("ignore")
 from hummingbird.ml import convert as hb_convert  # noqa: E402
 
 _ALL_SKLEARN_MODELS: Set[Type] = set()
 _LINEAR_MODELS: Set[Type] = set()
 _TREE_MODELS: Set[Type] = set()
 _NEURALNET_MODELS: Set[Type] = set()
@@ -85,15 +85,15 @@
 
 
 # pylint: disable=too-many-public-methods
 class BaseEstimator:
     """Base class for all estimators in Concrete ML.
 
     This class does not inherit from sklearn.base.BaseEstimator as it creates some conflicts
-    with Skorch in QuantizedTorchEstimatorMixin's subclasses (more specifically, the `get_params`
+    with skorch in QuantizedTorchEstimatorMixin's subclasses (more specifically, the `get_params`
     method is not properly inherited).
 
     Attributes:
         _is_a_public_cml_model (bool): Private attribute indicating if the class is a public model
             (as opposed to base or mixin classes).
     """
 
@@ -102,28 +102,28 @@
 
     _is_a_public_cml_model: bool = False
 
     def __init__(self):
         """Initialize the base class with common attributes used in all estimators.
 
         An underscore "_" is appended to attributes that were created while fitting the model. This
-        is done in order to follow Scikit-Learn's standard format. More information available
+        is done in order to follow scikit-Learn's standard format. More information available
         in their documentation:
         https://scikit-learn.org/stable/developers/develop.html#:~:text=Estimated%20Attributes%C2%B6
         """
         #: The equivalent fitted float model. Is None if the model is not fitted.
         self.sklearn_model: Optional[sklearn.base.BaseEstimator] = None
 
         #: The list of quantizers, which contain all information necessary for applying uniform
-        #: quantization to inputs and provide quantization/dequantization functionalities. Is empty
+        #: quantization to inputs and provide quantization/de-quantization functionalities. Is empty
         #: if the model is not fitted
         self.input_quantizers: List[UniformQuantizer] = []
 
         #: The list of quantizers, which contain all information necessary for applying uniform
-        #: quantization to outputs and provide quantization/dequantization functionalities. Is
+        #: quantization to outputs and provide quantization/de-quantization functionalities. Is
         #: empty if the model is not fitted
         self.output_quantizers: List[UniformQuantizer] = []
 
         #: The parameters needed for post-processing the outputs.
         #: Can be empty if no post-processing operations are needed for the associated model
         #: This attribute is typically used for serving models
         self.post_processing_params: Dict[str, Any] = {}
@@ -226,26 +226,26 @@
         """
         if not self.is_compiled:
             raise AttributeError(self._is_not_compiled_error_message())
 
     def get_sklearn_params(self, deep: bool = True) -> dict:
         """Get parameters for this estimator.
 
-        This method is used to instantiate a Scikit-Learn model using the Concrete ML model's
-        parameters. It does not override Scikit-Learn's existing `get_params` method in order to
+        This method is used to instantiate a scikit-learn model using the Concrete ML model's
+        parameters. It does not override scikit-learn's existing `get_params` method in order to
         not break its implementation of `set_params`.
 
         Args:
             deep (bool): If True, will return the parameters for this estimator and contained
                 subobjects that are estimators. Default to True.
 
         Returns:
             params (dict): Parameter names mapped to their values.
         """
-        # Here, the `get_params` method is the `BaseEstimator.get_params` method from Scikit-Learn,
+        # Here, the `get_params` method is the `BaseEstimator.get_params` method from scikit-learn,
         # which will become available once a subclass inherits from it. We therefore disable both
         # pylint and mypy as this behavior is expected
         # pylint: disable-next=no-member
         # FIXME: https://github.com/zama-ai/concrete-ml-internal/issues/3373
         params = super().get_params(deep=deep)  # type: ignore[misc]
 
         # Remove the n_bits parameters as this attribute is added by Concrete ML
@@ -254,46 +254,46 @@
         return params
 
     def _set_post_processing_params(self) -> None:
         """Set parameters used in post-processing."""
         self.post_processing_params = {}
 
     def _fit_sklearn_model(self, X: Data, y: Target, **fit_parameters):
-        """Fit the model's Scikit-Learn equivalent estimator.
+        """Fit the model's scikit-learn equivalent estimator.
 
         Args:
             X (Data): The training data, as a Numpy array, Torch tensor, Pandas DataFrame or List.
             y (Target): The target data, as a Numpy array, Torch tensor, Pandas DataFrame, Pandas
                 Series or List.
-            **fit_parameters: Keyword arguments to pass to the Scikit-Learn estimator's fit method.
+            **fit_parameters: Keyword arguments to pass to the scikit-learn estimator's fit method.
 
         Returns:
-            The fitted Scikit-Learn estimator.
+            The fitted scikit-learn estimator.
         """
 
-        # Initialize the underlying Scikit-learn model if it has not already been done or if
+        # Initialize the underlying scikit-learn model if it has not already been done or if
         # `warm_start` is set to False (for neural networks)
         # This model should be directly initialized in the model's __init__ method instead
         # FIXME: https://github.com/zama-ai/concrete-ml-internal/issues/3373
         if self.sklearn_model is None or not getattr(self, "warm_start", False):
             # Retrieve the init parameters
             params = self.get_sklearn_params()
 
             self.sklearn_model = self.sklearn_model_class(**params)
 
-        # Fit the Scikit-Learn model
+        # Fit the scikit-learn model
         self.sklearn_model.fit(X, y, **fit_parameters)
 
         return self.sklearn_model
 
     @abstractmethod
     def fit(self, X: Data, y: Target, **fit_parameters):
         """Fit the estimator.
 
-        This method trains a Scikit-Learn estimator, computes its ONNX graph and defines the
+        This method trains a scikit-learn estimator, computes its ONNX graph and defines the
         quantization parameters needed for proper FHE inference.
 
         Args:
             X (Data): The training data, as a Numpy array, Torch tensor, Pandas DataFrame or List.
             y (Target): The target data, as a Numpy array, Torch tensor, Pandas DataFrame, Pandas
                 Series or List.
             **fit_parameters: Keyword arguments to pass to the float estimator's fit method.
@@ -337,23 +337,23 @@
             elif getattr(self, "random_state", None) is not None:
                 # Disable mypy attribute definition errors as it does not seem to see that we make
                 # sure this attribute actually exists before calling it
                 params["random_state"] = self.random_state  # type: ignore[attr-defined]
             else:
                 params["random_state"] = numpy.random.randint(0, 2**15)
 
-        # Initialize the Scikit-Learn model
+        # Initialize the scikit-learn model
         sklearn_model = self.sklearn_model_class(**params)
 
-        # Train the Scikit-Learn model
+        # Train the scikit-learn model
         sklearn_model.fit(X, y, **fit_parameters)
 
         # Update the Concrete ML model's parameters
         # Disable mypy attribute definition errors as this attribute is expected to be
-        # initialized once the model inherits from Skorch
+        # initialized once the model inherits from skorch
         self.set_params(n_bits=self.n_bits, **params)  # type: ignore[attr-defined]
 
         # Train the Concrete ML model
         self.fit(X, y, **fit_parameters)
 
         return self, sklearn_model
 
@@ -370,23 +370,23 @@
 
         Returns:
             numpy.ndarray: The quantized input values.
         """
 
     @abstractmethod
     def dequantize_output(self, q_y_preds: numpy.ndarray) -> numpy.ndarray:
-        """Dequantize the output.
+        """De-quantize the output.
 
         This step ensures that the fit method has been called.
 
         Args:
-            q_y_preds (numpy.ndarray): The quantized output values to dequantize.
+            q_y_preds (numpy.ndarray): The quantized output values to de-quantize.
 
         Returns:
-            numpy.ndarray: The dequantized output values.
+            numpy.ndarray: The de-quantized output values.
         """
 
     @abstractmethod
     def _get_module_to_compile(self) -> Union[Compiler, QuantizedModule]:
         """Retrieve the module instance to compile.
 
         Returns:
@@ -404,15 +404,15 @@
         verbose: bool = False,
     ) -> Circuit:
         """Compile the model.
 
         Args:
             X (Data): A representative set of input values used for building cryptographic
                 parameters, as a Numpy array, Torch tensor, Pandas DataFrame or List. This is
-                usually the training data set or s sub-set of it.
+                usually the training data-set or s sub-set of it.
             configuration (Optional[Configuration]): Options to use for compilation. Default
                 to None.
             artifacts (Optional[DebugArtifacts]): Artifacts information about the compilation
                 process to store for debugging. Default to None.
             show_mlir (bool): Indicate if the MLIR graph should be printed during compilation.
                 Default to False.
             p_error (Optional[float]): Probability of error of a single PBS. A p_error value cannot
@@ -442,15 +442,15 @@
 
         # Find the right way to set parameters for compiler, depending on the way we want to default
         p_error, global_p_error = manage_parameters_for_pbs_errors(p_error, global_p_error)
 
         # Quantize the inputs
         q_X = self.quantize_input(X)
 
-        # Generate the compilation inputset with proper dimensions
+        # Generate the compilation input-set with proper dimensions
         inputset = _get_inputset_generator(q_X)
 
         # Retrieve the compiler instance
         module_to_compile = self._get_module_to_compile()
 
         # Compiling using a QuantizedModule requires different steps and should not be done here
         assert isinstance(module_to_compile, Compiler), (
@@ -487,15 +487,15 @@
     def predict(self, X: Data, fhe: Union[FheMode, str] = FheMode.DISABLE) -> numpy.ndarray:
         """Predict values for X, in FHE or in the clear.
 
         Args:
             X (Data): The input values to predict, as a Numpy array, Torch tensor, Pandas DataFrame
                 or List.
             fhe (Union[FheMode, str]): The mode to use for prediction.
-                Can be FheMode.DISABLE for Concrete ML python inference,
+                Can be FheMode.DISABLE for Concrete ML Python inference,
                 FheMode.SIMULATE for FHE simulation and FheMode.EXECUTE for actual FHE execution.
                 Can also be the string representation of any of these values.
                 Default to FheMode.DISABLE.
 
         Returns:
             np.ndarray: The predicted values for X.
         """
@@ -542,34 +542,34 @@
 
             q_y_pred = numpy.array(q_y_pred_list)
 
         # Else, the prediction is simulated in the clear
         else:
             q_y_pred = self._inference(q_X)
 
-        # Dequantize the predicted values in the clear
+        # De-quantize the predicted values in the clear
         y_pred = self.dequantize_output(q_y_pred)
 
         return y_pred
 
     # pylint: disable-next=no-self-use
     def post_processing(self, y_preds: numpy.ndarray) -> numpy.ndarray:
-        """Apply post-processing to the dequantized predictions.
+        """Apply post-processing to the de-quantized predictions.
 
         This post-processing step can include operations such as applying the sigmoid or softmax
         function for classifiers, or summing an ensemble's outputs. These steps are done in the
         clear because of current technical constraints. They most likely will be integrated in the
         FHE computations in the future.
 
         For some simple models such a linear regression, there is no post-processing step but the
         method is kept to make the API consistent for the client-server API. Other models might
         need to use attributes stored in `post_processing_params`.
 
         Args:
-            y_preds (numpy.ndarray): The dequantized predictions to post-process.
+            y_preds (numpy.ndarray): The de-quantized predictions to post-process.
 
         Returns:
             numpy.ndarray: The post-processed predictions.
         """
         return y_preds
 
     @abstractmethod
@@ -577,59 +577,59 @@
         """Dump the object as a dict.
 
         Returns:
             Dict[str, Any]: a dict representing the object
         """
 
     def dumps(self) -> str:
-        """Dump itelf to a string.
+        """Dump itself to a string.
 
         Returns:
             metadata (str): string of serialized object
         """
         metadata: Dict[str, Any] = self.dump_dict()
         return json.dumps(metadata, cls=CustomEncoder)
 
     def dump(self, file: IO[str]) -> None:
-        """Dump itelf to a file.
+        """Dump itself to a file.
 
         Args:
             file (IO[str]): file of where to dump.
         """
         metadata: Dict[str, Any] = self.dump_dict()
         json.dump(metadata, file, cls=CustomEncoder)
 
     @classmethod
     @abstractmethod
     def load_dict(cls, metadata: Dict[str, Any]) -> BaseEstimator:
-        """Load itelf from a dict.
+        """Load itself from a dict.
 
         Args:
             metadata (Dict[str, Any]): dict of metadata of the object
 
         Returns:
             BaseEstimator: the loaded object
         """
 
     @classmethod
     def load(cls, file: IO[str]) -> BaseEstimator:
-        """Load itelf from a file.
+        """Load itself from a file.
 
         Args:
             file (IO[str]): file of serialized object
 
         Returns:
             BaseEstimator: the loaded object
         """
         metadata: Dict[str, Any] = json.load(file)
         return cls.load_dict(metadata=metadata)
 
     @classmethod
     def loads(cls, metadata: str) -> BaseEstimator:
-        """Load itelf from a string.
+        """Load itself from a string.
 
         Args:
             metadata (str): serialized object
 
         Returns:
             BaseEstimator: the loaded object
         """
@@ -681,15 +681,15 @@
     def predict_proba(self, X: Data, fhe: Union[FheMode, str] = FheMode.DISABLE) -> numpy.ndarray:
         """Predict class probabilities.
 
         Args:
             X (Data): The input values to predict, as a Numpy array, Torch tensor, Pandas DataFrame
                 or List.
             fhe (Union[FheMode, str]): The mode to use for prediction.
-                Can be FheMode.DISABLE for Concrete ML python inference,
+                Can be FheMode.DISABLE for Concrete ML Python inference,
                 FheMode.SIMULATE for FHE simulation and FheMode.EXECUTE for actual FHE execution.
                 Can also be the string representation of any of these values.
                 Default to FheMode.DISABLE.
 
         Returns:
             numpy.ndarray: The predicted class probabilities.
         """
@@ -708,15 +708,15 @@
 
     def post_processing(self, y_preds: numpy.ndarray) -> numpy.ndarray:
         y_preds = super().post_processing(y_preds)
 
         # Retrieve the number of target classes
         n_classes_ = self.post_processing_params["n_classes_"]
 
-        # If the predictions only has one dimension (i.e. binary classification problem), apply the
+        # If the predictions only has one dimension (i.e., binary classification problem), apply the
         # sigmoid operator
         if n_classes_ == 2:
             y_preds = numpy_sigmoid(y_preds)[0]
 
             # If the prediction array is 1D (which happens with some models such as XGBCLassifier
             # models), transform the output into a 2D array [1-p, p], with p the initial
             # output probabilities
@@ -810,16 +810,16 @@
         Args:
             deep (bool): If True, will return the parameters for this estimator and
                 contained subobjects that are estimators.
 
         Returns:
             params (dict): Parameter names mapped to their values.
         """
-        # Retrieve the Skorch estimator's init parameters
-        # Here, the `get_params` method is the `NeuralNet.get_params` method from Skorch, which
+        # Retrieve the skorch estimator's init parameters
+        # Here, the `get_params` method is the `NeuralNet.get_params` method from skorch, which
         # will become available once a subclass inherits from it. We therefore disable both pylint
         # and mypy as this behavior is expected
         # FIXME: https://github.com/zama-ai/concrete-ml-internal/issues/3373
         # pylint: disable-next=no-member
         params = super().get_params(deep)  # type: ignore[misc]
 
         # Remove `module` since it is automatically set to SparseQuantNeuralNetImpl. Therefore,
@@ -829,16 +829,16 @@
         # Remove the parameters that are auto-computed by `fit` as well
         for kwarg in QNN_AUTO_KWARGS:
             params.pop(kwarg, None)
 
         return params
 
     def get_sklearn_params(self, deep: bool = True) -> Dict:
-        # Retrieve the Skorch estimator's init parameters
-        # Here, the `get_params` method is the `NeuralNet.get_params` method from Skorch, which
+        # Retrieve the skorch estimator's init parameters
+        # Here, the `get_params` method is the `NeuralNet.get_params` method from skorch, which
         # will become available once a subclass inherits from it. We therefore disable both pylint
         # and mypy as this behavior is expected
         # FIXME: https://github.com/zama-ai/concrete-ml-internal/issues/3373
         # pylint: disable-next=no-member
         params = super().get_params(deep=deep)  # type: ignore[misc]
 
         # Set the quantized module to SparseQuantNeuralNetwork
@@ -861,15 +861,15 @@
 
         Values of dtype float64 are not supported and will be casted to float32.
 
         Args:
             X (Data): The training data, as a Numpy array, Torch tensor, Pandas DataFrame or List.
             y (Target): The target data,  as a Numpy array, Torch tensor, Pandas DataFrame, Pandas
                 Series or List.
-            **fit_parameters: Keyword arguments to pass to Skorch's fit method.
+            **fit_parameters: Keyword arguments to pass to skorch's fit method.
 
         Returns:
             The fitted estimator.
         """
         # Reset for double fit
         self._is_fitted = False
 
@@ -893,15 +893,15 @@
                     setattr(self, arg_name, torch.from_numpy(attr_value).float())
 
                 assert_true(
                     isinstance(getattr(self, arg_name), torch.Tensor),
                     f"Parameter `{arg_name}` must be a numpy.ndarray, list or torch.Tensor",
                 )
 
-        # Fit the model by using Skorch's fit
+        # Fit the model by using skorch's fit
         self._fit_sklearn_model(X, y, **fit_parameters)
 
         # Export the brevitas model to ONNX
         output_onnx_file_path = Path(tempfile.mkstemp(suffix=".onnx")[1])
 
         BrevitasONNXManager.export(
             self.base_module,
@@ -918,16 +918,16 @@
         # Create corresponding numpy model
         numpy_model = NumpyModule(onnx_model, torch.tensor(X[[0], ::]))
 
         self.onnx_model_ = numpy_model.onnx_model
 
         # Set the quantization bits for import
         # Note that the ONNXConverter will use a default value for network input bits
-        # Furthermore, Brevitas ONNX contains bitwidths in the ONNX file
-        # which override the bitwidth that we pass here
+        # Furthermore, Brevitas ONNX contains bit-widths in the ONNX file
+        # which override the bit-width that we pass here
         # Thus, this parameter is only used to check consistency during import (onnx file vs import)
         n_bits = self.base_module.n_a_bits
 
         # Import the quantization aware trained model
         qat_model = PostTrainingQATImporter(n_bits, numpy_model)
 
         self.quantized_module_ = qat_model.quantize_module(X)
@@ -949,44 +949,44 @@
         float_module = torch.nn.Sequential()
 
         layer_index = -1
 
         # Iterate over the model's sub-modules
         for module in self.base_module.features:
 
-            # If the module is not a QuantIdentity, it's either a QuantLinear or an activation
+            # If the module is not a QuantIdentity, it is either a QuantLinear or an activation
             if not isinstance(module, qnn.QuantIdentity):
                 # If the module is a QuantLinear, replace it with a Linear module
                 if isinstance(module, qnn.QuantLinear):
                     layer_index += 1
 
                     linear_name = f"fc{layer_index}"
                     linear_layer = torch.nn.Linear(
                         module.in_features,
                         module.out_features,
                         module.bias is not None,
                     )
 
                     float_module.add_module(linear_name, linear_layer)
 
-                # Else, it's a module representing the activation function, which needs to be
+                # Else, it is a module representing the activation function, which needs to be
                 # added as well
                 else:
                     activation_name = f"act{layer_index}"
                     float_module.add_module(activation_name, module)
 
         return float_module
 
     def _get_equivalent_float_estimator(self) -> SkorchNeuralNet:
         """Initialize a topologically equivalent estimator that can be used on floating points.
 
         Returns:
             float_estimator (SkorchNeuralNet): An instance of the equivalent float estimator.
         """
-        # Retrieve the Skorch estimator's init parameters
+        # Retrieve the skorch estimator's init parameters
         sklearn_params = self.get_params()
 
         # Retrieve all parameters related to the module
         module_param_names = [name for name in sklearn_params if "module__" in name]
 
         # Remove all parameters related to the module
         for name in module_param_names:
@@ -1013,21 +1013,21 @@
 
         Values of dtype float64 are not supported and will be casted to float32.
 
         Args:
             X (Data): The training data, as a Numpy array, Torch tensor, Pandas DataFrame or List.
             y (Target): The target data,  as a Numpy array, Torch tensor, Pandas DataFrame Pandas
                 Series or List.
-            random_state (Optional[int]): The random state to use when fitting. However, Skorch
+            random_state (Optional[int]): The random state to use when fitting. However, skorch
                 does not handle such a parameter and setting it will have no effect. Defaults
                 to None.
-            **fit_parameters: Keyword arguments to pass to Skorch's fit method.
+            **fit_parameters: Keyword arguments to pass to skorch's fit method.
 
         Returns:
-            The Concrete ML and equivalent Skorch fitted estimators.
+            The Concrete ML and equivalent skorch fitted estimators.
         """
 
         assert (
             random_state is None
         ), "Neural Network models do not support random_state as a parameter when fitting."
 
         # Fit the quantized estimator
@@ -1166,15 +1166,15 @@
 
         # Set the new pruning amount
         pruned_model.base_module.n_prune_neurons_percentage = n_prune_neurons_percentage
 
         # Enable pruning again, this time with structured pruning
         pruned_model.base_module.enable_pruning()
 
-        # The .module_ was initialized manually, prevent .fit (for both Skorch and Concrete ML)
+        # The .module_ was initialized manually, prevent .fit (for both skorch and Concrete ML)
         # from creating a new one
         # Setting both attributes could be avoided by initializing `sklearn_model` in __init__
         # # FIXME: https://github.com/zama-ai/concrete-ml-internal/issues/3373
         pruned_model.warm_start = True
         pruned_model.sklearn_model.warm_start = True
 
         # Now, fine-tune the original module with structured pruning
@@ -1182,15 +1182,15 @@
 
         return pruned_model
 
 
 class BaseTreeEstimatorMixin(BaseEstimator, sklearn.base.BaseEstimator, ABC):
     """Mixin class for tree-based estimators.
 
-    This class inherits from sklearn.base.BaseEstimator in order to have access to Scikit-Learn's
+    This class inherits from sklearn.base.BaseEstimator in order to have access to scikit-learn's
     `get_params` and `set_params` methods.
     """
 
     #: Model's base framework used, either 'xgboost' or 'sklearn'. Is set for each subclasses.
     framework: str
 
     def __init_subclass__(cls):
@@ -1224,15 +1224,15 @@
 
         # Quantization of each feature in X
         for i in range(X.shape[1]):
             input_quantizer = QuantizedArray(n_bits=self.n_bits, values=X[:, i]).quantizer
             self.input_quantizers.append(input_quantizer)
             q_X[:, i] = input_quantizer.quant(X[:, i])
 
-        # Fit the Scikit-Learn model
+        # Fit the scikit-learn model
         self._fit_sklearn_model(q_X, y, **fit_parameters)
 
         # Set post-processing parameters
         self._set_post_processing_params()
 
         # Check that the underlying sklearn model has been set and fit
         assert self.sklearn_model is not None, self._sklearn_model_is_not_fitted_error_message()
@@ -1323,38 +1323,38 @@
         return y_preds
 
 
 class BaseTreeRegressorMixin(BaseTreeEstimatorMixin, sklearn.base.RegressorMixin, ABC):
     """Mixin class for tree-based regressors.
 
     This class is used to create a tree-based regressor class that inherits from
-    sklearn.base.RegressorMixin, which essentially gives access to Scikit-Learn's `score` method
+    sklearn.base.RegressorMixin, which essentially gives access to scikit-learn's `score` method
     for regressors.
     """
 
 
 class BaseTreeClassifierMixin(
     BaseClassifier, BaseTreeEstimatorMixin, sklearn.base.ClassifierMixin, ABC
 ):
     """Mixin class for tree-based classifiers.
 
     This class is used to create a tree-based classifier class that inherits from
-    sklearn.base.ClassifierMixin, which essentially gives access to Scikit-Learn's `score` method
+    sklearn.base.ClassifierMixin, which essentially gives access to scikit-learn's `score` method
     for classifiers.
 
     Additionally, this class adjusts some of the tree-based base class's methods in order to make
     them compliant with classification workflows.
     """
 
 
 # pylint: disable=invalid-name,too-many-instance-attributes
 class SklearnLinearModelMixin(BaseEstimator, sklearn.base.BaseEstimator, ABC):
     """A Mixin class for sklearn linear models with FHE.
 
-    This class inherits from sklearn.base.BaseEstimator in order to have access to Scikit-Learn's
+    This class inherits from sklearn.base.BaseEstimator in order to have access to scikit-learn's
     `get_params` and `set_params` methods.
     """
 
     def __init_subclass__(cls):
         for klass in cls.__mro__:
             # pylint: disable-next=protected-access
             if getattr(klass, "_is_a_public_cml_model", False):
@@ -1414,15 +1414,15 @@
     def fit(self, X: Data, y: Target, **fit_parameters):
         # Reset for double fit
         self._is_fitted = False
 
         # LinearRegression handles multi-labels data
         X, y = check_X_y_and_assert_multi_output(X, y)
 
-        # Fit the Scikit-Learn model
+        # Fit the scikit-learn model
         self._fit_sklearn_model(X, y, **fit_parameters)
 
         # Check that the underlying sklearn model has been set and fit
         assert self.sklearn_model is not None, self._sklearn_model_is_not_fitted_error_message()
 
         # Retrieve the ONNX graph
         self._set_onnx_model(X)
@@ -1474,15 +1474,15 @@
         output_quantizer = UniformQuantizer(params=output_quant_params, no_clipping=True)
 
         # Quantize the bias using the matmul's scale and zero-point, such that
         # q_bias = round((1/S)*bias + Z)
         self._q_bias = output_quantizer.quant(self.sklearn_model.intercept_)
 
         # Since the matmul and the bias both use the same scale and zero-points, we obtain that
-        # y = S*(q_y - 2*Z) when dequantizing the values. We therefore need to multiply the initial
+        # y = S*(q_y - 2*Z) when de-quantizing the values. We therefore need to multiply the initial
         # output zero_point by 2
         assert output_quantizer.zero_point is not None
         output_quantizer.zero_point *= 2
         self.output_quantizers.append(output_quantizer)
 
         # Updating post-processing parameters
         self._set_post_processing_params()
@@ -1497,15 +1497,15 @@
 
         assert q_X.dtype == numpy.int64, "Inputs were not quantized to int64 values"
         return q_X
 
     def dequantize_output(self, q_y_preds: numpy.ndarray) -> numpy.ndarray:
         self.check_model_is_fitted()
 
-        # Dequantize the output values
+        # De-quantize the output values
         y_preds = self.output_quantizers[0].dequant(q_y_preds)
 
         return y_preds
 
     def _get_module_to_compile(self) -> Union[Compiler, QuantizedModule]:
         # Define the inference function to compile.
         # This function can neither be a class method nor a static one because self we want to avoid
@@ -1513,15 +1513,15 @@
         def inference_to_compile(q_X: numpy.ndarray) -> numpy.ndarray:
             """Compile the circuit in FHE using only the inputs as parameters.
 
             Args:
                 q_X (numpy.ndarray): The quantized input data
 
             Returns:
-                numpy.ndarray: The circuit's outputs.
+                numpy.ndarray: The circuit is outputs.
             """
             return self._inference(q_X)
 
         # Create the compiler instance
         compiler = Compiler(inference_to_compile, {"q_X": "encrypted"})
 
         return compiler
@@ -1537,26 +1537,26 @@
         return y_pred
 
 
 class SklearnLinearRegressorMixin(SklearnLinearModelMixin, sklearn.base.RegressorMixin, ABC):
     """A Mixin class for sklearn linear regressors with FHE.
 
     This class is used to create a linear regressor class that inherits from
-    sklearn.base.RegressorMixin, which essentially gives access to Scikit-Learn's `score` method
+    sklearn.base.RegressorMixin, which essentially gives access to scikit-learn's `score` method
     for regressors.
     """
 
 
 class SklearnLinearClassifierMixin(
     BaseClassifier, SklearnLinearModelMixin, sklearn.base.ClassifierMixin, ABC
 ):
     """A Mixin class for sklearn linear classifiers with FHE.
 
     This class is used to create a linear classifier class that inherits from
-    sklearn.base.ClassifierMixin, which essentially gives access to Scikit-Learn's `score` method
+    sklearn.base.ClassifierMixin, which essentially gives access to scikit-learn's `score` method
     for classifiers.
 
     Additionally, this class adjusts some of the tree-based base class's methods in order to make
     them compliant with classification workflows.
     """
 
     def _clean_graph(self) -> None:
@@ -1572,15 +1572,15 @@
     ) -> numpy.ndarray:
         """Predict confidence scores.
 
         Args:
             X (Data): The input values to predict, as a Numpy array, Torch tensor, Pandas DataFrame
                 or List.
             fhe (Union[FheMode, str]): The mode to use for prediction.
-                Can be FheMode.DISABLE for Concrete ML python inference,
+                Can be FheMode.DISABLE for Concrete ML Python inference,
                 FheMode.SIMULATE for FHE simulation and FheMode.EXECUTE for actual FHE execution.
                 Can also be the string representation of any of these values.
                 Default to FheMode.DISABLE.
 
         Returns:
             numpy.ndarray: The predicted confidence scores.
         """
```

## concrete/ml/sklearn/qnn.py

```diff
@@ -54,16 +54,16 @@
 
 # QNNs do not support serialization yet
 # FIXME: https://github.com/zama-ai/concrete-ml-internal/issues/3134
 # pylint: disable-next=too-many-instance-attributes, abstract-method
 class NeuralNetRegressor(QuantizedTorchEstimatorMixin, SkorchNeuralNetRegressor):
     """A Fully-Connected Neural Network regressor with FHE.
 
-    This class wraps a quantized neural network implemented using Torch tools as a Scikit-Learn
-    estimator. The Skorch package allows to handle training and Scikit-Learn compatibility,
+    This class wraps a quantized neural network implemented using Torch tools as a scikit-learn
+    estimator. The skorch package allows to handle training and scikit-learn compatibility,
     and adds quantization as well as compilation functionalities. The neural network implemented
     by this class is a multi layer fully connected network trained with Quantization Aware Training
     (QAT).
 
     Inputs and targets that are float64 will be casted to float32 before training as Torch does not
     handle float64 types properly. Thus should not have a significant impact on the model's
     performances. An error is raised if these values are not floating points.
@@ -146,20 +146,20 @@
         # is raised.
         X = check_dtype_and_cast(X, "float32", error_information="Neural Network regressor input")
         y = check_dtype_and_cast(y, "float32", error_information="Neural Network regressor target")
 
         # Call QuantizedTorchEstimatorMixin's fit_benchmark method
         return super().fit_benchmark(X, y, *args, **kwargs)
 
-    # Skorch provides a predict_proba method for neural network regressors while Scikit-Learn does
-    # not. We decided to follow Scikit-Learn's API as we build most of our tools on this library.
-    # However, our models are still directly inheriting from Skorch's classes, which makes this
+    # skorch provides a predict_proba method for neural network regressors while scikit-learn does
+    # not. We decided to follow scikit-learn's API as we build most of our tools on this library.
+    # However, our models are still directly inheriting from skorch's classes, which makes this
     # method accessible by anyone, without having any FHE implementation. As this could create some
     # confusion, a NotImplementedError is raised. This issue could be fixed by making these classes
-    # not inherit from Skorch.
+    # not inherit from skorch.
     # FIXME: https://github.com/zama-ai/concrete-ml-internal/issues/3373
     def predict_proba(self, X: Data, fhe: Union[FheMode, str] = FheMode.DISABLE) -> numpy.ndarray:
         raise NotImplementedError(
             "The `predict_proba` method is not implemented for neural network regressors. Please "
             "call `predict` instead."
         )
 
@@ -176,23 +176,23 @@
 
 # QNNs do not support serialization yet
 # FIXME: https://github.com/zama-ai/concrete-ml-internal/issues/3134
 # pylint: disable-next=too-many-instance-attributes, abstract-method
 class NeuralNetClassifier(BaseClassifier, QuantizedTorchEstimatorMixin, SkorchNeuralNetClassifier):
     """A Fully-Connected Neural Network classifier with FHE.
 
-    This class wraps a quantized neural network implemented using Torch tools as a Scikit-Learn
-    estimator. The Skorch package allows to handle training and Scikit-Learn compatibility,
+    This class wraps a quantized neural network implemented using Torch tools as a scikit-learn
+    estimator. The skorch package allows to handle training and scikit-learn compatibility,
     and adds quantization as well as compilation functionalities. The neural network implemented
     by this class is a multi layer fully connected network trained with Quantization Aware Training
     (QAT).
 
     Inputs that are float64 will be casted to float32 before training as Torch does not
     handle float64 types properly. Thus should not have a significant impact on the model's
-    performances. If the targets are integers of lower bitwidth, they will be safely casted to
+    performances. If the targets are integers of lower bit-width, they will be safely casted to
     int64. Else, an error is raised.
     """
 
     sklearn_model_class = SkorchNeuralNetClassifier
     _is_a_public_cml_model = True
 
     # pylint: disable=too-many-arguments
@@ -246,15 +246,15 @@
         self.history_ = history
         self.initialized_ = initialized
         self.virtual_params_ = virtual_params
 
     def fit(self, X: Data, y: Target, *args, **kwargs):
         # Check that inputs are float32 and targets are int64. If inputs are float64, they will be
         # casted to float32 as this should not have a great impact on the model's performances. If
-        # the targets are integers of lower bitwidth, they will be safely casted to int64. Else, an
+        # the targets are integers of lower bit-width, they will be safely casted to int64. Else, an
         # error is raised.
         X = check_dtype_and_cast(X, "float32", error_information="Neural Network classifier input")
         y = check_dtype_and_cast(y, "int64", error_information="Neural Network classifier target")
 
         classes, y = numpy.unique(y, return_inverse=True)
 
         # Check that at least two classes are given
@@ -272,15 +272,15 @@
 
         # Call BaseClassifier's fit method
         return super().fit(X, y, *args, **kwargs)
 
     def fit_benchmark(self, X: Data, y: Target, *args, **kwargs):
         # Check that inputs are float32 and targets are int64. If inputs are float64, they will be
         # casted to float32 as this should not have a great impact on the model's performances. If
-        # the targets are integers of lower bitwidth, they will be safely casted to int64. Else, an
+        # the targets are integers of lower bit-width, they will be safely casted to int64. Else, an
         # error is raised.
         X = check_dtype_and_cast(X, "float32", error_information="Neural Network classifier input")
         y = check_dtype_and_cast(y, "int64", error_information="Neural Network classifier target")
 
         # Call QuantizedTorchEstimatorMixin's fit_benchmark method
         return super().fit_benchmark(X, y, *args, **kwargs)
```

## concrete/ml/sklearn/qnn_module.py

```diff
@@ -9,15 +9,15 @@
 from ..common.utils import MAX_BITWIDTH_BACKWARD_COMPATIBLE
 
 
 class SparseQuantNeuralNetwork(nn.Module):
     """Sparse Quantized Neural Network.
 
     This class implements an MLP that is compatible with FHE constraints. The weights and
-    activations are quantized to low bitwidth and pruning is used to ensure accumulators do not
+    activations are quantized to low bit-width and pruning is used to ensure accumulators do not
     surpass an user-provided accumulator bit-width. The number of classes and number of layers
     are specified by the user, as well as the breadth of the network
     """
 
     def __init__(
         self,
         input_dim,
@@ -36,34 +36,34 @@
 
         Args:
             input_dim: Number of dimensions of the input data
             n_layers: Number of linear layers for this network
             n_outputs: Number of output classes or regression targets
             n_w_bits: Number of weight bits
             n_a_bits: Number of activation and input bits
-            n_accum_bits: Maximal allowed bitwidth of intermediate accumulators
+            n_accum_bits: Maximal allowed bit-width of intermediate accumulators
             n_hidden_neurons_multiplier: The number of neurons on the hidden will be the number
                 of dimensions of the input multiplied by `n_hidden_neurons_multiplier`. Note that
                 pruning is used to adjust the accumulator size to attempt to
-                keep the maximum accumulator bitwidth to
+                keep the maximum accumulator bit-width to
                 `n_accum_bits`, meaning that not all hidden layer neurons will be active.
                 The default value for `n_hidden_neurons_multiplier` is chosen for small dimensions
                 of the input. Reducing this value decreases the FHE inference time considerably
                 but also decreases the robustness and accuracy of model training.
             n_prune_neurons_percentage: How many neurons to prune on the hidden layers. This
                 should be used mostly through the dedicated `.prune()` mechanism. This can
                 be used in when setting `n_hidden_neurons_multiplier` high (3-4), once good accuracy
                 is obtained, to speed up the model in FHE.
             activation_function: a torch class that is used to construct activation functions in
-                the network (e.g. torch.ReLU, torch.SELU, torch.Sigmoid, etc)
+                the network (eg torch.ReLU, torch.SELU, torch.Sigmoid, etc)
             quant_narrow : whether this network should use narrow range quantized integer values
             quant_signed : whether to use signed quantized integer values
 
         Raises:
-            ValueError: if the parameters have invalid values or the computed accumulator bitwidth
+            ValueError: if the parameters have invalid values or the computed accumulator bit-width
                         is zero
         """
 
         super().__init__()
 
         self.features = nn.Sequential()
         in_features = input_dim
@@ -72,15 +72,15 @@
 
         if n_layers <= 0:
             raise ValueError(
                 f"Invalid number of layers: {n_layers}, at least one intermediary layers is needed"
             )
 
         if n_w_bits <= 0 or n_a_bits <= 0:
-            raise ValueError("The weight & activation quantization bitwidth cannot be less than 1")
+            raise ValueError("The weight & activation quantization bit-width cannot be less than 1")
 
         for idx in range(n_layers):
             out_features = (
                 n_outputs if idx == n_layers - 1 else int(input_dim * n_hidden_neurons_multiplier)
             )
 
             quant_name = f"quant{idx}"
@@ -228,15 +228,15 @@
         Raises:
             ValueError: if the quantization parameters are invalid
         """
         max_neuron_connections = self.max_active_neurons()
 
         if max_neuron_connections == 0:
             raise ValueError(
-                "The maximum accumulator bitwidth is too low "
+                "The maximum accumulator bit-width is too low "
                 "for the quantization parameters requested. No neurons would be created in the "
                 "requested configuration"
             )
 
         # Iterate over all layers that have weights (Linear ones)
         layer_idx = 0
         for layer in self.features:
@@ -248,16 +248,16 @@
             # Compute the fan-in, the number of inputs to a neuron, and the fan-out, the number of
             # neurons in the current layer.
             # The fan-in is the product of the kernel width x height x in_channels while the fan-out
             # is out_channels
             fan_in = numpy.prod(layer_shape[1:])
             fan_out = layer_shape[0]
 
-            # To satisfy accumulator bitwidth constraints each dot-product between an input line and
-            # weight column must not exceed n_accum_bits bits. We thus prune the layer to have
+            # To satisfy accumulator bit-width constraints each dot-product between an input line
+            # and weight column must not exceed n_accum_bits bits. We thus prune the layer to have
             # at most max_neuron_connections non-zero weights
             if fan_in > max_neuron_connections and layer not in self.pruned_layers:
                 pruning.l1_unstructured(
                     layer, "weight", (fan_in - max_neuron_connections) * fan_out
                 )
                 self.pruned_layers.add(layer)
```

## concrete/ml/sklearn/tree_to_numpy.py

```diff
@@ -15,15 +15,15 @@
 from ..onnx.convert import OPSET_VERSION_FOR_ONNX_EXPORT, get_equivalent_numpy_forward
 from ..onnx.onnx_model_manipulations import clean_graph_at_node_op_type, remove_node_types
 from ..quantization import QuantizedArray
 from ..quantization.quantizers import UniformQuantizer
 
 # pylint: disable=wrong-import-position,wrong-import-order
 
-# Silence hummingbird warnings
+# Silence Hummingbird warnings
 warnings.filterwarnings("ignore")
 from hummingbird.ml import convert as hb_convert  # noqa: E402
 
 # pylint: enable=wrong-import-position,wrong-import-order
 
 # pylint: disable=too-many-branches
 
@@ -37,15 +37,15 @@
         framework (str): The framework from which the ONNX model is generated.
             (options: 'xgboost', 'sklearn')
 
     Returns:
         onnx.ModelProto: The ONNX model.
     """
 
-    # Silence hummingbird warnings
+    # Silence Hummingbird warnings
     warnings.filterwarnings("ignore")
 
     extra_config = {
         "tree_implementation": "gemm",
         "onnx_target_opset": OPSET_VERSION_FOR_ONNX_EXPORT,
     }
     if framework != "sklearn":
@@ -155,23 +155,23 @@
     )
     return q_y
 
 
 def tree_onnx_graph_preprocessing(
     onnx_model: onnx.ModelProto, framework: str, expected_number_of_outputs: int
 ):
-    """Apply pre-precessing onto the ONNX graph.
+    """Apply pre-processing onto the ONNX graph.
 
     Args:
         onnx_model (onnx.ModelProto): The ONNX model.
         framework (str): The framework from which the ONNX model is generated.
             (options: 'xgboost', 'sklearn')
         expected_number_of_outputs (int): The expected number of outputs in the ONNX model.
     """
-    # Make sure the ONNX version returned by hummingbird is OPSET_VERSION_FOR_ONNX_EXPORT
+    # Make sure the ONNX version returned by Hummingbird is OPSET_VERSION_FOR_ONNX_EXPORT
     onnx_version = get_onnx_opset_version(onnx_model)
     assert_true(
         onnx_version == OPSET_VERSION_FOR_ONNX_EXPORT,
         f"The ONNX version returned by Hummingbird is {onnx_version} "
         f"instead of {OPSET_VERSION_FOR_ONNX_EXPORT}",
     )
 
@@ -191,15 +191,15 @@
         # FIXME: https://github.com/zama-ai/concrete-ml-internal/issues/2778
         # The squeeze ops does not have the proper dimensions.
         # remove the following workaround when the issue is fixed
         # Add the axis attribute to the Squeeze node
         workaround_squeeze_node_xgboost(onnx_model)
     else:
         # Add a transpose node after the last node.
-        # Sklearn models apply the reduce sum before the transpose.
+        # sklearn models apply the reduce sum before the transpose.
         # To have equivalent output between xgboost in sklearn,
         # apply the transpose before returning the output.
         add_transpose_after_last_node(onnx_model)
 
     # Cast nodes are not necessary so remove them.
     op_type_to_remove = ["Cast"]
     remove_node_types(onnx_model, op_type_to_remove)
@@ -262,15 +262,15 @@
         x (numpy.ndarray): The input data.
         framework (str): The framework from which the ONNX model is generated.
             (options: 'xgboost', 'sklearn')
         output_n_bits (int): The number of bits of the output. Default to 8.
 
     Returns:
         Tuple[Callable, List[QuantizedArray], onnx.ModelProto]: A tuple with a function that takes a
-            numpy array and returns a numpy array, QuantizedArray object to quantize and dequantize
+            numpy array and returns a numpy array, QuantizedArray object to quantize and de-quantize
             the output of the tree, and the ONNX model.
     """
     # mypy
     assert output_n_bits is not None
 
     assert_true(
         framework in ["xgboost", "sklearn"],
@@ -279,18 +279,18 @@
 
     onnx_model = get_onnx_model(model, x, framework)
 
     # Get the expected number of ONNX outputs in the sklearn model.
     expected_number_of_outputs = 1 if is_regressor_or_partial_regressor(model) else 2
 
     # ONNX graph pre-processing to make the model FHE friendly
-    # i.e. delete irrelevant nodes and cut the graph before the final ensemble sum)
+    # i.e., delete irrelevant nodes and cut the graph before the final ensemble sum)
     tree_onnx_graph_preprocessing(onnx_model, framework, expected_number_of_outputs)
 
     # Tree values pre-processing
-    # i.e. mainly predictions quantization
+    # i.e., mainly predictions quantization
     # but also rounding the threshold such that they are now integers
     q_y = tree_values_preprocessing(onnx_model, framework, output_n_bits)
 
     _tree_inference = get_equivalent_numpy_forward(onnx_model)
 
     return (_tree_inference, [q_y.quantizer], onnx_model)
```

## concrete/ml/sklearn/xgb.py

```diff
@@ -333,15 +333,15 @@
         self.enable_categorical = enable_categorical
         self.use_label_encoder = use_label_encoder
         self.random_state = random_state
         self.verbosity = verbosity
 
     def fit(self, X, y, *args, **kwargs) -> Any:
 
-        # HummingBird and XGBoost don't properly manage multi-outputs cases
+        # Hummingbird and XGBoost don't properly manage multi-outputs cases
         # FIXME: https://github.com/zama-ai/concrete-ml-internal/issues/1856
 
         assert_true(
             (isinstance(y, list) and (not isinstance(y[0], list) or (len(y[0]) == 1)))
             or (not isinstance(y, list) and (len(y.shape) == 1 or y.shape[1] == 1)),
             "XGBRegressor doesn't support multi-output cases.",
         )
```

## concrete/ml/torch/compile.py

```diff
@@ -67,22 +67,22 @@
 
     Take a model in torch or ONNX, turn it to numpy, quantize its inputs / weights / outputs and
     finally compile it with Concrete
 
     Args:
         model (Union[torch.nn.Module, onnx.ModelProto]): the model to quantize, either in torch or
             in ONNX
-        torch_inputset (Dataset): the calibration inputset, can contain either torch
+        torch_inputset (Dataset): the calibration input-set, can contain either torch
             tensors or numpy.ndarray
         import_qat (bool): Flag to signal that the network being imported contains quantizers in
-            in its computation graph and that Concrete ML should not requantize it
+            in its computation graph and that Concrete ML should not re-quantize it
         configuration (Configuration): Configuration object to use during compilation
         artifacts (DebugArtifacts): Artifacts object to fill during compilation
         show_mlir (bool): if set, the MLIR produced by the converter and which is going
-            to be sent to the compiler backend is shown on the screen, e.g. for debugging or demo
+            to be sent to the compiler backend is shown on the screen, eg for debugging or demo
         n_bits: the number of bits for the quantization
         rounding_threshold_bits (int): if not None, every accumulators in the model are rounded down
             to the given bits of precision
         p_error (Optional[float]): probability of error of a single PBS
         global_p_error (Optional[float]): probability of error of the full circuit. In FHE
             simulation `global_p_error` is set to 0
         verbose (bool): whether to show compilation information
@@ -92,17 +92,17 @@
     """
 
     inputset_as_numpy_tuple = tuple(
         convert_torch_tensor_or_numpy_array_to_numpy_array(val) for val in to_tuple(torch_inputset)
     )
 
     # Tracing needs to be done with the batch size of 1 since we compile our models to FHE with
-    # this batch size. The input set contains many examples, to determine a representative bitwidth,
-    # but for tracing we only take a single one. We need the ONNX tracing batch size to match
-    # the batch size during FHE inference which can only be 1 for the moment.
+    # this batch size. The input set contains many examples, to determine a representative
+    # bit-width, but for tracing we only take a single one. We need the ONNX tracing batch size to
+    # match the batch size during FHE inference which can only be 1 for the moment.
     # Use batch size > 1 in FHE once it is available
     # FIXME: https://github.com/zama-ai/concrete-ml-internal/issues/758
     dummy_input_for_tracing = tuple(
         torch.from_numpy(val[[0], ::]).float() for val in inputset_as_numpy_tuple
     )
 
     # Create corresponding numpy model
@@ -154,24 +154,24 @@
     """Compile a torch module into a FHE equivalent.
 
     Take a model in torch, turn it to numpy, quantize its inputs / weights / outputs and finally
     compile it with Concrete
 
     Args:
         torch_model (torch.nn.Module): the model to quantize
-        torch_inputset (Dataset): the calibration inputset, can contain either torch
+        torch_inputset (Dataset): the calibration input-set, can contain either torch
             tensors or numpy.ndarray.
         import_qat (bool): Set to True to import a network that contains quantizers and was
             trained using quantization aware training
         configuration (Configuration): Configuration object to use
             during compilation
         artifacts (DebugArtifacts): Artifacts object to fill
             during compilation
         show_mlir (bool): if set, the MLIR produced by the converter and which is going
-            to be sent to the compiler backend is shown on the screen, e.g., for debugging or demo
+            to be sent to the compiler backend is shown on the screen, eg, for debugging or demo
         n_bits: the number of bits for the quantization
         rounding_threshold_bits (int): if not None, every accumulators in the model are rounded down
             to the given bits of precision
         p_error (Optional[float]): probability of error of a single PBS
         global_p_error (Optional[float]): probability of error of the full circuit. In FHE
             simulation `global_p_error` is set to 0
         verbose (bool): whether to show compilation information
@@ -227,24 +227,24 @@
     """Compile a torch module into a FHE equivalent.
 
     Take a model in torch, turn it to numpy, quantize its inputs / weights / outputs and finally
     compile it with Concrete-Python
 
     Args:
         onnx_model (onnx.ModelProto): the model to quantize
-        torch_inputset (Dataset): the calibration inputset, can contain either torch
+        torch_inputset (Dataset): the calibration input-set, can contain either torch
             tensors or numpy.ndarray.
         import_qat (bool): Flag to signal that the network being imported contains quantizers in
-            in its computation graph and that Concrete ML should not requantize it.
+            in its computation graph and that Concrete ML should not re-quantize it.
         configuration (Configuration): Configuration object to use
             during compilation
         artifacts (DebugArtifacts): Artifacts object to fill
             during compilation
         show_mlir (bool): if set, the MLIR produced by the converter and which is going
-            to be sent to the compiler backend is shown on the screen, e.g., for debugging or demo
+            to be sent to the compiler backend is shown on the screen, eg, for debugging or demo
         n_bits: the number of bits for the quantization
         rounding_threshold_bits (int): if not None, every accumulators in the model are rounded down
             to the given bits of precision
         p_error (Optional[float]): probability of error of a single PBS
         global_p_error (Optional[float]): probability of error of the full circuit. In FHE
             simulation `global_p_error` is set to 0
         verbose (bool): whether to show compilation information
@@ -293,29 +293,29 @@
 
     The torch_model parameter is a subclass of torch.nn.Module that uses quantized
     operations from brevitas.qnn. The model is trained before calling this function. This
     function compiles the trained model to FHE.
 
     Args:
         torch_model (torch.nn.Module): the model to quantize
-        torch_inputset (Dataset): the calibration inputset, can contain either torch
+        torch_inputset (Dataset): the calibration input-set, can contain either torch
             tensors or numpy.ndarray.
         n_bits (Optional[Union[int, dict]): the number of bits for the quantization. By default,
             for most models, a value of None should be given, which instructs Concrete ML to use the
             bit-widths configured using Brevitas quantization options. For some networks, that
             perform a non-linear operation on an input on an output, if None is given, a default
             value of 8 bits is used for the input/output quantization. For such models the user can
             also specify a dictionary with model_inputs/model_outputs keys to override
             the 8-bit default or a single integer for both values.
         configuration (Configuration): Configuration object to use
             during compilation
         artifacts (DebugArtifacts): Artifacts object to fill
             during compilation
         show_mlir (bool): if set, the MLIR produced by the converter and which is going
-            to be sent to the compiler backend is shown on the screen, e.g., for debugging or demo
+            to be sent to the compiler backend is shown on the screen, eg, for debugging or demo
         rounding_threshold_bits (int): if not None, every accumulators in the model are rounded down
             to the given bits of precision
         p_error (Optional[float]): probability of error of a single PBS
         global_p_error (Optional[float]): probability of error of the full circuit. In FHE
             simulation `global_p_error` is set to 0
         output_onnx_file (str): temporary file to store ONNX model. If None a temporary file
             is generated
```

## concrete/ml/version.py

```diff
@@ -1,3 +1,3 @@
 """File to manage the version of the package."""
 # Auto-generated by "make set_version" do not modify
-__version__ = "1.0.0"
+__version__ = "1.0.1"
```

## Comparing `concrete_ml-1.0.0.dist-info/LICENSE` & `concrete_ml-1.0.1.dist-info/LICENSE`

 * *Files identical despite different names*

## Comparing `concrete_ml-1.0.0.dist-info/METADATA` & `concrete_ml-1.0.1.dist-info/METADATA`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: concrete-ml
-Version: 1.0.0
+Version: 1.0.1
 Summary: Concrete ML is an open-source set of tools which aims to simplify the use of fully homomorphic encryption (FHE) for data scientists.
 Home-page: https://zama.ai/concrete-ml/
 License: BSD-3-Clause-Clear
 Keywords: FHE,homomorphic encryption,privacy,security
 Author: Zama
 Author-email: hello@zama.ai
 Requires-Python: >=3.8.1,<3.11
@@ -23,16 +23,16 @@
 Classifier: Topic :: Software Development :: Compilers
 Requires-Dist: boto3 (>=1.23.5,<2.0.0)
 Requires-Dist: brevitas (==0.8.0)
 Requires-Dist: concrete-python (==1.0.0)
 Requires-Dist: fastapi (>=0.93.0,<0.94.0)
 Requires-Dist: hummingbird-ml[onnx] (==0.4.8)
 Requires-Dist: numpy (==1.23.5)
-Requires-Dist: onnx (==1.13.0)
-Requires-Dist: onnxoptimizer (==0.2.7)
+Requires-Dist: onnx (==1.13.1)
+Requires-Dist: onnxoptimizer (==0.3.10)
 Requires-Dist: onnxruntime (==1.13.1)
 Requires-Dist: protobuf (==3.20.3)
 Requires-Dist: python-multipart (>=0.0.6,<0.0.7)
 Requires-Dist: scikit-learn (==1.1.3)
 Requires-Dist: scipy (==1.10.1)
 Requires-Dist: setuptools (==65.6.3)
 Requires-Dist: skops (==0.5.0)
@@ -59,15 +59,15 @@
 </p>
 <p align="center">
 <!-- Version badge using shields.io -->
   <a href="https://github.com/zama-ai/concrete-ml/releases">
     <img src="https://img.shields.io/github/v/release/zama-ai/concrete-ml?style=flat-square">
   </a>
 <!-- Link to tutorials badge using shields.io -->
-  <a href="https://github.com/zama-ai/concrete-ml-internal#online-demos-and-tutorials">
+  <a href="#online-demos-and-tutorials">
     <img src="https://img.shields.io/badge/Learn-%20Tutorials%20and%20demos-orange?style=flat-square">
   </a>
 <!-- Zama Bounty Program -->
   <a href="https://github.com/zama-ai/bounty-program">
     <img src="https://img.shields.io/badge/Contribute-Zama%20Bounty%20Program-yellow?style=flat-square">
   </a>
 </p>
@@ -175,15 +175,15 @@
 
 - [Sentiment analysis with transformers](use_case_examples/sentiment_analysis_with_transformer): a gradio demo which predicts if a tweet / short message is positive, negative or neutral, with FHE of course! The [live interactive](https://huggingface.co/spaces/zama-fhe/encrypted_sentiment_analysis) demo is available on Hugging Face. This [blog post](https://huggingface.co/blog/sentiment-analysis-fhe) explains how this demo works!
 
 - [CIFAR10 FHE-friendly model with Brevitas](use_case_examples/cifar_brevitas_training): code for training from scratch a VGG-like FHE-compatible neural network using Brevitas, and a script to run the neural network in FHE. FHE simulation shows an accuracy of 88.7%, but running inference with FHE is still a work-in-progress.
 
 - [CIFAR10 / CIFAR100 FHE-friendly models with Transfer Learning approach](use_case_examples/cifar_brevitas_finetuning): series of three notebooks, that show how to convert a pre-trained FP32 VGG11 neural network into a quantized model using Brevitas. The model is fine-tuned on the CIFAR data-sets, converted for FHE execution with Concrete ML and evaluated using FHE simulation. For CIFAR10 and CIFAR100, respectively, our simulations show an accuracy of 90.2% and 68.2%. True FHE inference is a work-in-progress.
 
-- [FHE neural network splitting for client/server deployment](use_case_examples/cifar_brevitas_with_model_splitting): we explain how to split a computationally-intensive neural network model in two parts. First, we execute the first part on the client side in the clear, and the output of this step is encrypted. Next, to complete the computation, the second part of the model is evaluated with FHE. This tutorial also shows the impact of FHE speed/accuracy tradeoff on CIFAR10, limiting PBS to 8-bit, and thus achieving 62% accuracy.
+- [FHE neural network splitting for client/server deployment](use_case_examples/cifar_brevitas_with_model_splitting): we explain how to split a computationally-intensive neural network model in two parts. First, we execute the first part on the client side in the clear, and the output of this step is encrypted. Next, to complete the computation, the second part of the model is evaluated with FHE. This tutorial also shows the impact of FHE speed/accuracy trade-off on CIFAR10, limiting PBS to 8-bit, and thus achieving 62% accuracy.
 
 - [Encrypted image filtering](use_case_examples/image_filtering): finally, the live demo for our [6-min](https://6min.zama.ai) is available, in the form of a gradio application. We take encrypted images, and apply some filters (for example black-and-white, ridge detection, or your own filter).
 
 More generally, if you have built awesome projects using Concrete ML, feel free to let us know and we'll link to it!
 
 ## Citing Concrete ML
```

### html2text {}

```diff
@@ -1,8 +1,8 @@
-Metadata-Version: 2.1 Name: concrete-ml Version: 1.0.0 Summary: Concrete ML is
+Metadata-Version: 2.1 Name: concrete-ml Version: 1.0.1 Summary: Concrete ML is
 an open-source set of tools which aims to simplify the use of fully homomorphic
 encryption (FHE) for data scientists. Home-page: https://zama.ai/concrete-ml/
 License: BSD-3-Clause-Clear Keywords: FHE,homomorphic
 encryption,privacy,security Author: Zama Author-email: hello@zama.ai Requires-
 Python: >=3.8.1,<3.11 Classifier: License :: Other/Proprietary License
 Classifier: Programming Language :: Python :: 3 Classifier: Programming
 Language :: Python :: 3.9 Classifier: Programming Language :: Python :: 3.10
@@ -11,16 +11,16 @@
 Classifier: Topic :: Scientific/Engineering Classifier: Topic :: Scientific/
 Engineering :: Artificial Intelligence Classifier: Topic :: Scientific/
 Engineering :: Mathematics Classifier: Topic :: Security Classifier: Topic ::
 Security :: Cryptography Classifier: Topic :: Software Development :: Compilers
 Requires-Dist: boto3 (>=1.23.5,<2.0.0) Requires-Dist: brevitas (==0.8.0)
 Requires-Dist: concrete-python (==1.0.0) Requires-Dist: fastapi
 (>=0.93.0,<0.94.0) Requires-Dist: hummingbird-ml[onnx] (==0.4.8) Requires-Dist:
-numpy (==1.23.5) Requires-Dist: onnx (==1.13.0) Requires-Dist: onnxoptimizer
-(==0.2.7) Requires-Dist: onnxruntime (==1.13.1) Requires-Dist: protobuf
+numpy (==1.23.5) Requires-Dist: onnx (==1.13.1) Requires-Dist: onnxoptimizer
+(==0.3.10) Requires-Dist: onnxruntime (==1.13.1) Requires-Dist: protobuf
 (==3.20.3) Requires-Dist: python-multipart (>=0.0.6,<0.0.7) Requires-Dist:
 scikit-learn (==1.1.3) Requires-Dist: scipy (==1.10.1) Requires-Dist:
 setuptools (==65.6.3) Requires-Dist: skops (==0.5.0) Requires-Dist: skorch
 (==0.11.0) Requires-Dist: torch (==1.13.1) Requires-Dist: tqdm
 (>=4.64.1,<5.0.0) Requires-Dist: typing-extensions (==4.4.0) Requires-Dist:
 uvicorn (>=0.21.0,<0.22.0) Requires-Dist: xgboost (==1.6.2) Project-URL: Bug
 Tracker, https://github.com/zama-ai/concrete-ml/issues Project-URL:
@@ -121,15 +121,15 @@
 respectively, our simulations show an accuracy of 90.2% and 68.2%. True FHE
 inference is a work-in-progress. - [FHE neural network splitting for client/
 server deployment](use_case_examples/cifar_brevitas_with_model_splitting): we
 explain how to split a computationally-intensive neural network model in two
 parts. First, we execute the first part on the client side in the clear, and
 the output of this step is encrypted. Next, to complete the computation, the
 second part of the model is evaluated with FHE. This tutorial also shows the
-impact of FHE speed/accuracy tradeoff on CIFAR10, limiting PBS to 8-bit, and
+impact of FHE speed/accuracy trade-off on CIFAR10, limiting PBS to 8-bit, and
 thus achieving 62% accuracy. - [Encrypted image filtering](use_case_examples/
 image_filtering): finally, the live demo for our [6-min](https://6min.zama.ai)
 is available, in the form of a gradio application. We take encrypted images,
 and apply some filters (for example black-and-white, ridge detection, or your
 own filter). More generally, if you have built awesome projects using Concrete
 ML, feel free to let us know and we'll link to it! ## Citing Concrete ML To
 cite Concrete ML, notably in academic papers, please use the following entry,
```

## Comparing `concrete_ml-1.0.0.dist-info/RECORD` & `concrete_ml-1.0.1.dist-info/RECORD`

 * *Files 11% similar despite different names*

```diff
@@ -1,55 +1,55 @@
 concrete/__init__.py,sha256=MXQ2ILkTW4ziKnd7dVxTEVOpHe8clGF34EXxmBCF_pk,284
-concrete/ml/__init__.py,sha256=MkPEAQmtmfFHFJWlw1gMaVr07GLjPuIBv85lNFSp0aI,1722
+concrete/ml/__init__.py,sha256=T19QQpjz-kkkO5Dov4vPfmYN7bB5LH-EOQjJLTNrFmg,1722
 concrete/ml/common/__init__.py,sha256=dXaklwVfEYsMLqyH_YFOjGuxnyzoZuApIm8dKLefybw,95
 concrete/ml/common/check_inputs.py,sha256=wj_oyESYWHemVsPRBC25OQc8a2pX2j7fYGpw8lCeKUA,2519
 concrete/ml/common/debugging/__init__.py,sha256=_eh1MBBmnk2o1M4YGbH0Cbr_cEPjFOVz5wsp080HonE,101
-concrete/ml/common/debugging/custom_assert.py,sha256=qrTn_5Nu2hxsrPB-9bM_6NFTOQ9MakFaomT9XuHXpKU,2377
+concrete/ml/common/debugging/custom_assert.py,sha256=H5ESt7cYnDT2PnUXvDaAvUJR2CKHwsHQ4QWtMLvcLZw,2377
 concrete/ml/common/serialization/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-concrete/ml/common/serialization/dumpers.py,sha256=YQge8j1daH-zCeC6mwYkkOrQERd7oyrwiFIM9hhqtyM,1310
+concrete/ml/common/serialization/dumpers.py,sha256=BnoB6S1_TW3tebA0ZQxDFUN3XUnsfVUfsk8yoUXyVW0,1318
 concrete/ml/common/serialization/encoder.py,sha256=TsZ4_FnfWpgk87CpeQDCyqANGmsJ3OlEKmWXBG9fiJk,2018
-concrete/ml/common/serialization/loaders.py,sha256=eCiLRIsTljMQsuzTLYj7wxjdhb4eyub3E-kFDV8ufCo,2856
-concrete/ml/common/utils.py,sha256=BWwck2DQk-6fVYw2mJ6bO9BPYnASm5RUP25Zh_sfGbA,17216
+concrete/ml/common/serialization/loaders.py,sha256=-ea3oVuHdVRnpMmNFqIHQg71DRfkAGHzaDjwCI2qJKI,2888
+concrete/ml/common/utils.py,sha256=vwPkVK9tItA0k36H_4twivAHJTrlLBKLA_xkXscQh4c,17217
 concrete/ml/deployment/Dockerfile.server,sha256=TbSoyy195L9ttQnEf4YJ70yMuphZRHZvGep1GI4kRlM,235
 concrete/ml/deployment/__init__.py,sha256=jgaPZMSlsJy1KF45EI92rQ7e2CKEpmLdaWcwwFEPeZ8,121
-concrete/ml/deployment/deploy_to_aws.py,sha256=Cpc5Rg7VR73zMxd1IAmLD3j8ClpNsCIL71Qn_GJm6i8,17568
-concrete/ml/deployment/deploy_to_docker.py,sha256=jqKy9sIFwa-xLCOQAqe9olGYQ5iLt48sUx_4x5rvKIg,3800
-concrete/ml/deployment/fhe_client_server.py,sha256=9w32jHS9PB0D9lanfWO7eSpXU1wF4J5pE9ZOdRuuiuE,15080
-concrete/ml/deployment/server.py,sha256=vb2X9aLE0kfb4fiZikokxNVE-MnBtcVWabtF3ENCYzM,4321
+concrete/ml/deployment/deploy_to_aws.py,sha256=zQO20lozs8X6E3u11bpfK-0KOgT0UXYJvPPDAIqnCZs,17843
+concrete/ml/deployment/deploy_to_docker.py,sha256=c7N9XhbgBajyLiu_TvPRke5kSi6_GdmRtOOPSl-LpoE,3800
+concrete/ml/deployment/fhe_client_server.py,sha256=F8qKjip0-sxOsF-73VUNN1MGuXjfyicl2EKp9_KW4PI,15084
+concrete/ml/deployment/server.py,sha256=CB6y1v9NPq3NqOE0spzKUMNywFNmhEta8zsn5YIu42o,2590
 concrete/ml/deployment/server_requirements.txt,sha256=gyd07Mp8qZPWhuz7QI1kgYS3JbjcPbZK4cL9RDOR_x8,33
-concrete/ml/deployment/utils.py,sha256=yQbGiq2vBaEX2hoZ36z1q4I7KfFACoTaSROGo--Jcps,3285
+concrete/ml/deployment/utils.py,sha256=aZFmDRUW58JHAJLK5c--UANMx1sN8C-ioz4JPcPQV-Q,3293
 concrete/ml/onnx/__init__.py,sha256=LiK0TOCAo7c5-TBk4zcO3PEQmdASN0NgLDYIYUXl464,19
 concrete/ml/onnx/convert.py,sha256=LC1YtDL_miK0QKYXzwYwLyBt1PTOXqk247BPS714EnA,3472
-concrete/ml/onnx/onnx_impl_utils.py,sha256=1KVD5LkzGt1AKJF0x5qIa2_idQacpws-cW30LWhLO_Y,9401
-concrete/ml/onnx/onnx_model_manipulations.py,sha256=qbWOR40TJy-CoD6p4Cc9xIgu9BGo0xoVrtqty3MXZSc,9684
+concrete/ml/onnx/onnx_impl_utils.py,sha256=cQ5mfY0VCUMClOy3OGVeddISjWsdoylLu8GundXC4JY,9401
+concrete/ml/onnx/onnx_model_manipulations.py,sha256=fjy-Vqa7Cl6F-O4sft1xKa0z1LZu6X6SnrXj7UtadDQ,9685
 concrete/ml/onnx/onnx_utils.py,sha256=V39Fip97h4F85PlhiQduQPFnNizpGjyGyCHFYOHPon8,20192
-concrete/ml/onnx/ops_impl.py,sha256=YYMyuLg2T9C3V3PGMBgbpRV_sOccA-g1-RnyaNAutIs,58040
+concrete/ml/onnx/ops_impl.py,sha256=2Zx8-C0iwuFTbZc9oz0GJxhvKMH7TswosUINeCu08cw,58045
 concrete/ml/pytest/__init__.py,sha256=5Zk0w3T-MYPF3485ir5kZ1dP7x_VoMIreDMKnJuyBCo,101
-concrete/ml/pytest/torch_models.py,sha256=L43QyEEu63vPTYRGdkt-gyLFpt2p8-31D_Ct-dNCE0c,42028
-concrete/ml/pytest/utils.py,sha256=RsOLNe9Ha2kBjk_vTXfM7_VHu_Emu1mYQnoVxF117Tg,8787
+concrete/ml/pytest/torch_models.py,sha256=M9cNkChOKRNYHgQbJ-oHYpQdfswzJxHC21fDG12kqeU,42036
+concrete/ml/pytest/utils.py,sha256=wU1jyx00hZt7m0fn7ljHzHYsepLMpjoPL1KneGqlIdc,8786
 concrete/ml/quantization/__init__.py,sha256=AMEpmLBppr9jRyHOaz9X6dND-u6urnDnkW06X_80JWQ,1081
-concrete/ml/quantization/base_quantized_op.py,sha256=Od8BimxvZC6MMdX15laqwUAh0JZ1O7quCewWT6i0aXs,35250
-concrete/ml/quantization/post_training.py,sha256=A6W2K-m4RA_Usx6EfuOluqE3OPO_oVMIMScflFQrp5o,43507
-concrete/ml/quantization/quantized_module.py,sha256=PzkKfuZNoGxKLjD8OzWgcl64Qt2860ziYdbMBMWu2Q4,24344
-concrete/ml/quantization/quantized_ops.py,sha256=7PGQTAQpnZcztL8vyXkkqoIZb_PY_u8KS4Lns2d8Aq0,80967
-concrete/ml/quantization/quantizers.py,sha256=dKngjgtunqBZzZwCPg-qneW69Pgr9ufpCIqcuxurwHo,37503
+concrete/ml/quantization/base_quantized_op.py,sha256=LVJvV56XGK7mHmjzvt-V7V4Rjc9wNfcQDyitV24xTS4,35262
+concrete/ml/quantization/post_training.py,sha256=bxQjVU1UBp2-Y0DB9j9JzezO88D6mGI18gkEJOY0K1I,43525
+concrete/ml/quantization/quantized_module.py,sha256=2k4PzzRxISm9z6IHYMj-EsesoeW6A986Ef-muINR-3A,24352
+concrete/ml/quantization/quantized_ops.py,sha256=PgpCUZlm5byWCL39wMFfIeBuyEq9AvXVyyQKXkylL4Y,80991
+concrete/ml/quantization/quantizers.py,sha256=wlDKxzcFV4EyPhijGcj-EugIMuS_yINCbTg5i_6fEAQ,37549
 concrete/ml/search_parameters/__init__.py,sha256=-TGfmte3AvAn2ajLxLd-FCbru6_Ml9DiKeG7ooR819Q,77
-concrete/ml/search_parameters/p_error_search.py,sha256=rQe56M66QmZKiUfk1bRVxTYRKDSC2u5XhfG3mx4lC2s,21176
+concrete/ml/search_parameters/p_error_search.py,sha256=SnQ1eIzHNQqIGWPbNMg4obDuN4jV0KDsPPVuCpJbkMU,21193
 concrete/ml/sklearn/__init__.py,sha256=PPR2RuegE_0-OOA9QhYyZ75lxm7rZiAWI4MqeQwRoCo,4679
-concrete/ml/sklearn/base.py,sha256=je41FdKhLGT3wyqmaeCyKgVh5CbVhtm-utXj4sF73Uk,62686
+concrete/ml/sklearn/base.py,sha256=TEnFPsOpq-0uESbAuunxgiOcF-WWx5LaRIMAbmllLA0,62708
 concrete/ml/sklearn/glm.py,sha256=xca19CFi-QhSTNm9I3anlK7PxEqDx2e1Z9MXw_NKJF8,13633
 concrete/ml/sklearn/linear_model.py,sha256=kQOGeIZjXUUmpSvxCWe-XKd02iY5MuXebvakEWJScvA,25212
-concrete/ml/sklearn/qnn.py,sha256=-VIkCtZO89uTjrNBY4fgcarIqnrqqPZ8OSRB2Nt6B4A,13339
-concrete/ml/sklearn/qnn_module.py,sha256=5jxigI42-aCG0_ko1u6aV6RXOjBhSD9afSorvD-V-n4,12567
+concrete/ml/sklearn/qnn.py,sha256=yyXbHBvhK5eFjVAiSz3pTZwOjNDZJslVSSvJjh833s8,13342
+concrete/ml/sklearn/qnn_module.py,sha256=ZHYlY5N_-UOkooYlQbwNzXKYZCSTyvwO01AFcWl1v1Y,12572
 concrete/ml/sklearn/rf.py,sha256=QSqXnHua77SLFX5crYe5s-XWsmz4AR06QTWSC7uuIOU,12243
 concrete/ml/sklearn/svm.py,sha256=bmfwiJcb28LzPbfiaGiNeHtEug8tzmoBHes_ee4cNgs,10614
 concrete/ml/sklearn/tree.py,sha256=1d2uOLER4HfVWb3tr3nHMEyFTQdpaoq2bjrSPkSHDD8,10394
-concrete/ml/sklearn/tree_to_numpy.py,sha256=PMK6VQskkZhW6YzhtCTl2pcg1BvZc5QpHdKY2KfnGw0,10920
-concrete/ml/sklearn/xgb.py,sha256=cHq7HyONowCiS-rCTh1TX_b-v0Ns7Vec0xm4DmB0GT4,20074
+concrete/ml/sklearn/tree_to_numpy.py,sha256=NU79gTlaSVqDbIYSNlL9Mo5kB0aQBejegy_P_iFExgQ,10923
+concrete/ml/sklearn/xgb.py,sha256=A_QcoICqydvF5JINuOtd2HISf88RJ_fF96ZEmgZjSHs,20074
 concrete/ml/torch/__init__.py,sha256=aUQ25-hJdirZCa20KqaVMrt2PnpnSXBCVw7QvLQ_BoQ,83
-concrete/ml/torch/compile.py,sha256=gwWBFuNdkQocDupvpw-l9LT6XKSEWS7ZS6ju8oshc_o,17532
+concrete/ml/torch/compile.py,sha256=FFYscm4Wwv9ZUjHQ7J6v86OpSRKUlMsvURXa8GiqxJI,17531
 concrete/ml/torch/numpy_module.py,sha256=PKJmuGL0Q7nzfXsbwz9Mr1J-w5MDGkpzRnZY86TylOI,3174
-concrete/ml/version.py,sha256=SQu4i6SxaSjblZS1KOTjixyzzbdHd9y4oE9gZ0CY6nY,124
-concrete_ml-1.0.0.dist-info/LICENSE,sha256=Q2TUW9iqL5JOnEcNTstSvNPYRnF-iQi24FUla8oUEwE,1546
-concrete_ml-1.0.0.dist-info/WHEEL,sha256=vxFmldFsRN_Hx10GDvsdv1wroKq8r5Lzvjp6GZ4OO8c,88
-concrete_ml-1.0.0.dist-info/METADATA,sha256=UU16LiaRSne7CAAWzLY7wN1pBxb1vdNXtTZ489KT928,11662
-concrete_ml-1.0.0.dist-info/RECORD,,
+concrete/ml/version.py,sha256=VH8_hqfSpN7fFhrn1tQXy2_-8GHNBxF6HCq6zT8oWwU,124
+concrete_ml-1.0.1.dist-info/LICENSE,sha256=Q2TUW9iqL5JOnEcNTstSvNPYRnF-iQi24FUla8oUEwE,1546
+concrete_ml-1.0.1.dist-info/WHEEL,sha256=vxFmldFsRN_Hx10GDvsdv1wroKq8r5Lzvjp6GZ4OO8c,88
+concrete_ml-1.0.1.dist-info/METADATA,sha256=bJXvQ2oO4iC3_Chk8edE522tTlUoa6SYq7t8nOv7CSQ,11617
+concrete_ml-1.0.1.dist-info/RECORD,,
```

